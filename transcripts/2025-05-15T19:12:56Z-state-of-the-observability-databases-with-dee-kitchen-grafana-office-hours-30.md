# State of the Observability Databases with Dee Kitchen (Grafana Office Hours #30)

In this Grafana Office Hours, we talk about the state of observability databases (Grafana Loki, Mimir, Tempo, and Pyroscope) and ...

Published on 2025-05-15T19:12:56Z

URL: https://www.youtube.com/watch?v=6Ph5nvicm6M

Transcript: Okay. Hello everyone and welcome to another Graphana office hours. It's been a while since the last one, but this one's a special occasion because we're actually in person and there's no box, but there's no hotel room. Yes. And it is an ungodly hour at 8:00 a.m. Yeah. And I had to clean up, tidy up in my hotel room, even though you can't see that much from behind. But hi, I'm Nicole Vanderhovven. This is D Kitchenen, our super special guest for today and my co-host crime always in trouble. Jake, just perennially in trouble. But D, why don't you introduce yourself? Uh, so I'm D Kitchen. I'm a VP of engineering here at Pana Labs. I run the databases team which is Loki Vir Tempo Pyroscope and some of the work around influx data dog translators uh graphite and open telemetry. Awesome. What? How long have you been at Graphfana? Five years next week. Oh wow. Five years in two weeks. Congrats for your five years next week. That's a lot. Nothing from us. Sorry. Maybe you should give us something. I feel there is internal swag though for 5 years though, right? There is actually something really cool that you get at five. There is. Do people know about this? Like at that 3 years we get a little light. They don't know. We get a little LED light. Mine's in Portugal. I don't travel with it. We get like shoes and then supposedly Yeah. I don't know the secret. Richie will not tell me what the secret five-year item is. He refuses to tell me. Find out what the 10 year is. Yeah. Well, do you tell us what you do? Like we know your title, but what do you actually do? So, I'm a manager and a lot of that is people management, but the shape of a manager at this level when you're at VP or near CTO or kind of that level, it's not onedimensional, right? When you're an entry- level manager, you're typically doing the sort of health of team. Uh you may be doing some project management. Uh and as you go up, you're also doing more customerf facing things. Uh you're accepting and protecting some of the legal sort of things like perhaps security and compliance. Um you're also looking after and shephering other managers, which means nurturing and helping them grow so they can protect their teams and look after them and so forth. And throughout all of this, you've still got to be the holder of some product vision. Um, you got to know where you're going to go, what you're trying to achieve. That was going to be my next question. It's like, how do how do you feel you juggle each of our databases? I mean, some companies only have one database they develop and we develop four. Um, yeah. So, fundamentally the architecture is similar. So, conceptually they're very easy to do. If I was trying to do what I do, which I manage what 100 engineers or thereabouts, uh if I was trying to do this and I had wildly different products, let's say a chunk of the kafana stuff and a database, this would be too hard because they don't have alignment. They don't have similarity. There's no shared components. But the architecture of the ingest model and the the read model is so similar between the sort of Mamir Loki tempo pyroscript that you really only have to understand the signals. But I've been an engineer for so long that I understand the signals. So it doesn't feel too different from Well, we should probably say upfront that this is going to be about the direction of the databases. So that means there's a lot of things in the future that we're going to be talking about. Please don't hold us to any of this because if you do then we won't get D back on to talk about this stuff in the future if they think that they'll be held to what they say. We're going to be talking about things that we're currently investigating. Some things are already built there. Some are already in GA. Some things are just conceptual right now. Some things are built for one database but not for another. So, you know, just just disclaimer up front. Yeah, it's worth saying that the the engineering culture here is very relaxed. Um, and there's a sense of urgency and a sense of prioritization, a sense of what's important and listening to our customers, but we actually only ship things when they're ready and we are willing to investigate something, perform an experiment and say that didn't work. There's something better. Let's do the other thing. I like that no vaporware policy. No. Well, I mean I I I would hate to be in a company where that wasn't the case, where it was just like, "No, we've committed to it and we're going to die building it even if it's wrong." Oh. So, so I was a c a company in my past and part of the engineering organization would mention what they called uh Gartner driven development. Uh and that's this like you know the magic quadrant has done this we must add that feature and it goes on the road map and there's no real passion for it. There's no real understanding why you need it. There's just a we must tick the box and we just don't do that. Right. Far does not do that at all. We build the right thing. We ship it when it's ready. Yeah. So, you're talking about the architectural similarities between the databases. Maybe we could talk a bit more about like just in general terms. It seems like it was an ingesttor architecture, right? So, typically we have like an ingesttor and a distributor and querer. there are things that are common across all four at least that's how they all started. Um is this something that you think is changing in the industry as a whole or because we're sort of blurring those lines a little bit adding some components in there. Um yeah, so the model of how we do this and at the moment some of the databases are still doing it in kind of a I don't want to use the word legacy because it's still bestin-class. Um but it resembles more how Cassandra would do the sort of ingestion. Yeah. And it's more a typical distributor system. You got distributor. The distributor is going to sort of talk back to the client. It's going to uh then the distributor's job is to hand it to three different ingesters. And that's replication practically. And when you've got it handed to three, then you can immediately the distributor can act back to the client and say, "We got your data from this point. We cannot lose it and everything is safe." Um, and that's great, but you're running replication factor 3, which is kind of a high cost and it still leaves you at some risk, right? You can lose an injector, you can even lose two in the worst case, but you can't lose all three. So if CSP fundamentally has an issue, you're still at risk. Um and there is a sense in the last decade, two decades that the industry has say sort of consolidated around object storage as the data sort of storage of every single thing. Um and a lot of cloud native apps are literally object storage. That's what they really mean. Um but the new sense in the last probably about 101 15 years is that the way to uh ingest is actually uh CFKA right it's the CFKA API um and that's all good but CFKA had its own problems KFKA is difficult to run it's difficult to to build that skill it's you don't you're still prone to the same problems um and a couple of years ago what started to emerge were companies doing CFKA API. It's just the implementation of the API. So if you think about object storage, uh we think of the S3 API and but you know GCS, Google cloud storage uh that implements the S3 API. It's not AWS. It's not it's just object storage, right? So, so the API the interface became the the standard uh for storage and the CFKA API has become the standard for sort of buffering and ingestion is buffering and now we can design newer architectures which Mamir has first where basically Mamir is writing the distributor writes straight to a CFKA API. Um now we use warpstream. Um there are other open source projects in this area. Warpstream is now owned by Confluent and Warpstream is the CFKA API backed by object storage. Yeah. Which gives you the replication there, but it also gives you the durability guarantees object storage. So, so now you have a choice over how you want to run that component. Uh, and that's a lot more flexibility. It's a lot more standard and it opens up other possibilities like because you can suddenly have multiple readers from it. Yeah, the adjust model is a little bit more simple. But out of CFKA you get more flexibility. Does that decouple you then? So in in our current architectures we look to basically have readers that will read the nearest data from our right our ingesttor component. Does this decouple our write and read path in these new architectures? Um yes but uh and and always has a but because you're speaking to someone from engineering so we're just going to go oh it depends everything. Um so so yes there is a decoupling that actually happens but um what actually goes on here is that in the first phase right if we wrote it from scratch perhaps we would just come along and just go uh distribute it straight into a cafka the warpstream sort of object cafka API to our object store uh and then we would just create different readers that come off the back of that and query path would hit that but that's a lot of changes at once. Yes. Yeah. And any sensible engineer is going to slow down and go don't change commit a PR, right? Which is 100,000 files and two million lines of code. Don't do that. Break it down to small things and validate each thing. Yes. And then and then prove that B piece with the least change that you can achieve. So the least change that we can achieve is that we can make the ingesttors just read from CFKA themselves and put the CFKA in gain all of the durability reduce the replication factor on the ingesttors but today leave the read path to go to the ingesttors as they were uh for the very most recent data. Um and that's the safest way of doing this because it's mission critical, right? No one is going to use an observability uh product which isn't mission critical and that reliable. So we are not going to change 50 things at once. We're going to change one, prove it, gain the reliability, change the next thing. So it's a very long path. Exactly. that if your alerts don't fire um that this is this is action stations because we're meant to be the ones telling you that you have problems, not the fact that we have problems because we've lost your data. So, I was curious how a change like this filters on through the databases. Is it something like that you said or or that is said at a higher level and then every all of the other teams try to investigate how to do it? Because to us it just seemed like, you know, Jay and I are more involved in Loki. So we heard about their implementation or trying to implement warp stream and kafka and the decisions around that then it's like oh mamir's done that oh pyroscopes looking into this oh you know so it's like is it just one person does it and everybody else follows or so this goes back to the engineering culture within uh Grafana and especially within the databases um we give very high autonomy to engineers um and so I'm really not there and Tom my boss CTO we're really not there dictating We're not saying you must do this. What we're saying is have you considered would you experiment with do you think that this is something that could work here? And we might seed the thought or we might hint at a priority but we actually leave give the engineers an unbelievable amount of space to choose the time and sequence of things. Um and for a while um everyone has come to it for a different reason. The MIR came to it because we would run a a large sale and we might have you know 500 injesters and the reliability of injesttors and that model when there was only like a couple hundred injusters the reliability was really high the anti-affffinity worked well everything was cool but it gets disproportionately expensive and kind of like how RAID gets less reliable at massive scale uh the ingesttor model kind of has some of those sort of uh concerns around So they came to it from a reliability perspective. Can we get the capper API in and the durability of objects? Um Loki has come to it from a perspective of uh you know currently when they write three copies from the ingesttors they have to dduplicate that. Yeah. And there's a lot of work that goes on and logs is a volutric signal right. So this that massively increases the storage requirements, the IOPS that you're going to do to do this writing and dduplication, the burden on the sort of compaction type work that you actually have to do. And so you know they came at it from a different perspective. Maybe we don't have to do this work if we only read once from CFKA and we can guarantee that CFKA has handled the replication and the durability. then it can streamline everything below and that should also increase the velocity of the engineers because they don't have to work with always thinking of replication factor three um tempo came at it from a different perspective um because tempo are also a volutric and when I say volutric if you think about um metrics and profiles these things are aggregate they say like one bit of data x time period and it represents all the things in that time period. But with logs and traces, you know, if you've got an API that gets hit once in a second and it logs 10 times and writes two traces, you're going to get 10 logs and two traces. But if it gets 100 times, then all of that volutrically explodes. So, Tempo also has this volutric thing. They need to go faster. They wanted to simplify the mental model as an engineer. um with Pyroscope um they wanted a streaming architecture to handle sort of the burst loads and sort of make it a little bit more reliable but also increase developer speed. So they're all coming at a different ways. What we actually do is we share the expertise, right? Our teams talk to each other. Yeah. Um, and on Loki, like Ed, Owen, they're going to go and speak to um, Marco on Mir and they're going to say, "How did you how did you do that?" Like, "Can we learn from you?" Like, um, we're actually at an offsite. Yeah. This is what we're not in a random hotel room. I just invited them over to my hotel room. So, we're actually at an offsite and a database. Yeah. Um, and we've had these sessions for the last couple of days where they're actually going away into rooms and they're actually saying what did you learn about like how to use the cafer API? What what were the problems with integrating it? Um, what were the problems with the operationalizing or you know we want our TCO to be low and in part we want to maintain the margin but we want to pass on that cost saving. So we want an effective sort of TCO. We we have to concern ourselves with, you know, what's the reliability, what's the cost, what's the speed and maintenance and ease of operation. So, and they all talk to each other. There's a lot of like, oh, can we copy your homework that's been going on and that's a good thing. Just gives us a good vibe. Like, I really enjoy seeing them interact and go, "Well, hang on. Why did you do that?" And it's the curiosity between teams is what I like. And then, you know, there's the ability to be honest about it, too. you can debate the the fact that was that a sensible decision or not. Um, so we do have a question, but I feel like we could hold that just a little longer when we start talking get into more specifics, especially um around our databases. But before before we got to that um we do have some initiatives that we've talked about with this offsite um and some things that we've already implemented our databases and D I wondered if you could talk a little bit about where you know we currently are say with hotel compatibility where we're going from a more broader sense um and then maybe talk about some of those other features we've kind of implemented across the board like drill down apps. Uh okay so which one of those do you want first? Let's let's start with um hotel compatibility. I like hotel. I'm an hotel fan. Um so it depends on the signal. Um for tempo this is traces built on Jerger. Jerger was one of the first hotel signals. The the maturity and um you know traces were so influenced and derived from uh Jerger that tempo on day one was 100%. Right. It's far um tempo also also from day one um it it started on parket and honestly yes we use parket but we also um you know everyone who uses park ends up introducing minor changes so are we perfectly universally compatible with park but it is a column store right so um tempo from day one was a column store and that really helps because one of the struggles that um let's say Loki or or have had is the um the the metadata the the sort of resource data around hotel um and because you know if if you think about it in in traces that makes total sense you're going to send the trace you're going to send the span you've got these fixed number of fields you've got some other fields that will actually vary and it's a very very structured signal um and most of the value of that is actually in the structure the actual trace is a tiny amount but most of the value is in the structure Um if we go on to profiles, profiles is incredibly structured right and so Pyroscope um is is able to take say you know the heap the stack um and what is known from inside the application and inside sort of the kernel and what's going on in the CPU memory that's very very structured and it's all very well known there's a couple of formats for it you know whether you're using PR and so forth but there's um incredibly struct a lot of structure there and they don't really have an hotel sort of metadata issue. Interesting. Um, in part because hotel haven't fully adopted everything around profiles. Yeah. At this point, but also in part because people who use profiles actually only lightly add meta metadata. um you maybe have just application metadata a very sort of light sort of uh line and you might have if it's like ebpf profiling um some information about the host but people don't go beyond that because the actual structure provides such rich data that they didn't actually have to add more sort of uh enrichment to it. No, so profiles once it's accepted should be fine. uh polar signals are doing great work there in that space as well. So powerscape and polar signals together um are really leading that sort of industry. Um yes I work for kfana still willing to mention competitors. Um now the the more interesting ones are Mimir and Loki. Yeah. Um so Mimir um from metrics let's just be clear right Prometheus is king. It's king across the entire industry. It remains king. Uh if you go if when we go to a customer and we say what are you doing for metrics today you know some few will say data dark but the vast majority will say Prometheus. Yes. And whether that's Prometheus directly and my gosh we see some massive Prometheia and whether that's Prometheus directly or whether it's using like Mamir or Thanos or Cortex or Amazon managed or Google provided it's all Prometheus. Um, and you know, Otar is going to have to pry that out of people's hands on the value of the schema, the name schema, saying that's the signal you've left behind. Let's bring it. But no one should underestimate the difficulty of making that change because the Prometheus client, the the remote, right, and everything else, it's so damn efficient. Right? When you run the hotel collector, it is not as efficient. And when someone's got a fleet and they've got like 10,000 machines, 50,000 machines, 100,000 machines, when you talk about efficiency, you're talking about, you know, not just like, oh, it's only a couple of percent of the CPU on that one machine. You multiply that and that is real dollars. It's huge. So, there's a lot that has to be done to really shift things. Um, now's metadata in a Prometheus store and the state of Prometheus and stuff. At the moment the metadata is mapped effectively comes part of labels or you know you get it from the info sort of section and so forth. Um it doesn't handle the same volume and the same degree of enrichment and and that's you know for the vast majority of use cases that's fine but if someone comes along with an SDK and they want every single field and they want to light up 300 fields uh you're going to run into trouble. But I'd also question what you're doing. Yes. Because if you're going to send one megabyte every like 10 seconds with a single then I'm going to be sort of like what do you value here because the hotel schema is absolutely phenomenal and and I love it but I'm also just like there is a trade-off and attention here in what you want because in metrics you want timeliness. Yes. Right. Alerting of it is your critical system and that's a really important thing. um perhaps on your laptops. Look at that just in front of me. On a home machine or a Linux server and you're for a hobby project, run this stuff. Who cares, right? Because it's a few points of some CPU core and you know these are Apple M1's or M4s, whatever. I just got like whatever. But when you do it at massive scale, the efficiency starts to matter. And so there's still a a bit of issue there. And you're also having to retrofit. Like you've got 20 years of code bases that are all were in graphite replaced by Prometheus. Prometheus is everything. And now you got to retrofit and change all the instrumentation. Yeah, that's huge, right? And then when we go to Loki, Loki is far more interesting. The logs hotel stuff is far more interesting because most log store were designed for essentially I wouldn't say unstructured. Unstructured stuff is what I type in WhatsApp. Um but but log lines are semistructured, right? They're always semi-structured. They they have a time stamp. They have a log level or a HTTP method or status code and then they have some text at the end, right? But they're semi-structured. Half of it's structured. But actually over time, if you look at the last decade, this wasn't like from Click House didn't cause this. Click House wrote it, but you know, structured logs became more and more of the thing. Logs are interesting. People think they're just text, but they're not. They are logs existing multiple things. You do get sort of text logs. You do get semistructured logs. You do get fully structured logs like JSON line logging uh or log bump. Um but you also get like binary logs. Yes. No one ever talks about that. Binary logs are what you have in databases for like transmitting stuff. They're still logs. Um but you know Loki and most other sort of log databases um observability use case ones are based on semiructured data. um you know someone's gonna put a Python stack trace at the end and you're going to have to deal with that. Yes. And and so forth. Now the hotel compatibility for that we if you look at how elastic used to do things and why Loki came about elastic indexed everything which was it was efficient but expensive. Yeah. Yeah. Well, it also created some problem because they were indexing everything upon ingestion. Right. If you had a massively bursty sort of flow of logs, then there were times when the ingestion path cannot keep up. And at that point, the the back pressure would cause like scaling issues, um, drop logs and so forth. And no one wants that, right? You got to keep every log. Yes. So, so Loki's view originally was do as little ingestion as possible at that point, let everything in. And then at query time, we would brute force and do all the parsing and everything else, right? And that was great, but you know that was the best we could do and the best anyone could do say you know five years ago right? But now we're just like well actually five years is a long time in tech right and you know click and column stores do exist. Splunk got better too. Let's can't ignore the elephant in the room here. Splunk is the actual big player here. Um Loki's tiny. Click house is Tanya. uh Tanya is that one? Yeah, sure. Yeah. Um but Splunk is the market, right? And and that's the one that everyone should pay attention to. But um you know the capabilities did come along and you know people now use their observability logs for far far more than observability. Yes. Yeah. And that's the real thing that when we think about the hotel stuff, we're actually thinking that they are enriching it not just with the actual normal hotel metadata, not just with all of the actual these HTTP logs and we're going to have all the dates. They're actually enriching it in addition with things like and it's associated to this customer order and that's just joining with business data. Yeah, we're flowing into a different talking point. Yeah, we're flowing into a different talking point which is um in essence the the hotel added this pressure for structured capability onto more structured capability onto the um the Loki system and then at the same time this rise in blending business information into logs um I I use an e-commerce example of a a you know a transaction detail or customer ID but it it works across the entire Right. If you're a SAS provider, you're going to add customer sort of tenants IDs in there or whatever. Um, and you end up with support teams. They're not observability. Yeah. And they're coming along going, I want to look up everything related to this customer order because apparently the payment f and they will search, you know, a couple of pabytes of data for a keyword and and it's like that's painful. We need some structure here. Um, so well maybe we could just talk about like the state of of hotel compatibility for all of the databases. Where are we at? We already said traces and profiles. Traces spot on. Everything's good. Profiles um pretty damn good because we're in those conversation. Dimma from Pyroscope is in conversations with the oper telemetry people. Um the Mamir is um we're pretty good. We're we can actually do first and most things at the moment but we had a lot of upstream components which were in Prometheus which means we also have to go and uh spend time with the Prometheus Prometheus community to improve what they're doing and help support them and making Prometheus hotel compatible as well. Um and then Loki's Loki is probably the weakest but it's not bad right it's compatible it's capable um but does it do absolutely everything? Well, actually, no vendor does, but at the same point, it's pretty damn good. Yeah. And also the the user experience takes a bit to catch up. Yeah. Well, it is important to say that hotel does not define the query language. Yeah. Right. Does not define some of the user experience there. Defines a protocol for sending it over the wire uh and naming, you know, and these sort of metadata sort of schema conventions. It doesn't define other things. Yeah. So, we're I think we're we're making progress for sure, but like there's a little things with Loki when you're querying on on metadata that's like, oh, this could be a little bit better on on an experience level. Um maybe we could also talk about the architecture. We talked about the about Warpstream and the new streaming architecture. This one's specific to Mimir. Um, Yarun Slot says, "When do you expect to have Graphanomir 3.0 OSS with a new architecture available? And do you expect that Loki Tempo Pyroscope will follow soon?" Um, yes. Okay. What about But he's asking for a date. Yeah. Tomorrow. The so actually versioning is an interesting conversation but um the the Mamir 3 it's effectively already exists right we run it. Yeah. Um so we already use the internally there's a code name for this which is sigen. Sigen is the wife of Loki and it's the Norse god of integrity which we I like that. That's cool. Data integrity. We already have that deployed 100% in in our in turn in our production cells. Not quite 100%. Okay. Right. So we're in the vast majority of AWS and GCP sort of areas, but we're not yet on all of the issu stuff. So so slightly different speeds. So we we don't put things in the OSS and say yes, it's stable until it's baked, right? We're not going to until we've used it. So we're using it uh in Grafana Cloud. Um we monitor it. we work with it closely. um we pay attention to what's happening and when we say yeah that's totally something that you know works um then we release it but it's already available right you can already use it there's an experimental flag um you can take some reassurance on the fact that we're running it in production um so in that respect it's already available but we do want to finish the Azure piece as well right we do want to make sure it works across the top three cloud service browsers um and then we'll cut so that soon. I think it's like an like as a as you raised a good point there and we had a conversation on this yesterday. It's between like different levels of adopters of our product and it's like 3 mir 3.0 is currently sitting in that very early adopter state phase. It's obviously all being contributed into our open source repo and it's really for people to investigate and those people that are confident enough to set up the new architecture and then the documentation and how we want to do the proper release cycles will catch up when we feel more comfortable to do so and it's interesting it obviously implements the cafka API uh it should work fine with cafkers but we are predominantly using it with warstream yes um but it is the cafka API um and we have very high confidence Warstream and we're seeing very impressive numbers not just in things like TCO's savings which will enable us to sort of be more competitive in pricing but it's not just that it's the reliability improvements are great um but there are some also there I'll throw out some complexity to people and then can deal with that um there are some issues right and one of the issues is nothing to do with Mamir 3.0 and it's to do a hotel. Interesting. Um so the there is a Prometheus uh remote write um by default enables like 50 concurrent sort of streams of data. Um most of the collectors and agents default to a very low concurrency. And when I say low, they didn't have any paralyz. Um and and even though that they now support paralyzability, I'm assuming they default their default value is also one. Oh, great. Wow. So, so that's a problem. And it's a problem because when you introduce a CFKA and you got a batch, right? And a batch, right? Anyone who's used any CFKA knows a batch, right? Takes about 500 millisecond. Um so you have a tiny latency. It's imperceptible in the sort of I want to receive metrics within a second or two. imperceptible at that level because we were already down in milliseconds anyway. Yeah. But 500 milliseconds to a client um can be a backing up of data and can add back pressure and because it's continuous thing that back pressure may never be sort of alleviated whatever word. Um and so that can be an issue for hotel. Interesting. So so that's a pressure because when people deploy Mamir 3.0 Oh, uh they're going to have to think about their hotel agents and they're going to have to make sure that they pay attention to the dock and and increase the the um the uh concurrency the paralyzers paralyzation. But for every other agent out there instantly concurrent paralyzable already you're basically already doing like 50 or 100 victometrics do 35 in the agent and and things like that means that there's no back pressure no noticeable nothing else but there is back pressure in the hotel client so how about Loki tempo and Pyroscope uh wait what was the question again the architecture warp warpream stuff warp stream stuff oh yeah so on screen like can we Imagine that. Uh Tempose is all really quite job. Um come along and because they already had the column store in place and they're just adding the W stream stuff, theirs will be a quick follow. Um low keys is a little bit harder. Um, and because of the columnar storage too, adding it on the front end is not so bad. But to take any advantage of it, we want to immediately add some components uh just behind like dual writing some sort of um some storage sort of information because for Loki team adding that simplification in the and it is a simplification when you get a RF3 to RF1 you can think about it easier. So I mean in that respect adding that from a developer experience means that they can immediately take advantage of it to improve sort of creating column store type indexing of more of the structured data as it arrives and so so these are they're not tightly coupled but it feels like the best way to pass on the value of doing the change is to provide the feature. Yeah. That an actual end user can actually use. So that one's a little bit further behind. Pyroscope. Pyroscope is the weirdest one exception. Yeah. So, Pyroscope is an interesting exception because it's not using warpream. Um, as I said, engineers have very high autonomy and can investigate different ways of doing things. We don't dictate you must do this. Um and they felt in their case that that what they had already in Pyroscope was so much of what was being covered by what CFCO would actually do um that they would rather do a small change in a different direction to reach that architecture but that without requiring such a significant change. Yeah. uh and the third party component or we spoke to a lot of customers who were just like do you want a cafka and everyone's like yes right and we spoke to OSS people just like would you be willing to add a cafka and people just like it's open source yes right so so all of that is good but Pyroscope has taken the different path of it it's only a small change to actually achieve the same sort of guarantees uh benefits and trade-offs so so they've gone a slightly different way because they've essentially implemented a lot of their own fashion. Yeah. Yeah. So, they might align in future. We'll see. Okay. So, if I understand it correctly, and correct me if I'm wrong, um, Loki, Mimir, and Tempo either already have, um, CFKA, Warpstream, or are looking into it as the solution. But Pyroscope, then Loki, then Loki, Pyroscope, Pyroscope just took that and they're trying to put it into Pyroscope rather than introducing another component. So, Pyroscopes is really, really interesting because it's not um as I guess the word is modular. um and it's more integrated. It's a much richer the benefits that they get from it are really quite astonishing. Um and and it's a great experiment to run against two databases. Yeah. Because we can actually see and trade off and go which one's better. Yeah. Um and perhaps in time we sort of go the benefits of the deep integration are better and perhaps over time we go oh the benefits of CFKA were better because CFKA offers other kind of things which is like let's say you're migrating from an old cluster to a new well you could just adjust all your readers to point at the existing CFKA and you've done your migration so so you can there's more flexibility that comes out of doing these things. Yeah. So I think it'd be quite good to jump back to this question that we had earlier because we we've sort of talked a bit about the ingest um and now like we had a question about the storage later and um I guess another elephant in the room is column storage. Yeah, it seems to be the trend on the block for absolutely everyone these days is let's have a column database let's use colummer storage. Um what's how do you perceive will our databases move into colummer storage? What are we looking into this? Maybe let's let's read the question as well. Hello. After using oops after using the LGTM stack for two to three years, my or is switching to click house as it was faster to generate analytics and reports and compression is better as it can target the data type of each column. The team that introduced clickousebased solution claims that they can build the service and wrappers around it so that the dev experience remains the same or simpler. What will be the future of LGTM? So it looks like they're they're they're comparing ClickHouse a a database with a columnar store um and talking about the advantages of that. They were talking about the analytical capabilities and saying that is something that the LGTM stack is not universally better at. Also a big question at 8 in the morning. Sorry question before breakfast. Uh I don't know if the person's still online. Um I wonder if they're actually This is all recorded. So uh yeah. So is that means they're not online? Oh, it means it doesn't matter because they can come back to it. No, I want to ask. Oh, okay. Well, you can try. Um I wonder whether the question is scoped just to the observability question or whether it's to all BI data that is being output because that's a really important distinction. Um when we consider column stores, it it you kind of got to go over the history of databases. Um the column store is not a new technology. Yeah, this stuff's been around since the 70s, but actually people found it easier to actually work with relational and you know, you go back to was it Cobb and the sort of denormalized systems um and maybe it's not cobb. Wow, I got that name in my head, but whatever. Uh I once learned something. So you go back to that and and people work with relation data and it makes a lot of sense, but it has trade-offs, right? you can reason about the data easier, but the trade-offs are that it's harder to query and you'd end up creating these data warehouses to make it easier to do different type of queries and there are two types of queries and I'm going to mention them and people can follow along which is you have the OLTP um and this are transactional T is transactional right so this is basically looking at um think of select statements where you just get um just you're returning rows but these are rows which are raw rows and then your OLAP um which is the A is O A and the A is analytical and these are where you add like a group by and you're doing sums and maxes your grouping and SQL's interesting there um because that made it really easy to explain but these these two types of queries are what drives a lot of why you would want to store things in certain way so Loki was built originally for the observability use case where you're coming along and you're saying I have a a Linux server and something happened on that Linux server in this 10-minute window and I want to go down and I want to find the row the log line um where something went wrong. Right. So, so that's a transactional query and the vast majority observability use cases are transactional queries of that nature and something like metrics would be used to identify the moment where you know you get your big spike and then you go along with Loki and you say for that query window give me these log lines right show me the error set and over time the logs have the use of logs have evolved and they've evolved towards um you know people why do they want to look at dashboards all the time but at the same time instead of relying on let's say alerts or SLOs's or driven by metrics or driven by traces or other things they're actually just saying we don't need metrics as much we can derive our metrics from our logs and we will use our logs to drive our dashboards and we will make our dashboards from you know all the logs we've got and we wrote 50 terabyte of logs today and we are going to make a dashboard that shows 30 days of logs that is heavily metrics driven and it's heavily analytical because the dashboards were all count by some group by yeah because you can't have actually all the and Loki was phenomenal at that and actually you know there were some other log source as well which are phenomenal at that at low volume but they struggle when you start going I've added to my log um the I know transaction amount right just the transaction amount no PI just sort of like we've sold £17. I'm British. Switch to a different language. That's 500. Whatever. And then um which I'm the odd one out here. Someone's converting the right currency converter in Grafana. And then they would plot these uh graphs which showed sort of like transaction value of time from the logs. And now they're doing business analytical queries from log data. And we're we're far from the observability use case. we've kind of strayed um and in that um you know Loki doesn't shine as much so so and column stores do shine but there are trade-offs there are trade-offs and tentions in everything we do and it depends well that's the thing it's not like one thing is better than the other like better for what well if you are a company that has uh lots of different vendors in your environment and you don't have control of how those log lines were being written, then using something like Loki remains the very best thing you can do like by far because you can take these semistructured um sometimes fully structured logs and you can basically apply these like real-time query time transformations to pass them and produce these analytical queries. But the performance is not there, right? Every Loki user knows this. I'm not going to hide the fact, right? If you run it at phenomenal scale, Yeah. and you perform a give me all the transaction amounts over 90 days, it's going to hurt, right? And and you're going to get like you're not going to get a subsecond answer. You're going to get like a 10 30 second answer depending on how much resource you've thrown behind it. Now, if you own every line of software code that you are running and you have full ownership and you can change and retrofit all of your um applications to use a fixed structure and then you can express the fixed structure in a clickhouse table or you know any other sort of column store type thing. then you have done the work manually as an engineer to define the schema right and you've set it enshrined it and perhaps you materialize views of it as well um and at that point because you've done the work to do the I want these indexes that's what you're doing when you're creating tables you're creating you're creating these indexes then the performance on analytical queries is phenomenal that same question takes subsec under a second the catch if you then want to go your Clickhouse store and you actually want to rehydrate all those lines and you actually want to say I was using it for say a security uh scenario, right? We took our HTTP logs and we turned them into a clickout store and that was our integral sort of like record of everything that happened. um to re when you query and you try and produce an an OOLTP query out of a OLAP store uh you effectively are querying all the columns and then restitching it back together. Yeah, it's expensive in another way and slowed in another way, right? So there are trade-offs here. Yeah. Um and organizations that look at column s have to consider these trade-offs. Um do you store things twice? Right. Was the actual log line important to you in a security case? Yeah. Yeah. What whites space is used is critical to you. Yeah. Or like for a compliance use case too. Exactly. Right. So, so there are some sort of struggle areas as well. Um so you really got to go back and understand what's going to happen inside the database. Is your data like really really precious and it has to be fully retained in an auditable and sort of super secure record sort of thing. Um, do you care that it sort of comes out the same way? Do you care about actually, you know, what's your use case? Are you quering predominantly, you know, OLAP style thing or predominantly no LTP type thing? You are you willing to accept the trade-offs there? Um, and then there's halfway house, right? Click house is actually really really good. Um, damn it, I used it when I was in Cloudflare. Um, so so it's a a great store, but you do need to know your trade-offs. you do need to know what it will shine at, what it will not shine at. And to most companies um if they come along with these questions um today I'm saying you probably want a mix of tools right for your observability logs um and for some of your other logs um you probably do still want to look right and if you have a heavy analytical sort of solution and you don't need to really go down to log lines anymore and you don't have some of these security or other scenarios you probably will probably find that click house will fit. Um that said, we have a project. You're going to ask, right? So, yeah. Yeah, absolutely. We are we are looking into that. So, we have a project. It's pretty early days still though. Yeah. And this is uh us learning uh and it's us learning. So the Loki team is learning from Tempo and Tempo have already shown that uh they can use parket as a um yeah learning but then being like nope that's when we're not actually um they're pretty close to being compliant on parket but there were a couple of things where someone else was using parket and that other people use parket broke compliance and then knocked down one of our dominoes. Um but you know that was sort but on the um no tempo have had great success with comto. Um so Loki are now looking at a parket type storage. Park is interesting. I'm deviating again whoever this person is standing actually being like oh my god they're all over the place. Um park is interesting because park uh file format as a column store is actually really really good and you'll see all the benefits that click house column store has. Um, click house when you run it oss quite painful to run. Yeah. But in their cloud, they back it with object storage. That's nice, but they get all the benefits and they're not sharing that. Um, and but with parket we will put it into the well, not park, but park is defined for the file system. It's really good for seek actions. Um, that's not good for for object storage. Is this a level of detail which is too much? No, no, no. They'll love this. We'll resurface you if we Yeah. Yeah, that's fine. Right. So, so park is a format which has the index interly throughout the file and it's really good for seek actions where you actually read forward, read the point and then read forward. Um, but it's not good for object storage where you want to probably put a larger file down and do far fewer IOPS to the object storage to reduce your cost. The moment Loki's cost a little bit high, it can come down even more. Um, and when you do that, you what you really want to do is a fixed index sort of point and then your data to follow because then you could do sort of tables like partial block type fetch fetching because you know S3 and the the object storage API that could do these sort of partial chunk type fetching out of an existing large file. So we want to write a large file which is a column store. We're experimenting with this at the moment, which means I'm not promising a um we're just we're just about to have some breakout sessions on it. So that's the stage that we're at. So we're performing an experiment of a park derived format which is more object storage native um to provide far more clickouse performance inside Loki or when Loki is used for analytical queries. Uh that's the thing that's probably big news. But if data objects yeah we're calling them data objects but um if anyone actually it doesn't have a cool name like parquet um oh no are you going to come up with a name right now? I will come up with a name on the spot. Oh no. Well um that person that you were we were whose question we were answering MCS114 says they agree with Loki strength for flexible log format as we have thousands of microservices. The plan is to use vector to do the transformations and there's a plan to standardize log formats and fields. So looks like observability use case in which case low key still shines. It still shines merely because of that read use case right. If you're actually primarily uh concerned with the analytical queries like just seeing dashboards of logs over time then yeah this click house will do that better. But when you're actually using it in a um in an instance, you're doing the instant response and you're trying to go down to individual log lines, then you get annoyed that it's not as fast. I just wanted to raise the other point in that. It was an interesting one, and I think again it's down to like trade-offs where they were talking about, well, we're going to build a series of APIs on top of Click House that makes it feel like you're using LGTM underneath. And I think that it's one thing to keep in mind and maybe to ask the people that are implementing this for you. The trade-off here is you need to convert those quer like those queries into SQL. So if you're using promql at top, if you want to use the services on top of graphana, those services that you want, they're used with like Prometheus, you need to convert them. Depends how well they develop those APIs in order to use the speed of click efficiently. Actually, that's an interesting thing. Query languages. I'm sending your talking points all over the place. Well, actually, maybe we could because we only have less than 10 minutes. Maybe we could Wow. I still would really love to talk about adaptive telemetry and the drill down apps. So, maybe we could go through the state of adaptive telemetry for for all of them. So, adaptive telemetry for people that don't know. Oh, maybe you should actually describe this. Should I? Yeah, that's what you're here for. We're here to ask you question. Telemetry is a kafana cloud feature. It's not an OSS feature. Yeah, we should start with that clarification. And the purpose of adaptive telemetry is let's step back and go what's the problem with observability? And one of the large problems is that when you were in the monitoring world and you would add obser you you would add monitoring to answer specific known questions. And with observability, you're adding a lot of instrumentation so that you can answer the known questions, but you can also in the time of an instance answer your unknown questions. Um, so you're gathering far more information and it gives you this ability to reason about what's happened and everything else. Um, but that means that the volumes of data have exploded and your costs too have exploded. Yeah. And you might only still be reading a similar amount of data to what you used to have. So your value hasn't gone up, but your cost has gone up. You you aren't using it anymore, but your cost has exploded. And there must be some way, you know, some people will come along and go, I want a quot. I want to say that you cannot write more than this much works. You cannot go above this many active series in um in a Prometheus world. And the problem with anything that is a quot is everything is equal, which sounds wonderful until you stop and then go damn. Yeah. And what happens is that if a quotota were kicked in, let's do metrics because it's the easiest thing to think about. If a quotota was kicked in and we said right we exceeded some uh 100 million active series this numbers too high I don't know what customers open source users we see customers like hundreds of millions of active series um say say 100,000 series is probably a good like yeah yeah good good open source but still so even 100,000 active series right you perhaps don't want to spend on 150,000 so so you put a quot in and say at that point. Throw away everything above this. Now, that's crazy because one or two things are going to happen. Either your quot is time based and you consume for the first two/irds of an hour and then you just go we're going dark third of an hour or you're going to say we're applying this quot and anything above is dropped but everything is equal. The critical signals that you're going to use, the critical metrics and labels that you're going to use may go above the quotota be the ones that are dropped. Yeah. And what you really want is some way to prioritize. You want to say this stuff is really important to us, but that stuff isn't. Right? And so when it comes to receiving the data, you want to keep the important stuff and say, you know what, if I've never queried it and I never ever use it and it's not in an alert and it's not in the am I just hoarding just Yeah. you know, and am I going at home and there's boxes of data everywhere, printed all the log records out. So, adaptive telemetry is a system that we first introduced in metrics um and now we have in logs um and we're working on the others. Um and adaptive telemetry is a system that reads the usage of all of the telemetry based on some criteria. Um, for metrics, it's whether it appears in dashboards, you're querying it, you query it via the CLI, you put it in alerts, it has a proximity to a time and event like something going wrong in a machine. Um, and that's all important for gradations of uh importance and that stuff will always be kept and then you have the ability to manually or automatically say the unimportant stuff um, aggregate it, discard it, whatever. Um and what that does it means that you can observe everything you can instrument he heavily but you don't have to have this sort of sticker shock of the price at the end of it and that means that your costs are under control but ignore the costs what it really means is that your value is very high for the cost that you are paying right and that's the real sort of benefit of it uh in low key that's based on patterns like log line patterns um and we're currently looking at we are going to complete the cloud Right. Whenever we learn something from one database, which is really good, the other teams listen, they learn to so so we are working on uh adaptive traces. I don't think that's a shock to anyone. No. Um and we talked about it just in graphicon too. Yes. Yeah. That's good. And profiles adaptive traces and profiles are out. Yeah. Yes. Yes. So we are working on adaptive telemetry as a whole family thing. Um but it is in graphana cloud only and it it's not oss because of the the massive integrations across all of the systems. This isn't a you could use tempo by itself and gain adaptive learn adaptive traces or you can use Loki by itself and gain it because for us to understand usage we have to see everything and that's very much a very deep integrated thing. Yeah. So and it's like we're we're also syncing the own cost of running these services that are basically checking whether you know you're operate like your the alg the algorithms that we're running to make sure that your data is ready for adaptive. So so in graphana cloud we give that for free. Yes. Right. Well you're paying for graphana cloud and when we added these features we did not charge you more for these features even though it lowered your bill. Yeah. Right. So so that's an incredible incredible thing. Um but it makes customers happy. Right. And And and that's it. That's the point is that we want we want them to be with us for these reasons. We want them to trust the fact that we know what we're doing with your data and we you know well it's also for us like this isn't we also use this. We also want to lower our cost. We also want to increase our value, you know, because we we use all our own stuff. So we want this for ourselves. So why would we then not make it available to to other people? Exactly. And so some of our own systems and we use all this stuff with automatically applying rules across all of our signals which will probably terrify some people. Ninja Sloth has a question just on this. Adaptive metrics or inline aggregation seem to become a necessity at scale. Has there ever been talks about this functionality in OSS but with more manual user generated aggregation rules like M3? Yes. Um sure there are talks. How to do it is a little bit more tricky. So that's really interesting as well because M3 added that functionality and we were inspired by that and that was caused us to go and actually say what can we add um and but M3 haven't innovated on that in the OSS anymore right that they've actually put all of their secret source back into their own database and that's the proprietary stuff whereas we've um taken the capabilities that we've actually built a lot of it is in there but we haven't yet surfaced it externally now I actually do think that we will eventually no date um eventually have to come around this because I do think that there is similarities here with other for me one of the forms of innovation is to see something in different space and bring it over and for me this is very much like in you know a a firewall or a waffle or something where what you actually want is it's a rules engine and a rules engine um these are common in other parts of tech and you know we we should possibly be looking after we've got them to a decent maturity to align them in some way to actually say rules, transformations, aggregations, enforcement should happen, you know, closer to the edge and we could investigate that. But that's a very open future. It feels like in a year or two we will be approaching this point where we say we've got it to a point where we would consider trying to align this and now surfacing and and that's not to say you couldn't create like a poor man's version with Prometheus with something like recording rules or something like that where you could take a much more extensive data generate some recording rules and bring them into a more central way of doing it. M no why do we do this in cloud and not in oss and part of the reason is the innovation speed right if we put this in oss and you all used it um and you would use it we would not be able to change it right and if we got something wrong we cannot fix that we baked it in right and it's going to be wrong it's going to be painful it's going to be a horrible experience and you're going to experience us throwing away everything you just built right so we don't want to do that so we build in cloud we make our mistakes right mostly to ourselves. Um, and then we you say it's stable enough. It's Yeah. Yeah. I I think we can do that. Can you un stop your camera? I think we're still going on on mine at least. Uhhuh. Is that still broadcasting? Can you stop your camera? Yeah, I can just get remove myself. Well, that's interesting when you're So, yeah. So, just did get out, right? And I then scrape time aggregations with the likes and the alloy component to keep these closer to the source. Yeah, I mean you could do it at different levels, but I mean I think if if the point of adaptive telemetry is to lower cost, increase value, you can do that at multiple layers. So you're talking about doing it on the collector level. You could go further back. You could do it like if it were logs or something. Um what should be logged? What shouldn't be logged? There's education that's around that too. I I disagree with that because then you're actually taking a regression back towards monitoring and you're only going to have the the signals that are in the code the instrumentation. No, I'm talking like logs that you don't really need like there are a lot of people who still do like to print to log and there's just some console logs that are not but if adaptive telemetry throws it away and then during an instant you said I really needed this I tried to query it then we can unagregate it we can undrop it right and tempo has this ability that we're we did we announcer I do not because we introduced the warpstream component And we can consider things like um recent read playback. Oh yeah. Right. So if there was a rule to like drop things um and then someone started querying perhaps we can just go back and put the cafe client to read earlier and replay that bit and then put it back out even though it's supposed to be aggregated. So even in an instance you can immediately get it back. Right? So adaptive telemetry is way more mature, sophisticated, and capable than merely performing aggregations hardcoded at any level because it's dynamic. Yeah, I just um know from like load testing that sometimes people just want to keep absolutely everything like every request they want to return in a log in a log line the entire response body of every single request for response body. Yeah, that's PII nightmare. Yeah. Yeah. literally. Yeah. Um, so it has to be like at all levels and not all of it is adaptive telemetry. That is one of the layers. Um, but you could also, you know, if if you're trying to get more value out of it, alerting, like you said, you don't want to be looking at a dashboard forever. Anyway, sometimes people have all the data, but they don't have the alerts on it. They might even have a great dashboard. Humans are the weakest link until you actually have to respond to the instant. And then humans have the strongest link. Yes. But also like to to the point of why don't we OSS this? We were just in a talk yesterday, an internal talk where we were talking about how how much handholding it takes to get to even get a team to say yes to we will drop your logs, you know, like it's not it's so it it feels like a disservice to just put that out in the world and not support it at all and not walk you through like here are the ways that you can kind of tweak it. It's interesting, right? So the what do we OSS? We oss virtually everything and but what we're ossing is like all of the capabilities, the building blocks and you can pick and choose. You can compose the solution. Um but some of the very deeply integrated, very opinionated, very this is how it will all come together as a very tight solution that is on the OSS and a lot of that's to do with that we have to learn ourselves. We have to carry on innovating. You have to be able to like not calcify it, sort of fix it in this moment in time, sort of just have it sort of acrue into a fixed state. We have to be able to evolve it and the best way to do that is in our own cloud. So there is a real tension there between what we do open and what we don't. But I do think going back to the question about um opening up and everything else, I do think we will reach a point where that is not fundamentally a sacred thing. It's not changing very quickly. we can know what the capabilities should be across all the signals. We can build a rules engine that would work across all of the signals and that it has that essence like a W where you don't WS don't care what the application is. I think there's a space that eventually that will come but you know there still be stuff which wouldn't come with that like all of the backend weird stuff it's deeply integrated into Grafana cloud that tries to find out what you use when you use it and how to recommend the rules that probably wouldn't be because that's so deeply integrated some of that's even integrated into our billing system you know we're not shipping our billing system yeah okay so I'm going to be the bad guy over time's got places to be. So yeah, so taking this all this from the cache brain, let's see if we can summarize what we've done so we can finish up. So we've talked a bit about the new architectures that are are potentially coming in and one has already come in from Amir. We've talked about the power of CA CFKA especially during ingest. We've theorized a lot around column storage and paret. Um, we've talked about the future of hotel with all of our databases and the state of where we're at. And then we've talked about our adaptive telemetry. Um, and where that future might go as well, but not to be, you know, that was theorized once again. Was there anything else that you just wanted to throw out there? Those future directions. I think we went in enough directions. You revealed enough how to come up with names for things. Yeah. Every project, every implementation for every database is different. Yeah. Uh the naming of things like so Sigan was the name of the Wstream project, the the Cath project. Loki have decided Thor. Obviously Thor being this sort of the brother of Loki and being even stronger and more well behaved and everything called there's rhythm cuz there's like the best way to be like a good drummer and having tempth and I'm just like I can't keep track. We should leave them on that. All right, thank you everyone. We'll see you next time. Thanks for watching and thank you D for also joining

