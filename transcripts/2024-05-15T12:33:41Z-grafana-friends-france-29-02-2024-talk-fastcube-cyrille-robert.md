# Grafana &amp; Friends France - 29/02/2024: Talk: Fastcube - Cyrille Robert

Observabilité des données gérées dans Denodo Platform Le cas d'utilisation « Observabilité des données » illustre comment, ...

Published on 2024-05-15T12:33:41Z

URL: https://www.youtube.com/watch?v=x2TmX8nA6m0

Transcript: euh donc déjà bah par rapport au slid vous verrez la différence entre des éditeurs et une ESN sur la qualité des slides impressionnant euh donc déjà m'excuser pour ça et puis le le le jeu l'exercice qu'on qu'on vous propose ce soir c'est de dire bah voilà Vincent nous a présenté des nodo data plateforme data virtualisation et donc très rapidement exploitation de données métier en vu de d'aller dans la vraiment la gestion de donné métiers apporter de la valeur avec ces donné métiers avec la BI le reporting et puis maintenant et de l'autre côté JB qui nous a présenté l'observabilité et là on retrouve vraiment le l' les logs les matrix l'opry le réseau le SNMP donc de monde qui sur le papier en étant évidemment bien bien entendu assez caricatural assez différents et qui ont du mal à à discuter parfois ou ou voir la complémentarité donc l'exercice que on va dire un peu différent un peu différenciant qu'on qu' qu'on propose ce soir c'est de dire bah ces deux mondes là ou ces deux là ces deux acteurs là sont on va dire très bons dans leur domaine et ben en couplant les deux peut-être qu'on peut faire des choses intéressantes donc le pour ce soir on a eu choisi le sujet de la data observabilité donc data den Noodo observabilité grafana mais bon où est-ce que ça ça commence donc bon l'agenda je passe donc en préambule je vais quand même présenter Fas cube euh très brièvement donc pH cube c'est une ESN qui s'est créée en 2017 et très rapidement voilà les domaines d'intervention de F cube dans donc dans la data data management data plateforme data intégration et et entre autres bi Analytics ainsi de suite et on a également un pôle qui est un peu différent de des autres qui est le PM euh où on travaille plus avec des Directions Financières euh avec des outils commbard IBM Cognos non maintenant c'est planning Analytics en voilà c'est ce genre de on a une un liste de partenaires avec lesquels on travaille et euh donc les différentes technologies donc dans le monde de la data bah en haut on a des nodo mais on a également partenaire avec bah des des offres du marché assz standard snoflex databrix conflient type Koui et sur la partie B al j'ai regroupé sous le pôle bi je peut-être des gens serai un peu vexés mais voilà grafana euh bord pour la partie EPM et puis après bah comme beaucoup de monde pour bi est tabou voilà une ESN des partenaires et on travaille en collaboration avec les émitteurs pour déployer leur solution dernier petit mot concernant la société on est présent donc on a trois sites en France un au Canada et on a également une équipe à Casa qui nous permet bien entendu de d'être d'avoir des offres adaptées parfois déjà pour l'Afrique du Nord et le Middle East mais également pour la zone yier à des imaginez bien des tarifs mirchore pas des tarifs parisiens voilà j'en ai fini j'ai fait ma ma BA présenter la société quand même donc en préambule pourquoi ce sujet data observabilité bah déjà c'est parce que comme certains par mon JB est quand même assez ancien on a fait pas mal de projets data dans notre vie et on a eu la la vague des MDM des datal des clusters àp des Data war House et je sais pas pour vous mais si je fais une un retour sur expérience il y a quand même pas mal de projets où ça c'est pas forcément fini très bien ou dit plus poliment la valeur ou le retour sur investissement qu' pu était pas forcément énorme le cas le plus typique c'est enfin pour ma part ce que j'ai vécu les Adou on fait desal qu'on injecte de la donné on injecte de la donné et puis un moment donné quand on veut l'exploiter bah c'est très compliqué et voilà deuxième chose pareil sur une base expérience on a déployé des de l'obser abilité métier d'observabilité orientée métier et on a eu en fait une grosse surprise c'estàdire qu'on a eu un énorme succès et une énorme adoption par la communauté la volonté avait été de dire on va faire du donc de l'observabilité applicative voilà mais en visant le métier et et cette ça a été une vraie successor ce qui fait que au bout de quelques semaines toutes les acteurs ouvraient graphana tous les matins pour voir si euh en l'occurrence c'est un projet de transformation finances donc tous les tous les beaucoup de processus métier et tous les acteurs d'entreprise et ben aller tous les matins voir si pour faire simple si c'était vert ou rouge et grosso modo si c'était si leur domaine était vert et appeler leurs collègues à côté si le après un cas particulier et puis par rapport au sujet aujourd'hui évidemment c'est une synthèse de de beaucoup de thèmes et chez certains un client on a eu des des demandes dans tel domaine tel domaine donc on on propose aujourd'hui c'est en fait de reprendre tout ça dans une initiative globale pour aujourd'hui bon j'ai j'ai mis des petites asrix j'ai tendance à utiliser le Monot opération euh exploitation et je sais que le mot opération en particulier pour beaucoup de monde a une are peut avoir une aification opération commerciale ça peut être des des organisations métiers donc veuillez m'excuser ce soir quand moi je vais utiliser le mot opération c'est Mo le côté support maintien en condition opérationnelle vraiment le côté exploitation voilà et puis pour le reste on va avancer donc lorsqu'on bah on travaille entre autes avec nos amis de denodo sur des des projets clients donc on aujourd'hui on a quand même pas mal de de de missions qui viennent de la volonté de beaucoup d'entreprises de de se transformer de de devenir dat cintrique c'est on va dire il a peut-être mux un effet de mode mais toujours est-il que beaucoup de sociétés ont nommé des CDO en mettant la data au cœur de cur au cœur de l'organisation et de la transformation des métiers donc un projet de de transformation data cintrique transformation digitale mais orientée autour de la la data très rapidement on va se dire mais quand on va déployer ça ça va toucher tout le monde donc c'est-à-dire que le CDO le management de manière générale dit je vais faire un gros je vais transformer mon organisation je vais transformer mon est-ce que je vais est-ce que ça va servir est-ce que tout ça ça va fonctionner est-ce que la data si je reste concentré sur la data elle va être utilisée est-ce qu'elle va être utilisée partout par tout le monde et est-ce que chaque acteur chaque producteur de données va se sentir responsable de ces données parce que généralement c'est je le fais évidemment toujours pareil de manière curale c'est un problème d'IT moi mes données c'est dans mon fichier Excel et ça me va très bien et je veux les donnés des autres je veux pas partager mes données avec les autres donc on a un challenge par rapport à la direction qui investit dans cette transformation évidemment dans le but d'apporter après très rapidement on a des Data hers ou des des managers de Data ou de Data plateforme qui arrive à qui on va demander tu es responsable de la donnée donc tu dois veiller à ce que ta donnée elle soit bonne disponible de qualité donc ça déjà un premier enjeu donc garder le contrôle de la donnée et très rapidement et là c'est un c'est un historique plus intégration à partir du moment où vous allez partager la donnée les consommateurs dès qu'ils auront un problème et dès que vous faites de l'agrégation de données donc des nodo multisources et puis à la fin on a un super KPI si la personne vous app en disant le K il est pour il va appeler la data plateforme il va pas se dire ah oui mais c'est normal c'est parce que dans un coin la donnée éit pas bonne donc c'est-à-dire que très rapidement le Data honer ou par délégation le Data va devoir prouver et trouver d'où vient l'anomalie de la data je sais pas si certains de vous ont déployé de la data intégration de manière large dans les entreprises très rapidement c'est le bus et le ai et les ETL qui sont la route cause de tout c'est jamais les applications et donc il faut prouver que la donnée qui est mise à disposition pour vos processus métier elle est sous contrôle ça veut pas dire qu'elle est forcément toujours parfaite mais quand il y a une anomalie vous savez et vite et vous savez vite identifier la la source du problème on peut dire que fréquemment c'est la la source la data source qui est fréquemment à l'origine mais ça peut être un change un déploiement une P matériel enfin n'importe quoi l' a évidemment des challenge parce que tout ça c'est de l'IT à un moment donné faut que ça marche euh il y a de la data il y a du volume il y a de la performance il y a si on on trraîne un peu on a la dette technique donc comment on fait et puis à la fin j'ai euh et c'est peut-être pour moi si je devais garder un point c'est lorsqu'on a une stratégie data et qu'on commence à publier de la data au niveau de l'entreprise ça ne fonctionne que si l'utilisateur a confiance dans la donnée parce que vous lui demandez généralement de passer de ces fichierers Excel qui s'échange avec ses copains en disant maintenant tu n'utilises plus tes fichiers Excel tu viens chercher les données sur ma data plateforme dans mon MDM dans mon datalec et la personne si elle n'a pas confiance et si de temps en temps elle a la preuve que c'est pas B et ben votre datalec et ça je l'ai vu M main de fois il est pas utilisé vous avez vos 40 machines votre cluster à dou et personne se connecte dessus parce que personne n'a confiance dans la donnée et celui qui gère ne peut pas prouver qu'elle est bonne enfin n'a pas prévu de donc euh ces enjeux là et ben en fait on a une des populations totalement diverses et variées on a de l'IT on a de la data sa tri et très rapidement on a en fait toute la communauté d'entreprise et ben en fait la proposition c'est de dire bah tous ensemble pour le pour le bien de tout le monde on va regarder si notre donnée est sous contrôle je vais utiliser parfois le mot est-ce qu'elle est en bonne santé est-ce que ma donnée elle elle est en bonne santé ou estce ou inversement ou est-ce que j'ai un problème et on appliquera le traitement parfois de cheval si nécessaire donc là je vous propose évidemment si vous faites Google data observability vous allez avoir des des des propositions des définitions diverses et varié là c'est vraiment notre vision de la DAT observabilité comme je dis vraiment si il y a une chose à garder c'est comment à la fin obtenir gagner la confiance et évidment garder la confiance des consommateurs de données et répondre évidemment aux autres enjeux success facteur pour la data observabilité comment on s'adresse à une audience la plus large possible on met tout le monde ensemble l'interface utilisateur c'est bête à dire mais ça joue énormément si c'est lent si c'est pas très beau si c'est pas explicite s'il faut être un spécialiste pour comprendre les indicateurs ça ça marche pas trop le multilevel c'est de la vue synthétique et puis à un moment donné quand même être capable de descendre un niveau de détail mais pas avoir que du détail mais inversement pas avoir que le KP qui est toujours vert parce que on va faire en sorte qu'il soit toujours vertorisation ça paraît évident mais j'arrive enfin le matin tout va bien mais je veux peut-être voir si pendant la nuit pendant le weekend les jours précédents tout s'est bien passé donc capable de me promener dans le temps pour vérifier que tout s'est bien passé une vue donc avoir accès aux données donc opérationnell données d'exploitation à 360 parce que en fait comme tout peut impacter ma donnée vraiment au sens bah un petit événement dans un coin c'est là où on va avoir besoin d'un maximum de choses et pour finir on va dire c ASP vraiment data le linéage vousou les dépenses ou les relations entre les données j'ai une donnée que je publie mais à un moment donné elle dépend d'un d'une autre donnée elle agrège de données elle dépend d'une base de données elle dépend d'un serveur elle dépend de et ben ce lineage cette cette relation vont également à momentn permettre de synthétiser l'impact parce qu'on peut dire voilà tout va bien sauf que j'ai une machine qui est arrêté oui mais si elle est arrêté il y a problème donc c'est de pouvoir montrer l'impact derrièreon j'ai avancé donc j'ai pas d'heure je vois pas où j'en suis euh donc là la proposition que l'on fait une nouvelle fois basée sur certaine expérien et l'agrégation de plusieurs expérience c'est de dire lorsque je veux pour pour savoir si ma donnée est en bonne santé déjà évaluer la santé de ma donnée en fait ici j'ai un pointeur non il y a pas de pointeur laser sur celu et ben très rapidement je vais avoir besoin de savoir dans alors évidemment c'est des catégories qu'on vous propose ici on peut les redéfinir les grouper ou les spliter mais c'est de dire je veux savoir si ma donnée elle est disponible donc je vais voir est-ce que mes données sont disponibles est-ce que mes tout mes batch mes hôel mes pipelines sont fonctionn ainsi de suite en tas de choses est-ce qu'elle est disponible est-ce qu'elle est en sécurité parce que ça reste quand même un fondamental on doit la donner on doit maîtriser qui la consomme et ainsi de suite surtout dès qu'on commence à avoir de la donnée confidentielle est-ce qu'elle est en qualité donc qualité est-ce que elle est in tech consistant on va voir derrière si vous connaissez voilà des gens on définit ce qui était la data quality enfin les catégories de contrôle dans la data quality et d'autres si mon système d'information bah qui supporte tout ça il est opérationnel parce que un moment donné c'est bien beau ma donné pere de qualité mais si si mon système il a la ramasse si le si il commence à se donner des signes de fatigue ben ça m'intéresse et alors terme de déomination je sais pas si c'est bon terme mais c'est est-ce que ça tourne correctement est-ce que ça tourne ça m'a donné peut-être de qualité mais à un moment donné dans l'observabilité donc on là ça va être plus le CDO le management est-ce qu'elle est utilisée on a connu des clusters à doublees qui étaient là qui consommaient l'électricité mais pas une seule connexion utilisateur dessus donc c'est également vérifier que comment est utilisé le système dans tous mes données dans tous mes data product pour utiliser la la terminologie lesquels sont les plus utilisés lesquels ne sont pas utilisés par exemple et puis bah par extension euh les performances les incidents s s'il y en a si j'avais caractérisé et je je l'ai mis dans le slide sujet qui peut on va dire lancer des débats très rapidement aussi euh on a eu j'ai on a eu une mission très spécifique là-dessus qui est de commencer à construire un modèle économique parce que c'est bien beau ça mais à un moment donné avec l'évolution hein euh c'est de commencer à imaginer un modèle économique euh Vincent je sais pas où il est passé voilà commencer à montrer que des nodo lui-même commence à apporter de l'information pour le phops puisque c'est quelque chose qui arrive de plus en plus et j'ai fait l'impasse sur l'emprunte carbone qui risque d'arriver un jour où il faudra associer ce type d'indicateur donc c'est j'embrasse un un maximum de sujet on peut en rajouter on peut en enlever mais c'est vraiment ce partager ce côté avoir la vision à 360 de tout ce qui fait m'a donné et qui mea dit que si tout va bien sur tous ces critères là ça va ma journée va être calme et je peux vraiment travailler avec mes utilisateurs et eux consommer donné en confiance ainsi de suite là j'ai juste repris les le premier niveau à plat alors je vais juste vérifier l' parce que je n'ai pas de montre et je voudrais pas vous assassiner 12 minutes ah bah ouais donc je vais pas vous faire ces slid là pas tous euh mais euh juste extraire une ou deux informations c'est de dire voilà euh derrière chaque case catégorie sous catégorie le principe des des quelques slides et je vouslais défil c'est de dire bah en fait tout ça bah au départ il nous faut de la data il nous faut des données c'est des données d'exploitation fréquemment c'est des données de diverses et variés donc si je prends par exemple pour la qualité il faut qu'on ait accès au au data product donc aux données métier qu'on a accès au lineage qu'on a accès potentiellement au aux Data Source qui a des qui ont permis de construire ces ces data product parce que j'ai un indicateur qui me sort par exemple un nombre de factures bah peut-être que je vais vouloir vérifier si c'est bien le même nombre de factures dans la data source alors c'est que un nombre mais si j'ai un gros écart ou si j'ai un écart il y a petit pe donc on peut avoir besoin d'accéder au datas pour faire ce type de chose le code qui est déployé les changes est-ce que des modifications ont eu lieu ou pas approuvé pas approuvé et évidemment tout ce qui s'est passé avant par rapport à ce domaine là donc une fois donc j'ai besoin de ces données là ces données là à un moment donné bah il va fa que je les récupère que je les travaille je les processe je les transforme donc on va retrouver la notion de couche sémantique pour quand même à un moment donné comprendre euh ce que leur donner une signification et puis derrière sortir des informations donc qui vont rentrer progressivement dans dans les cases donc de dire bah je vais coder des règles de Data Quality de contrôle de consistency de contrôle d'intégrité donc très fréquemment c'est du SQL donc à soit avec des outils dédiés soit à la main en SQL avec des stles des outils comme desbt ou dans des nodo avec des vues mais derrière c'est du SQL donc cette logique de Data Quality je la mets dedans et je vais sortir ça je vais sortir la version synthétique je vais sortir je vais détecter les changements techniques donc un changement soit de de Data Modelle soit de CAES mais très rapidement ce qu'il va savoir c'est les changements prévus puis ceux qui sont éventuellement pas prévus est-ce que j'ai un change qui est associé à mon changement ou pas dans des noo par exemple parce que pour rester quand même dans le thème de la soirée euh si la personne a des droits c'est très simple hein il change de et mais ça peut avoir un impact et et c'est très bien c'est l'agilité mais ça peut être intéressant de savoir qu'il y a eu un changement comme disait JB c'est qu'à un moment donné la route cause bah très rapidement si on a un incident puis que grosso modo dans la même minute il y a eu un changement de code livraison déploiement d'un nouveau code peut-être qu'on peut regarder de ce côté-là c'est pas forcément la route cause et c'est ça voilà donc tout un tas de de de rapport qui vont venir alimenter on va dire un indicateur ement je je synthétiser comme ça un indicateur dans chacune des catégories donc on n pas beaucoup le temps si ça vous intéresse un peu après ou ultérieurement de rentrer plus dans le détail ce sont évidemment des propositions ça aucune prétenension d'être exhaustif mais c'est des cas quand même très rapidement qu'on a rencontré et pour lesquels on un moment donné il se dit alors parfois on l'a fait avec un petit script un petit point S et puis on se dit mais en fait si on regroupait tout ça et ben à la fin ce serait ce serait sympa quoi comme ça parce que ça répond plusieurs personnes sont souvent intéressées par le même indicateur donc là sur la sécurité je fais une toute petite parenthè sur la sécurité pourquoi parce que autant sur tout le reste on va se dire oui bah c'est bon je vais regarder je vais agréger je sur la sécurité euh ce qu'on propose c'est de dire les ACL de vos systèmes VO c'est de la data vous récupérez par API vous allez dans vous voilà voilà sur telle donnée sur telle table voilà les rôles qui sont autorisés voilà les rôles voilà les droit de chaque rôle on c'est très bien bah la proposition c'est de dire on va extraire ces ACL le consid que c'est de la data donc on va la structurer on va la structurer dans une matrice parce que à un moment donné l'humain a aussi besoin de comprendre les données donc quand on parle de sécurité très rapidement un des modèles pour présenter la donnée c'est une matrice de sécurité enfin en l'occurrence des matrices de sécurité et la proposition de c'est en fait je vais avoir ma matrice de sécurité que j'ai spécifié en tant avec mon dans le cadre de mon projet et ben je vais récupérer la data brute mes ACL je vais reconstruire cette matrice et je vais la comparer et si j'ai mes deux mes deux matrices sont identiques ça veut dire que je suis sous contrôle en terme de sécurité il y a pas eu de changement d'ACL unc contrôlé après le changement il peut être normal mais au moins on sait si on est en phase entre ce qui spécifier approuver et ce qui est réel j'ai pas précisé j'ai un gros background réglementaire avec la pharma et prouver l'auditabilité et prouver que ce qui est pécipié est déployé c'est c'est on va dire c'est une seconde nature et puis après ça peut s'appliquer également aux données sensibles et sur la sécurité je m'arrêterai là par rapport aux autres c'est en fonction des technologies par exemple des nodo on a la capacité de de faire de l'impersonate c'est-à-dire que on peut vérifier no no matrice de sécurité nos rôles nos permissions mais à un moment donné on est si on est un peu paranous on peut se dire je vais faire une requête en tant que et je vais vérifier que effectivement en tant que je n'ai pas ou j'ai la data c'est ceinture et Botel peut-être mais ce test là il est ultime voilà et entre autres pour des données masquées ainsi de suite euh je peux dire on peut assez facilement se tromper lors du déploiement et une donnée masquée devient non masquée et ben si on a ce double contrôle voilà c'est vraiment le ceinture plus Bretel je me connecte comme JB et bah JB non tu as pas le droit parce que voilà voilà pour la disponibilité on va retomber sur la disponibilité des datas sources avec des mécanismes du style on va vérifier bah que le mot de passe pour se connecter enfin on va se connecter régulièrement et donc on va détecter des problèmes que très très fréquents des règles de firewall qui changent des mot de passes qui expirent des mot passes ok ce genre de chose c'est tout bête mais c'est le quotidien et pourquoi ma donnée elle est pas bonne bah c'est normal parce que le mot de passe pour connecter à la base de données il est il est bloqué ah ouais mais je fais comment or une nouvelle fois quand vous recevez l'appel si vous êtes en charge vous dit mais votre données est pas bonne bah si elles sont bonnes il y a pas de raison qu'elles soit pas bonnes ben non c'est un mot de passe qui qui qui a expiré et qui qui bloque toute la chaîne voilà et puis sur le run le statut tout ce qui est bon on va dire assez classique qui s'est connecté quelle requette on exécuté combien de temps donc ça donc je veux pour aller très vite donc mettre en place de la table observabilité c'est quoi c'est on a besoin de se connecter et de collecter des données euh de transformer manipuler ses données de et de et de stocker le résultat de ces transformations généralement voilà de les publier et les visualiser bah ça pour certains d'entre vous qui ont travaillé sur des projets de type data data plateform c'est ni plus que ni moins qu'un projet data comme un autre c'est le même modèle lorsque vous faites de la data dans la finance machin c'est exactement même donc la proposition c'est de dire vos données d'exploitation V données de production on va les G pe-être c'est un projet data comme un autre c'est pas les mêmes c'est pas des données m mais c'est des données alors là derrière c'est vraiment come dans la formule Meetup il y a je sais pas si pour certains vous êtes familier on a du mal à savoir quelle va être l'audience donc j'ai fait quelques slides beaucoup plus technique en se disant voilà il y a des gens qui potentiellement sont plus intéressés par la technique donc première des choses donc pour répondre à ce besoin d'abilité donc là j'ai j'ai remis un peu le CH en fait si vous prenez ces grand fonctionnalités et ben de dire bah des noo et là désolé JB si si tu je te vexe grafana c'est très bien mais dès qu'on va vouloir manager transformer processer avoir de la logique métier ben dans grafana comment dire même les transformations c'est bien mais à un moment donné c'est un peu limité quand même voilà et dès qu'on est multisource alors là on on commence à rentrer dans les choses extrêmement compliquées pour dire impossible là où denodo en fait c'est naturel des noo vous lu donner les sources il agrège les sources on agrège on agrège on agrège voà ouais mais des Donau comme l'a dit Vincent c'est un middleware il y a pas d'HM il y a pas de couche de présentation il y a pas de couche de stockage c'est d'ailleurs c'est on prend la donnée on la virtualise et et on la sert à la volée et donc mais à un moment donné ce que j'ai calculé il faut quand même que je per donc en fait en couplant les deux et ben en fait je réponds à tous mes besoins mes capabilities qui vont permettre de déployer de la tabilité je traite ma donnée je la je l'agège et je la visualise on a le meilleur des deux mondes si je peux dire ça qui vraiment collabore c'est pas l'un ou l'autre là j'ai listé un tas de points euh je vais pas les énumérer mais pourquoi grafana bah l'adoption la navigation time series time base parce que on est dans le domaine de laibilité on n'est pas dans power bi on n'est pas dans tableau c'est vraiment l'axe temps on se promener dans le temps on veut réduire la fè on ve grandir on veut aller dans le passé ainsi de suite et puis bon l'alerting j'en parleris mais un modèle aussi très très important dans le qui retour d'expérience fait le succès de ce type de de projet c'est quand on commence cette démarche là là et ben on commence avec quelques indicateurs sur auquels on a passé puis après on va dire ouais non finalement je vais changer ceci cela et donc la capacité de modifier de rajouter des dashboards de rajouter des données d'agrécher des données c'est quelque chose de extrêmement facile avec avec grafana parce que gross chaque dashboard est un fichier unique un fichier JSON donc ça c'estez facile de déployer là où une application power bi on va retomber sur un cycle de vie d'une application avec des phases donc pour comparer euh parce que vous imaginez bien qu'on est eu mais avec pourbil je peux voir des camomber des histogrammes et des gens pourquoi tu je vas chercher un autre outil oui mais c'est pas la même philosophie c'est pas la même philosophie et l'agilité euh est essentiel parce que très rapidement on a un incident ah zut je l'ai pas détecté peut-être que j'ai une donnée la donnée je l'ai mais j'ai pas mis l'alerte j'ai pas mis le filtre ou j'ai pas le voilà et donc on a envie d'aller assez vite et des noo bah virtualisation des tas de choses pareil donc là vraiment le désolé pour le ça va être on va dire très technique mais une nouvelle fois c'est en fonction de potentiellement des gens qui pourraent être intéressés le la la configuration que l'on a mise en place chez nous et quand je dis che on l'a mis en place c'est-à-dire qu'elle fonctionne évidemment on est dans un Lab on n'est pas sur un volume d'activité mais on part al il nous faut d'oir une data plateforme on est on est on est on est partenaair des nodo donc on va on a choisi des nodo comme data plateforme métier pour démarrer voilà aller sur des serveurs donc très rapidement la première chose qu' qu'on a fait parce que nous on n pas de SPL on n pas d'élastique pas ce genre de chose bah on a commencé à déployer no nos exporteurs promo tu c'est tac tac tac on les a fait passer par un par un kfk parce que c'est c'était élégant voilà et puis tout ça pouf ça part directement dans le grafana cloud JB na pas dit mais sachez qu'il a une version free de grafata Cloud qui déjà vous permet de faire des choses extrêmement poussé parce que les limites de la version Free sur les volumes sont déjà hautes on peut aller assez loin c'est le nombre d'utilisateur on va dire la principale restriction donc on met ça donc là on est vraiment sur le du système classique des log des métriques ce genre de choses ah pardon c'est pas le bon bouton après bah on se dit oui mais maintenant on va rentrer dans la data observabilité donc je vais utiliser la puissance de denodo donc dans denodo première des choses que je vais faire c'est que je vais configurer tous mes toutes mes data sources donc dans des ça Data Source donc je vais me connecter à la Directory parce qu' moment donné j'ai des rôles mais j'ai des utilisateurs donc JB mais en fait à quoi il a droit JB potentiellement la plateforme g que des rôles et c'est dans l'activ il y a rôle user voilà après on a les outils des Noodo Solution Manager qui nous donne l'information du système denodo en lui-même au sens serveur le skeduler et ça c'est des informations qui sont accessibles potentiellement fréquemment par des protocoles différents savoir dans des Noos si un job planifié dans des no a fonctionné aini de suite c'est une API reste c'est pas une entrée dans une base de données donc pour aller chercher cette information là bah dans des nodo il y a une Data Source reste et et on on récupère très rapidement ainsi de suite plus des petites choses va dire homemade pour aller chercher des données JMX de type notification qui ne sont pas supporté par le JMX exporteur de la communauté voilà donc on a fait ça et puis une fois qu'on est dans le mononde des Noodo ben on commence à attaquer la partie structuration de données donc un premier modèle de données donc on va rester sur des choses assez classiques session utilisateur rôle privilège job voilà un petit data sourceur bit donc on va pinguer entre guillemets à toutes les toutes les X minutes toutes les datas sources et pas uniquement les databases mais n'importe lesquel et après dessus on va commen on va vraiment construire des des vues en l'occurrencef des données qui agrégé porteuse de signification donc nos indicateurs qui vont nous permettre de couvrir la roue complète et puis bah tout ça une fois que denodo a fait tous ces calculs là et Ben denodo et ben il va pousser ou mettre à disposition ces données là pour gfana et on va avoir en fait une connexion bidirectionnelle donc par exemple casfk Connect va prendre des données dans des nodo va les envoyer là en mode job le loky exporteur il va prendre des données de il va les porter dans dans ley et par ici des des nodo pardon grafana la partie dashboard va pouvoir se connecter au à des nodo ou à n'importe quelle source de donné donc soit au data classique soit avec un élément qui est qui est sympathique dans le mon graphana qui est le private data connector qui a un petit agent onprem qui vous qui permet à grafana cloud d'accéder à toutes vos données et comme on peut accéder par exemple à des Nono une fois on est dans le cadre de not Lab en au DBC et ben on a un chemin au DBC au lieu de haute si on préfère donc ça tout ça c'est c'est pas ça marche euh si voilà on on a fait le choix et a dernière petite chose dans le cas de notre Lab pour calculer nos indicateurs à un moment donné on a besoin de seuil de configuration enfin de de master data on va dire pour vraiment calculer indicateur le choix qu'on a fait et nos fameuses matrices de sécurité dont je parlais et ben on les stock dans sharepot online et denodo euh c'est facilement se connecter à un fichier Excel parce qu'il adore les fichiers Excel hein parce que c'est ta première source de données mais en l'occurrence des fichiers Excel directement euh sur SharePoint Online euh donc un on récupère la donnée et au passage on bénéficie bah du de l'édition web de fichier excel euh collaborative c'est plus simple que de copier un fichier deer un fichier à un endroit on édite le fichier Excel et bomm il devient disponible tout de suite voilà et là pour finir des captures d'écran rapide donc c'est pas tout à fait euh le modèle que je vous ai présenté qui avait qui au départ est on va juste un graph généré dans Excel pour présenter le concept ça c'est un écran dans grafana c'est un panel voilà on a utilisé je l'ai marqué dans ser mais j'ai pas précisé pour pour faire ce type de c'est on n'est pas en standard des des des panels grafana c'est le plugin Eart en fait toute la bibliothèque àach earts il existe un plugin qui vous donne accès et donc on a des compléments pour arriver à des présentation ben sexy et ça voilà c'est-à-dire que là pour pour l'exemple on a dit bah mon mon job qui fait un refresh cash et ben il s'est mal passé mon ch voilà et ben ça me permet de me dire où c'est un impact donc ça c'est la vue synthétique et puis après évidemment derrière vous pouvez imaginer qu'il y a toutes les toutes les vues détaillées les tops de requêtes les tops de ceci enfin ça c'est pas un souci et est-ce que on peut faire donc dans les pareil pour faire du linage on peut avoir ce type on a également le côté network si on VEF après c'est on va retomber sur les problématiques B c'est comment je faire faire du joli du compréhensible pour une population mais le choix et c'est là on va dire le retour d'expérience quand je vous dis si vous arrivez enfin c'est ce que nous on avait fait sur un programme de transformation finance c'est que c'était ça le entre guillemets la homepage le dashboard principal et donc les gens tous les matins ils allaient voir alors le problème c'est qu'après grapana ça devient un outil de production c'est que quand il est pas disponible il fait mais attends voilà et c'est là où parfois un mode un peu pilot sandbox quand on l'ouvre à des business par que VO les communauté larg ça devient un outil à proprement parler sur lequel bah il faut qu'on s'engage en terme de disponité et de qualité voilà après d'autres là un peu gadget dans des Noodo vous avez des traces de requette puisque une vue d'une vue d'une vue d'UE jusqu'un Data Source et ben euh on sait entre guillemets amusé à convertir ça en tempo en pardon en Open télémétrie voilà le format c'est open télémétrie mais envoyé à tempo et là c'est la représentation d'une d'une requête des nodo dans le temps donc une vision gant là où dans le design studio c'est quand même moins sexy on va dire ça comme ça et là c'est pour mettre en évidence la la sécurité je vous ai dit voilà quand on comp la sécurité évidemment on va on va on va sortir un Boulen bon ou pas bon voilà mais pour vous montrer c'est de dire voilà on a un fichier excel qui on va dire de type pour un type de privilège on a un fichier Excel de type matrice et dans grafana alors là c'est vraiment pour montrer côté visuel on est capable de présenter cette même Matri la vraie on a un indicateur qui va vous dire c'est bon c'est pas bon et on peut cliquer mais derrière on va voir quelle est la matrice on s'est pas amusé dans l'exemple ici à faire des changements de couleur mais on pourrait dire bah cette cellule là en fait c'est celle-ci qui est en déviation et on pourrait lui mettre un code couleur en disant voilà c'est le niveau de privilège pour tel groupe pour tel vue qui qui est en décalage avec l'aspec des petites visualisation sympa conclusion voilà donc pour nous on est on est convaincu pour vécu c'est que garder le contrôle de ces données et gagner la confiance des utilisateurs c'est clé pour réussir un projet de transformation data et pas répéter ce qui s'est produit depuis des dire nombre d'années sur toutes ces systèm autour de la data la confiance comme j'ai dit et en couplant ganaodo en fait c'est très simple très rapide alors évidemment si vous n'avez pas des nodo on peut dire pourquoi mais si vous l'avez déjà c'est très rapide d'aller dans ce domaine-là le simple par contre le voilà le le le verrou psychologique c'est de dire je peux virtualiser de la donnée d'exploitation je suis pas obligé de virtualiser que de la données finan que de la donnée euh je peux également virtualiser de la donnée d'exploitation parce que j'en ai besoin pour sécuriser en l'occurrence euh la qualité des données et dernière chose avec euh Vincent UGB lorsqu'on a un peu préparé pas peut-être pas assez préparé on s'était dit mais en fait il y a d'autres sujets mais on n pas le temps d'aborder mais on avait imaginé on a déjà pensé à d'autres use cas donc aller beaucoup plus loin dans le dans le phops euh parce que des nodo propose des données financières sur des nodo mais on va chercher les données typiquement des acteurs cloud on merge on coin et on présente quelque chose de de beaucoup plus fin et surtout de beaucou temps réel tu as parlé de l'enrichment c'est sur du Kafka un Kafka des nodo prend le un l'enrichit et et passe la main à grafana tu as également présenté parce qu'il y a une question donc toutes ces données d'explloitation elles sont voilà Gén disponibles mais ces systèmes là n'ont pas forcément une gestion de sécurité fine et donc des noo en mettant des noo entre guillemets en surcouche de sécurité sur ces donnéesexloitation bah vous pouvez se dire bah je peux commencer à les partager avec une audience plus large puis que je maîtrise la sécurité là où généralement d'autre c'est un compte technique qui a accès au log et ben les données de log par exemple des logs de type JSON et ben je vais pouvoir anonymiser masquer des données euh en fonction bah du des des rôles de la personne euh c'est c'est assez intéressant et là on va alors là on est dans le dans la bonne euh non non mais plus sérieusement et je m'arrête là en fait qu'est-ce qu'on voit dans les grands groupes je on n pas fait de sondage mais les grands groupes pour l'avoir vécu les gens du Réau ont leur outil Cisco qui monitor parce qu'il répond exactement à leur besoins on a les gens qui font du Microsoft qui on leur SCOM et ça répond exactement à leur besoins il y a l'équipe opération qui a fait un investissement sur splunk mais contenu du volume leur splunk n'est utilisé que pour le le système et donc en fait dans les grs groupes dans lequel je suis intervenu ou voilà c'est qu'on va avoir comme dans tout beaucoup de de technologie d'observabilité on a de l'élastique ainsi de suite et et ces éditeurs là vous disent bah d'abord vous en choisisez un vous faites un programme de transformation vous mettez tout dans un seul outil d'observabilité et après vous serez les rois du monde sauf que votre programme de transformation comme un programme de Data plateforme il va durer 2 ans 3 ans voilà et à la fin vous aurez un et potentiellement vos besoins onont éuer et puis vos budgets potentiellement ils auront fondu aussi comme des au soleil et ben une idée mais je c'est de dire mais en fait quand on a dit que embrasser toutes les données dans une seule plateforme en fait on y arrive pas il y a des gens qui ont dit mais ça on va sortir le principe du datamh c'estàd qu'on va départementaliser enfin par domaine plus exaant pour utiliser la terme on va faire le datam c'estd qu'on va redescendre d'un cran pour que les gens soient suffisamment autonomes pour gérer le les données dans leur domaine on va donner un peu de liberté parce que dans à l'échelle d'un département a ils vont y arriver là où à l'échelle globale la seule gouvernance va bloquer l'exécution des projets et ben sur l'observabilité moi j'ai le sentiment que tôt ou tard pas nous parce qu'on est pas un acteur le garde on discute pas avec les gardeurs mais tôt tout tard quelqu'un va dire mais en fait vous avez investi des millions de dans plunk on va pas vous demander dans plunk mais les données qui sont pas dans on on va avoir besoin de les agréger pour les calculer et ben pour les agréger il nous faut quelque chose auudessus multisource et pour aller encore plus loin qui transforme et processe ces données là pour vraiment présenter des indicateurs synthétiques présentabl à une large pour une population ha à un momenté il faut être pragmatique un grafana STAC répondra les it seront contents mais dès qu'on va vouloir aller sur des des utilisateurs du business et leur montrer et leur prouver que l'on garde que le système est sous contrôle et ben c'est là où le processing de données la préparation de données et l'esthétisme des rapports devient essentiel et je suis désolé j'ai fait trop long [Applaudissements]

