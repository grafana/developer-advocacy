# Grafana Mimir Community Call 2024-02-29

Grafana Mimir Community Call from 29th of February 2024. We have discussed experimental support for autoscaling in Helm ...

Published on 2024-03-24T09:34:46Z

URL: https://www.youtube.com/watch?v=6cBDbxDAJ7w

Transcript: this is February 29th 2024 this is the refund Community go um welcome everyone and to everyone who's watching afterwards uh we have a fairly short agenda um we'll go through it um that be itaps time for questions um I want to kick off with announcing the auto scalings in the H chart so Yan beat kind on GitHub um was kind enough to contribute like push this uh push this work over the edge and get it to mer in domain so now Distributors rulers qu front and quers can be Auto scaled um there's still some rough edges around how the migration Works um and and I think it doesn't quite support deployments with requested CPU of less than one core um but we'll be working through these and tracking all the progress in in an issue um yeah so this was released in the weekly 278 ham chart release um and will'll be part of the 5.3 with these not quite sure when it will be stable but um it will be at some point um CRA do you want to take the histograms Point yeah uh so about Native histograms uh which is kind of a new way to represent histograms uh in the more efficient Series so uh we've been uh busy working on some optimizations uh and by V not I don't just mean mimir developers but like community members and other companies um and so some optimizations have landed regarding the promit prq engine um that were done by the community and also uh specifically for mimir so shout out to Yuri one of our team members um there's probably more to come because I do see some I mean we see some opportunities to for further optimizations but the code is so complicated that every time we try to change something it literally takes weeks to to iron it out and and get it merged if not longer um the other thing that is worth mentioning that in promit there's a new function related to Native histograms which is the histogram average uh which simply takes and divides the sum by the count which you know sounds very trivial but if you do it with regular prom then you have to calculate those two separately basically and it's not very efficient so it's a nice shortcut and makes everything a bit more consistent because we were just missing this function um and then the last thing is that uh we started to work on adding more like turning more of the mimir emitted histograms into native histograms um this has a long long time been coming but um now now it is in in progress uh so I hope by the next uh release we can have all of the at least latency related histograms admitted as native sys as well and we can start uh updating our dashboards alls recording rules to take advantage uh so you can actually start the migration uh yeah and I haven't wrote it in the agenda but obviously we need to document this so uh it's not super exciting but a lot of work at least cool thanks um Sean do you want to talk about the strap series um not sure we can not sure I have much to add um but yeah um Sean brought up rigging down the strap size um in the the promethus TB had so all the series [Music] are divided into I think by default 16,000 Stripes um so that access to them is so we get less lock contention um I think it's a like the value of 16,000 is something we inherited from Prometheus and there it was optimized for a lot more um a lot more concurrent rights a lot more series but in Meir there's one TB head per tenant so when you get a lot of small tenants um having very large strip size has has like diminishing returns so we end up paying for memory which doesn't help you in any way um yeah I checked in one of our clusters and the stripes uh the strip series were about 9% of overall Heap so if we just drop that um we might be able to get some mem back sorry no go on so is the suggestion to use a different number or to make it configurable or what's the idea it's already configurable so I think I think that's pretty solid okay I I do Wonder I've been trying to dig through the old Prometheus commits to see where the the 16,000 default like was chosen and if there was like a reason for it um but I cannot find uh I found the commit it was added but I can't find a PR uh for it that would be like indicate was back in 2017 it's just been like that forever so um but yeah in my experience that's like a wildly High stripe count for um incurrent access Maps so I that's what just I was wondering if there was a reason for that that massive number but I can not find it uh but I think dropping it down to I mean in our case we're running many thousands of tenants so it's using um you know four or five gigabytes per per ingestor um so we could probably drop it down to like 128 or something like that and I doubt we'd see any additional contention um we'll probably have to do a few rollouts to to to verify that we don't see any performance like deg degradation there but uh I expect um I mean and then you'll get diminishing returns right going from 16,000 to to 1024 is a huge benefit but then you know going down to 512 won't won't change as much and you you only see a benefit if you're running a lot of tenants in a cluster if you're running you know five five to 10 tenants or something I don't know what the average mayir cluster tenant size is but uh most people probably don't get to the point where this is is a significant train yeah a lots of people choose to have all their metrics in a single tenant as well um so they can run at a fairly High scale but still have everything under the same tenant um but even then you're saying that like something like 16,000 M that's still not make sense yeah because I think in my experience what you end up seeing is the stripes are to reduce contention right so if you have you know 16 threads or 16 routines or whatever go routines writing at a time or accessing at a time then the chances of them colliding with 16,000 is like negligible right it's there's almost no chance but the chances of them colliding with a thousand is still pretty unlikely so you don't you don't get that much benefit you get serious diminishing returns for for larger values of a strike map y do you have that commit now that you've pied it out uh yeah because I'm curious as well I remember I think the theb stuff in preus used to be in a different repository at some point yeah it was in um it's now in Prometheus junkyard tsdv yeah but uh I think we were just just looking at the issues at least in the Box scrub promus box scrub and and they were moved so I think those should should have been moved to promus issues and PRS oh so you're saying if I just switch this over oh right thanks oh wait no this is just one massive uh oh cool um do we have any other discussion points anything questions anything to announce sounds like I know um we get a recording and then I upload it um all right um I was going to ask if uh if youon can post results or like um after you experiment with strap size um I think it's something we've sort of neglected for a while but it could be a nice change of defaults perhaps in prus as well yeah I am um we also tuned we just did yesterday we dropped the uh what was the parameter uh the block storage head chunks right buffer size bytes from four megabytes to one megabyte um and that saved us quite a bit there as well um I mean we're running on nvme drives so the the memory buffers it's GNA but depend on what you're writing to right but uh for us that was a really big benefit we saw absolutely no change in performance um but we we dropped about 30% of our Heap it's pretty solid that's up how much um do you know how many tenants you run on a single investor um yeah so on average I think it was active tenant was about 1,500 on a single per adjuster oh that's quite a lot yeah we have a well so this is our stage cluster so that we're running fewer we're running the same number of tenants but with like each one is smaller than than in our production cluster so um I I don't think these setting changes will help production quite as much because uh it'll it should spread out across a larger number of inors so each ingestor might only have a few hundred active penants but yeah Co it's um yeah thanks for sharing folks thanks for joining everyone um see you see you next month how you doing bye bye bye

