# Grafana Tempo Community Call 2025-05-10

Join our next Tempo Community Call: ...

Published on 2024-05-10T11:33:13Z

URL: https://www.youtube.com/watch?v=wejNoDf21j0

Transcript: all right welcome to the Temple Community call May May 2024 uh we have some big things to discuss today we have a pretty packed agenda um I'll post that again so it'll show up in the recording uh throughout the call if you come up with questions you are welcome to unmute and ask none of this is a caned presentation you're welcome to make this conversation you're also welcome to post in chat or whatever way makes sense uh all community members are invited to add agenda agenda item so don't feel like uh you can't discuss something just because it's not on there uh questions are encouraged we're just here to talk about Tempo mainly in the doc we kind of have the few things we want to focus on today but like I said uh this is open for all to to bring up their on topic so we're talk a bit about park4 which was just merged and there's our latest iteration of Park A4 and what's going on there Tempo 25 which should come out this month we're targeting end of month uh some architectural changes Mario is gonna review those is that right Mario uh yeah I think he was nodding before okay thanks Mario so some the changes I think a lot of people here to see and I did kind of at the channel twice because I really want people's input and I want some visibility on this in the community we have some changes coming up that we are strongly considering Mario's going to go over why we're considering those um the reason behind it the advantages is we see the disadvantages like the concerns and we welcome everyone to express or ask questions or get involved and let us know what they think about some of that and the final one is the future of serverless and SJ is welcome to review that if you would like I'm not sure I don't want to commit him to it um but he is kind of reviewing it on our side SJ are you up for that or do you want me to chat through that one yeah I can take it awesome okay so SJ will discuss the the serverless questions we have um in the future okay so let me get some names down does anybody mind if you don't mind putting your own name in or if somebody from our side doesn't mind just like kind of reading the list and slamming some names down I would really appreciate it all right so let's start at the top we kind of went through the items there b parket 4 so we're on our fourth iteration of Park um three was all about dedicated columns and we saw very nice performance improvements with those columns um okay four is uh an addition or an update for features so three was mainly a performance Improvement four is mainly a feature Improvement uh we have a number of new columns uh in the paret uh format so there is an increase in footer size the increase in fter size translates to a slight regression and we're kind of quantifying that on our side right now we're running this in Dev we intend to roll this out to our larger clusters over the next week or so um and then to our production clusters probably in the next month I don't we don't really have a timeline but that would be my guess assuming no no concern that changes so we do expect a small regression and we might work on that in park a 5 uh the main reason like I said is for some nice features so Park 4 added some columns some of those are contributed by Andreas uh of red hat thank you to Red Hat they've been a great partner with Tempo added a lot of nice features uh and these columns were mainly to add uh Trace level Trace level statistics uh such as the the list of services uh Services present in the trace as well as errors and spans per service um and so we're going to use this and I think Red Hat intends to as well to enrich the UI so you're doing a trace Q query right now you get about like a bunch of Trace IDs and you have some basic information like the root service so with this information we can decorate the results a little bit more and we can add here the five or 10 or whatever services that participated in the trace in the results as well as number of spans or errors in each um uh in each service so this is a really cool Edition from Andreas he got involved while Park K4 is being worked on and got those nice features in and we will hope to get those in the graphon UI soon enough and they'll be in the uh they are they will automatically be in the results the search results if you're using Park form and then the other columns that were added were because we are need to extend the trq language and we need this format to do that but we want to add support for events links and arrays these are elements of uh the otel spec that we did not support before so we are excited to um we're excited to add some sport to that and I'm gonna real actually can somebody else do this can somebody uh find the uh we have a trace Cel extensions read me a doc in the repo I should have linked it but we have a number of exensions planned uh that we are going to add I guess also kind of in the next three to six months next couple releases of tempo expect support these for these uh features and we'll hope to get some maybe in two5 I don't know we'll try to get some in two5 if not definitely the next release should have some nice support here but I think people have been waiting on those a lot I would like them as well more and more of the otel clients or sdks are using arrays especially for things like HTP headers uh and we want to be able to support that in tql uh thank you for whoever pasted the extensions link if you want to take a look at some of our uh some of our proposed syntaxes or comment uh feel free okay so par4 is exciting new feature it will be in Tempo 2.5 it will not be default in 25 it will be but it will be configurable and I'll add details in the blog post when we post it why it's not default and what you would do if you want some of these features immediately okay 25 coming up hopefully at the end of the month month it would be at our normal Cadence our normal Cadence is about for a year about once a quarter uh and the end of this month would put us on exactly that three roughly three months after the previous release um I've linked a change log list there so kind of the raw change log from the whatever it's just like not in a release so you're welcome to kind of glance through that and I've highlighted a few that I thought were worth discussing in this meeting uh of course once 25 comes out there'll be a blog post that you know reviews all the major changes release notes they'll be the normal set of docs around that um but B Park the original is going to be removed support for will be removed uh in 2.5 and it we haven't it hasn't been default since like 21 or 22 so it will make the upgrade path require some of those intermediate versions uh but to simplify the code base and speed up our tests and a lot for a lot of reasons we're cutting out the old paret and there'll be details in the blog post help people know okay now there's now you can't SK from like 21 to 25 kind of you have to go through all the minor versions um uh we are adding support for additional metrics Trace scale metrics will remain kind of this beta feature we're contining to work on that uh Mario will touch a bit on why that's still beta and what we're looking to do to make it GA when he's going to talk about some of these architectural changes because they're related to GA and Trace ql metrics but you will see two new functions so in two4 we supported just rate of spans uh we will have support for quantiles these are actually both already merged in domain quantiles or histograms on float values so duration would be the obvious one but we were having some fun doing quantiles of HTTP status code which is meaningless but kind of funny to look at but you can do it in Trace ql quantal is a htb status code histograms on htb status code if that's what you're interested them um so a new tql metrics features again we'll go over it in a bit about why that's not GA yet and what we're looking to do hopefully in the next release to make that GA and then finally Zach caught a long running issue that we've always known as something we've wanted to improve and never had a chance but Tempo is no longer running as root um in the docker image so this is a nice security uh a nice security Improvement no long using that root user ID or group ID um I do think this will require some changes Zack are you want to talk about that at all kind of put you on the spot here feel free all good um yeah so the approach we've taken with the Jon and we will do this in Helm also is to have an init container uh that runs his route just on Startup and then this will Chone all of the existing data um because we're going from root to non-root and so we need to make sure the data is writable and readable by the new Tempo user um so there will be changes is if you fall outside of the helm or the jonet then yeah feel free to ask questions jump in the slack uh come chat with us we can help you work through some of that um but chances are that you will require uh some manual intervention there uh that will be another great thing Zack let's sync when we get this blog post together I want to make sure we have some good steps in there and make sure people have the visibility on that change because that will definitely catch some people yep for sure thank you cool all right um so par 4 upcoming it'll be in 2.5 2.4 five hopefully by end of month that's the goal right now and I don't necessarily see anything that stands in the way of that uh but you know how software goes some days you think everything's running smooth and the next day it's not so much um so hopefully the last week of this month uh and let's move on to I think what a lot of people are here for and Mario's going to walk us through some of these architectural changes were I thinking about why we're thinking about them uh and I will hand it off to him um yeah thank you Joe okay I prepare a couple of slides so hopefully it'll be easier to follow um is this involves um details on how uh sample works and some um like in depth details uh I I'll try to do a quick version but if anyone has any questions um you don't have to wait until the end of the call I just type them or unmute and and ask um all right so first um I wanted to be uh give a bit of uh some context on what we're thinking about this architectural changes uh and all of this sort of boils down basically to uh the new uh metrics capabilities that we're adding to tql and some of the challenges that we've been facing to make um this T metrics perform to the to the level we we basically we want and we were uh hoping um so uh well first of all as most of you will probably know especially if you're running a distributed uh deployment of tempo uh for durability and high availability uh it recommended that uh temper run with a replication factor of three and that's the also the way we run it internally and uh well that this brings um some challenges some that are specific to trical metrics some that are not and the first one it's the the most obvious and that is that there is redundant data basically so temper runs compaction but compaction takes a while to to dedup all the data and even when when it gets to um like high levels of uh compaction so it has done basically multiple rounds of compaction uh it's very difficult that it gets to complete the duplication so uh you have essentially only one copy um so basically what this means is that is um basically in every read this is not specific to trisal metric just in any read uh we're read more data than necessary and opening files and reading from object storage uh it's expensive in especially in CPU time and in memory and it can add up to um a lot of time uh in in queries especially when we're looking to um make them very fast um and the other challenge the other main challenge is that um duplic data impacts the way that we can do charting at the efficiency of the queries and this is more specific to tral metrics uh so so for tql and idlabs uh you can usually D at the highest level because when you merge all the jobs but for metrics that not the case uh because you want the metric to be D duplicated so if you're pting a rate you want the actual value without all the D duplication the D duplication has to happen at the job level um so what this means is every job will have to inspect all the spans of a given Trace uh maybe this is get gets too much into detail but um very some up um every job is opening every file this is not very efficient uh and adding that to the fact that we have redundant data replication factor three is a big challenge for trco metrics um so we tried to overcome this with a number of different approaches most importantly sharded compaction this was an attempt to Shard compaction so um instead of so what we wanted was to create charts of ranges in in the blocks so um all the all the bces from this like from this ID to this ID from this range will be in a certain block and the next block would cover another range and so on to sort of try to mimic the way we do charting inquiries uh but in the end this um tuning tuning compaction with uh this new technique would Prov to be very difficult and results were uh underwhelming uh for what we were looking for the b a PR request um that is I think Mark as draft if you're eous you can go um loc at it up all right so after this um mess of me rumbling about technical challenges that we didn't overcome comes the architectural changes um okay so now you have the context replication factor three is um we have to run with replication Factor 3 for durability and ha but is a huge challenge for tri schol metrics uh so what we thought was well if we cannot make rf3 work uh or we cannot make TR schol metric work on top of rf3 how could we make rf1 so the current architecture for Tempo is this one uh we have the ders writing to the Injustice and the queriers uh reading from the back end so from S3 or GCS and also the Injustice so we have this kind of so the the injustes in the end have this kind of Duality which and which they belong to the right path uh and to the read path um and essentially there is no way we can run the Injustice with a replication factor of one and continue having the requirements that we were mentioning before uh so what we thought was uh what about if we introduced the durability VI another way instead of having uh the rights duplicated to the injustes and in this case the durability would be introduced by a durable que um possibly kava maybe yes maybe not um um that's why the initial Topic in the agenda was called just Kafka question um so essentially with the new with this new architecture the right path would be limited for uh to the Distributors and this new queue um and then the read path uh the the would be moved basically to the read path they would consume all the data from from the queue uh and then flash blocks because we have durability coming from from the queue we can create rf1 blocks essentially um so yeah this is the high level um architecture I don't know how much more to talk I could talk about this for probably longer than an hour I don't want to say myself more than I've already done um I don't know if anyone has any questions or wants to comment on anything um but yeah basically we wanted to discuss this change uh that we're thinking especially because it introduces a new dependency uh and we know people will have uh very very strong opinions or not so strong about having running CFA or any other alternative and in general introducing uh a new dependency uh to Tempo so we believe this is uh this would be a massive Improvement uh because it will lower TCO it would make uh the right path and the re path uh more reliable it would ease uh and lower a lot of the responsibility that the injustes have now which are this very beefy component that we have right now they're very heavy um and they're very difficult to do um and it would definitely make uh trade school metrics uh work much much better and um to we would allow us to get to the point that we want uh to get to um so yeah we want to hear your thoughts on yeah introducing a new dependency and making these architectural changes um yeah that's I hope this was like understandable think you did good Mario uh uh right I think the main point right we're looking for a q to push our durability and availability concerns onto to get down to rf1 trace ql search will be significantly faster tcco will go down and metrics will go from not really viable at a couple hundred megabytes a second to viable at a couple hundred megabytes per second which is what we're looking for um so right any questions or concerns please raise them uh another element of this that I I really like I mean there's obvious things to be concerned about but I'll point out something I think is a strong Pro here which is right now it's only possible to run Tempo essentially an rf3 everyone is forced to do a reputation level three which of course incurs an amount of cost extra costs on the system if you have this architecture you can configure Kafka uh in an rf1 style mode or an rf3 style mode and you will have a like a slider between availability durability and cost that you can't control now so you could run if you run rf1 kofka with the new Temple architecture we're considering I would cons I would think that TCO would be vastly lower um if you run with rf3 Kafka our current expectation it will be slightly lower um so I like that there's this this new tunable uh in the tempo architecture that gives people the choice to run a much cheaper version of tempo despite uh I do know there'll be concerns I do hope that most people will be okay with this cop is a pretty well-known technology I think a lot of people have access to it but it is you know every dependency is complexity and we recognize that all right uh one question if you don't mind me asking uh what what is the impact on ingestor you have seen with this change specifically around the right performance I know we are changing the replication Factor so that definitely will impact how much rights the ingestor are going to be doing second thing uh uh any benefits we have seen in terms of search performance because of this architecture change if if any or if if this has been tested in De I just I'm just asking sure uh I can address that Mario do you want to take that I can I can um yeah I'm I'm not sure understand do you mean with this architecture already um yeah so uh have we deployed this architecture anywhere and and kind of did did a first round of a POC to see what kind of impact it has on the inors in the long run right well we haven't deployed this uh anywhere still uh we wanted to get feedback uh very early uh before um like shipping this anywhere okay um but what we expect would be um so the Injustice would become way way smaller because they would have uh less responsibilities um they probably use surely less CPU less memory because they will need to have data um like in memory and in disk uh for way longer essentially all the responsibilities that Injustice have right now uh are mostly delegated to the to the queue and they're um the Injustice are moved into this um more not completely stateless but less uh massive uh component to a like a more lightweight one and um in terms of performance improvements um is tough to say uh we think the the the performance Improvement will be uh massive for uh Trace K metrics for ID lookups probably are less noticeable is still although it still should be um much higher just because the the challenges as outlined at the beginning uh impact differently that the different uh queries but yeah I don't know I'm I'm not sure number I I totally understand I was just curious I was just curious if if this has been deployed or if you did a small PC or something like that yeah well it should be like a significant uh change I would say as we experiment with this we will absolutely share numbers with the community as we start seeing them on our side so as soon as we have numbers you will have numbers as well um I'll also point out the compactors will be significantly cheaper to run so the the current largest the current most expensive thing that compactors do is take a paret trace turn it into a Proto Trace do a combine Logic on them because when we see two with the same Trace ID we combine and then um rewrite that as one big Trace in the output block and so that should become significantly less important with rf1 it'll still happen some because of traces that were broken across blocks but it'll be significantly less common and so I think compactor costs might go down by a quarter a half we're making guesses here and we will absolutely share uh the numbers when they come through uh we do we do I wish Marty were here because I know he's run some experiments on rf1 metrics and seen that we know for sure is like three times faster or or even more because the ddop was so costly so that's the primary reason we're going this way but we do expect across the board to see improvements TCR reduction and search Improv okay thank you sure thanks for the question that was good um if there's no more we'll move forward a bit with sjes I do encourage anyone to ask other questions so maybe think about it some while SJ is going through serverless and ask questions about the serverless stuff he's about to discuss and then questions over the whole agenda everything we've discussed is also fair game so once Sage is done we'll open it up and say please type in the chat or please unmute whatever you're comfortable with and we'll answer any questions people have or concerns uh so maybe maybe kind of think on that while SJ has gone through his piece and be ready to Chuck something in if he got something s what you up to yeah so uh I guess I'll introduce seress first because I'm assuming not everybody knows what it is uh so serverless is a component of Temple uh that allows you to spill over your access queries uh so querer will spill over those query and it'll spin up uh Cloud functions or you know Lambda or Cloud run functions and execute query and then return the results back uh we added it uh super early uh when we added support for search and we added it in Google Cloud run and ews uh Lambda we never gotten around adding it to Azure uh correct me if I'm wrong so uh yes uh so we don't support an uh it was helpful in the early days of search but uh as Tempo matured we saw that we could tune uh our search performance and get better value by keeping the search within queriers and not letting it spill over uh so right now most of our clusters do very little spill over where they spin up these uh functions uh so we are thinking or asking Community if anybody is using them these serverless functions at scale getting values from it like are you significantly getting value from these functions because we've seen significant challenges uh so the major one is cold start uh latencies from these spillovers will mostly be in seconds instead of it being milliseconds and that's quite High uh in my opinion another one is uh it's hard to observe these functions because they are very shortlived uh the only metrics that we have are the metrics that queriers are providing that hey I spawned a function we don't get logs from these functions we don't get metrix sure you can do it but then it'll be lot more work and uh painful to do another one is every cloud provider has their own way of deploying these functions and that's why we only support the like two Cloud providers uh as of today and I'm assuming people who run their own data centers will have no way of running these functions uh and also AWS has deprecated go 1.x runtime which we are using uh so that all these things have made us think like is is it worth keeping serverless uh can we remove this you know or I guess another question that I want to ask is will you be sad if we remove Tempo seress you know uh you know another thing is we did uh as I said we did a lot of tuning work internally and we wrote those down in our public talks so uh those findings and those recommendations are available to everyone uh and my thinking process is that you can use those recommendation in tune your queriers and maybe spawn you know like at 10 20% extra queriers and get better predictable performance uh than doing serverless uh so yeah cool that's serverless we haven't made a decision uh so we came to community uh ask the important question will will be sad what emoji best describes your face if we say no serverless uh so s right so the kind of thing that sparked this conversation SJ went over all the good points but AWS drops support for the framework we're using and we're trying to find resources to you know upgrade to the newest framework and then we start asking questions that even worth it so that's why we're here asking you all um and SJ has a project this quarter to review that so any input from the community would be helpful if no one has input then we're just going to do what makes sense for us maybe that's continue to support it if we feel like we're getting the value or if it's simp the code basee and it's just the value is not there we'll just drop support so uh if you do have strong opinions please let us know in slack or perhaps through a GitHub issue or discussion in fact it might be smart for us to start a GitHub issue Sage called investigate serus or consider dropping serulus call to the community for like request the community for comment and that way they can jump in that issue and and drop some uh comments if if someone does really want us to keep the surus support cool all right here we are uh roughly at the any any other topics team I think we roughly covered our our thoughts today um if you have questions feel free to yeah ask in chat or unmute uh otherwise we might just ramble on about coffee and whether it should be cold or hot and um whether tea is any good because I think it's terrible I'm with who's the what was that the soccer coach from the really super popular show that just came out he hat t uh nobody knows this thank youed okay yeah Ted lasso he was always down about te um so yeah Ted lasso exists all right crew uh if there are no questions we appreciate oh here we go what's up one question sorry uh Factor PR open for compact to I know that's in currently in draft mode you guys have been talking about it and reviewing it so my question is uh uh are we still planning changes in compactor with the new uh architecture change we are going to be bringing because again the work for compactor would be significantly reduced as we talked about it uh so uh what is the end goal and what is the thought process on that moving forward right um yeah that's a great question so uh yes um definitely so we did all this work uh mainly thinking about Trace skill metrics and when we didn't get the um the results that we were hoping for we sort of froze it and move it to draft that doesn't mean we want to just uh drop all the work that we did uh and we intend to com in back to it uh once we um like wrap up what we're currently working on for TR SK metrics uh just I think the it's is still very useful uh as you say especially I think for ID lookups when you're quering uh a trade for ID should have a very nice Improvement um um but yeah um we yeah we intend to come back to it uh but just we haven't priz it uh we yeah we decided to to froze it for the time being thank you yeah if anyone has interest like it's we encourage to to take a look at it and yeah or uh I also have a sneaking suspicion unconfirmed that more smaller blocks are going to be better for search as well as metrics than fewer Big Blocks so forever ago tempo's goal was to make giant few as Big Blocks as possible to make Trace by search very quick um as it's matured and the codebase is matured we don't need that anymore and the sharding does a good job of reducing the number of blocks have to be looked at for Trace by search which would give us the ability to have far more blocks and stable Trace by search and the blocks will be smaller and we talked about this a bit ago but Park A4 increases the footer size the footer size is a fixed cost for all queries and if we can reduce that size I think it's going to uh end up being a benefit for trace geometrics and search it's kind of a it's kind of a guess and I do would I do want to expend some resources on the next quarter or two next couple months on that PR see if we can't get smaller blocks see if we are seeing an impact like I expect uh on search so that that's something of a guess but I we definitely not abandoned it it's on the back burner for the moment for us to to get metrics done and uh hopefully this Q architecture done but it's it's definitely still in the mix got it [Music] thank cool all right crew uh thank you everyone for showing up especially Anonymous Tempo user my favorite Tempo user appreciate you showing coming in and chat with us everyone take care uh hopefully you're not too upset or shocked about some of these changes hopefully you can see some of the benefits we see or once we get some numbers out there you will maybe be uh persuaded um regardless uh engage us um let us know what you think ask in slack I personally struggle to keep track of slack some of the other team members do GitHub discussion I always try to chime in on I GitHub issues I watch closer but all of these methods of engaging us are encouraged uh so let us know what you think um post some questions we will do our best to get back to you and keep this conversation alive so we're all aware of the changes and are kind of on board with it but thank you all and uh hopefully we'll see you next month we'll see you when we see you thank you take care bye everybody thanks

