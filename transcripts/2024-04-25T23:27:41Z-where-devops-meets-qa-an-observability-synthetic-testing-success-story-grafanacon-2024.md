# Where DevOps Meets QA: An Observability &amp; Synthetic Testing Success Story | GrafanaCON 2024

Goal: to monitor transactions and proactively test uptime. At GWI, the DevOps team was passionate about their Grafana ...

Published on 2024-04-25T23:27:41Z

URL: https://www.youtube.com/watch?v=LdBeT9Dh_sw

Transcript: um hello everybody I hope you have um a good expression after lunch and be here with us we're are going to shake things up uh I'm yanis we are going to have a topic today um a different one about the Synergy we had between devops and qaams at gwi in order to introduce synthetic testing internally uh and build an observability culture for different things so yasas is in Greek I know there are a lot of people from Greece uh in the audience um I'm leading QA team on gwi is anybody QA person today oh we have one great we need to speak after so I'm B nothing I love everything that has to do with product and Engineering I really love testation and I'm also grafana Champion which is an honor uh for me kalispera my name is theonis I'm leading the devops team at gwi I'm obsessed with automations just love automating stuff and I enjoy collecting board games more collecting than playing uh lately with a young kid it's very exciting to be here thank you great um I think the best thing to to approach this talk is to identify the problem we were trying to fix internally so let's describe first the problem problem with some questions with the audience uh first I need to know if you have tests in place anybody in the audience but you don't get alerts proactively anybody not you do the audience please anybody oh we have one all right um if you have tests but it's mostly for your backend services not more end to end flows come on man right and if you have tests uh but you don't actually measure web performance okay we got it um so we started like last year building um uh monitoring metrics uh with devop Stein uh we wanted to embrace let's say observability with all these tools but we actually didn't uh cut uh the issues proactively from production our clients were annoyed and they are actually pinging us all the time so we started embracing um the tools from grafana stack and we was trying to figure out how the QA team and the DeVos team can work together to build a pet project that we're going to show today especially in the demo and alert actually our engineering teams proactively when something is happening on production in time before we have a look uh into code let's talk a bit about our stack no surprises in here I'm sure you must have heard about these keywords a lot over the last couple of days internally we're using a rhyme to describe these kind of things so we say that hey L is for locky the tracks are logs G is for dashboards T stands for Tempo the Tracy stuff and P for profiling when code is Dove and then we have good old promethus for all the metrics and stuff like that now not to over complicate things we also have alert manager in the background for our alerts and we also have Thanos for the Prometheus Federation so loads of things over there cool but uh let's step a little you know a step back let's take a step back and see why reliability is hard I think everybody is aware in the audience um you know complex architectur our system has around 100 micros Services right there are different layers um you have the client facing layer you have the um the actual service layer your apis which are the middleware so the you know we are not like 10 years before uh we had a monolith back then you know a few databases now everything is more complicated you have different programming languages uh for each of your layers you can have different protocols to communicate like grpc HTTP TCP so really it's hard to develop stuff uh for your product it's even hard to test stuff for your product and even harder to monitor right uh but someone would say like uh this is true but that's why we need to build observability but it's a little bit more complicated than that so let's talk a bit about our high level solution okay don't don't don't forget that you know as a devops person it's irritating you know to push for observability for monitoring so much but then you always getting notified first by the client when something goes wrong so our high level idea was to take those k6 scripts those k6 end to endend tests and then wrap them on a container and then wrap them on top of a Cron job and then have them in our cluster running very often every few minutes um so one of the original things that we wanted to do we wanted to automate things as much as possible when it comes um to kubernetes but those Chown jobs because they were running in inside our clusters it means that they had out of the box all the nice observability observability monitoring tools that also supported the rest of our applications which means Prometheus metrics alerts and Myriad of nice grafana dashboards this also means that if a script fails for some reason then we get notified for it right away okay because out of the box we have alerts that are connected with our on call tool of Genie In this case and let's say if a user cannot log in then because our entry point our first thing our first scenario we tried out was users cannot log in in our platform so they cannot essentially use our platform form so then the on call Engineers will get notified proactively for it right away that was our Pet Project it started a year ago and during Fridays great so let's get a little bit technical do the demonstration um um about the project um I will start one second not here yep so this is our P project uh the first thing about the qaim right the QA is a function that you know not every company is actually having QA nowadays but it's really critical to discuss how the quality Engineers are actually embracing uh the tools that we have from grafana especially k6 so the first thing was to discuss with our team internally how we approach the architecture of writing this type of test that we wanted to do uh with our devops team we use a k6 browser to simulate actions with um you know with a browser uh and the team already used playright if you know playright it's a web automation framework so it's pretty similar what k6 browser actually uh do but uh we used it for our endtoend functional testing um so it's a different scope here we want to measure web performance in see if the our application is actually functional so what the team discussed is to have abstraction as much as possible so um I will give you an example uh we have here um a login um Authentication uh test so you will see the abstraction that we show is we want to have page objects this is a design approach for functional testing each page of of our application needs to have the complexity of the case6 API we need to hide the API and we need to make sure we reuse uh things and we have some abstraction on top of our test so you see we import the login page over here um and uh the actual function of the test we start a browser from k6 you will see we don't have any k6 API calls anywhere we call the login page and we do stuff with our user if I go to the page which is actually under different folder here you will see the main function uh that hides the complexity you will see a bunch of stuff from the library uh the main idea this is the abstraction that we did with the QA team the main idea is that uh we wanted also to have stable results if we don't have if we have flacky tests or you know flacky execution everybody's annoyed um so what we did is we work together with our uh engineering teams to have stable unique locators so if you hear the term locator in web automation it's like we refer to elements in the Dom of our application uh so we can actually use and do some simulations Etc so um this is k6 behind we use uh you can use CS say sex path uh it doesn't matter but they need to be uh really stable um so this is the one part the other part is we wanted to feed data to our tests we didn't want static data so what we use is Secrets uh we use a tool called shops um if I can find the yaml here to show you yes this is a basic example so soaps is a tool to decrypt encrypt uh secrets uh you need to have access on your gcp um geing so uh be careful about that um when you deploy of course we need to have a Helm chart to decrypt the secrets so for that reason we have uh over here values yaml for the helm chart so you will see in this particular example in the helm chart we can overwrite the secrets these variables for our tests so we have different variation of the same tests and we overwrite for example the URL that we need to navigate to our application can to support multiple environments so I can run my stest on staging or production for example um you can have different schedules but this is um a typical example of the helm chart and how we actually override these tests be mindful these tests are running constantly like every uh 5 minutes the will talk about this uh we have a chrome job for that and the last part is what we actually you know monitor here what are these tests are doing so there are three two things that we want to validate functionality if it's broken be careful you don't want to test everything with k6 um you know we use playr to do functional testing k6 it's not like want to cover you know everything for our application we T basic cases and we want to make sure we functionality is not broken constantly uh and the we performance is not decreased because k6 can provide metrics for this and the last part is okay what about visualization I mean what we are trying to visualize we created the dashboard which is over sorry here okay this is this is coming from uh the community which is great uh we just use it to to extend a little bit some stuff and uh make a more developer friendly dashboard for our in teams um you will see the Google web vitals do you know Google web vitals what it is the core web vitals anyone raise your hand yeah a lot of people okay it's a it's a it's a way from Google to to monitor uh the page experience you know and cas6 it's really good because actually in the latest versions they can um export this and we export them on prometh server and then we can actually fetch them on our graph dashboard we use heat Maps um the the heat M it's a nice addition because you can see exactly the data point that it's failing for example and we can actually go with our engineering teams and um figure out what's uh what's the problem we have another widget because these tests are running every 5 minutes with the crown job which is the top uh left so we can see how many tests are failing how many times our tests are actually failing uh so that that will be the interesting part okay sorry all right let's have a look some devops things it's not nice it's cool right all right okay with production out of the way we have to solve several they like difficult things so the first one was how what about uh local development we wanted people not having to worry about Helm charts about difficult kind of things and the main question was how would engineers test their new scripts and we want to automate again as much as possible because wake up in the back end is doing loads of stuff with Secrets recoding decoding variables we wanted to make this completely seemingless for the engineers we wanted to be able to try this thing locally so initially we started um by wrapping all the kubernetes logic behind some make files and behind some bus Scripts and make it very easy for people to to run their tests first we started by creating Docker file for arm architecture we're using Mac OS and uh k6 had some weird things that we needed to F tune for local development other than that again everything it's under make files people could run simple commands and then you um build their template or like debug with sell or just run and those things under the hood they will encode things overwrite Secrets overwrite environment variables it's going to be very very seemingless for the engineers to test stuff so let's give it a try and then we can have here so let's say you are an engineer and you just want to run your very signning test about if a user can log in in your platform you just have to do make run and then the scripts will take care of everything and then it's going to take a few seconds for things to run hopefully it will not fail and then k6 will just export some um um okay let's see and and then k6 will will show some some metrics and those metrics will also be exported to to promethus as well okay fast internet connection here today guys stop Wi-Fi Ian nor normally takes a few seconds so even okay so it failed failing is fine it's fine it doesn't matter it always has to do with network don't forget that this brings a browser and then it actually tries our platform okay so uh it can fail for a few things in there but anyway this is a good case because it failed and then we can see why it failed we can see all the logs here and then um we can either rerun it or fixed or proceeding and this for for the engineers this is a very good approach they don't have to worry about anything that's going under the hood so with this thing in place the next thing to test out is what is actually happening when when Cron job fails and before we understand what is happening let's have a look on the Cron job object itself so if you are familiar with kubernetes it's nothing um too technical here the interesting thing to see is the back of limit of three so because those grown jobs are connected with alerts with onall teams more than one you don't want to have flakiness H or like Lads of false alarms when you um bring your atten to the engineers so with the the golden rule for us is if a job if a test fails three times then it definitely means that something is off okay so we try each job three times before we say that something is wrong and then we need to notify people so this one is interesting here other than that the rest of the object it's for someone who is familiar with Prometheus it's nothing major the only very interesting thing to see is the screenshots folder so in order to give as many information as possible about what is going wrong we take advantage of kix's ability to take screenshots if something is failing so Engineers can use it or for debugging purposes to see what's happening really under the hood some of those scenarios can be very very long okay so loads of things can go wrong in there now k6 the problem is that k6 it's not nodejs even though it looks code like node which means that we cannot just take a screenshot and then just use a gcp play and then upload it h to to gcp unless okay there are a few things that we can do with this we can compile k6 so it includes the library and maybe it will work but in our case you know where devops people want automate stuff we have other Alternatives so we save the screenshots locally and then we on a persistant volume on a PV in kubernetes and then we just connect this PV with a with a GCS bucket so here we can have a look in storage class Helm chart and then we can see the the provisioner that is responsible for it okay there are multiple provisioners so in this case again we just store a screenshot with a specific name um and then we upload it to to GCS okay and this is how our current job is looking like then the last part before um we move on is the alert manager okay what's happening with the alerts what what's going on in there so in our clusters we have alert manager promethus alert manager that's just sitting around and then looking for specific crds this is an alert manager configuration it looks fancy it's doing something very simple it just says that all the alerts on this specific name space will just go on this specific receiver here and then we want our Engineers to have complete ownership of their applications so everything is under Helm charge no devops have been H to write this kind of configuration okay Engineers they are solely responsible for this now in production here we would probably have also OBS gen configuration but we omit it uh so you know it's going to be uh smaller H yeah so in this case it just points stuff to a slack Channel nothing more fancy than that so this is our alert and then for the prometheo stuffff it's h it's similar we have a Prometheus rule crd Prometheus is just wa waiting for specific configuration as well to be able to use it and we just leave the expression to cube metrics so all the difficult part to figuring out if a Cron job is failing or not we just leave this this thing to cube to cube metric it would be interesting to see graan 11 you know if we just feed it this specific Prometheus rule how um what's going to come up with so maybe we can come back to this but yeah it just says if something is failing just there is an alert now something that's interesting also to notice here is that we don't wait to find an alert normally you are waiting for a few minutes just to get rid of the fless but don't forget that we have already retried our Chron job three times okay so if something is is going wrong it's failing three times then you just fire the alert right away um okay some something other interesting to discuss is how we push this data from um our coron job to Prometheus because normally Prometheus it's it uses the pool model so it just pulls stuff it pings your service every x amount of seconds to pull data now our coron jobs now it failed but normally they're really fast okay they take just a few just just a few seconds so it's quite possible that the cron jobs will not be up when Prometheus decides to pull them and um there reason a way you can fix this is by having um a Gateway in the in between so Crown jobs send stuff to the Gateway and then Prometheus pulls Gateway now we didn't go down this route it has some some downsides for us so instead we used kix's ability to use a remote right for for Prometheus uh but for this it's an experimental filter in Prometheus and you need to have it enabled in your server before you can be able to use it but yeah k6 remote right goes right away to promeo server everything is working out of the box all right let's take a step back we have a Chon job it's doing something very simple brings up a k6 browser that logs the application tries to log in and then it gives thumbs up if it if it's working otherwise it's failing if it fails three times cjob is failing completely in this case promethus this will fire and when this fires then the alert manager in this Nam space will see that something is happening and then it will just create an alert for it now let's have a look how those alerts look like here we have a test um and has like loads of nice information about it let's say you're noncon engineer you can see right away um like you know when it failed why it failed classers and stuff like that but the interesting thing in here is those two lines so you can visually see the error yes it's me yes that's myword all right uh I think we don't see it let's try again I yeah okay so Engineers have visual cue about what went wrong in this case we just changed some of our secrets to show like another random user so it's failing okay it's failing well so Engineers can just go there and see what went wrong um in the first place now most of the times that's not going to be a surpris we just have blank screens in there so if something's taking a while to load if something throws an error it's difficult and visual cues they're nice for debugging but they're not very nice if something really goes wrong to your application then in that case people have also here they can check the logs and that will go right away to Loy and then Loy here will have all this nice information that we also get from the crown jobs and then they will figure out what is wrong and further debug now Engineers don't forget that this is out of the box we didn't do anything special like everything was supported for the rest of our applications as well and this is why it was very fast for us to to iterate on this um so we also have profiling in here so if you're for some reason you want your test to have profiling pyroscope would be here or if you had traces Tempo would be in here most of the times for tests we don't use profiling or tracing but anyway if you want the stuff are supported out of the box cool and that's it we're going close to the end with some challenges all right I can I can work for we have some minutes hopefully um yeah so first chalenge QA and observability and devops right this is what we started uh so um um as I said not all of the companies are having quality in their process and we start to you know to think about how the quality Engineers can benefit from observability in the first place so if you have observability and quality teams you already know where you are uh if you don't you need to ask yourself you know what is unknown what we are trying to cover how we can alert teams and actually the mindset of a quality engineer especially when he's doing exploratory test thing is to understand you know what's wrong with the application why a new feature is not working uh and these conversations are actually really happening with our devop team and this is how we actually give birth to you know to the wake up project in the first place uh so we need to embrace the unknown do you one thing we kept asking constantly ourselves was why buil it internally why not use you know an external setting paid SAS service for this and obvious answer here is money similar Services they can get very expensive they charge per scenario and we knew in our cases scenarios would have plenty of them so it can escalate like very quickly also don't forget that was a pet project for us and we had to really bypass all the budget discussions like you know the difficult discussions and stuff like that also it like we have made so much effort internally to have everything under the same platform by using grafana it was was a lot a lot of effort like to have people just logging on one thing for everything so it just it felt weird just to have an external service just for this Ed Case and last but not least we really did not make anything like new everything was already in place all the dots were there so it really it was like no effort for us also engineers and QA Engineers were already familiar by using k6 to write tests challenge accepted so the J traction we built it the engineers need to use it right so um three main pillars here we have our project which is a wake up it's nice because we should tell about the name in the beginning after the lunch to wake up people so uh we have people they need to use it we have the process we need to integrate it in our delivery flow um so for the people it's really easy because our front end teams were writing react in typescript k6 it's on top of JavaScript it has also typescript support so it will really nice and easy to use it our QA te using it for already you know building load test for the back end uh the main was the back end Engineers that they had the different programming language and but the learning C was really nice and smooth so the process is you need to have strategic goals with your business so meaning that okay what is the goal here we need to measure web performance right uh and that's the the whole point the business needs to be aware that it's really nice we need to have page experience user experience and uh not any issues with our performance and last but least and the last thing the one that drives me crazy lately is endless fine tuning especially with web vitals in place people you know you need to F tune your scripts a lot because you really want to avoid having false alarms and flag tests because it's connected to many on c teams people can get very irritated which makes completely sense so as an engineer you have to be very very um you you need to F tune your test a lot so with the the way for us to solve this thing is by having very strict processes around it in Wake Up's case it means that when you're writing a a new test you find unit and then it needs to leave to a slack test channel for a couple of weeks or a couple of Sprints until people give a thumbs up that it's stable enough to get into the production Rota but it drives me crazy really

