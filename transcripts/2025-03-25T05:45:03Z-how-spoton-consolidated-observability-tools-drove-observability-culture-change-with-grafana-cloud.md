# How SpotOn Consolidated Observability Tools &amp; Drove Observability Culture Change with Grafana Cloud

In this talk, Jeremy White (VP of Engineering) and Sai (Engineering Manager) from SpotOn share how they revolutionized ...

Published on 2025-03-25T05:45:03Z

URL: https://www.youtube.com/watch?v=wG0GG0U3Jf8

Transcript: all right so my name is Jeremy white I'm a VP of engineering at spoton I oversee our core Services Partnerships supportability and most importantly the infrastructure team which is what helped with the observability I'm Sai I'm an engineering manager at spoton uh I lead Cloud traffic and uh absorbability Team U recently part of the Great Migration all right so as far as spoton spoton we are a restaurant business uh operating system uh we help restaurants be able to run their business so they can focus more on the cing more on the restaurant the hospitality side of things our goal is through technology to be able to help make them more successful as far as the landscape at spoton um it's rather complex because we have a a broad sense of products but also Technologies we have a lot of client-based things in terms of uh the handhelds that are are running that are going to the tables whether it's the kitchen display systems in the back whether it's helping run the network for the restaurants um and a lot of the backend services for like online ordering and things like that we recently went through this journey of moving from we had four different uh observability tools moving over to grafana consolidating all of those um and being able to pull in data from all these disparate uh disparate systems as far as our team it's about a 30 person team in terms of infrastructure specifically the team that we did to do the implementation here was five people on observability and this is to support hundreds of Engineers across a lot of different products so just to give you kind of like a sampling of some of the areas of products and why it's kind of a little bit more complicated I think a lot of companies don't just have that one product you have a lot of different areas with that comes subject matter expertise so what we've done as part of this is broken down into domains so four of the domains here that you can see are just a sampling of some of the products the the important thing here is that this is why this tagging and this taxonomy was so important to us was because we have such disparate teams that focus on different areas how do we get alerts how do we get data how do we get things to the teams that matter so that they can solve the problems best all right so as far as taxonomy goes one of the biggest challenges we had was who owns a service uh get blame was probably one of the most common tools used to find out who you touched it last it's yours uh this is not great when you're in an emergency setting or anything along those lines so we needed some way to be able to consistently give the correct answer not someone that wrote that system you know several years ago there might be a new team that's managing it uh same with we found that a lot of the teams are not familiar with what resources what different pieces make up their system right they're focused primarily on the code they don't necessarily understand all the other pieces that come together so our goal with this was to try to get some of that transparency around how we've organized all of our systems and who those systems and components belong to so at a high level we broke it down based on audience so uh the way we see it is that the products and services are what our customers or what support or sales are going to actually reference and talk about the domains are what we just call our groups of teams right so each one's going to range between 20 and 100 depending on how much area that's being covered there um and then systems and components are the actual assets that we're building so these are the microservices the systems that actually get deployed or the clients that go to the Android clients that type of thing um every one of our systems and components now uh the the goal is to have a uh someone that owns that and then when you know which domain owns it we have a product leader and an engineering leader so even as teams within that domain change you at least have consistency of you know you go to that domain you know who the leaders are you can at least get your answer there even if things change behind the scenes as your priorities and your in your teams shift so to double click more on systems and this is not something we came up with we leverage backstage which you'll see why that's important later on um we've used their system domain model um so we simply adopted that uh this is where when you define a system it's a it's a collection of resources which are going to be your databases any of the infrastructure type resources uh apis are how you're supposed to interact with the systems and most importantly the components uh are the effectively the microservices or the services so this gives gives us a way of kind of grouping those together um to give like one common view for this and so what we did is uh when we implemented this we went through and we had over 1,800 repositories and we made it a requirement where every single of our GitHub repositories required this metadata file for catalog info in that we Define a lot of other things but the key part you'll see here is that we specify the domain that means we know who owns that repository um from a a domain perspective so therefore we know their leader and then we can drill down from there if we need more information uh this went from instead of having thousands of repositories and services and trying to figure out who owns what at least now we still have those thousands of repositories but they're grouped by domain so it's a smaller subset to be able to drill into all right uh before we talk about the enforcing of tagging part I just wanted to call out on this five labels um the observability strategy at spoton uh starts with a simple idea it's universal language so these five labels domain environment Resturant location cell and service are absolutely ubiquitous across all uh our observability system these aren just arbitrary values these actually in a way Define our systems right so you can actually let anybody in the company ask some meaningful questions to your observability system like is this service part of this domain is the database part of this service so it's absolutely essential so we all know that labels are just key value pairs it's it's useful but in order to have a scalable observability system it's absolutely essential all right so now we have this taxonomy um the next thing is we got to implement it we vers in the taxonomy what's next so we started off with something very simple AWS right makes sense we we can apply the tax at the creation time um we all agree that automation is the right way to go but it's Al crucial to understand we eliminate the human input from this Loop uh that's where backstage comes in backstage allows you to decorate your metad metadata for each of the repositories and it also provides those metadata via an API that's what we leverage we leverage backstage as a source of Truth and we built an automation around our infrastructure as cacd and we query back stage hey what's the common what's the what's the critical metadata for this particular component and we get them and apply them at the deploy time so this is easy pretty straightforward we apply the tax for all the AWS infrastructure but here's the kicker once these tax gets applied in the infrastructure these becames these becomes uh Prometheus labels when grafana scrapes them directly uh and there's also other incentives when solving tagging problem right you you check the other boxes like cost and complains all right so now we wanted to extend the AWS pattern into the kubernetes right um which which means that hey can we just apply apply the tax at the deploy time is it possible to apply the tax at deploy time of course it is right you can apply the hel chart but then we took a step back and what that meant is we actually had to apply like across 750 applications across multiple resource um so is it even worth it we are exposing some risk so we were wondering is there any way that we can actually mitigate this complete uh redeployments and figure out a way to actually do it within the cluster and that's what we did so the delivery and the compute team which is part of infrastructure domain came up with this interesting idea where they built a custom homegrown label controller which actually gets deployed as a system resources across all our kubernetes cluster and that label also does the same thing it calls back home which is again the source of Truth and asked for the same question hey can you give me the critical metadata which is part on domain and subdomain tags for this particular component and it applies them uh at the kubernetes higher level objects and the kuber takes care of the rest it it it propagates the tax to the lower level objects all right so now we've covered two big chunks of infrastructure tagging we're doing pretty okay consistent at that point then we asked ourselves hey is there possible to extend the standardization technique into the application level layers and then there is application Telemetry so we all know that open Telemetry plays a big part in the standardization and our answer was twofold open Telemetry is the foundation and custom the G for Developers the customer SDC Cas for developers is absolutely crucial from a central absorbability standpoint because you can use that an opportunity to expedite open Telemetry adoption within the company so how did we do it we made them dead simple we created much of the boiler plate and removed all the cognitive load for a developer to instrument applications so that they can focus on the business Telemetry and and all of our sdks across four different languages injected the same set of labels like service name service version and deployment um environment um the good incentive for for any developer to actually start instrumenting application uh absorbability and if you are in the grafana world the moment you instrument your application and you deploy it in a cluster you get the red metrix dashboard out of the box for free which is actually a good incentive all right so this is a simple example of how our SDK works you can see a developer actually Imports the spot- on all SD okay um and then they are trying to create two important signals for order ID one is a metric and there is a logger but if you notice there is no context of labels at all but the moment they create those signals in the second uh uh snapshot you can actually see the service name version environment gets added automatically and the same thing for the locks as well this is how we intertwin the signals graphon on open Telemetry loves this stuff all right so this is your last resort right now we have taking taken care of your infrastructure you've taken care of your application layer the final thing this is the last Hub before your data reaches into graphon cloud in other ways this is your last chance for standardization um fortunately we had one Telemetry pipeline so it's pretty easy because the allly offers the programmable Pipeline and we are pretty much IC at that point so it was easy to push configs but the question is what kind of standardization would one apply right so we we took a step back can ask this question if things go wrong what are the three things that people care about what service where is it running and who owns it it all boils down to three things and that's what we did we tried to embed these three context as labels into all of our signals so the first one is K metadata it it answers the V question and it covers the operational context the second one is the domain labels it's the who question covers the business context and third one is the service metadata which answers the what question and covers the technical context the this is what happens inside a pipeline right you the at a in a at a fundamental level the pipeline takes a raw un structured data and passes through a processing layer and does the extract transform and standardize and applies the label before it reaches graphon Cloud so on the right side you have an example where the Pod name container name becomes service name name and remember we applied the tax at the kubernetes now with Alloys service Discovery we tap that part uh domain and subdomain labels and use them for all the signals so all your signals metrics traces and logs will have the same set of labels all right this is the reaping benefits time now we' have done all the hard work to make and standardize all your Telemetry signals across your landscape so this one you can see the first picture is a thesis service which is actually part of a payments domain it's trying to make a call to another service Omni Channel which is actually part of another another domain and it's actually a log and then the third picture is thesis emitting a metric you can see three different signal types all having the same domain and critical information across service context this is where we make actional actionable insights possible the foundational dashboards this is one of my favorite things um that happen automatically um this is this is basically leveraging graas native integration since we have solve the problem of labeling ad Source all we had to do was just consolidate and create a dashboard based on the domains you can now filter the business domains instead of worrying about technical details and now we've created a a structured dashboard where application theme can actually tap into this pull those into their dashboards and have a HFI uh service view uh and this is also very very interesting for infrastructure domain in general because now you have this bird side view of all infrastructure pieces from one single context and the best part is you don't have to solve for new Services right because the tabbing system is in place the moment a new database gets created it will automatically show up here all right the alert management uh the pre grafana problems with alert Hur us really bad uh we had a task to consolidate an inventory of all the alerts and we couldn't complete we got to a point where we have 8 70 alerts and some of the cases we had the alerts were still calling back to the old employee which is bad really bad um and all of them had similar patterns no standardized alert classification um poor hygiene no no ownership and all that um so when when our time came to solve the alerting problem during the migration uh we had to pick a few good rules the good philosophies the first one was no alert left behind meaning every alert will have a home and it will always find its owner at the right time and we also wanted to classify based on the convention makes simp things easy which are actually very tribal to your organization right and then we made uh the alert rules infrastructure as code as well so that people don't have to worry about it and this one is slightly Draconian but it's also paying off us uh paying off uh in the longer run where we this is how we actually enforce standardization and make sure we back that philosophy the smart alerting was built on that philosophy right um graphon takes good chunk of um things that we had to do here so once you have labeled all your infrastructure and application the allowed sources are Live Wire in some ways right because they have the source labels and when they start firing all those alert instances will have those labels for you then they are subjected into the classification roles and they get routed based on the domains and sity which is something that is one of our rubrics and finally based on the sity it will find its home all right this is basically a life cycle of an alert at spoton right so graph makes it extremely easy to create an alert from a dashboard so we saw earlier that we have created the dashboard based on the domain and you see a green little heart there which is a indicative of good uh healthy system and the moment the alert fires all the alert instance will have the domains and the subdomain information which actually was configured through code as the notification policy so all these labels when they get fired get subjected to this policy and we'll find the right home the third one is an example of a particular database from guest domain uh with the sity major it is out for delivery to a slack Channel because it is Major if it was critical it's going to page an incident uh this is this is one of my favorite alert from a platform engineering point of view we all have talked about alert ownership for a while right the interesting thing about this slide is you've got two infrastructure alerts but it goes to two different teams the cube not cubn not ready is a compute platform alert because they're responsible for maintaining the compute platform the other one which is also an infrastructure alert but it is actually doesn't go to a devops or an infrastructure team it's actually a dependent snsq that's owned by a development team of a service so now we've got a fine grain demarcation it usually should have gone to a devop team but now this alert goes to the team that owns the service all right so one of the big things here to to handle was clear responsibility right uh when we had emergencies or when we had hot issues it was a bit chaotic because it was hard to understand like who owned things again you'd ask who owned a service you'd get three different answers if you ask three different people um so it was very inefficient we would actually call do an all handson deck and it got to a point where you get close to burnout right since you don't know who to pull in you pull in everyone you can it's kind of like the drowning man syndrome um this also proved out some challenges with trying to be able to articulate this to support and sales teams right if you can't articulate where the problem is or who owns things if you're using those different languages uh that caused us some challenges so one of the things we did and it kind of Segways into the last conversation is we did establish some of these roles for whenever there are incidents uh we had a first responder right that was the one that was supposed to be handling the issue and confirming whether or not it was an incident Commander we did add an external Communicator because we did find it was difficult during incidents to have someone actually communicate with sales communicate with support and kind of put it in a language that was not always technical um so that was a new role that we added on and where this worked out really well with the irm tool is because we knew when an incident came in which domain it was associated with you can see in the bottom screenshot there we automatically assign those roles by looking at the label of that incident coming in so we we actually paged The Incident Commander we paged the first responder we paged the external Communicator for that domain so no more trying to pull in everyone all at once we can now be focused and pull in the team that really is relevant for this particular issue now we can still pull in other teams if necessary right because each domain will have a first responder so you can still go outside of your domain but this really drastically cut back on that problem we had where pulling everybody in all the time uh also the slack feature of this for the little robot uh reaction anyone that's used the irm tool and being able to pull that into the irm tool is phenomenal huge fan of that so thank you gra on that uh this also Al allowed us to be able to do emergency access so one of the problems we had is who do you grant access um because of the taxonomy with the domains and we knew every resource which domain it was associated with we now have access control that when someone's on call they can say I need emergency access for the payments domain and that will grant them access to just those resources so we're able to compartmentalize it based on that Convention of the domains being applied pretty much everywhere also our dashboards are in domain folders so again when you know you can kind of focus in on just the dashboards that are relevant to you or if you do need to understand how another domain is doing you can look that up uh same with our run books and our standard operating procedures that we found that being able to drill into those areas really helped kind of reduce the cognitive load where you could focus on just your area uh even though we have a lot of services and a lot of things across all of spoton uh one of the other we got this was a big culture shift right um we stopped because of those foundational dashboards and things like that that s had shown we stopped asking Engineers to create dashboards that included CPU memory things like that those those became just baked in we were able to really shift Engineers to focus on what are the metrics that really matter because dashboards should be telling the story right we wanted support and people outside of engineering to be able to look at these dashboards and at least be able to grock what's going going on um and and this actually kind of turned into even product and sales being able to look at these and quickly be able to answer questions of what's working and what's not um this then manifested itself into as you're building new features people started to decide all right what kind of metrics do I want to start putting into these new features so that they show up in these dashboards that support is going to see or so that product can actually use these to to see as the product behaving the way that we would have expected um so this really started to bridge that Gap and something we tried a couple times to implement us a Low's and we always struggled trying to get that buy in now we've got teams that are actually defining us a Low's without us asking like they start to see the value because they're trying to create these dashboards that are not geared towards engineering now um so that's been a huge Boom for us at spot on um and then cost efficiency uh one of the one of the main reasons we made a switch was we had we were getting overage costs every month and trying to defend why are you spending this much more on a tool that you've already established here's how much you're going to spend every month we would have to go on a treasure hunt trying to figure out which team turned on a feature added extra logs added extra metrics it was very difficult Now by being able to break those out by domain we can show how much each domain is spending and we we stop calling them offenders right and they're now consumers do you want to be spending this much into your system we can compare different teams to one another whereas before I could say why are you spending x amount of dollars now I can say you're spending x amount of dollars but is other team spending why why is there such a discrepancy it fast-tracked a lot of discussions we had before where we would simply hit a wall at trying to be able to get to conclusions all right so but I've all I've attended few conferences before and this one is mainly to cater some of the absorbability central absorbability teams in the audience right because every time I go to a conference I always wonder like how come a team of five or six can actually change a cultural problem of a company that's just not possible right so but there are few tenants that we have learned along the way so I thought I would share with with you folks U Pick a problem space observability in some ways are like the Lego pieces like you you you pick a problem space spend enough time on it tackle it move on to the next one it will actually will yield compounding gains like the things that we did for tagging right so it was actually unintentional we were trying to solve cost problem and compliance problem and the moment we entered into the Prometheus and grafana World they loved it right so we took advantage of it and then the convention over configuration simpl Simplicity is not simple right so I think the convention should be more obvious within your company and it will last long um uh the reusable components is uh another thing so that you can actually spend your time as a platform engineering team on other things ultimately in our opinion the goal of an obsorb team is to provide useful data to your organization to make quality decisions and finally uh we were able to complete the migration but more than that we were able to build a solid foundation to propel us to other things right and it was only possible because observability is a team sport it's like a poow everybody chimes in everybody has got some uh rewards to read and it's it's also a support of your R&D team infrastructure and grafana being part of the same team so thanks thanks everybody cool thank you

