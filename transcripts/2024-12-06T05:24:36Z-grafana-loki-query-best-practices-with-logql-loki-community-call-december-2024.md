# Grafana Loki Query Best Practices with LogQL (Loki Community Call December 2024)

In this December's Loki Community Call, Cyril Tovena, Senior Principal Engineer and LogQL guru walks us through a Grafana ...

Published on 2024-12-06T05:24:36Z

URL: https://www.youtube.com/watch?v=d61nQX-n91c

Transcript: let good hey everyone welcome to aloki community call for December and I am Nicole vanderhoven and I'm here today with these two lovely people hi guys returning once again I haven't been cancelled Jay Clifford um currently calling out of Vegas at the moment at AWS reinvent so if you're about come and have a chat awesome hey guys I'm h s principal engineer in the Loki team super excited to be here and talk about l i just real I know this is totally not what you're here for but why do you have a pyroscope thing in the background and how can I get one actually so are you this is a good question I used to uh so I it's been like six years that I walk at graa and I changed team couple of time and my previous team was the bioscope team and uh my manager last year sent me this for Christmas basically that's so awesome why do we not have that that's so cool Okay so we've also got Julie Julie would you like to introduce yourself too hi I'm the technical writer that works on the Loki team awesome so today we were going to talk about log ql this is going to be everything about what it is from from a beginner's point of view but then also like a lot of best practices around how to use it we've sourced questions from the community but feel free to drop any in the comments down below if you have any that come up as you're listening to us sirel would you like to start us off with defining log ql like what is it and how's it different from other query languages yeah um actually maybe we can talk about the history of but um I think I never I never had a chance to talk about how did they came together this this language so I think it was like in uh 2020 so one year after Loki just uh started uh the only thing you could do on Loki was like selecting logs and filtering so that was the basic language so filtering as in like just add a world and it will search for that world and not nothing else you couldn't do passing that you can do nowadays or or Metro Co and so uh Tom Wilkey the the the creator of Loki wanted to have a a better language or something that would uh will be on pair with you know the the vibe of the project and so he asked me to start working on that and and uh he also asked me to include other external uh people from uh for building that design doc so it's actually not just uh me who work on that but also uh someone do the community may know whose name name is uh fedck Branch so fedck is a maintainer of uh promo is is very big on community uh cuity community and is also the CEO of of Polar Polar signal um if you know the company and uh he helped me basically design so I did couple of like session with him uh and we designed together with Tom uh the the language and I think there's still the design doc out there that is like uh public so maybe we can share that after it's pry fun to go through because it's it's quite you know quite different from the actual implementation but you can see what we wanted to do with the with the language um so that's the that's the history of it uh it has obviously changed over time it's like uh now we are like three more years on top of it so a lot of addition have been added uh but in essence it's a way to query your logs uh and uh to you know query your logs in a whether they are stru un structure uh so search but also uh you know query metrics from your logs that's what that's what L is and So based upon that how's it kind of structured is it like promql what's the differences you know what what does it kind of look like yeah um so the main idea of the language was to be a bit a bit the same as the line follow key you know the line of look is like log like Matrix well like promus but for logs uh and so for the language local the idea was to be exactly like uh promo uh but obviously promo doesn't have the same uh you know filtering uh content uh features so couldn't be exactly the same so we Tred to uh to to use like the pipe operators for all those different uh style but then keep everything that is like Matrix the same as promotus so it's heavily inspired by Pro prom for sure uh and it is done this way so that if you already know promql you should be it should be quite easy for you to uh get started with lql without relearning everything from Scotch okay and can you tell us um because one of the you already mentioned about the pipes in log ql and that's one of the things that differentiated a little bit uh could you could you talk to us about that and why you decided to add them yes so we so it's good question so we had a we we we had to choose which operator will be will be best for uh searching and the reason why we went with pipe is we thought that this will look like a CLI or like a the same command that you use in your in your bash script in your piping command uh after one on another so you're like filtering and then you do a pipe and you do passing and you do another pipe and you might be uh um converging into metric so that's the main idea for using the pipe operator so that it looks like a it looks like a CLI command that you're writing um there's it's actually other uh competitors or or vendors are doing the same so pipe operator I think it's the same in splank I think if that's the that's correct so so does like the pipe operator also lean into this idea of schema at query is this based like how how does that kind of work into the the story we are we passing the log lines through these pipe operators yeah so ly doesn't really actually know about the schema or or structure so the the the pipe uh combined with uh par operator allow you to actually transform uh something that doesn't have a schema with into something that has a schema the schema is actually Dynamic so it it's not like set in stone for a given log line it can be actually different from One log line to another uh and we we create the schema using the labels so we actually add more labels as you start passing your log line and and and this the the structural data um so it's combined with a with the passer nice very cool and so I mean you you've touched on it a little bit so we've done a little bit around sort of saying you know there's a there's log queries where we can have that search concept but you've also deployed metrics queries so you know take us a little bit through what metrics query are and sort of when when would they be deployed what what are their use cases for the user yeah um metric qus are are basically when you want to so the most basic metric qus are like counting logs over time so you can count um you know how many errors you have uh usually I think the the most powerful ways of using the metrio are when you combine them with filters so you're you're for instance searching for something and then over time you're counting how many occurrence of that search is happening but there's also ways to um transform some of the fields like if you have a latency for instance uh inside your log line uh you can transform the log line into a quantile or or if you have a count you can transform that into a right so yeah so the the idea of U metrico is is either uh you know counting the the log line itself or trying to you know transform or you know rate or count what's inside the the log line itself okay and then everything else would be considered a log query everything that's not in a numerical form yeah everything everything that is not wrapped with a function is a log query yeah yeah okay and then maybe let's also talk about where you can run these queries because uh you can you can definitely use them in Gra F I think that's probably the way that most people would use it but there is also another way that I don't have that much experience on and it's log CLI what is that and what is the reason for having something like that yeah so log C is a tool that has been existing for a while we actually built it if I remember correctly because uh we wanted to be able to use some of the uh uh query feature without sing for graph to implement all all the the features uh I actually don't use it myself nowaday very much because I'm pretty fulfilled with either graph or explore logs uh but it's quite useful if you want to grab the data and then uh start doing you know a pipe operation with your bash so you know you can grab a stream of logs and then start maybe passing it with another tool that you have on your uh on your machine uh so it's basically a CLI but could be useful if you want to download for instance very quickly or stream the data into another another tool that's that's the that's the idea I noticed with sort of the CLI there was some really cool features around basically looking at like you know the active series and the cardinalities like the number of labels and stuff like that can you also get those features in grafo um or is that purely like a log CLI only feature yeah that that feature about uh knowing about the the the labels and the series is just uh in the CLI but I think we're looking at trying to get it into graph at some point there's a lot of uh there's a lot of cool feature that we could do that you know from the Loki API that we're not doing right now uh but with explor logs we're trying to touch on more features like this so you know watch out watch out for the space for sure yeah I think that would be very cool we had like um a user um end up labeling by timestamp and I feel like the log CLI might have saved them that point realized that log streams would have went through the roof so yeah that's the Lox like this this specific feature of loel is what our team is using internally when you know a customer is having issue like they're using the the that tool to try to figure it out uh what is the label status basically for that customer U which is to be fair not like we definitively want something easier so that everyone can actually do it yeah yeah AG that' be cool I wonder if we could um have a little bit of a demo of just like a some simple query for for log ql just so we see what we're looking at then maybe we can drill down into it and ask more questions yeah uh let me see so I was let me see what I have here if I have any logs in gra on a play here s sorry that's all right let's let's do that's what he comes here for he wants to be put on the spot uh never shared my screen with this one so let's see what's gonna happen oh I can share just a tab that's perfect all right let me let me know what you see right yeah I can see myself too now that's that's perfect all right great um so I'm I'm uh yeah we talked about unless I'm using play. graph .org so you can you can do the same U I should mention so I think play is nice to you know start doing your own queries and see what you can do with lq there's also in the documentation I think Julie you probably know more than me on that but there's a tool in the doc that allows you to also play and validate uh the uh the queries right yeah y I'll I'll just say one thing before we get any further can we make this just a little bit bigger so it's clearer on the on the presentation yeah let's do that thank you and also the the tool that serial mentioned is called the logl simulator and that is in the documentation I will find the link yeah I use it uh couple of times to uh to to try and validate the query so it's pretty useful um all right let me try to get the page bigger all right do you want me to zoom is it good yes I think that's good yes yeah thanks well if I zoom in it crops it and that is the the link down there for the the page where you can find the simulator it's just a good tool if you're if you're going to have to put in um a bit of your log and then you can test out different queries to see what it needs to be yeah all right uh so thanks J right all right so what we're going to do is I'm going to start doing like a uh you know uh from from the start basic basic uh explanation of how loal work and then maybe after we can try all the type of Co um so the first part that you so I know it's very so it's very danting at the beginning because you usually start with something empty like this um so in case you're a bit uh scared about that you can always use export logs so I'm just going to quickly show you export logs right if if I show this tab right so expor logs doesn't require you to write any logs if you ever uh feel stuck you can use expor logs to for instance do some uh filtering uh I want to show you this before getting started so we actually also have a Loki Community call All About Explorer log or we spent an hour just on this so I'm going to leave a link in the description as well so you can if anyone is interested in that this is a great great I think this is the best way to get started queries like before you even start to write a query maybe we've already written it for you and then you can just copy that query and modify it that's how I learned yeah definitively so uh this is what I did I just went to a explore log and then click on uh open and explore and you can see the query uh so that's a good way to uh to to start all right let's go back to explore so the first part uh which is in the in those like Curly bracket is is what we call the uh stre selector so it allows you to slice and dice the the data so the streams are unique label Set uh of uh of your logs um so if I want to see let's see if it's going to show me so I have those labels in that environment so if I want to see for instance for a given name space let's say the MIM namespace uh all the logs that's that's how I do it and I can I can also combine it with all the uh selector so let's see let's pick another one so I could do services so there's only one uh cluster yeah there's only one all so I'm going to do name space so you can also do uh rig xes so this is the the operator to do rig xes so I can say I want everything that contains uh mimia for instance so like this uh and that's going to include all the mimo name name space it's not just uh one of them and then after uh I can combine so there's multiple uh cluster I'm going to select this one and this is how you slice and the data and those queries are always fast so you should not shy away from actually executing them uh just to make sure that you're looking at the right data so you execute them and you look like you look at uh you know the other labels that you're seeing to see if you're on the on the right uh path for what you want to select um so this is one of the most important part of low key making sure you correctly select the right uh uh label set because that's the fastest way to reduce the amount of data that you're gonna that you're going to quy I know there's a lot of people that you know go in and do like uh you know cluster and then uh like this right everything that's usually the worst that that's the worst way of doing it because it's definitively going to ask too much of ly so you want to you really want to try to form the start only pick what you need um does aut matter seral the the the how you specify the labels do they does the order of specifying labels matter or you know should you start from a specific label first yeah yeah so actually uh it does it do what matters not necessarily the order because the query uh um engine will reorder them by by uh name uh one matter though is that if there is um I don't think that's our okay is here let me see if we have one label uh that is always so we we had a case earlier but let's say if we had level and there are well that case for instance that does matter at scale U it's not you should not add it if it's um if it's always the same label don't don't try to use it like it's not helping Loki it's actually asking one more thing for Loki it's trying to filter that one and this was not really required to filter with it uh but the other doesn't really matter I I like that you said that um the label set that is chosen is actually really important for the performance of Loki we do have a um label best practices page in the documentation that's in the chat as well um because there's there's a lot of labels that we see that are for example Dynamic that that change all the time that's not the best use of them and it's just going to slow down your use of lowkey yeah for sure Matt fine actually has a question here uh is there a way to prevent users from running a log ql that might be detrimental to Loki so something maybe that isn't very performant or like the one that you said there were labels in the query that weren't actually necessary and maybe it makes the query long running is there a way to stop people from doing that um there's there so there's ways to mitigate those issue in low key with uh um uh tenant configuration so it's like tenant specific configuration I think we call them override in low key or limits maybe I think we call them limits in low key uh and we can limit some specific tenant to run with less resources or we can uh make sure they don't query seven days you can log them down to maybe an hour uh we don't I don't think we have anything specific to log down a specific query or stop specific queries we do have that in Cloud but that's uh not open source uh but if you if you haven't really want something like this you will basically write a proxy and start looking at uh the queries and try to stop some queries if they're like too wide for instance so not exactly one query but we can we have a some way to mitigate uh queries like by by time for instance or reduce the the amount of uh resources that one tenant can use and I guess also asking the other community question we have here I'm going to interpret it a little bit so it's based you know more Upon Our querying rather than the full multi-tenancy setup but is it possible with' log ql to query um multiple tenants or um within within you know so if you have sent logs to multiple tencies can they all be queried at one time or just you have to set up different data sources for Loki in query those individually yeah you have to uh you have to actually uh uh do it in individually in open source um that's a we have a an Enterprise feature that does exactly that so anything that is like related to Enterprise multiple tenant tenants uh you know authentification we always consider that that's like Enterprise features so you need to uh get lowy Enterprise uh but anything that otherwise you can either build that yourself uh or oh yeah I'm looking at the question did I answer correctly question yeah unfortunately not like m tendenc is only uh in Enterprise nice thank you very much all right let's get back to it um so this is log format uh logs and uh the next operator that is very important uh is filtering uh and I think you should always try to filter first before doing any other operator so we set that at the beginning when you do a pipe operator uh you actually every logs that go through the pipe is the one that match or past the the previous operation so the way it works is that the more you filter and the more your a operation at the end the better it is um so if you if you want to start searching for worlds then you use the pp call Operator um so I can search for uh let's say something like 29 I think 29 it's not not that much right so that's the the PIP operator that just reduces the amount of uh data you can do the opposite so that's the opposite everything that doesn't have uh 29 right and uh if you uh after that if you want to start uh so there's other operator there is the the rigex one and the non-match rigex those one are interesting in few cases uh they are interesting if you are doing a match sensitive so if if I'm looking for uh something uh like for instance uh cortex but I wanted to match whether it's like with a Capital C or not so this is insensitive case matching uh so this is very useful to use a rigex like that and it's actually looki is smart enough and is not going to execute a real rigex it's just GNA actually uh do uh uh another operation so it does U optimization of CH another reason for using Ras in my in my opinion is when you want to do all so if you want to match uh either cortex or uh duration for instance one or the other then you will you will use and that is also a fast uh because the way does it is again optimize it a way in doing two uh contains operation now if you start doing uh uh this for instance so if I do 29 do this is exactly the same operation uh and that's Could you actually sorry it might be uncomfortable on your end but could you increase the could you zoom in a little bit more while we're looking at queries yeah see I just caught Jade going like he's very close yeah does that work better yes I think that's good thank you all right so I was saying that U rigex matching usually is something that there's a good reason for using it is like Cas and sensitive or if you want to do all operation but try to avoid uh regex operation as much as possible they are very slow and they're slow actually for a good reason uh it's because they are uh executed in a way where they cannot use recursion uh and so that's why it's very slow this is the go rigex uh um matcha that we're using and it's it's known to be not the fast fastest out there uh and I don't think there is way to make it faster so I suggest that you do as much as the other filter that you can uh and once you're done with the the filter and you need something more uh uh sophisticated then you can start using rigex after but try to reduce the amount by having multiple uh matches of equality or inequality first and then the rigex at the end um there's a new addition that I want to talk about so all of those are about the content of the log line how to filter them uh so you can do the uh the opposite like this so let's do let's do it uh with like not matching right like this um there's a new one that has been added so I'm going to remove this one which is this this this new uh graph doesn't even know about it so we're GNA try we're going to try to use it uh for Strings you can use back back uh back back cheick I I I usually like to use back cheick because it's automatically will uh not required uh to ignore the backslashes right if I do if I do this for instance is going to complain right but if it's h if it's with a back tick then it won't complain so I don't need to basically ignore it like this and I can match a backlash so back check is quite useful for not uh caring about ignoring something right so this new operator is very because it's like a rigex but not rigex it's called pattern matching so you can uh try to match and this is the uh the equivalent of dot star so you can match anything that has cortex uh let's say cortex like this right and then anything after right uh and that's going to match anything that contains cortex but you can play with it with uh after there must be a duration equal something right and then that's going to still match everything because you can see the log line uh as as as this this matching pattern the same pattern uh and so the idea is like you just write how the the log line should look like and replace with uh things that you don't know uh and all things that are viable and that's actually much faster than rig X's uh this is basically the equivalent so if I if I do that this is the exact equivalent of doing Dot star um like this right and then cortex and then dot star you see what I mean so this is exactly the same equivalent query except that this one has only one operator is this one and it's much much faster to uh run this one as the the pattern so we call it the pattern filter it's much more uh faster to execute the pattern filter than uh the rigex matcher uh and it's it's brand new I think it's from 3.1 or 3.0 three. Z yeah three. Zer it's pretty it's pretty cool I use it a lot export logs use it too uh when you find a pattern you use that operator to select only uh only those uh pattern um I can try to find you a more complicated one right now so give me a sec s's working on that he said fana doesn't know about this but it is in the Loki do so if you want to go look it up it's in there well go I was G just gonna ask on that note does the pattern ingestor need to be U enabled to use that feature at all or is it that's just say directly a query line um no it doesn't it's actually just a query language addition yeah fantastic if you want if you want there's a new API that shows you all the patterns and the volume of it that requires the new PA inesta well you know we're talking about best practices here but I thought maybe we could talk about what good is like what is an actual good query and I was thinking we could talk about it in terms of like there's there's a of course it has to be functional you have to be able to find what you're looking for and S you already talked about how that really is not just about the query it's also about the labels beforehand um and then there's also performance we've talked about making things faster and then there's also cost so how does how do those three things interact like maybe you could talk about how we charge like if if people are using the Enterprise version of Loki for example what is the actual cost based on yeah so the cost of uh of so a good quer is not necessarily A quer that is not expensive by the way depends on on your use case some some sometime you have to to do an expensive query for sure uh but to talking about cost it's an interesting one because the cost is mostly related for low Kei uh in two parts is the CPU usage you're going to need to execute that query or the CPU resources that you're going to need so it depends on your deployment uh if you want your query to go fast you might have a very busty workload with tons of qu like 200 of them and you're going to pay for them you know most of the time even if you don't use them um so the CPU is one and then there is the object storage that is another factor in the cost the object storage is always lower I know a lot of people are like worried about object storage but object storage tends to be quite inexpensive so I I would say that CPU tend to be uh the highest and it's all about how you decided to deploy uh your your lowkey now at high scale obviously the bill can can be salty even for object storage if you're like storing you know terabyte or petabyte of data and when you query it you have to do a lot of requests to uh the object storage and that's what you get charged for you mostly get charged by number of operation uh and so depending on how many streams the more streams you're trying to itat and the longer the time range you're trying to it the more uh uh object operation we're going to have to do to uh to query I think that played quite nicely into that Community question um Nicole that we received so we received some Community questions beforehand um and I think you've kind of answered it there because they um they basically asked about querying and cost per query is Chunks per query a good metric to track the cost I'm assuming that chunks are S3 objects and the pric for a few cents for a thousand requests yeah exactly it's I think it's exactly like that few cents per uh per uh thousand request uh so yeah it is's a good way to measure the cost but again not it doesn't draw the full picture it does gives an idea for sure the more chunks you have the more expensive that query will be uh but it's also it also depends on for how long you've run that query on all those chunks and how how much CP you've burn in the cloud so yeah I think it's I think it's a good uh measure the more chunks you have for sure will be more expensive but it's not enough to really know exactly like if you have for instance 10,000 chank to fulfill a qu and it times out you may have not actually had the time to download download them and to use CPU um so I will tie it always to definitively uh chunks and also CPU that you're scheduling okay so I think we can say that a good query is one that is as performant and costeffective as possible while still being functional like it still does the thing that you need it to do for sure yeah okay and one thing that I learned um that seems very basic but I didn't know it until recently is that when Loki parses a query the query it it does so from left to right and so the order that you place them in matters and I mean that seems obvious but like I didn't really think of it that way so it it should be um you should put the you should know that it's going to be executed the way that you're writing it yeah and so you know go on you can talk about it no we talked about it in the beginning where uh the idea of L and the pipe was to kind of make a feeling that you're using uh CLI tool and the way CLI tools you you know with pipe works is that you execute something and then you move it to the next uh function using the pipe operator uh so that's the same idea they are executed like all your operation are executed left to right so you want at the left to have the the uh operation that are less expensive but filtering the most and at the far right you want everything that is expensive and you know uh what you what you're okay to execute on a small s set of the queries so on on that note then I've got my um favorite question to ask what are the top five mistakes you've seen people make in log Q what's like you know recurringly you see with customers and community members that you're like ah stop doing that yeah some of them are are some of those mistake that I see sometime I I wonder like did we really do a good job of adding those operator like you know and maybe maybe we made some mistakes so uh obviously the top the top one is using rigex too much I see people using rigex all over the place and there's one reason why we keep adding like alternative to rig xes like the the this patn filter and uh and optimize those also is because they're like sucking on CPU a lot uh so if you use rigex only when you really need them not because that's what you are more used to use it should be like there should be a reason for using RX and no other alternative uh the second one is definitively not using the right stream selector so something that is way too wide compared to uh what you what your query needs to do which ties into a bit maybe sometimes you're not having the right labels um the the third the third one will be uh using uh label format on Metric queries uh label format is is expensive so label format is for instance you want to add a new label based on the combination of uh two other label let's say uh I have cluster and environment and I want to concatenate them into one label uh and so you can do label format M uh and it's using the uh go template syntax so it's it's viable you can template viable like this so I can say cluster Dash Dash and then uh what was it uh what do we say m for instance do M oh I need to provide it a name uh new label right so if I do this operation so I'm just going to remove that one up if I do this uh I'm I'm basically combining into a new label which is called New the combination of those two with a with a dash so it's it's a template one of the big mistake that I see is overusing that usually you want to use that for mostly for like logo but I see that being used sometimes in uh metric coise if you do need to like play with labels and rename them please use transformation it's much better to do it uh on the graph on side once you have all your label and you can combine those label doing them on the quiz here it's very expensive uh the same applies for line format so line format re rewrite the line uh the the results so if you do line format and I'm doing uh the same template this time I'm going to Output as the line only uh those two label uh sometime I see like matri quiz uh that does like a count of a time and has a line format that that's a big mistake because it it's just actually con the same thing you can remove the line format it's not really required so misusing those label format and line format in metric qu are a big mistake that I often see uh every day in in the customer that work with us uh that also answers uh the question from Navin bot saying please also tell us what not to do while querying Loki what to avoid yeah so I think I've been big on don't use rxes already Yes uh so I've I've touched on one very quickly like don't use a label that it's on every stream it's not going to make it faster for sure um don't use so another one that I see very often is uh so we need the we need gon logs uh but oh we're going to do it with this one which I don't think it's just in logs but I think we have we have maybe a Jon log somewhere I'm just agreeing with Matt fine here I'm still a bit mind blown over the line format and the the label format that's a pretty cool feature I didn't know we had so oh yeah you can you can even add images if you want in that one serious geez um so that's J what I often what I often see is people doing the Jon pass and then based on the the those values doing like uh for instance I don't know uh m method uh method equal uh post so I think that's a that's a good that's a good use case but the gist and parel being at the top here will definitively run the on each uh log line so you really have to think about there's millions of log line coming in into the pipe and so if you can reduce as much as possible before executing a basa it's much much better uh and what I what I often show to people is that if you have a if you have something like this and that's what you're and that's what you're looking for then you better you better just put it put it as a filter right uh like this uh so you need to use the quote so this um is is this might actually lead nicely onto another commun Cas we just got from map um he kind of wants you to touch a little bit on structured metadata as well so with with that be a good would a method be a good example of what should go in structured metadata the by that key value pair for like say post get um and how do you query by structured metadata yeah so structure metadata is new and uh all done because we'll make them better uh over time and we are already working very hard nowadays to make them like a first uh citizen uh so the key to key to be aware about the structure metadata they were supposed to be solving the UN passing too much at query time time uh problem like this one exactly this this problem here uh if uh if I'm doing this on millions of log line that could be slow and not necessarily because of that operation but because of the the passing operation so structure metadata allow you to have data and then without passing you can start doing filtering like this uh right now I don't have that as a structure metadata so that's why I'm not getting any it but that's how you would use structureal metadata and so very powerful for that specific use case you have uh couple of fields that you are always using for filtering or aggregating on uh and and they may be high cality and they don't really fit like workload uh uh style they're not like a pod label or they're not a namespace label they're more like uh you know about the log itself then that's a good use case of structural metadata uh and so I definitely recommend to start start using them they're not necessarily nowadays super fast uh compared to like not using structure metadata but they're definitively faster than using parer but we're working out on on making them uh even faster every day so we're going toward using more structure metadata and if you use low key with open Telemetry you'll see that we leverage uh uh the structure metadata more and more uh so that's the yeah that's that's briefly on metadata so um we also got a community question and this person says any plan for log ql to support nested queries yeah that's a good question uh I I was you know I I I would love to say yes tomorrow yesterday that's what we heard yeah I honest honestly this is something we'll do U I just don't know when it's not it has never never really been prioritized like we always found something more pressing to do than this one uh but it's a it's a cool it's definitively a cool feature so I guess the idea for those who don't don't really understand what nested query is is you do a query that gives you a set of values and then you start filtering with that set of values uh so it's like one query to find uh data and that you're going to use for another query to filter with that data uh so we do want that we do want to do this I don't have an answer on when we'll do it right now we're heavily working on like the storage itself and so once we'll be happy with storage is probably where we going to start looking at uh those kind of ques and I guess sort of moving back to query performance um is there any sort of tools that we have in order to measure say you're you're running a query and it's running slow can you can you look at like the performance of a query um say in terms of like CPU U utilization Ram anything like that just to see how expensive the queries you are that are running is there any sort of like performance metrics um the in term of metric itself I think it won't it won't really tell you exactly like the L the metrics that Loki are giving are not really useful for give you know a set of qu that are slow um what we use internally the most is either tracing or logs uh so tracing is very very powerful uh if you have a lowkey cluster uh in your environment and you should definitively activate tracing if you have issues with query performance because not only you're going to learn about um how Loki works by looking at Trace like all the subqueries that are executed and you know how the index is uh is is being executed but also you're going to learn uh you know about why it's being slow um however we do have and maybe I can uh show you uh we do have logs uh in in uh in in low key uh and I can show you give me a sec I'm gonna switch tab could you imagine if um a log aggregation solution didn't have logs of its own yeah that would be funny should this stop so uh it's only funny if it's not us yeah so Loki logs heavily everything related to slow queries or queries in general uh and you can leverage those logs to try to understand what's going on uh we have one which is very popular the way we find it uh can you see my screen no you can yes right so it's always on the we always look at the qu frontend so depending on your deployment it might actually be the read pod or or just low key if it's a single uh deployment but we look at the uh let me make it bigger we look at the metric. go uh that one usually has a lot of information per query um so if I run it this is in in our Dev environment uh so I'm asking for uh the queries that are uh slower than uh 10 second and I can see uh a lot of information for each each line that I'm going to have here will be about someone that made a query so luckily we don't have a lot we just have four of them uh this is Dev so each each of those log line have tons of data about like cash it index how many inors have been quered uh you have even the uh Bloom ratio filter if you're using the bloom filter um yeah latency in general and then you have the qu itself somewhere hidden in the mix uh let me see if I can find that yeah so there's a bit of like log setion here how deep does it go do you then have logs for the logger of the logging platform yeah definitely so this was this was the qu this was like someone quering awvs uh with a specific filter so you can see that quer was uh slow well uh you can see how much data it was going so it was going like 250 Gigabyte and you can see the throughput which is usually a good measure usually if you want to know if uh a query went well or not this one was 10 second but it was 24 gigabyte per second to be fair that's quite fast you know we like L can go faster than that but if you if you're doing uh 251 gigabyte in less around 10 seconds it's not that bad um so that's how I will I will use Loki itself like query Loki for the logs uh and we can share that quer also uh in the in the future I I'll make sure to share it after yeah well if you could just put it in the private chat we can copy it over actually where's the private sh let me see and then we have a few questions that I'd like to go over to yeah let's go that's what you're here for right yeah because I think that's a that's a handy one for using lowkey to query itself pretty much and I guess you can also get things like the the throughput the total throughput of of logs at any given point of time not just for a specific one yeah you can use leld then on top of that specific log to start getting clusterwise uh metrics like how much should put per second of qu you you have uh and by the way we we uh we touch on that on like uh cost we uh on graph and a cloud make uh we charge or only charge if you're quing too much data and we charge on how much uh uh data you're querying okay so I just posted the thing you sent the query for that um in the chat as well naven bot says there's some extra labels coming on in the latest Loki that are detected level and service name are they also increasing cardinality and if yes how do you remove them a that's a good question uh and there's a funny story about those so first I want to make things clear detected label is not a label it's actually a structure metadata and fun part if you think it's a label a label then yeah that's that's good because you're using it as if you thought it was a label but it's not uh service name is a a label and it's not a bad one in our opinion because it's usually automatically using a another La label so it wouldn't increase the city because you already have a a label somewhere uh could be container could be application that is very similar but for us we needed standardization so we looking at all the uh label that you're ingesting and we try to find one that looks like a service name and we use that one so it doesn't increase directly the cardinality uh and as tons of customer that were you know quite reticent about the idea of having it but as we removed it for some they were complaining to not have it anymore it's good for customer themselves too to have like standardizations not just just for us it's also necessary for explore log so if you removed it you would break explore logs right yeah that's that's true you also needed for explore logs same for the detected level so it doesn't necessarily you can remove them if you want by the way um there's a limit I think uh that that you can use but to remove it uh so if if you want to remove it do it but as Julie said you you'll struggle a bit more with uh explore expore logs okay Matt fine has a question too as Loki continues to adopt native OTP what changes should we be aware of with log ql that's a good question to be fa right now not much um because the structure metadata in the Loc language itself are considered as like just labels right at the end of the day uh so they they you know already all the operation that exist in Loki are supported for the structureal metadata because they are you know thought as labeled um so not not a lot you can just leverage all of them as if they were label um I don't think we'll ever do specific feature for them but I'm not I'm not you know I cannot predict uh the the future yet um so I don't I don't think uh anything special for uh uh OTP just like structure metadata basically we we'll we'll you know you'll be able to use uh the OTP metadata as structure uh metadata I mean it sounds like no no changes to be aware of by Design because that means that you're using it just like you would label so that's that's what we're trying to get to yeah I think the only matter yeah that's that's correct I think the only maybe uh change we might do in the future will be on the UI to try to make it more clear that those structural metadata are like the same for OTP we might we might try to improve the experience for you uh in the UI but in in language itself I don't think it yeah by Design like you said there's no difference okay um can you talk about the different factors for for what makes a query fast and lowkey performance in general so we've talked about the the query the structure of the query itself we've talked about the labels from the beginning too um what about things like uh the the deployment mode yeah they want if people are wanting to make some make queries faster what can they scale up for if they're using the distributed deployment mode yeah um definitively if you want to if you want to go the fastest possible deploy it as we deploy it so it's in in distributed mode uh because that's how you can scale out the Reds uh the the most as opposed to the other deployment method for sure um what else could I say uh on the r path you don't necessarily need to uh scale much more than the quers themselves and then tune the configuration to use them so there's like a parm configuration in Loki which is I think chir parm or something like that uh and that that will tell how much paradism they could be for a quer but also there's another one that tells how many uh queres in parallel can be served by a choir usually you want to tie that to the number of CPU that you schedule for the choir but the choirs are the one that you want to uh schedule and I highly suggest you try to do auto scaling for the quiros um the qu front end is another one that we don't necessarily have to scale it that much more than like five to six but usually two is for most people is enough uh or two or three depending on on the if you want High be or not uh and then after it's like index Gateway maybe you might need to scale that a bit because it it it fulfills uh uh index quaries um so you know if you SE tracing the index Gateway struggling you might need to scale that up but other than that on the Cy path that's pretty much it you just need to uh yeah scale the the the the quos so that actually plays good into another Community question we had which was around um the results cach um what is actually oh my God I had that cute already on the same mind what cached and when criteria to enable the cash and how to tune cache configuration and behavior maybe we can sort of break this up but you know let's sort of talk about what it's actually used for and what's actually stored in the results cache and how do we use it at grafana labs in our s yeah yeah so the it's good question the result cach is is uh is a bit like trickster uh caching so Tri trickster is a project that allows you to cash tsdb or or time series database uh queries but the idea is that a metric query usually uh has multiple step uh multiple points for each steps and it catches it catches those point and if you're doing a bigger query suddenly if you're doing like six hour then 12 hour instead of quering the six hour you already did is going to use that so that's what the res cache is doing is actually going a on Metric Co sto stalls a lot of the metri co we don't we don't really cash log qu uh we actually only cash L queres when they don't match uh so if you're doing a filter query and it doesn't match anything we can actually cach that you didn't match anything for that query so that next time you do that query again we don't we won't search for the same time range and we search for the rest of time range but we don't cash lqu and I can explain why we don't do that is because most of the Lo qus are limited uh so they're limited to a thousand result so we haven't seen actually everything in that qu we just have in a thousand and so caching wouldn't be really useful if you Falls in between the time range because we don't know how much there is more in that time range uh so we don't we rly cach logs themselves just non uh match non match or Mets uh I highly suggest you use the rle cache it's quite powerful for uh dashboarding uh if you use a low Kei for a lot of dashboard it will make your query faster for sure because you're going to avoid uh redundant queries uh and when you're doing like heavy searches and the recurrent like the same searches I highly recommend to use the rle cache there's another cache it's the it's called the Chun cache um that one is also very important because it allows you to uh cache the chunks of data themselves uh so it's unrelated to qu just you know data the row data that we don't know from the object storage those one are also cached uh and this one you can scale up as much as you want and allows you to basically avoid hitting the object storage and we actually fill that cache as we flush data so when Loki inest data as it flush uh chunks of data it will also flush it into the uh chunk cache and so if you do rest and queries will actually avoid going to the object shortage and directly quate a cach so it depends on your environment but it's actually also a good one to use you can sorry goad I say you can tier the cach right as well you can have like a tier one cash in a tier two yeah yeah that's corre I think there's like multiple way multiple Cher there's like in memory caches and then M cash and yeah I think that's pretty much it but yeah on the on the note of like not going to object storage I know that when aqu query is run you first look at the Loki looks at the the ingestor and because they also have they also save some of the um some of the things that they've ingested so they might be able to query that they do query that first is there like a performance benefit to having more ingestor but for the read path like if you have more of those things that are already available does that does that have any effect on it at all um it's it's very so it's very r that you need to uh scale ingestor just for serving better ribs usually the inors are are actually pretty fast at fulfilling R qu they're mostly doing flushing so it's pretty hard that you need to do that but if you feel like your queries are faster for WR data it is because it's not quering the object stage and quering data that is in memory for sure uh but no I don't I don't I mean I wouldn't recommend to do that if you feels like if you are at this point you may have another problem somewhere okay um we are almost out of time but I I still want to go through a few questions if that's okay like even if it's rapid rapid fire sorry what's that and we're almost out of time but is it okay to do a few more questions yeah we can yeah no bo let's go great okay so we have a community question for loads around 70 to 100 gigabytes of logs per day which Loki which deployment mode is feasible simple scalable or distributed um I would so this is a this is a good one because I don't I don't know if we uh if we have talk too much about this but I think simple scalable one is we're slowly trying to uh decommission it and we think that distributed wall is worth it it's it's much better in in most of the cases so either go single binary when it's enough for you or go distributed because that's going to give uh give you much power and much much more fature proof so my will be distributed for sure great next question how do we optimize the Loki read path where Rex matching times out with Loki microser I think dist yeah distributed mode yeah that's a good that's an easy one because just don't do regexes use the alternative try to optimize your qu like with everything we said today uh we haven't touched on Metro Co and I'm sure you can find a lot of information in the documentation about that but try to look at your qu and and if you can do maybe a um before your UH rigex do just a search of a w that is also part of that rigex it will be faster so do as much as possible to uh reduce the amount of data that comes into each of your pipe by placing uh search at the top so you know contains something okay another one I'm looking to get my most expensive queries and whether they're efficient or not for example 1 GB of logs that fetch 100,000 chunks is not great it's not for sure so um I'm not good at doing math right now but usually a Chun of data should be around 1.5 to two megabyte of of w of data so if uh if if for one gigaby of log you have a thousand 100,000 CH sounds like you definitely have an issue somewhere and it's most likely related to having a label that should probably be a structure metadata somewhere so a label that inflate the the cardinality so I guess for for this person they're saying they're trying to find out which which those queries are like try to see if they are efficient or not probably the one that you you showed yeah the metric yeah the metric. goo the one the the the query I shared about the query frent um it's actually not just the query front but if you find that log line about that quy it does talk about how much uh chunks it downloads and then you can start figuring out the match and which one which which streams may be problematic there's also there's also a cool function in the CLI as well that allows you to like run a query and see which chunks it touches or like the chunks that come back from what I remembered like as I played with it so maybe look at the log CLI as well yeah the log CLI does output the stats so every query when you do a query it's good point Jay every time you do a query the API return return the result and statistics and lock C actually shows you uh as part of the output the stats and in graph I think there's like a there's a button qu qu inspector when you do qu and you can see the stats and and everything is there you can you can check check it out awesome that's great navine has another question if I'm getting one lck logs per second what should I increase lowkey right pods or increase the limits in limits config so one lck is a an Indian I think it's like an Indian unit of measurement that's 100 ,000 so I guess when you're dealing with that many logs per second what would you increase first or is the answer both or something else I would I mean the the the answer is a is it depends but to be fair looky in term of ingestion and I think that's why it's very powerful popular because it's very powerful in ingestion so you know it might actually do fine with that amount of log on one CPU so should you increase the ly right pod I don't know you should probably look at the CPU of that pod and if it's you know close to the Limit yeah you might you might need to do that so it really depends on on the log itself you can do one you know with just one CPU I mean actually I don't know if we have ever shared that but 90% of the community is using a single binary in L key so it's pretty it's a big statement that you know without actually scaling it does already you know scale scale well uh yeah the limits is uh so ask about the limits I mean the limits are mostly for operators trying to guard rails about mistakes that maybe some of your tenant are doing so yeah it's also possibly an answer like maybe you should if if there's one tenant in your cluster that is definitively sending uh bust of logs and you don't want that then there is like rate limiting that you can put in place using the limits okay I think this is the last one how can you test and Benchmark Loki queries before deploying them in production yeah that's a good one um so I have there's multiple answer to that to that question um if you uh so there's there's definitely we have a Casey script I think in the repo yes you got you get the right answer ding ding ding yeah so you can yeah you can use the K so we have a a set of k6 pluging I don't I don't remember how we call them but you can use those uh pretty sure they are in the repo I used them myself a long time ago uh and they are they are useful for like making sure for instance uh your ingestion is is doing well uh you can even simulate some queries I do find that if it's specifically for the re path uh so for the right path I think k6 does such a good job that you should probably use that if you want to make sure that you know you can substain a specific uh load on the r path there is one trick that is funny to do is try to match something and then not match something it's actually going to run L and it's going to run Loi and you're gonna have a sense of how fast it can go uh so you run a query on Loki and you say match F but also don't match F and what's gonna do is GNA be forced to quer everything uh for that stream and to execute every log line so it's going to unpack every log line and it allows you to see how fast it can go d D so was a bit chaos testing yeah yeah oh yeah don't do that in prodution for sure and on that note okay that good and then also um so like I think that was for for like benchmarking the the performance of loky queries right but we already mentioned that if it's if you're just trying to test whether a query works the the simulator would be really good for that you don't even have to have lowy deployed you can just have the the log there um so that's a handy little tool I think that's that's all the questions that we have for now and we're already over so thank you for staying a little bit longer um and thank you for coming on to talk about this can you come back please yeah we should talk about the meters we haven't had time to touch on them so there were a couple of them so yeah we can do uh we can do it another time for sure also all the reasons to use log format format label that was cool so many so many things yeah all right thank you so much everyone for being here and the four the three of you for joining me on a Thursday night for some of us yeah thank you i' have F at awvs uh reinvent Jay and we've had about 30 Loki people come up already uh to the booth so it's been crazy all good things so it's been awesome cool great and if anyone watching wants to know what this is a community call is like a pretty much monthly call when we have something to talk about especially new releases it is with an engineer or a couple Engineers from the Loki team so you can directly ask them questions and if it's about a new feature then we usually have the people that worked on it to talk about that so straight from the horse's mouths and and if you'd like to get involved then there are links in the description below to find out how you do that we you can also follow subscribe I hate saying that but like we will always have these um there will always be a notification for on the grafana channel too when we're going live and this will also be available immediately afterwards so you can always do that if you want to get started and just want to hear people talk about new things and there's always the community stuff as well you can go to the community forum and go into the Loki section and we'll make sure that those questions get answered anything else from anyone before we go think you said it all Nicole absolutely CR all right thanks everyone have a good rest of your day wherever you are in the world bye

