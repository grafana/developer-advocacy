# Tempo Community Call June 2025

Published on 2025-06-13T13:45:57Z

## Description

Grafana Cloud is the easiest way to get started with Grafana dashboards, metrics, logs, and traces. Our forever-free tier includes ...

URL: https://www.youtube.com/watch?v=RIK0OY0QW9o

## Summary

In the June Tempo community call, attendees discussed several updates and features related to the Tempo 2.8 release. Key presenters included Joe, Kim, and Tiffany, who shared insights on the release's features, such as breaking changes, performance improvements, and new metrics capabilities. They highlighted the removal of serverless support as a significant change and discussed enhancements to the TraceQL language. Additionally, a new MCP server for Tempo was introduced, allowing for integration with large language models (LLMs) to facilitate natural language queries about tracing data. The session featured a demonstration of using this LLM functionality, showcasing its potential for generating queries and providing insights into trace data. Community members engaged in discussions about future developments, documentation needs, and the architectural transition for Tempo, particularly concerning Kafka integration. Overall, the call emphasized collaboration and feedback within the community to enhance Tempo's capabilities and usability.

# June Tempo Community Call Transcript

Hello everybody, and welcome to the June Tempo community call. Today, we've got a number of exciting topics to cover. So without further ado, let's jump in. As always, if you have any questions, comments, or anything you'd like to discuss that’s not on the agenda, please raise your hand or put it in the chat. We'll be more than happy to address it.

Let me share my screen here momentarily. Or Joe, are you just going to take over right away? 

**Joe:** Yeah, maybe I should just share because I do have something. 

**Speaker:** All right, Joe, why don't you kick us off?

**Joe:** Sure. Oh, Chrome's being weird. Can you all see my notes?

**Speaker:** I can see the Tempo community call.

**Joe:** Awesome. Let's talk about 2.8 then. Do you want me to go ahead?

**Speaker:** Yes, you're sharing.

**Joe:** 2.8 is technically half cut, I suppose. The image exists, and the release notes are there. Thank you, Kim. Everything's done except the blog post. I usually wait for the blog post to share in the public Slack channel, so you have not been informed of this yet. But everything exists, and Tiffany, the blog post is coming soon.

**Tiffany:** Yes, it’s in preview. Just need to double-check that nothing carried over incorrectly, and then it should be good.

**Joe:** Cool! So maybe in an hour or so, the blog post will exist, and it’ll be a good chance to highlight the important stuff. But since you're on the community call, you might want a deeper dive. We're not going to cover everything since it is a big release, but we’ll touch on some points here. We have talked about a fair amount of this in the past, so if you’ve been on previous community calls or subscribe to our YouTube channel, you probably know a fair amount of what we're about to discuss.

### Breaking Changes

Breaking changes—always a few, but these are generally very minor. I don’t think any of these are super critical. You probably aren’t using a lot of the features that we’ve changed. Some metrics were renamed, and the largest breaking change, which we announced a long time ago, is the removal of serverless. Hopefully, you’re not using serverless, as we’ve been discussing this for a long time. 

Other than that, I don’t think anything critical is here. Jenny Fam extended the behavior of our max attribute setting, which might be another change you’re using, maybe even unknowingly. But these are the most significant changes in Tempo 2.8.

### New Features

Feature-wise, there are more metric features, including some overtime top K and bottom K metrics. We've added a "most recent true" option, which lets you get the most recent results. This is a query hint, which is a weird element of the trace language we don’t talk about much. We added it as a query hint to guarantee getting the latest results, which has been a long-requested feature. 

Our goal is to eventually make it the default behavior, but we're not quite there yet. So check out "most recent is true" if that's something interesting to you. We also added support for the parent ID span parent ID in TSQL, along with lots of enhancements, performance improvements, and bug fixes. 

We've been heavily focused on TraceQL correctness for the last month or so and will continue for the next couple of months. We made this feature-rich language as fast as we could but ended up writing some bugs—just a few! We now have people focused on tightening up the features, getting good tests in place, and really improving the reliability of the language. 

### Feedback and Questions

The core functionality works well, but it’s the weird combinations of parameters and specific selectors that can sometimes trip it up. So, you should see in Tempo 2.9 or maybe 3.0 a long list of bug fixes and enhancements. 

**Siraj:** I had a question about nil and trace.

**Joe:** I don't want to talk about nil and trace. Surj, I'm not even sure I remember all the details except that we tightly constrained how it can be used. 

Let me show you the PR if you're interested. 

**Speaker:** Can you find the PR?

**Joe:** Oh, it's here. Thanks! This PR goes into detail about how we handle nil and how we should handle it moving forward. 

So, 2.8 is exciting! Use it, and hopefully, you'll see some nice performance improvements, particularly in memory usage for those with larger traces. Please give us your feedback and let us know how it goes. 

Do we have any questions or thoughts?

**Speaker:** If I can jump in for a second…

**Joe:** Oh, sure!

**Speaker:** I’d say if you want a good preview of what we've done, our 2.8 release notes are pretty solid. I want to call out that there’s a cool image because I like images in things. 

**Joe:** Sure, go ahead and share your screen if you want.

**Speaker:** There’s an image showing memory and performance usage, which I think is awesome. Marty did this work, finding that we were over-aggressively pooling slices of values for parquet. We didn’t notice for a long time, and as we started taking bigger traces from more customers, this got out of hand. We fixed this, and now the memory usage of compactors is significantly more predictable and lower.

**Joe:** That’s great to hear! The release notes are fantastic, and Kim puts a lot of effort into them. Tiffany wrote our blog post this time, and there’s going to be a video, which is exciting. 

Any questions about 2.8? You’re welcome to hold these until the end or ask now. This is a casual presentation, so feel free to jump in. If not, we can move on. 

### New MCP Server for Tempo

We built an MCP server for Tempo. I suppose I'm demoing that. We have a PR for this, and an MCP server is a protocol published by Anthropic. It exposes tooling to an LLM in a way that allows you to call into something, and many databases are adding support here.

We have a PR now to add direct support into Tempo. We got our fun robot emoji, and this will add a shim between the query front end and the LLM. 

**Joe:** Let's start with Claude. Claude is an agent interface and is great for coding. It knows Tempo exists and has a list of tools. It will call them if it thinks it’s the right thing to do. 

Now, we can ask simple questions. Does anybody have a question? 

**Speaker:** Can you bump up?

**Joe:** Oh, yes, thank you! 

Does anybody have a question for Claude about traces or green traces?

**Speaker:** Show me only green traces. 

**Joe:** Sure, let me see what it does. 

This is just synthetic data created by a K6 script. If you enable the MCP server in Tempo, it warns you that you're about to pass your tracing data into an LLM, so think about it carefully. 

Claude is now looking for attribute names and values. It’s interpreting “green” as “good.” 

It found some attributes, and now it’s looking for HTTP status codes. It’s doing a normal trace search and found 22 traces. 

**Speaker:** Can you ask it for angry traces? 

**Joe:** That’s an interesting one. It’s now looking for things over 400, which is the range for errors. 

This opens the door for what I want to do next in Grafana, which is to expose this in the Grafana UI. Instead of using a CLI, I want users to be able to type natural language queries directly.

### Future Integration with Grafana

**Matt:** Will you be able to ask it in natural language if it’s integrated with Grafana?

**Joe:** Yes, this is an open protocol, and there’s definitely an opportunity here. My next task is to get this done, and I’m very excited about this cool feature. 

**Speaker:** Can you write a haiku about my slow endpoints?

**Joe:** Sure! Let’s do that. 

It’s important to onboard new developers easily. For someone new to tracing, I think that’s where it shines. It can tell you how services interact and what is slow. 

**Speaker:** How do you find errors and suggest fixes? 

**Joe:** Let’s see if Claude can do that. 

It’s doing a pretty good job identifying errors. It interprets status codes correctly and suggests looking at spans. 

I think this will be valuable when integrated into Grafana, showing users what it did in a sequence of steps.

### Community Engagement and Questions

**Speaker:** What are your thoughts about the integration? 

**Joe:** I think it’s very valuable and fun to use, and I’m excited for the community to engage with it. 

**Andreas:** What is the new role of the inester in the new architecture? 

**Speaker:** The inester holds data for a short time while the block builder builds the blocks. 

**Andreas:** What about the future of the monolithic tempo? 

**Speaker:** We want to make tempo deployable and easy to use in monolithic mode. 

**Andreas:** Would it require Kafka? 

**Speaker:** We don’t know yet. We want to ensure there’s a solution that doesn’t require Kafka, but we need to figure it out. 

### Closing Remarks

Thank you all for joining today. We covered a lot of ground, and we’ll see you next month. This will be uploaded to YouTube, probably tomorrow or Monday. If you have any follow-up questions, feel free to reach out. 

Thank you! Bye, everybody!

## Raw YouTube Transcript

Hello everybody and welcome to the June Tempo community call. Today we've got a my number of topics, so some that are pretty exciting. So without uh further ado, let's kind of jump in. As always, if you have any questions or have any comments or have anything you just want to talk about that's not on the agenda, raise your hand, put it in chat, find a way um to get it to us. we'll be more than happy to uh talk about take a look at or do whatever is requirement. All right. So, let me share my screen here momentarily. Or Joe, are you just going to immediately take over? I guess technically two first.
 Yeah, maybe I maybe I should just share because I do have a thing. Okay. Yeah. All right, Joe, why don't you share? Kind of kick us off here.
 Sure. Oh, Chrome's being weird. Maybe hopefully I can. Can you all see my my notes or this?
 I can see tempo community call. Awesome. All right. Talk about 2.8 then. Do you want me to talk about 2.8?
 Oh, you're sharing.
 All right. Uh 2.8 is technically like half cut, I suppose. Uh the image exists, the release notes are there. Uh thank you, Kim. the release. Everything's done except the blog post and I normally kind of wait for the blog post to throw it in the Slack channel, the public Slack channel. Uh, so you have not been informed of this, but it all exists. And then Tiffany, the blog post is coming soon.
 Yeah, it's in preview and um, just need to double check that there's no nothing that carried over incorrectly and then it should be good.
 Cool. So maybe in an hour or so the blog post will exist and we'll it'll it'll be a good chance to highlight um you know of course in the blog post the stuff that we think is most important but you're on the community call so you probably uh you know maybe want a bit of a deeper dive. Uh we're not going to go into all this because it is a big release but maybe we'll touch on some of the points here and we have talked about a fair amount of this in the past. Uh so if you've been on previous community calls and or you know subscribe to our YouTube channel thing list and you know aggressively keep up with uh tempo community calls then you probably know a fair amount of what we're about to talk about. Um so breaking changes always we have a few right these are generally very minor and I don't think any of these are uh super important you probably aren't even really using a lot of the features that we have changed some metrics renamed perhaps um this is the largest we announced this a long time ago though the removing of serverless um so hopefully you're not using serverless we've been talking about taking up forever it's probably the biggest breaking change um other than that I don't think anything super critical is here. Uh uh Jenny Fam extended the behavior of our max attribute uh uh setting. That's maybe another change you might be using maybe uh even unknowingly. But th these are the uh most important or the biggest changes I guess in the uh tempo 2.8. Uh feature- wise uh more metrics features some over time top K bottom K. We've added most recent true which lets you get the most recent results. Shocker. Uh this is a query hint which is a weird element of the trace language we don't talk about much. Uh we added it as a query hint. This ability to guarantee getting the latest results uh to give us some more flexibility and eventually removing it. I think our goal is to make it default. It's just not quite there yet. So we have this ability now which has been a request for a long time to not get any results which is the default behavior but to get actually exhaustively search backwards in time until you have the most recent results. So check out most recent is true if that's something interesting to you. Um and then we added support for the parent ID span parent ID in TSQL. Lots of enhancements. Not going to get into these. Lots of performance improvements. lots of bug fixes. Uh we have been pretty heavily focused on um TraceQL correctness the last month or so and for the next couple months. That's a big deal. We made this giant crazy featurerich language um as fast as we possibly could and happen to have written some bugs, you know, just a few. And we have people focus now on maybe slowing down the features. we feel like we built a solid feature set and instead get all the all the things tightened up, get some good tests in there and really um uh write uh improve the reliability of the language. So the core stuff all works. It's always these like weird combinations of parameters and uh particular combinations of selectors and attributes and such that can sometimes trip it up. So yeah, you should see in tempo next 2.9 or maybe 3.0, maybe 2.9, maybe 3.0. Um, you'll see probably another long list of bug fixes, uh, enhancements, those kind of things. Siraj had a question. Nil and trace I don't want to talk about nan trace. Surj I'm not even sure I remember all the details of n and trace except we explicitly very tightly um constrained the way it could be used. Instead, it used to be a very uh kind of just like an element of the language you could use in a lot of different places. Let's see. I will show you the PR if you're interested. Can I find the PR? I thought I maybe not whatever. There's a PR that talks in detail about how we handle nil and how we should handle it forward. Uh you find it sash if you find that we'll post it in the thing. Um oh it's here. My bad. It's this one. Thank you. Uh so right this is kind of gets into the details of like disambiguate use of non existence and equality. Uh so yeah if this is like interesting to you shocker if it would be but if it is jump into this particular PR if you too want to know all about mill handling and trace this PR gets into the details cool 2.8 Right. It's exciting. Use it. Hopefully, you'll see some nice performance improvements. Really nice, um, memory improvement compactors for those of you with larger traces. Um, uh, give us some feedback. Let us know how it goes. Uh, do we have any questions, thoughts?
 If I can jump in just for a second.
 Oh, sure. Yeah.
 Uh, I would say if you want to get kind of also a good preview of what we've done, our 2. release notes are pretty solid and especially um I really want to call out that there's a cool image because I like images in things. Uh and I can probably share my screen if you want me to. That way it's not or did you click on the link? You clicked on the link. There's an image there of memory and performance usage which I think is awesome. I just wanted to call out because I really like seeing these images in release notes because they're better than words at least for me. They are better than words. Uh yeah, this shows Marty did this work. He found uh we were over overaggressively pooling uh slices of values for parket. And so we didn't notice for a long time and we just started taking bigger and bigger traces from more and more customers and this got out of hand. And so we fixed this and now the memory usage of compactors is significantly more predictable uh and also lower which is good. I think we all agree that's good, right? But it's better. Sometimes sometimes you want the graphs to do this. Sometimes you want the graphs to do that. Just got to got to figure out what direction you want to go and make sure they do it. It's easy. Uh all right. So, uh release notes are fantastic. Can puts a lot of effort into these. Um Tiffany wrote our blog post this uh this time. There's going to be a video. Uh it's going to be exciting. Any questions about 2.8? You're also welcome to hold these to the end or ask now. This is not obviously a super prepared presentation. So, uh it is casual. Feel free to ask. If not, we can move on to uh we built an MCP server for Tempo. I suppose I'm demoing that. Um so, we have a PR for this. An MCP server is a protocol I believe published by Anthropic. Question mark. Does anybody know for sure? I'm getting some nods. Okay. Um, it's a protocol to expose tooling to an LLM in a way that allows to like call into something. And a lot of databases are adding different kinds of support here. And so we have a PR app now to add direct support into Tempo. We're kind of staring at now. We got our fun robot emoji. Um, and this will basically add a shim between the query front end and uh so and the uh and an LLM. So it's just kind of like is a protocol supported now by various tools like cloud code uh which we'll look at in a second. Um so uh I have we're going to look at cloud code. We're going to look at this also. I wrote like an opinionated agent um pretty quickly using cloud code. I was like please just go write this and it did okay. I had to kick it in the butt a bit but it mostly did it. Um and also uh we'll look at just kind of like chatting with our traces I suppose. Let's give this a shot. Um, so this let's let's start with claude. Claude is a cloud codes are just an agent uh interface. It's great for coding. It's currently my favorite way to interact with these things in coding. Uh, but we have set up tempo is an MCP server. So it knows this thing called Tempo exists. Um, it knows that it has a list of tools. um and it will call them if it thinks it's the right choice or whatever. If if uh if it thinks it's the right thing to do, it'll actually start calling these. I'm exposing documents. Uh so it knows how to write trace QL queries and then like okay, how getting attribute names, getting attribute values, getting trace by ID, metrics, of course, search, and then we can like ask simple questions like oops, I didn't like it. Does anybody have a question? Anybody have a question? That's a good one. Joe,
 can you uh bump up?
 Oh, yeah. Yeah. Thank you. Uh, good call. Does anybody have a question for Claude about traces for most green?
 What is the most Show me only green traces? Sure. Show me only. I have no idea how I was going to interpret that, but sure. Before we It'll It'll It'll do some stuff. Um, this is the trace data it's working on. This is just some synthetic data. Um, if you do enable the MCP server and template, it warns you that you're about to potentially pass your tracing data into an LLM. So, think about it hard. Um, but this is the data it's working on. So, this is just all synthetic stuff created by um, you know, like K6 script essentially. But, thank you for the uh, call on bumping this. You do have to tell it a lot like, oh yeah, I super want you to do what you know the tools. Um, so first it's got the attribute names. I don't It's going to maybe look for some attributes probably related to color because I asked for green ones. It's probably going to interpret green as um it's probably going to interpret green as uh uh good. In fact, that's kind of what it's doing. This is interesting, too. It asked for attribute values, but it did incorrectly. It asked for HP status code. It got an error back and it was capable of figuring it out. So, it added span dot in the front. Um, so it asked for just this which doesn't have a scope, but when you ask for attribute values, you need a scope. So it got that error and it said sure I'll add span in front of that. Um, so now, yeah, it's looking for HTB status codes 2299, which are green. It supposes it doesn't really know. Uh, but yeah, there you go. There's your green traces. It's doing a normal trace search. So it did status code greater than 200 less than 300 or greater than equal 200 less than 300 which is we all know like the stat range of status codes that's correct and a good and now it's telling me oh I found 22 traces um here's some stuff that's happening here's some of the uh what is it uh here's some of the uh operations I suppose that are currently succeeding that are green all right we could have some angry traces So I presume it's going to interpret angry to mean slow or in error perhaps. Oh no. Okay. So we all know that LM is very context oriented, right? I said angry and it's sticking with HTB status code. Now it's looking for things over 400 which is you know four and 500s is kind of the range for uh sadness and status code. It's doing another search. Now it's just looking for things over 400. now that it's kind of like bound to a path in a sense like it's it's using its previous context to kind of inform what it's going to do moving forward. But I love this because um I mean I don't want to really generally ask for angry traces. Uh but this really opens the door for what I want to do next in Graphana which is not this interface is great I suppose but I really want to expose this in the graphana UI. So like in explore you can do natural language queries. I assume people will not ask for angry traces. because I assume they'll ask for things like, you know, let's try something more normal. What are our slowest HTN points? Oops, I spelled that wrong. It probably won't struggle with that. Oh, sorry, Matt. You opened it. You open rested your hand. I didn't see.
 Yeah, I was wondering like what if you can you ask it if there's any similarities between the 403 trac uh bad traces to the uh like whatever it was complaining about there.
 Okay. Yeah. Uh we can do that in a sec. Good call. Um, this is an interesting thing, too. Claude is pretty aggressive. It thinks it knows everything. So, it was like, I know how to write your slowest endpoint query. I'm going to write a metrics query. And it was wrong. Syntax error. And then it tried again. Just I don't know where it's getting these things from. It's just kind of like trying randomly, right? Uh, and then it asked for the docs. Where did it do that? Oh, it did not. It did a search. My bad. It still hasn't figured it out. Come on. Ask for the docs. Generally, it asks for the docs after a little bit. That's why I have this agent. We'll briefly look at this one. I wrote a custom one and it's because I can set a prompt and the prompt says, "Go ask for the docs, you dummy, before you start trying to write TraceQL queries and it does a lot better job of uh writing the queries." Um,
 what is a prompt, Joe?
 What is a prompt? I think I know prompt is a system prompt, I suppose. I'm I'm learning a lot of this myself. I'll tell you, I'll be honest with you all. Like, I use these tools like everybody else does to some extent. Um uh and the system prompt is like a set like a set of text that pushes the very generic LLM in a specific direction. So I can share with you the system prompt. It's all open source for uh this thing here. If I just go to main uh like I said this is pretty slap dash. I say things like the current time is so I passed the current time. The first time I was like hey please write a query over the last three hours. It thought it was sometime in 2024 when the LLM was generated, right? Because they build the models every few months. Um, but so I actually have to tell it the time right now for it to make correct queries relative to now, which I thought was kind of fun. Um, and then, you know, be concise and to the point. I'm just kind of prompting it, giving it um guide rails about what I want it to do and how I want it to behave so that when a person just comes and start asking questions about traces, they get better uh responses. And one of these somewhere in here is like uh use the docs. I also say things like if you're unsure, ask a question instead of making a guess. If you're if you're asked to do something not in the tools, just say I don't know what to do. Like claude will just try random crap. Uh unless you give it permission pound it to be dumb a little bit. Um somewhere in here. Oh, you be quick to use the docs before attempting to write trace, which makes it much better about like asking for documentation. I suppose I should bump that up to asking for documentation and um writing better queries. So this in this case it wrote some bad queries and just gave up. Like it didn't even bother the doc. Sometimes it does. A lot of times it does. Uh why don't you check the docs?
 We have a question Joe.
 Yeah. Matt Fine asked, did you say that Grafana Explorer if hooked into these MCPs that you could ask in a natural language? And is that feature coming soon? Uh so this is MCP right it's this open protocol uh it uh I've used it in cloud code obviously we're doing that right now I've used it in my custom agent I've used it in cursor a little bit which was kind of cool I was in cursor and then you could like point at a code line that had a span to find and say go find some of this crap in tempo please and it would it' do a good job of finding that. Um yes though to get to your question finally is because this uh to this is an open protocol there's definitely a that's kind of awesome there's definitely a opportunity here right to continue to integrate and absolutely the next thing I want to do and soon is strong Matt I don't know you but uh I work in software and soon can some of you even know what soon means right so sure soon uh I really would like to put a thing here I don't know what to call it tracing agent LLM thing and I want you to be able to type exactly the same things I was typing. Can you find slow traces, please? This instead of instead of this span dot or or span, I can't even do it. Duration greater than one second, I want you to be able to type a natural language query, find a slope trace, uh, and then it will do something very similar. It's going to call into the tools. uh it will then render it a whole lot more nicely than the CLI, right? It'll render it. Um that's right. The year Linux is the desktop OS of choice will be the same year we will present this natural language. But yes, this is my next task. This is my number one next task is to get this done. Um so I am very excited about this. I think it's a very very cool feature and uh it's got so much value so obvious value so immediately that uh absolutely it's what I'm going to work on next and I haven't done a lot of graphana devs so one of the biggest hurdles is me going to be you know kind of learning some of this is cool though I did some we have some fire emojis which is neat it's using the different things that's kind of cool oh so I did say hey why don't you use the docs please and it's oh yeah I'm really stupid I should use the docs and then it asked for the docs and then it wrote a perfect metrics query instantly I'm like, okay, sure. Uh, we prefer face emojis uh to fire and color. Thank you, Mario, for suggesting angry emojis. Let's see if it will uh nicely now give us something. It's frolicking at the moment. Okay, there you go. We got the terrified person. Uh this dude down here is very excited and just yeah got a spectrum of emojis right there for you. Are are you satisfied Mario? Would you like uh
 yes keeping keeping the context?
 What's that?
 Yeah, keep in the context. Uh can you ask for angry traces? I want to do to know what those were.
 That's a good point. That's a good point. You think since I asked for angry traces it would have used the angry emoji, but it didn't.
 Yeah. Can you write a This is where it gets fun, right? Can you write a ha coup about my slow phrases slow end points? End points. Uh, oops, I forgot the word about article cart. Look at that. 1.8 seconds. Users click away. I assume that's a haiku. Somebody who can count syllables or knows what a haiku is is welcome to correct me. Um, okay. So, that's all fun. But here we go. Article to cart. I don't even have to spell this right. I don't know why I'm bothering. Is slow. Can you suggest improvements? Now, this gets a little hairy. I've done this on things I actually know, right? Versus things I don't know. This is just in synthetic data. We all know, but I've done this on things that are now it's going to get an actual trace, which makes sense. It wants to analyze the trace structure, right? Um, uh, sorry. I've done the things I know a whole lot better and they're way more complicated and it's like 80% right, 50% right somewhere in there and then like a couple misleading things. It's not perfect. Uh and sometimes just very generic um very generic kind of suggestions instead of you know direct kind of you know nuanced like educated uh more educated suggestions. I think for someone who um I think for someone who is new to tracing or new to their services, I think that's the thing I'm most excited about. Like I am a new developer. This is this is what I typed. I I don't know Pyroscope very well. I went to the Pyroscope. I put it against Pyroscope traces. I said I am a new developer. Tell me how tell me how my services interact. I think this is where it's really shines things like this. Um because this will onboard people a lot easier. Be like, I don't know. Uh use our traces and go ask tempo real fast. It'll tell you what talks to what. Um it'll tell you what's slow, what we need to focus on. And it it'll do it does that pretty well. Once you're like, okay, give me deep analysis is I think where things fall apart a bit. And where the expert is going to uh you know, have more ability to to know what's happening. So it does this. It doesn't do this. Well, um, it's just looking at these. I I really wanted to use descendant operators. I I want to find a prompt that forces it to start doing like descendant different kinds of descendant queries when you ask these kind of questions. And it doesn't quite do that, but it it does something, I guess. So, there you go. All right. Um, uh, real quick, this is cool. So, I really wanted to I really wanted to explore the like agent loop. Um, so I wrote my own. Um here we're going to ask it something very similar. Show me um traces that are in error or find find errors and suggest fixes. Let's say that find errors and suggest fixes.
 Joe, can you uh enhance? Oh yeah, enhancing. Thank you for the reminder. Uh enhanced. Um so this I think it does pretty well. It's now okay. It found status codes. It did interpret it as HTTP status codes. Sometimes it's very variable. Sometimes it uses span status error. Sometimes it uses in terms of status code errors. It like it takes different paths each time. Um it is finding a a 403. This is again synthetic data. So we occasionally have an error, right? So it's found a 403 and it's probably going to tell me something obvious to do with the 403. Um uh but I enjoyed doing this because I wanted to explore the agent loop because I do intend to put this in graphana and I can explore that a whole lot faster as a go CLI than I can in Typescript front end go backend kind of thing that I'm less familiar with in the graphana world. So I did this as a way to get up to speed on how this might look and then the next step is to put this in graphana. Yeah. Um so yeah it's a little messy. I tried to say I mean another thing I had fun with is I kind of vibecoded I almost hate saying that vibecoded this whole thing. Uh I it made some awful awful mistakes. I had to kind of fight it off a couple patterns it was really hot on that were just bad. Uh but for the most part I was able to let it do its thing which was fun. Uh I like all of this styling I was like uh it's very plain. Can you style it please? And then it added a million colors and emojis everywhere and I was like okay please back off that it's way too much. and we found a balance. Um, so this is tempo is very difficult for me to use these tools regularly. I do for certain activities, but I can't just say add a new thing. I can do it so much faster and easier. Um, but this is a total blank slate. So, one of my goals in this total blank slate was to explore explore that kind of development and I had some fun. So, here we go. It's saying authentication is failing. I don't know. It's it's Oh, it's it's checking. It's suggesting go. I mean, this is not even a go program. I don't know why it's suggesting go. It's if length of tokens is who knows what this means. So, it does kind of go off the reservation occasionally. Uh, I don't know why it thinks I mean I don't know why it's giving me things to do in Go. This is uh this is fake data created by K6. So, there's no program creating this, but it's it's suggesting some weird suggest uh weird code changes, but it did correctly identify that we have some for3 spans. it is a um issue with authentication and please go look at the spans right uh and this will look a lot better in the in graphana because you'll actually see the trace nicely and then you could use that to link to logs or all the other integrations we have in graphana right um can you graph this is fun errors over the past three oh I I just like this because I found a asky graphing um um an asky graphing uh utility like I so I just had fun doing that. I wanted to show you all that I can it can actually dump graphs into the console which I thought was fun. Um all right, sorry I have just blabbed for quite a bit here. Um any fun questions to ask? I kind of just enjoy coming up with random things because it gives you random answers and it can be exciting to see what it does with somewhat sensible questions or questions about how we intend to work with this or how it works or anything really. So for um what you had a little bit above it was showing like things that were a query and then there was something that's so it's partially queries and partially other things because I saw one that was like name and it gave you a specific name for something.
 Right. Right. So let's see if I can go to the top here. There's a set of tools that you expose and this is all MCP like I didn't invent any of this. So we expose tools and you say I have a tool called get attribute names and here's the description of what you can do with that and this description is critical and here you can get attribute values if you want um you can get traces or whatever and you tell it the tools you provide a description this along with the prompt and then like the LLM magic will have it the LLM itself is like I want to get attribute names you've asked me a question that makes me think I need attribute names before I proceed. So when I said find errors and suggest fixes, it asked for the attribute name because it was probably looking if it found like something called span. It may have gone for that. But it found span.h.status code and it locked in on that. And then it was like, okay, give me some values, please. What are the values of that particular attribute and then uh what did it do? Oh, and then it said it had 403s. It asked for attribute names again. I couldn't tell you why. Um, but it did. And then it was like, okay, I don't give a crap about that, I guess. So then it did an actual query. Uh oh, it's talking about exception attributes, but it didn't go for that. So maybe a different path would have been to go look at the exceptions and on a different day, it may have preferred that. If I push it directly in one way or the other, it will do what you tell it. But if you very generically ask it for errors, it will find different paths often if you don't give it like I want HTTP errors or I want gRPC errors, right? So like if I went over to uh like my tempo dashboard, if I copy the stuff after the colon onto the query and just dump that to Trasql, that should give me something. But if I do like the if you give just a name, will that work in TraQL?
 No, these are the just the other endpoints. We have an endpoint for names. We have an endpoint for values. These are hitting like bespoke endpoints. Here's a TSQL query. So when it says TraceQL search, now it ran a TSQL query. That is a valid query right there. And this is what Tempo serviced. Here's this is a trace by ID lookup right there.
 Um here's another trace query. It's looking for exception events. That's kind of cool. It did it did have actually used the events in the end, didn't it? Um um what's the reason it's displaying it as a map or the data type?
 Oh uh this is when this is a detail of the Go MCP implementation. uh when you call a tool you just provide a map of string to any in this particular implementation and the string is the name of a parameter and the any is the parameter it can be like complex objects it could be a string you can like define all of the different um uh when you build the tool itself if you want to see that uh let me do my enhance because I remembered this time uh MCP tools oh no bad I think it's in this one mc CP when you define a tool you say those are resources tool you're like here's a tool it's called traceql search um there's a required parameter called a query uh its type is string there's a not required parameter called start and end if you don't provide these I'll search for an hour but if you do want to do a specific range you can provide start and end in this format and it does all this very well if you ask for yesterday it will correctly put a start and end for yesterday and scope your query correctly Um, and so this is this is how the interface between the LLM and the code works. So the the PR is essentially like exposing tools that are a little bit guesswork on my part. I expect this to iterate over the next months to get better and better as we understand how to do it better. Um, and then it's a shim, right? So it's the front end and then I get the call the handle call for the tool and I basically just translate that into a trace into a go or a tempo object search request and then pass it into our normal pipeline HTTP pipeline which just handles it like it normally would and then I just straight dump the JSON back at it. Um, I was told though, um, I did talk to Sirill who has a lot more experience here and his suggestion is to not give it JSON, but to find more compact summarized formats and it will reason better about them. So, that's a future thing. Like I said, I do think this is going to get better uh, as we understand more about how to build the shim correctly. This is like this was a week of work or so. This wasn't too bad. Cool. I have a question, Joe. So on the tooling when you you when you provide all the tooling up front. So let's say as you go and you ask it different sort of question and rely on another tooling it is smart enough to go fetch it or do you have to continue to feed it the dependency tooling chain? uh it will repeat you like I think we saw two things you talk sometimes it will use the tools in a way that I think is correct sometimes it won't is that thing like uh you remember in claude where there was no prompt it it ran three broken traceqlmetrics query and then gave up and just did a different query and then I was like should you use the docs like I prompted shouldn't you use the docs I was like oh yeah and then it pulled the docs it used the docs and it wrote a good query
 uh this side where I um uh where I had the prompt already. It used the docs correctly. If you you can prompt it to use a tool like hey use that tool right there and it will use it. If you don't it's going to make I'm going to put judgment in quotes here. Judgment calls about uh what to use and why. And it'll string three or four in a row while it's reasoning. And then after it's satisfied, whatever that means, uh it will dump a big old text blob back at you like this is what I discovered after running these four queries. And you can see its reasoning in the middle, which is fun. And I think this is useful for folks who are new to tracing or new to TraceQL or all these tools. It'll tell you like, I'm asking for attributes for this reason. I'm going to focus on HTTP status code for this reason. Uh I'm going to use metrics now. And so even this I think is going to be valuable um when it's in graphana like just to show people like I did these five things in this order. I think it's going to help people learn traceQL as well as help people who don't ever give a damn about TraceQL or any of this. They just go show me slow queries. Like why am I why am I typing 16 characters to learn slow queries? Like I just want I just want to see slow stuff. It will be fill that gap awesomely. Just show me slow stuff please. I don't care. um which a lot of people want and that's totally a fine way to use tracing. This is really cool. Yeah. Let's see. Write a K. No, Kurt Vonagget gets too depressing. Who's a good athor? Give me a decent author here. J write a James A. Corey style short story about my curtis. Is that his name? The guy who did the expand stuff. Is that Cor James A. Corey?
 I mean pseudo name, right? It's two guys. I think it's two people, right? I don't know their real names, but it's a pseudonym. Let's see if it gives us a good sci-fi adventure here. Lord knows how much money I just spent of graphana budget.
 Well, while that's churning, I'm going to go off script and ask something of our non-G graphana people here uh for uh how would you use it or how would you like to use that assuming this was integrated in a graphana front end? What kind of use cases would you use or would you have any you want any usage of this? We'll see if anybody you can either type an answer, raise your hand, whatever you want. See if we can get some audience participation. Or not.
 All right, Joe, tell us about the story.
 I have no idea. It's awesome. I mean, I think coffee is I've read all the books. It's been a while. They're talking about coffee. Uh some some not so work appropriate words. Uh but yeah, here we go. It's a sci-fi story involving Tao Station, which I think is actually in the book. Uh
 that is a real place. Yeah.
 Okay. This may be infringing on some copyrights of James A. Corey. I do not know. But right, it's Here we go. You got yourself a short story. Marco and Elena are trying to figure out some stuff about distributed tracing
 index out of range. That's you know that that hits home.
 Authentication service service started said pulling up a timeline. The display shifted showing the first red bloom at 10:30 station time 43. All right. All right. I'm done. Uh if anybody has any questions, let me know. You can find me on Slack. I'm on the public Slack. Uh if you want to chat about this in the in the repo there are a couple or sorry in the in the community call docs there are a couple of links to repos jump in if this is interesting to you help me develop it I intend to focus on graphana next what's up Andreas yeah I'm curious what is your experience with generating a bit more complicated queries like how
 sure ask me
 because I also like tried it once with the the cloud model but like sometimes it didn't even get a proper query like just my query was show me traces with errors. Sometimes it forgot the the braces at the beginning and the end. Sometimes it used the wrong attribute and I'm just curious what your experience is in the accuracy of the queries the generated queries.
 Uh yeah uh absolutely let's let's try something harder but let me talk about what I do list my service names. I don't even know what service names are in here. um it'll just ask for the resource service name. Uh my experience is it does a terrible job until it reads the docs and then it does a great job. Uh it after it's read the docs, I don't think I've ever seen it send an incorrect PQL query. Uh before it reads the docs, I see it mess up all the time and uh I will give it I give it the actual parse error. Uh, one of the cool things I think about this is I give it the parse error which it uses to correct and I think it's going to encourage us to improve our parse errors because uh, the parse errors are fed into the LLM and it does a pretty good job of responding to that and if we make them better and better then it will uh, respond uh, more correctly. All right, what services or what calls? Let's see what it does. What calls the o service? I will probably have to prompt it to use structural operators. Let me try this first. It'll probably call I think it'll ask for trace ids and just use that and then I'm going to tell it to use descend or not descendants. I'm say use structural operators please and let's see if it does something about that. This is kind of the next step for me I think especially as I integrate it into graphana is get a prompt a system prompt that's good enough that when you ask things like this oh it did it automatically. Uh it did it backwards though. So it's looking for things the off service is calling. It's looking for things that are downstream and I said what calls the off service. If I correct it, it will fix itself. But I do think this is an area for improvement and it'll probably be done through the prompt through docs. Um, another good thing feeding at the docs is I think we're going to add a lot more examples to our docs and I think we're going to have a lot more use cases in our docs which will both improve the docs for people and for machines in markdown format or HTML or some custom.
 Uh, I fed it I actually put this in the PR. Um, I don't I fed it directly to markdown. I split it up in a way. Uh, Kim, you're about to hate me. Sorry, Kim. Um, I purpose I cut out what I consider like the core of the docs of the TraceQl docs. I put those into a new file and then I added this stupid thing called this markdown MCV cut off. I basically cut everything above this to not confuse and I feed everything below. This is the actual docs that are served. I have mixed feelings about this. I think it's cool in some ways. I think investing in these docs will also help us invest in what we feed the LMS and vice versa. But I also think this is a little hard to realize. I mean, I try to put a comment, but this is easily easy to mess up, right? It's easy to forget about this and reconstruct the docs in some way that breaks something. So, I have mixed feelings. I talk about in the PR. Like, please give me comments. Hey, Kim, are you going to tell me I'm a I'm a crazy person?
 No, actually, I have a bunch of doc updates that I was doing to the TraceQl docs to restructure and make things easier to use. So we should talk and make sure that what I'm doing um gets integrated with what you're doing. Yeah.
 And also I've been playing with AI models with the docs. So there is now a C-pilot instructions file if you want to play with co-pilot with the docs. We're trying to kind of I copied it from one of our other repositories from another writer. So we're going to refine it to be more tempo specific. But if you want to play with co-pilot, there's a copilot's instructions markdown file under the GitHub area in 28. Cool. Uh I think one of the reasons the docs work so well is we have so many great examples that's like this is how you do this example. This is how you do this example. Uh and I think it's struggling to write the thing I asked for because I don't think we have a good example of that particular combination of structural operators. So the docs are really well written which is why this works because I'm like or who I think this is 90% Kim like here's a thing here's the example here's the idea here's the example bam over and over again the docs are like this and I think the result of them is it parses them really well and it uses them to write queries very very well uh and you can kind of see that throughout here's some more um okay it tried to do something it did find the shop back end I guess calls it. I don't even know if that's the only one. I'd have to go write some other queries to figure it out. But yeah, it's okay. It's giving you the spans. Um, so iterating on the prompt, getting it into Graphana sounds like it'll be, you know, tomorrow, right? But no, I got I have a lot of work to do for sure. But this is yeah my p my personal project my current goal is to push this forward because I think it's so valuable um and so fun to use and everyone can make James A. Corey write a uh future sci-fi novel about their traces if they want. All right. Uh anything else team? Marty is psyched. Marty cannot wait to get uh sci-fi novels about his distributed tracing data. I appreciate you all's time. I'll hand it back to Matt here to wrap us up. Um and we'll see you next one.
 All right. Um do we have any other non noncp related tempo questions or anything else? As we've got a few minutes here. Uh Andreas.
 Yeah, I actually have a question. Um what is the new role of the inester in the new architecture? So in the new architecture there's this block builder which creates the new blocks and puts it in object storage. So is the inester just there for kind of caching like that the quer is the quer is connecting to the inester to read the most recent traces which are not in blocks yet or what else is it doing? Yeah, essentially that it's uh just buying time for the block builder to build the blocks after these blocks to be discovered. So I'll just hold um like whatever much data is needed like say 10 15 30 minutes um in memory or disk just something that is very fast to um like consume and and read from um yeah and they won't be in Justin anymore so we need to find a a new name. I don't know if we should do like an internet poll or something. I've asked JGBT multiple times and all the suggestions are always bad. Okay, thanks. And second question, what is the future of the monolithic tempo? Will the monolytic tempo also require Kafka Kafka instance? We don't know yet. Uh it's possible that it'll be um I I want to invest some time uh to make tempo still deployable and easy to do so in monolithic mode because my impression is that most of the community users uh use monolithic mode. Uh we're very biased towards distributors because it's what we deploy in cloud. uh but we don't want to leave um monolithic to just rot out there. So I'm personally committed to that but there's nothing in the Roma that we can um that we can share right now. Um yeah hopefully there'll be some solution that doesn't require Kafka but um I guess we'll need to wait to see what we can come up with. Maybe like a version with less guarantees that it's in memory. We don't know yet. Okay. Thanks. Yeah. My my next question would have was um if there will be a future also like keeping the old state without the Kafka, but I think you already answered it. Uh I think most likely there won't be. uh it'll be it'll be too different uh and it will be a lots of uh a lot of work maintaining both architectures. Um then I I don't know uh I think yeah we're on the side of just removing classic tempo the classic architecture. Okay. And um is there any kind of big issue or do or like I saw a few pull requests where like also the one from you where you had a a graph nice graph of the new architecture but is there some kind of starting point where I could take an easy overview instead of just looking all the polygraphs. Uh we haven't made most of the documentation public yet. There is a lots of there are many diagrams uh and lots of documentations guides that we're using for rolling like doing the rollouts and just explaining all these uh changes but since we're still moving a lot of uh moving fast and I the the project has changed uh multiple times already on how we were gonna um just do it and and roll it out. uh wouldn't want to uh like share too much and then have to oh no no no but then we're doing it this other way and that multiple times. Uh but Kim and I were already working on documentation and uh just the first step I think will be just trying make all the documentation that we already have internally um just accessible to um to the rest of the community. Um yeah, Marty. Hey um Andreas. I guess Marty said we're kind of in the middle of the rearchitecture and we expect to deliver the other half like in the next release or two. Are you interested in running the intermediate architecture and that's why documentation would be helpful or is it more about just like getting up to speed ahead of time? Yeah, I'm just curious like about the changes getting out of speed because we are maintaining the tempo operator and there will be a few changes required there if we need to integrate Kafka of course
 and also kind of asking about the possibility of like keeping the last one like like previous architecture because it would make it easier for people who don't have Kafka on prem because I think it's easy to get it in the cloud if you're already on some cloud provider but if you have like an on-prem instance of do we need to open shift it's maybe not that straightforward to host your own kafka so that was my that's where I'm coming from um yeah I think we in that case I think we expect to have the documentation like in the final place as we get closer to the final architecture and hopefully some of the intermediate steps that we've had to do won't be necessary um yeah I mean, I guess we can see if that we have anything else that we would like to publish. Mario, uh, yeah, we could look into um publishing some some documentation that won't change and if it does, it will be very minor changes. Um, Andreas, are there specific docs that you're interested in? because if you can give us a list then we can see what we have that's stable like I don't have a specific list just kind of the general overview and big picture
 okay
 and like kind of the changes which will be required for the existing deployments I think honestly uh we have one more what we we're calling in ter like a phase right before we I think we're going to see the final way this all looks for us. Uh, at that point, we should just have a sync meeting. Andreas, Mario and Marty are the experts here, but uh, you should just get in a room and say, "This is what it looks like now. This is how we transition from point A to point B and hash it out." Um, and see what you can get done. This is definitely worth, I think, meeting together. Otherwise, it'll be very difficult to communicate all this. Sounds good. Yeah. All right, we also had uh Greg a few minutes ago posted a question in the chat of how can I tell which version of tempo you need to find a cloud instance? Some of the features don't seem to work. That kind of replied, can you give me the tenant ID? Looks like he gave a commit hash that sort of tells me we probably don't have an a magical way to view it from the front end. Uh and specifically, it seems like uh having trouble with getting the Neil queries to work. uh
 disambigulating existence or something.
 I was just going to look up if R202 had that change in it. It should that change is pretty old. And like Zach said, every what is in cloud right now is um generally about two weeks off of tip of main. We we deploy every week. We cut a version every weekend. We deploy to our dev and ops and the next week we week deploy broad. So it is very up to date. If you're having an issue with something, um, uh, you can file a support ticket. I mean, since you're here, if you want just want to, you know, chuck the, uh, query you're having issues with, we might be able to help you a little bit more directly.
 Yeah, unless we slip. Um, yeah, two weeks. So, Greg, also tomorrow, uh, tomorrow's Friday. Yeah, tomorrow will be a new version. Um, so you can try tomorrow.
 I will on a there's a health endpoint for our graphana.net that instance. Is there a similar one that gives me which version and which commit it's on?
 You know, I don't know if we expose do we expose the status endpoint? I hope not.
 Yeah, really.
 Um, but uh in your in your docs when you're setting up your instance, uh you'll be able to see uh where to send traces. That is an API. So, if we expose it, it's available there.
 Okay.
 And if we don't, then uh the short answer is no. I was just trying to uh check for a U resource attribute equal to nil. Did I misunderstand? Um I'm not sure who uh who knows about the nil.
 Uh okay. So equals nil. The PR is up. Uh but I think FAM's lost some momentum on it. Does not equal to nil is what works right now. Equals nil. the there's a pro, but yell at Jenny fam say I really care about this. It's in the works. That's what I want to hear, Jenny. It's in the works. I just misunderstood. I thought that Neil was all done. So,
 no, if you look at the PR highlighted, it's a really nuanced thing. It It's It's I don't know why we were super excited about it. It's a It's a very strange error that would has nothing to do with equals in not equal. that equals nil. Is it going to work?
 What I want to do is check for something that this is what I asked a few months ago. Um that I want spans, but I want where a value that may or may not have n may or may not have a value to only check for the ones that are not equal nil and it has a value. So that I get other spans back that don't even have that attribute.
 Okay. Yeah, that's what right that's what the equals no will do for you, right?
 Okay. Uh that I don't know. There's like Marty said there's a workaround in the issue. I don't know if it'll work for that.
 It worked for me. So
 Okay. Okay. Right. eventually after fam gets your PR in you'd be able to write that equals nil or equals the thing I care about and one of the things I'm looking forward to on the LLM integration is you know some of the especially the um like the greater greater than two things and and being able to work with that being able to say that in English help my other people a whole lot more
 I think thank So that's that's a great thing.
 Yeah, we've built so many features in it does so many things and they're not used. And I think this uh except for a very small percentage of users. So I think this integration, this natural language thing will as well as the drill down app was another great moment where we're like, oh, we built all this stuff and we're able to leverage it. And I think the LOM situation will be the same. we'll actually start leveraging things like descended queries and trace metrics more and getting value out of the things we've been investing in for years now. So, totally agree. I think it's going to unlock a lot of the features of Tempo that are um kind of hard to use right now.
 If it shows the Well, I know in what you were showing it shows the queries, but
 kind of like the the UI search now does show you what the query is. You would start with that. That would be a great thing too. I'm
 point definitely my goal is right to show the what the LM does and like it has that I think right now there's a button that's like go to explore with this query. I want to do that too where you can see the five things it did. You can choose any of those as a jumping off point and just go to explore because now you understand maybe about what it's trying to do and you want to make smaller edits. So I definitely want a similar experience um to what you're describing in the search. Good deal.
 Thanks, Greg. Go ahead, Kim.
 So, I have a question for the community about information architecture for the docs. We currently nest our instrumentation information underneath setup or actually under getting started. And so what we're thinking about doing is moving it out to be at the same level of all of our top level content. So our new structure would look like get started, how to set up pimpost specifically, and then how to set up your app to send traces. And my question is, does that order make sense? Would you expect to see information for instrumenting your app separate from how to set up everything or would you want to see how to instrument your app nested under setup? Don't all answer at once. I think I would do it as a main like
 a main line
 and not awesome because it could go either way and I want to do it in a sense that makes sense to people who use it. Cool. Well, we got one vote, so we'll do it that way.
 Yeah, you already know mine because we had a discussion on this and this.
 We've had a whole bunch of discussions on this. So thank you very much.
 Thank you. So uh there was a question about build information which commit is running in cloud. Uh we actually do expose build info. Uh so it would be your endpoint uh slash tempo/ API/ status/build info but it is behind OP. So you need a token that can read traces and you'll be able to hopefully read the the command that's running on cloud. Awesome. All right, I think that puts us at time. So I appreciate everybody uh showing up. We had a pretty full schedule today and covered a lot of ground. and we will see you next month. And uh this will go up on uh the YouTubes probably tomorrow if not Monday. So appreciate everybody showing up and we'll see you next time. Thank you.
 Thank you.
 Bye everybody. Bye.

