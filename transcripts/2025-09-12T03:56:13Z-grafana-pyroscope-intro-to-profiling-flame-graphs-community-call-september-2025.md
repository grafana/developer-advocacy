# Grafana Pyroscope: Intro to Profiling &amp; Flame Graphs (Community Call September 2025)

Published on 2025-09-12T03:56:13Z

## Description

This month we're going to have Bryan Huhta giving an intro to profiling and flame graphs.Have questions? Please bring them!

URL: https://www.youtube.com/watch?v=U7E9rDvRCK8

## Summary

In the September edition of the Pyroscope community call, hosts Brian and Tiffany introduced the concepts of profiling and flame graphs, aiming to educate attendees on their significance in application observability. Brian explained that profiling helps answer the "how" behind application performance, complementing logs, metrics, and traces that address "what," "why," and "where," respectively. The discussion covered various aspects of profiling, including its statistical nature, the significance of continuous profiling, and the visualization of profiling data through methods like top tables, call graphs, and flame graphs. They elaborated on how to interpret flame graphs, emphasizing the relationship between call stacks and resource costs, and showcased practical applications using Grafana's tools. Brian also demonstrated the use of the Grafana profiles drill-down feature and the AI-powered "explain flame graph" tool for enhanced analysis. Overall, the session aimed to equip participants with foundational knowledge and practical skills to leverage profiling effectively in their applications.

# Pyroscope Community Call - September Edition

**Welcome everyone!** 

In this September edition of the Pyroscope community call, we’re doing things a little differently. We noticed that many folks may not be familiar with what profiling and flame graphs are, so we decided to dive into that topic. If you have any questions, please feel free to ask in the chat. If you encounter any issues with the chat, you may need to create a channel to interact. Now, I’ll hand it over to Brian.

---

## Introduction to Profiling and Flame Graphs

**Brian:** Hi everyone! I'm Brian, an engineer on the Pyroscope team. Today, we’ll be discussing an introduction to profiling and flame graphs. This will be the first installment in a series focused on introductory to intermediate material targeted at profiling in general.

### Agenda
1. What is profiling?
2. What is a flame graph?
3. How to read flame graphs
4. Tricks for reading flame graphs using Grafana's profiles drill down app
5. Q&A session

### What is Profiling?

The first question is: **Why do we even want profiling?** Aren't logs, metrics, and traces enough to satisfy our observability needs? Traditionally, metrics help answer the question of *what went wrong*, logs explain *why*, and traces show *where*. Profiling fills in the gaps by answering the question of *how*.

Grafana offers a product called **Grafana Pyroscope**, which is a continuous profiling solution. This means we continuously profile running applications and build out flame graphs for those applications. You can query a profile for any given point in time rather than just for discrete profiles.

These profiles complement the answers provided by metrics, logs, and traces by giving you more insight into your programs.

### Profiling vs. Tracing

A common question is: **Why do we want to do profiling when we have tracing?** 

- **Profiling** describes the cost of running a program, focusing on the resource costs.
- **Tracing** describes the lifespan of a request or transaction, showing when events happen and their relationships.

Profiling is statistical in nature, while tracing is deterministic. When profiling, we aggregate stack traces, whereas tracing involves a tree of spans.

Profiling identifies how latency and slowness occur by pinpointing specific functions causing slowdowns, while tracing may indicate which services or operations are slow without detailing the specific functions.

### What is Profiling?

**Profiling** measures how a program spends its resources. Most profilers can measure common resources like CPU or memory, while some can measure more exotic resources like mutexes or thread pools. We build a profile by sampling stack traces during program execution and aggregate these stack traces over time.

### What Does Profiling Help With?

Profiling is useful for gaining an understanding of how your code behaves in its running environment. Some benefits include:

- Identifying hotspots in your program
- Validating assumptions about inefficient code
- Exposing bottlenecks in functions

### Visualizing a Profile

There are several ways to visualize a profile:

1. **Top Table**: A flat list of function names and their relative costs.
2. **Call Graph**: Visual representation of functions and their relationships.
3. **Flame Graph**: Commonly used for visualizing profiling data.

In a flame graph, the y-axis represents call stack depth, while the x-axis represents the cost of functions, not time. The widths of the bars indicate the relative cost of functions.

### Building a Flame Graph from a Profile

To create a flame graph, the profiler runs alongside your application, sampling stack traces at set intervals (e.g., every 10 milliseconds). These stack traces are aggregated to form a profile.

When building the flame graph, we combine common stack trace prefixes. As we add samples, the visualization becomes more refined.

### Understanding Flame Graphs

Nodes with open bottoms represent work being done, while "flames" group logical processes within your application. The horizontal order of the blocks in a flame graph does not indicate the execution order of the stack frames.

### Demo of Grafana's Profiles Drill Down

Now, let’s transition to a demo of Grafana's profiles drill down tool for visualizing profiling data. 

1. **Accessing Profiles**: In Grafana, go to the profiles section to view services being continuously profiled.
2. **Timeline and Flame Graph**: The timeline provides a time-dependent view, while the flame graph visualizes the profiling data.
3. **Top Table**: Use this to sort by self and total values of functions.
4. **Focus Block**: Select a block to zoom in on and examine its child functions.
5. **Search Functionality**: Search for specific functions to quickly locate them in the flame graph.

### Other Profiling Types

Beyond CPU profiles, you can switch to view memory allocation and other profiling types, allowing for a comprehensive analysis of your application’s performance.

### Closing Remarks

Thank you for joining us today! We encourage you to engage with us on our Slack channel under `#Pyroscope` or explore our GitHub page for more information. We will also be posting details about our next stream in the coming month.

If you found this session helpful, please share the link with others who might benefit.

**Thank you, Brian, for this informative session!**

## Raw YouTube Transcript

Hey everyone. Um, welcome to the September edition of the Pyroscope community call. Um, we're doing things a little bit differently this time where we figured that there are many folks out there in general who may not be actually familiar with what profiling and flame graphs actually are. So, we figured this time around we would go into that. If you have any questions, please ask questions inside of the chat. if for whatever reason it does not let you ask anything in the chat. You may need to go and create a channel. Um so yeah, please ask questions along the way and I will give it over to you Brian and not me, that's me. Hi. >> Hi everyone. I'm Brian uh an engineer on the Pyroscope team. So today we'll be uh we'll be talking about an introduction to profiling and flame graphs and this will be the first installment and hopefully a series of installments of introductory to um intermediate material point targeted towards profiling um in general. So our agenda today what is profiling? What is a flame graph? how we read flame graphs and some tricks of how to uh read flame graphs using graphana's profiles drill down app and then we'll end with a Q&A if there's any questions that come up. So the question everyone has is what is profiling? But maybe a better thing to say ask first is why do I even want profiling? Aren't logs and metrics and traces enough to satisfy our observability needs? And traditionally, metrics has fulfilled the role of a uh answering the question of what went wrong, logs the why and and uh traces the where profiling slots into this hierarchy um by answering the question of how. Uh so Graphfana has a product um called Grafana uh Grafana Pyroscope which is a continuous profiling solution. So that means we uh will continuously profile running applications and build out flame graphs um for or build out profiles for that application. This means you can query a profile for any given point in time and not for discrete profile uh discrete points in time or ask for discrete profiles. And so of these of these profiles um really does it really can help complement um the additional questions of the what, why, and where uh by giving you even more information and insight into your into your programs. And I want to touch on this before we get too deep. Um because a common question is is why do we want to do profiling when we have tracing? Um so I want to kind of compare and contrast uh the the problems each solution solves. Um where profiling describes the cost of running a program, tracing describes the lifespan of a request or a transaction. So profiling is definitely oriented towards understanding costs uh resource costs of your program. Um and tracing is a little bit more even oriented uh to describe when events happen on which service and how they what the relationship or lifespan of that uh event might be. Profiling is very statistical in nature and we'll get into that in the next few slides. Whereas tracing is very deterministic. um the when you you establish the starts and ends of each span that build out your trace uh as your code executes those spans will always start and end um at in the same places within with relationship to each other. Certainly tracing is statistic to some degree from the sampling perspective. You may decide to accept a trace or not accept a trace, but once you've accepted a trace, the the structure of that trace is very deterministic based on where you've selected to start and end your spans within your program. The data for profiling is an aggregation of stack traces. Um whereas the data for a trace is a tree of spans. Um likewise, like I just kind of alluded to earlier, the data for profiling is collected um by sampling the stack traces and data for tracing is collected by manually instrumenting spans. Um this is uh tracing often times will have those spans start and end at the beginning at the function boundaries. Um but this is not necessarily required. uh you can obviously have a span start and end in the middle of a function or any around any logical operation your program might perform. And lastly, uh profiling identifies the how portion of latency and slowness. Um so it'll tell you what specific functions were being used aggressively to cause slowdowns. Um whereas tracing will uh point to the where it'll tell you which service possibly which fun which uh general um logical operation that your program performs might be slow but it won't tell you which specific uh functions won't always tell you which specific functions cause that slowness. So we get back to the original question of what what is profiling? So profiling is a way to measure how a program spends its resources. So your common profilers most profilers will be able to measure common resources like CPU or or memory. Um some some profilers can measure even more exotic resources like mutex or or uh thread pools. Um, we build a profile by sampling stack traces during program execution. And we aggregate these stack traces together over some period of time to form what we call a profile. And continuous profiling just means we always do this. So we never stop trying to build that profile. What does profiling help with? profiling is quite useful in helping uh generally gain an understanding of precisely how your code behaves um on in the environment where it runs. So you don't need to set up an artificial um environment to try to get you know observability hooks inside your program. You can have it running in production and profiling can help you uh understand its behavior uh in production with production workloads. So this includes identifying the hot spots in your program. Um you can use it to use profiling to help validate where you sus suspect your hotspots might be. Uh discovering code that may be inefficient. Um so if you had an assumption on how code ran but it wasn't correct or maybe it was correct. Uh profiling can help validate those uh those inefficient code those assumptions about the inefficient code blocks. And it can also help by exposed bottlenecks. So if you have uh functions that execute that are slower than they should be or um are are constraining your system in some way. Uh profiling can help eliminate those functions. So when it comes to visualizing a profile, there's a few different strategies. Um I'll review some of the common ones. One is a top table. So we can see that a top table is uh just a flat list of function names and their relative costs uh that they contribute to the duration of the profile. So in this case we can see that we have um this insert stack trace function uh that's part of that's a method on the the tree symbols object and uh this is uh during over the duration of this profile this was responsible for 4.31 c seconds of CPU time um which uh accounted for um you know 1.57% % of the the profile. Uh and so FL I I I want to say that also mention that top tables are really good for finding mins and maxes. Um you can sort these columns and identify you know quickly the the minimum and maximum of of the various columns. So the top tables are really effective at doing this. Another strategy to visualize a profile is through a call graph. Um so in this case uh the fun the boxes represent functions um and the lines the arrows between the boxes represent uh the the stack traces or the the which functions are calling which other functions. And then lastly we have uh one of the final popular ways of visualizing profile is using a flame graph. Um and so here uh we we have a common example of flame graph and that in a second I'll kind of break we'll go into into depth on how we build this flame graph and how we understand this flame graph. Um I will say there's I just want to touch on that there's two common ways that we um you might see a flame graph in the wild. Uh one is where the root um starts at the top and builds its way down. Uh and the other is where the root starts at the bottom and builds its way up. Uh functionally there's no difference between the two. Um it's just a difference of how to render or how to build out the graph. So when we read a flame graph, uh there's two important components to keep in mind that the yaxis is our call stack depth. Um so this describes which functions call other functions. Um and so you you kind of get a treel like structure there. and the x-axis is the cost of the of the function or how much resources that function consumes. So it's not uh time uh timeline. So on the left isn't time zero and on the right isn't the end time but rather it uh the widths just refer to the relative cost of that function. So in this case we can see that our cost total cost is 4.62 62 minutes of CPU time and each one of these sub um bars is some component of that total 4.62 uh minutes. >> And I have a question that came up. So the question is do we use profiling only on code basis or can we integrate it with infra logging as well? >> Um typically you'll want it want to run your profiling on your own code. Um there there are other uh systems that's where you have the most control of profiling. Um and if that doesn't answer or if you have follow-up questions we can always get to those that too. Okay. So building a flame graph from a profile. So how do we get you know from from those sample stack traces to a flame graph? So as an application uh executes and it's it's running in in your environment, you'll have a profiler running alongside it, which is is simply another program. And what that program will do is every uh 10 milliseconds, it'll ask the ask uh your program's runtime which what stack trace is on the CPU at this point in time. And so the runtime will then respond and say and give provide that stack trace. And so we can see that uh here we have um five stack traces that have been sampled over the course of 50 milliseconds. And um when we when the uh the program the when the profiler asks for that stack trace sample, it assumes that whatever whatever function is currently on the being executed by the CPU or that that uh that function at the end of that stack trace. It just assumes that that function took 10 milliseconds to run. Um and we'll see how this makes a difference as that profile gets built. um that function may take more or less time but this is the statistical component of of profiling where we need uh data over time to to begin to build a more and more accurate picture of your system behavior. So the resolution minimum resolution is 10 milliseconds. So like I said we have stack trajease S1 through S5 that we've collected over 50 milliseconds. Um and I I want to touch on that uh these bars represent functions in the stack trace. So this first bar might be one one function and then that function calls another function so on so forth and we finally get to this final function which hasn't called additional functions. And so when we're building this uh uh flame graph, we'll take all of those stack traces and aggregate them together into one big bundle. Um and we call this a profile. And then when we want to render this profile, what we do is we uh combine all the common stack trace prefixes. So if we if we go back for a second, we can see that some of these stack traces share common prefixes like this. These three functions here match these three functions here. So S1 and S3 uh share a common stack trace prefix of these three functions. Um and likewise you can see that S4 also has that same common prefix. Now S2 has a common prefix of these two functions with all um S1 through S4. And so you can you can kind of get a sense for how we might combine um the common prefixes all of these stack traces. So when we do this we can combine these prefixes. And now you can see that how the time no longer starts from the left and goes to the right where we all of our stack traces are are unordered. And uh that's that's where we lose the um orderedness of of the stack traces and it instead becomes the width representing the cost. And so at this point you can already see even with these five s stack trace samples that we're starting to get to an object that approximates a flame graph. And you can imagine as we add more stack more samples um that this this uh visualization will only become more similar to a flame graph. So kind of tying this back to um to some code. So on the left we have some pseudo code and on the right we have a sample um flame graph that represents that pseudo code. So we've profiled the pseudo code and we've produced a flame graph. So here you can see in the main function we have the main our main loop calls two functions do little and do a lot and you can see uh them represented in this fling graph on the left or on the right where our main function um consumes the entire portion of the um the total profile uh and it calls this do a lot function and also this do little function. And you can see again that uh time does not go left to right where technically do little is called first in the code but it shows up second in the flame graph. So um just to reiterate that we are it's our cost left to right not not not the time. Um so you can kind of visually map the pseudo code to um relative the relative uh costs commented in the pseudo code to how those costs appear in the flame graph where this do a lot function um we spend 35 we spend 65 CPU cycles and then inside prepare we spend 35 CPU cycles so you can see that we have 65% in the do a lot and then we have a smaller portion executing this prepare and likewise with do little and and its call to prepare. >> I have a question. Is there a certain amount of like so so this example is like x CPU cycles etc. Is there like a certain amount like CPU cycles or time that is like mandatory for it to take in order for it to actually be able to show up inside of a flame graph? >> So there's Yeah. So we that's a good question. So here here we're being commented about CPU cycles. We actually don't measure profile. Most profilers won't measure CPU cycles. They'll actually measure CPU time. But uh it's kind of a nice way to think about it how CPU cycles equates to time. Um there's no minimum amount of time that uh a function needs to spend on the CPU in order to be profiled, but there is a minimum measurement that the function will be measured to have assumed to to cost, which is 10 milliseconds. So even if a function only takes 1 millisecond to execute, if it was on the CPU when the profiler sampled it, the profiler will just assume that it cost 10 milliseconds to run that function. If hopefully that makes sense. So you can't get a a resolution smaller than uh 10 milliseconds. Well, you could, but you can't I guess a better way of saying is you can't get a resolution smaller than however uh whatever frequency you sample at. So if you sample every 10 milliseconds, you can't get a resolution smaller than 10 milliseconds. If you sample five every five milliseconds, you can get a resolution smaller than five milliseconds. So for instance, if like do little only took five milliseconds, there's a chance that it may never show up depending on when you are sampling at. >> It's possible, but again, if you if you sample for long enough, eventually you will you will sample a stack uh stack trace that has do little being executed. >> Thanks. And this actually is kind of an interesting uh kind of side note which um a lot of profilers don't won't sample at precisely 100 milliseconds or uh precisely um 100 hertz. They'll sample at some, you know, sometimes prime number that's close to 100. Um to try uh kind of break, um typical typical cycles of landing of things that always fall between the the fence posts of every 10 milliseconds. So, um, if you sample at some, uh, prime hertz, then you can kind of break up, uh, natural cycles that might occur to try to help try to help hit those faster functions. So, some techniques to understanding a flame graph. Um here we have a slightly more complicated flame graph and it can be intimidating to try and look at this and understand um what you're looking for uh what things in this flame graph are valuable. So I want to cover some some strategies that you can use to to help understand these flame graphs. The first is that nodes with open bottoms represent work being done. Um, so you can see that uh these nodes that I've highlighted here have no additional nodes below them. So that means they weren't calling other functions to like pass on the work or ask for some additional work to be done. These were nodes that are actually performing the work. And so this might be you know some computation. You might have loops here uh that you know operate on on data structures. Um but these nodes are important to identify and you want to look at these nodes to understand if uh their width is um reasonable for the work that is being done in the code. Now I do want to highlight uh this doesn't mean that these open bottom uh nodes are always the last ones in the line. You can see that this one right here um is has one additional uh frame below it, but you can see that there's still an open bottom here, which means that this node right here is partially calling a second function to do some work, but it's also partially doing some work itself. Um so open bottom nodes don't have to be restricted to the very bottoms of the flame graph. um but they are the ones that are important to to consider when observing a plane graph. Then um this is a very uh uh non-traditional term but I want I like to call them the flames. So these are kind of um these these regions of the flame graph that uh kind of visually group themselves into a single flame or a single uh tree that's that's independent of the rest of the flame graph. Um you these approximate themselves to uh processes in your application and so this isn't process like a Unix process. This is processes and like uh you have the portion of your application that's serving HTTP requests for example um or a portion of your application that's uh performing some kind of batch job. Uh these are like the logical processes in your application and it's important to keep your eye on these as well and understand their width in in relationship to the rest of the flame graph. Is the width does the width seem reasonable? um in comparison to the rest of your flraph. >> Hi, question. Um is the horizontal order of the blocks the order for the execution of the stack frame? >> No, it is not. Um the horizontal order uh can be changed depending on your flame graph and how uh the profile is built. Um so some in some some tools here the horizontal order you can sort kind of by greatest greatest to smallest widths or vice versa. Um but yeah it certainly does not represent uh if if we have this frame coming before this frame that does not mean that this thing executed before this thing. Um it just happens to be placed uh left left of it. So the the only x component that we really focus on is is width in relation in proportion to the rest of the plane graph. But that's a great question. And so if you're running in a garbage collected language or a language with a heavier runtime, it's very often you'll find that you can identify the flame um for your runtime. And so this is obviously going to be language dependent. Some languages are going to have a much lighter runtime and thereby their flame is going to be less obvious. This this profile is um for a Go application. And so we can actually identify the flame uh which corresponds to the Go runtime. And so this can be helpful too. You might not necessarily be able to directly influence uh the cost of each one of these functions in this flame. Um, but if you see the runtime flame get too wide, there might be other things you can do uh in your code to indirectly influence it. For example, you might want to manage um your heap memory a little bit more closely. Maybe allocate try allocate fewer objects or reuse objects or put objects on your stack to try keep the pressure off of your runtime to um uh make your make your runtime work a little bit less and give give more CPU time to the rest of your program. Okay, so with this I do want to uh quickly get into a demo um to go over uh some of these strategies we've talked about in uh a concrete way with um with graphana in and Pyroscope um with our one of our visualization tools that we provide from graphana called profiles drill down. So if we come down, >> you accidentally stop sharing. >> Oh, let me let me quick share my Sorry about that. Okay, hopefully that's better. Um so if we come over to uh our graphana instance we can open up the side panel and under the drill down tab we have profiles and so profiles profiles drill down is the tool we use at graphana like I said to uh visualize um pro profiling data um in a variety of ways. So here when we come to profiles we can see that we're greeted with a list of all of the services that we're continuously profiling and we can uh um go and inspect any one of these services. we get a um a time series graph which I'll I'll discuss later for so you can perform you know kind of a high level uh uh at a glance analysis of of how that service is behaving. Um for our purposes uh we'll look at the checkout service and we'll go click on this flame graph icon uh button to go look at the flame graph for this service. So once we're here um you can see that we have this timeline on the top and we have uh the flame graph and top table um on the bottom. And so this timeline uh this is time dependent. So as as we go left to right, we're going forwards in time. And what this value on the timeline refers to is um this point in time's uh respective value um for the total profile. So as we go along we can see that if there's certain points where the profile gets larger and so that means that this total bar here for this for this period of time will will be larger than if we selected some period of time right before it. Um and so this can be helpful. You can see that we have a few spikes here uh which can indicate that um maybe there's something more work that's being performed by our application during these at these uh spiky times than than during than than during these more uniform load times. Um so that's the timeline. Now if we move further down we have the profile and the top table. Like I said, uh or we have a flame graph and the top table. Um there's a few different things. There's a lot of buttons here. Um you can toggle top table only, flame flame graph only, or or both. Um there you can uh I think it often is easiest to look at both at the same time. But if you're uh it can be helpful to look at the flame graph only because it does make it wider. So it makes it a little bit easier to dig into some of these more narrow frames. um if that's if that's something you you might be interested in. Um when you're looking at this flame graph, these uh the colors in the flame graph are do have significance. And so in this default mode, the colors represent um are they're colorcoded by package. So you can see everything that's pink comes from this runtime package. Um everything that's kind of like this. It's like a sea green or or or kind of pale green comes from the crypto package. Um this blue comes from um this Pyroscope Go package. And so in the default mode like I said these colors represent the package that they belong to. You can change this um and you can change it by to be colorcoded um by cost. And so this might be the more familiar uh weight that you've seen profiles in the wild or fling grass in the wild where um the more red the more yellow something is the less the the smaller the cost and it gets more red for greater cost. So this is another way of color coding these um color coding the the frames. So uh we have this top table on the left and this can be useful for browsing um uh you can order you know order the costs of of functions either by their self value or their total value. Um and so the it might not be immediately obvious what self the difference between self and total is. Um so self refers to how much time is being spent executing that frame uh in particular whereas total is the amount of time spent executing that frame and all subsequent frames. So if we uh focus on this block here, we can see that uh this reports a self value of 210 milliseconds and a total value of 620 milliseconds. So that means the total width of this um of this frame is 620 milliseconds of CPU time. And this bit that has no children right here um is uh 210 milliseconds of CPU time. Um so the self value kind of refers to you know the work that this function in particular is doing and the total is uh the work that this function plus all of its child functions did. >> So if it's like colored based on like packages and then those are both listed as runtime. Are they coming from different places and that's why they're different colors or what's making it different for the two different parts of runtime that are happening here? >> Yeah. So, I did a focus block and or I focused on this block and we'll get to this in a sec of of what this means. Um, but since I focused on this block right here, it kind of lightened um all the other all the other frames. So, that's why these colors change slightly. Um uh it if I was to release the focus, you can see that um all the subsequent colors back here are are pink again. But since I focused it, it changed um the parent nodes a little bit, the color of the parent nodes a little bit to reinforce that I was focusing on on a particular block. Okay. So, uh, back to the top table. Um, like like I said earlier, the top table is great for sorting by self and total. Um, you can also we also have a search function here. So, I can search um, maybe I care about my memory allocation. So, I can search for Malo and it'll uh do a reax match to find all of the Malaks functions with Malo in it. Um, and it'll also highlight those functions on the graph on the fl graph. And so this could be helpful if you have a very large coin graph uh and it's difficult to know precisely where your function is, but you know your function exists. So you might want to use the search uh bar on the top table. Um and that will highlight where these these functions exist on the flame graph. Um and so once we found a particular function, maybe I'm interested in this particular frame of malik. Um, I might care about uh more of this detail below this this frame. Um, but obviously it's too narrow where it's it's very challenging to understand or just visually see what these very narrow flames look like. So I can click on this uh and select focus block and that will zoom me in to uh f have this block fill the total width of the flame graph. um its value hasn't changed, but its relative size has changed. And that's given me the ability to get access to to see some of these more narrow uh flames that come off of it. So, this um focus block can be helpful to um kind of zoom in on portions of the flame graph so you have less noise you're looking at. Uh something that's interesting to observe here is that you can see um the resolution of the profiler. So we have this runtime.get GC mask um and it reported to take 10 milliseconds and same with this one and same with this one. So you can see that these functions may have taken less than 10 milliseconds to execute in total but we only can measure a resolution of 10 milliseconds. Um but as we collect more and more stack s uh sample more and more um stack traces uh we can get a a better and better understanding of the relative costs of of these functions um compared to the rest of the of the profile. >> I have another question. So, like right now you're looking at this and like for instance, if you want to look at a specific area that you can do a focus block, but is there a way to be able to like zoom in more to be able to see like say all the gray areas and you're like, "Oh, I'm curious even though they took so little they didn't like they're not showing up right now, but if you is there a way to like be able to zoom into a specific area?" Uh, so there's no like click and drag and kind of box a specific area. But if you do want to see more about these gray areas that are difficult to see completely zoomed out, the best approach is just to focus in on a nearby block. So here I focused in on this block and that kind of zoomed in all those gray areas. Now I can actually see them colorcoded pink. So this is obviously a lot of runtime work. Um, and so that's the best strategy for uh um bringing those gray areas into resolution. So another thing I want to show is um uh if we search for um I I know of a particular function here called place order. And so this is I know a function that you know is critical to how my application runs. And so I can focus in on this block and and uh understand um when when an order comes in and now I have to respond to this that request. I can get an understanding of uh what work is required to to serve that request. And so here um I can see we have uh a lot of um reax matches. So, this might be something I want to uh I want to um take a peek at, right? We clearly have um we're we're clearly compiling a reax every time an order comes in. And so maybe this is something we can do to improve by perhaps uh compiling this reax once at the start of the application and then reusing the compiled reax um for every request instead of compiling it per request. Um there's another uh um I just want to quickly Yeah. uh pre so there's there's another strategy we can we can use to understand um the cost of um a function which is we can use sandwich mode and so here I've uh I searched for um this invoke function and placed into sandwich mode and what this does is it kind of gives us almost two flame graphs um surrounding this invoke function. On the top we can see all of the people who have called uh the invoke function or all the functions that have called us this function and on the bottom we can see all the functions that um invoke in turn called. And so this can be helpful um especially if you have a small function that gets called very frequently throughout your application. Um it can be helpful to understand the cut where the cost uh of that particular function is coming from and the total amount of cost that that particular function um uh kind of you know for better for lack of a better term invokes or or the total amount of cost that that function requires to execute. And so um here we can see that um one of our one of the people one of the functions who calls this invoke function is is this metric service client export. Uh we know have another one over here um cart service client empty cart and uh shipping service client ship order. And so we have three people three functions who call this function. And so Sanders view is really good at um uh if you don't have one occurrence of that function in your flame graph, if you have many many occurrences and you want to understand the impact of that function uh kind of summed up, sandwich view can help answer answer those questions. And so the last thing the one of the last things I want to go over is that we've been looking at um CPU profiles, but you can of course switch to look at uh any number of different types of profiles. Now the different profiling types you see here are going to be dependent on um what profiling types your profiler supports. In the case of Go, it actually supports a wide variety of profiles. Um but for example, we could go and look at um the amount of uh memory that was allocated um for a particular prof uh for a particular application. And so we can see uh this flame graph represents this total amount of time here. And uh now instead of time um we're getting uh a measurement of storage. So gigabytes and this I mean we can um use this to understand uh you know this function right here um has consumed or has allocated um about a gigabyte of of memory uh in total. Um so that's this fun this function and all its children and this function in isolation has allocated about 700 megs of memory and so this just illustrates that you don't just need to pro aren't just able to profile CPU time um but rather there's a wide variety of different profiling types um that are available. And one of the last things I want to cover is that if you find a profile that uh is interesting and for whatever reason you want to uh explore it in ways that this tool might not allow, we have this ability where you can download a puff. And so this will just trigger um a download and now you'll have the raw profiling data. And if there's a tool you're developing yourself or a tool that you know about that has some feature that doesn't exist here, you now have the raw profiling data for this profile here. um and you can pipe that into the tool of your choice. So with that I will >> I wanted to say one thing. Could you share your screen again? >> Yes. >> Give me just a sec. >> Thank you. >> Okay. Um yeah. So, uh, for folks that are wanting to try things out, it's not this specific one, but if you go to play.garfana.com, you can go and play around with things. If you look on the top right here, especially if you're newer, um, or even just in general, um, there's the explain flame graph if you're going around on there, which can be super useful. Um, so like could you for instance just like click the explain flame graph for this one? It'll take a sec because it's pumping through a lot of the data. >> But yeah, so basically it uses AI, which everyone is talking about nowadays, um, to help explain what is happening inside of your flame graph, which can be super useful, but of course it takes time. So yeah, I'm just going to need to put in play.o.com or.org depending on um it redirects. So if you go there um you can play around with um Pyroscope. Um you can go into explore, you can go into drill down. Um there are several different um services and applications that are running over there for you to be able to try out. Um so there's like the right app which is in the docs if you feel like going through the docs as well. And I guess right now it's being a little bit slower than normal. Maybe maybe we can debug on the fly. Oh, I fixed it. Yeah. So, as as Tiffany was saying, um, explain flame graph uh will send um send this the flame graph or the profiling data to an LLM that we um and it will uh give us some some uh hints and tips of things that we might want to look at to try to understand um why the flame graph looks like it does and if we want to make it better, how to make it better. You can already see that like I was saying earlier, it's identified that the reax compile um is likely to be a problem. And in fact, during one of these for one of these recommended fixes, it's recommending we cache or pre-ompile our regular expressions. Um which seems like a reasonable thing to do. We don't want to call we don't want to compile reaxes um in a loop. And Robert's asking if you then need an LLM plugin for flame graph analysis. >> Yes. So this flame graph uh uses the graphana L1 plugin. Um so we don't call you know just make raw calls to uh an LLM um of of that you don't control. So the LM plugin will manage um which LLM gets called. You can bring your own LLM. um you get you have finer control over uh how the LLM gets used and the data that gets sent to the LLM. Okay. And so with that, um, that is the end of, uh, our demo. And I want to thank everyone who stuck around to watch this. Um, and join us over at our Slack channel. Uh, you can find us in hash Pyroscope at on the graphana.slack.com Slack. Um, feel free to take a peek at our GitHub page graphana pyroscope um and see what we have going on there and or you can engage us at community.grafana.com. >> And then so Robert also says um if so does get hints about what language Java go. >> Um the short answer is yes. The longer answer is uh we don't explicit in the prompts we don't explicitly suggest which language it uses but it the LM is actually quite good at inferring the language based off of the profiling symbols. Um so the go symbols are quite unique same with Java ornet. Um, and my experience has been that LM does actually quite a good job of understanding the language the profile uh language of this uh program that got profiled. >> Cool. Yeah. So, yeah, please yeah, come hang out with us um in the Slack um ask questions there. Um we'll be posting when the next stream is, which is next month. um and just details about that and then there's also yeah the forums as well. So yeah, thanks for everyone for coming. Um if you thought this was useful, please uh share the link which is going to be the same link um with other folks that you know. So and thank you so much Brian for doing this today. >> Absolutely.

