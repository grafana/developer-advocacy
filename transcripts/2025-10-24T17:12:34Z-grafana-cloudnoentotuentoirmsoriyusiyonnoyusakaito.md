# Grafana CloudのエンドツーエンドIRMソリューションのユーザーガイド

Published on 2025-10-24T17:12:34Z

## Description

Grafana Cloudのインシデント対応管理（IRM）ソリューションは、アラートやSLOの作成、オンコール管理、インシデント対応、 ...

URL: https://www.youtube.com/watch?v=v_f3Qkf7MAw

## Summary

この動画では、グラファなラボーの共同創設者が東京でのプレゼンテーションを通じて、IRM（Incident Response Management）ソリューションの活用法について説明しています。主なポイントとして、インシデント登録の複雑さやエンジニアのフラストレーション、アラートのノイズ問題、そしてデータを活用したオブザーバビリティの重要性が挙げられています。また、SLO（Service Level Objective）を利用したアラート作成やインシデント管理のプロセスを改善する方法、さらにチームの文化やプロセスの成熟を促進するための戦略についても触れています。参加者にはエンジニアが多く、彼らの視点を重視した話が展開されました。

## Chapters

以下は、ライブストリームの中での重要な瞬間とそれに対応するタイムスタンプです。

00:00:00 イントロダクションと目的の説明  
00:03:00 IRMエンドエンドの意味について  
00:05:30 エンジニアのプルリクエストに関する意見調査  
00:08:00 インシデント管理の複雑さについて  
00:12:00 インシデントレスポンスの文化的な課題  
00:15:30 オブザーバビリティによるデータ活用の重要性  
00:20:00 インシデントライフサイクルの説明  
00:25:00 SLO（サービスレベル目標）の設定方法  
00:30:00 アラートの管理と対応のプロセス  
00:35:00 インシデントレスポンスの実際のデモ  

これにより、視聴者は興味のあるトピックにすぐにアクセスできるようになります。

こんにちは。今日は、Grafana Cloud Incident Response Management（IRM）ソリューションについてお話ししたいと思います。私はGrafana Labsの共同創業者で、今日は東京に来れて嬉しく思っています。ライブでお話しする予定ですが、IRMソリューションを使って皆様のレスポンスをどう加速できるのかについてお話ししたいと思います。

まず、IRMエンドツーエンドのIRMの意味についてお話しします。ご質問があれば、スライドに質問を入れてください。内容が多く、時間がないかもしれませんが、もしQ&Aの時間がなければ、後で私を捕まえて質問してください。

では、最初に皆さんに質問です。エンジニアの皆さん、プルリクエストを引き出すのが簡単だと思われる方、手を上げてください。はい、こんな感じですね。次に、インシデントの宣言が楽にできると思っている方、手を上げてください。そう思いますよね、多くの組織にとって、インシデントの登録やその後のプロセスは複雑で手間がかかります。

こういった問題は、本当にその的なプロセスが必要なのか、あるいは修正してすぐに進めばいいのか、判断が難しいものです。チームとしてどうベストプラクティスを導入できるのか、どうすれば価値を高められるのか、そういった視点についてお話しできればと思っています。

PRを開くことを同じくらい快適にすることは可能だと思いますが、難しいことでもあります。そこには文化の問題があり、人、プロセス、ツールにまたがる課題があるからです。したがって、注意深い対応が必要になります。人、プロセス、文化の問題は複雑です。我々はエンジニアなので、エンジニアの観点から問題を解決したいと思っています。つまり、ツールを使ってプロセスを改善し、摩擦のないインシデントレスポンスを実現したいのです。

文化的な変化、つまり文化を成熟させるためには、オブザーバビリティのIRMを使うと良いと思います。データを使って複雑なシステムを理解することが重要です。オブザーバビリティネイティブのIRMは、ツールを使うことで、目の前の問題を理解することを目指しています。プロセスを簡略化し、一貫したフローを使って最終的な結論を導き出すことが大切です。

インシデント管理においては、データのすぐ横にテレメトリーを置いてプロセスに対応します。こちらはシンプルなインシデントのライフサイクルです。まず問題を特定し、問題があることが分かったら、対応します。特に顧客に影響を与えている場合は、お客様へのインパクトを最小化しなければなりません。

さらに重要なのは、このプロセスから何が学べるのかということです。このライフサイクルをどう改善し、検出、対応のプロセスをどう続けられるのかという観点が必要です。このプロセスを考えてみましょう。プロセスが崩壊していると、エンジニアの間でフラストレーションが生まれます。

例えば、問題があった際にアラートが多すぎると、エンジニアは「それは気にしないでください」と言うかもしれません。アラートが出ても無視したり、適切な対応者や対応チームが誰なのか分からないこともあります。適切なデータがなく、チームとして適切なダッシュボードやログが見つからないこともあるかもしれません。

個人的な経験を考えても、アラートが多く、次々に問題が起こる中で一時的なフィックスを提供し、次に進むことがあるかもしれません。しかし、再発防止のためには根本的な原因に対応しなければなりません。このプロセスは本当にフラストレーションを生じさせ、エンジニアは定期的に回ってしまうことがあります。

「アラート疲れ」という言葉がありますが、これは大きな問題です。特にお客様のユーザーの間でも、SLのレベルで問題があまりにも多く発生すると、ノイズが出てしまい、シグナルが見えなくなります。エンジニアとして、どうすればそのツールを改善し、その状況に対応できるのかを考えるべきだと思います。

では、ここでデモをお見せしたいと思います。まずSLOから始めます。これはシンプルなアプローチで、ベストプラクティスでもあります。アラートをどう作り、アクションにつなげるのかが重要です。エンジニアとして大切なのは、しっかりとエンジニアを午前3時に呼び出す必要があるということです。

まずは新しいアラートを登録・作成し、プロセスを簡略化するためにオプニマイズにこだわる必要があります。SLOレベルで様々な選択肢があり、難しい意思決定を排除するためにテンプレートを用意しておきます。成功の定義を明確化するために、インディケーターを基準として使います。

例えば、成功を達成するための基準として、リクエストが200ms以内に対応しなければならないというものがあります。具体的なリクエストに対して達成できた件数を比較し、成功率を計算します。クラスターごとに計算を行い、サービスに問題がある場合には、どのクラスターで問題が生じているのかを特定します。

次のステップとして、ターゲットを設定します。ライアビリティのターゲットをどこに設定するのかということです。私たちの提案としては、ガイダンスを用意していますが、サービスのデータを使って、信頼性に関連してアラートが複数出ているかどうかを確認します。例えば、エラーレートのターゲットを80%や85%に設定することができます。

次に、アラートの名前と説明を設定します。説明は簡単な言葉で記述すべきです。ペイメントサービスのリターンリクエストなどを設定し、最低限のラベルを2つ設定します。一つはチーム名、もう一つはサービス名です。これにより、トラッキングインサイトの構築が助かります。アラートが出たときにメタデータを利用して適切なチームにそのアラートを転送できます。

次に、簡単なやり方でアラートを作成します。自動的にボックスをチェックし、SLOバンドファーストバンの場合にはエラーが出ている場合、SLのオブリゲーションに問題があることが分かります。次に、アウトオブボックスで素晴らしいダッシュボードが示され、自動的にSLOのパフォーマンスが示されます。ここでデータがあって、進捗状況を確認できます。

問題があった場合には、潜在的な影響を受ける可能性のあるお客様を特定し、先手を打つことができます。様々な中核的なマトリクス指標をトラッキングし、チームのSLOの達成度合いを確認することが可能です。

ここでアラートが発生し、次の段階に進みます。ここでは、検出からレスポンスへと進みます。SLOのパフォーマンスが落ちている場合、通知のポリシーに従ってチーム名が指定され、ルーティングされます。チームを見て、オンコールのスケジュールを調整する必要があります。

このスケジュールは大変です。午前中にコールスケジュールが良くないギャップがあるという状況になっています。他にも様々なオプションがあります。ユーザーにとって使いやすい方法を提供する必要があります。ウェブインターフェイスに入ることもできますし、GoogleカレンダーやOutlookのカレンダーを使ってスケジュールを管理することも可能です。

加えて、シフトスワップをリクエストする機能もあります。ここでクリックしてリクエストを入れると、チームに通知が行きます。この時間は誰かがカバーしてほしいことが分かります。オンコールコンピュータでは、例えば寝ているかもしれませんので、このモバイルアプリで受信できるようにする必要があります。

Grafanaのコアアプリでオンコールのスケジュールやアラートを簡単に確認できます。インシデントがあれば、影響を受けているサービスやメタデータを確認できます。問題が解決した場合、インシデントクリアの手続きを行います。

インシデントが発生した場合、全ての自動タスクが設定され、ポストインシデントレポートが作成されます。情報をGoogle Meetで集め、必要な方に参加してもらって問題について話し合います。スラックやMicrosoftチームでも問題があれば、情報がそこに掲載されます。

インシデントレスポンスにおいて、役割が重要です。誰かが調整を行い、各チームが重ならないようにします。例えば、トムさんが調査やトラブルシューティングを担当することになります。他に取り込みたい方がいれば、その方を入れて通知を受けるようにします。

タスクを作成し、フォローアップをし、再発防止のために検討します。インシデントのコンテクストを知っている方がいる場合、そのデータを見て関連するものを確認します。例えばログパターンを見て、エラーが起きた理由を確認します。

これらのプロセスは非常にパワフルです。最近のデプロイメントやインフラ側で何かあった場合もトラッキングされます。これは学習フェーズであり、何を学んだかをドキュメント化し、チームと共有することが重要です。

全体のトレンドやサービスごとの状況を把握し、必要なリソースを確認することが重要です。これが全体的なライフサイクルであり、インシデントの検出から始まります。アラートのノイズに困っているチームには、重要なところにフォーカスを当てる必要があります。

最後に、文化的な課題は簡単ではありませんが、取り組むべき課題です。我々はエンジニアとして、まずはどこかのチームに集中的に取り組んでみてください。その後、フィードバックをもらい、改善を行っていくことが大切です。その得られた教訓を他のチームにも広げていくアプローチが良いと思います。

## Raw YouTube Transcript

こんにちは今日 はグラファなクラウドエントエンRM ソルーションについてお話をしたいと思い ますグラファなラボーのえコー ファウンダーであり ます今日は東京に来れて嬉しく思ってい ますライブでも今日は行おうと思っている んですけれどもアイライソリューションを 使ってどう皆様のリスポンスをえ加速 できるのかということをお話をしたいと 思いますけれどもまずirmエントエン エントエンドのirmの意味についてお話 をしたいと思いますご質問があれば スライドに質問を入れてくださいま内容が ただま多いのでもしかしたらあまり時間が ないかもしれませんもし9Aの時間が なければ後で私を捕まえて質問して くださいでは最初 に皆さ教えてくださいもしもです ねエンジニアの皆さんプルリクエストを 引き出すのが簡単であると思われる方手を 上げてくださいはいま大体こんなもんです よね例えば次にインシデントのえその宣言 が楽にできると思ってらっしゃる方手を 上げておいてくださいまそう思いますそう ですよね多くの組織にとって何か インシデントの登録を行う際えそしてその 後続のプロセスについては複雑ですしえ 手間がかかりますこういった問題というの は本当にその的なプロセスが必要なのか どうかあるいはえ修正してすぐ過に進めば いいのか判断が難しいもの ですえチームとしてどうベスト プラクティスを導入できるのかそして価値 を高めるためにはどうすればいいのかそう いった指点についてお話ができればと思っ てい ますまそういった内容について今日は触れ たいと思いますがー するということをPRを開くの同じぐらい 快適にするということは可能だと思ってい ますただ難しいことではありますがただ そこには文化の問題がありますヒト プロセスそしてツールにまたがるえ課題だ からですなので注意深い対応が必要になり ますえ人そしてえプロセスカルチャーの 問題というのは複雑な問題です我々は エンジニアですのでエンジニアの観点から 問題を対応解決したいと思っています つまりツールを使っプロセスを改善し そして え摩擦のない血管のないインシデント レスポンスを実現できればと思っています 文化的な変化つまり文化を成熟するために はビリティのirmを使って オブザーバビリティは成熟できると思って いますつまりデータを使って複雑な システムを理解するということです オブザーバビリティネイティブのirmと いうのはツールを使うことでえそして データを使い目の前の問題を理解すると いうことですプロセスを略化しまして一貫 的な一貫したプローでフローを使って最終 的なえ結論を導き出すということです そしてインシデント管理におきまして データのすぐ横にテレメトリーをすぐに 置いてそしてええプロセスインシデントに 対応するわけですこちらシンプルな インシデントのライフサイクルになります まず特定をします問題を特定をしまして 問題があることが分かりましたら対応し ます特にえ顧客にインパクトを与えてる ように場合にはお客様へのインパクトを最 消化しないといけませんさらに同時に重要 なこと はこのプロセスから何が学べるのかという ことですどうすればこのライフサイクルを 改善しそして検出そして対応のプロセスを 改善できるのかとそして改善のプロセスを どう続けられるのかという観点が必要です このプロセスを考えてみてください プロセスが崩壊していたとするとえ フラストレーションがエンジニアの間で 生まれてしまい ます例えば問題があった際にアラートが多 すぎるとアラートが来るとエンジニアはま それは気にしないでくださいとこれ実際 問題ではないんです無視しても大丈夫です よとあるいはもう勝手に治りますよという ようなエンジーもいるかもしれません アラートがが出てきてもそれを無視すると いうこともあるかもしれませんあるいはも ラートが出ても適切な対応者対応チームが 誰なのか分からないということもあるかも しれません適切なデータがないかもしれ ませんチームとして適切なダッシュボード が見つからないので適切なログも見つから ない具体的なサービスが特定できなという こともあるかもしれませんま個人的な経験 を考えてもアラートが多くてそしてえ アラートが大きくてそして皆様が対応が 求められるそしてですがま次々に問題が 起こってくるのでまとりあえず一時的な フィックスを提供して底適用してそして次 へ住むということもあるかもしれませんで もえその再発防止のためには根本的な原因 に対応しないといけませんえこのプロセス は本当にえフラストレーションが生じまし てえエンジニアもえ定に回ってしまうと いうことがありますアラート図れという 言葉がありますけれどもこれ大き問題です よくえお客様のユーザーの間でもあるわけ ですSLのレベルで問題があまりにも多く 生じた場合ノイズをうノイズが出てしまっ てシグナルが見えなくなってしまうわけ ですですのでエンジニアとしてどうすれば えそのツーリングを改善してその状況に 対応できるのかを考えるべきだと思ってい ますではここでデモをお見せしたいと思い ます こちらですまずsloから始めますこれは シンプルなアプローチでベスト プラクティスでもありますアラートをどう 作ってそしてアクションにつなげるのかと いうことですエンジニアとして大切なのは しっかりとエンジニアをえ午前3時にま 呼び出しがかかったとして実際sloと いうのが実際のアクションに直結すること が必要ですまず は新しいアラートを登録作成します そしてプロセスを乾燥化するためにオピニ まこだわりのなるソリューションを作ると いうことですsloレベルで様々な選択士 があってえそして難しい意思決定をえその 排除しえそのそのためにテンプレートを 用意しておきますそしてインディケーター を基準として使い にい成功の定義を明確化しておきますウブ ベースサービスだったり通常よくあるのは いくつかえ成功のための基準が設定されて います5え例えばこの例だといくつかえ 成功を達成するための リクエストえ200msのうちに対応し ないといけないエラーを特定をしないと いけませ ん具体的なリクエストに対してそして達成 できたえ件数を比較することで比率を計算 をします成功率を計算しますそして クラスターごとにそういった計算を行って サービスに問題があるような場合にはどの クラスターで問題が生じているのか特定に 利用します1つのクラスターは問題がいい 問題がない他だと問題が多いというような 場合にこのデモ環境で見ていきたいと思い ますけれども次のステップとしまし ターゲットを設定しますライアビリティの ターゲットをどこに設定するのかという ことですけもアウトオザボックスで我々の 提案としましてはガイダンス をすでに用意していますけれども何がこの サービスだと達成できるのか実際の サービスのデータを使っていますがこちら はえ信頼性に関連してアラートが複数ま出 ておりまして確認対となっています8% タートするのかエラーターゲットえ信頼性 ターゲット80%するの かえ85%に設定をするとあるいはえ達成 できない可能性もありますまず信頼性を 高めるために はえそのただアラートが多くななりすぎる 可能性もあります次名前とその デスクリプションについて デスクリプションSLのデスクリプション をどうするのか簡単な言語で描写すべき ですペイメントサービスのリターン リクエストだっ たりとシンプルなものを設定しますそして 2つのラベルを最低限設定します1つが チームネームもう1つがサービス名ですえ そしてトラッキングインサイトの構築にえ 助かりますアラートが出てきた時にメタ データがを利用して適切なチームにその アラートをえ 転することができますそして実際アラート をいくつか成しますま簡単なやり方を選び ます と自動的にえそのボックスをチェックして もらいますスローバンファーストバン スローバンの場合にはえ 出エラーが出してるような 場合SLのオブリゲーションに 可能性があります例えば大きなその停電に なった場合あるいはバン3が早いような 場合そこからデビューに行きまして全て 問題なく見えます そして次へ行きますそこからアウトオブ ボックス で素晴らしいダッシュボードが示されてい てそこで自動的にsloのパフォーマンス が示されていますこちらにデータがあって 自動的にここで えこをかけて物事がどう進捗してるのか 見ることができます問題があった場合には ま潜在的なえその影響を受ける可能性の なるお客様を特定をしましてえ先手を打つ ことができ ます様々な中核的なマトリックス指標を トラッキングをしてチームのsloの達成 度合を確認することができますえここで アラートがファイアーされてその次の段階 ですえここではこの検出えという部分え次 にレスポンス です大きく落ち込んでいますsloの パフォーマンスが落ちていますえこれは アウテリアされていますのでそこからえ ここで通知のポリシーをフォローし てチームネームが押しあるデモであれ ば東京コンチームにルーティングすると いう風に書いてありますのでチームを見て オンコールそからスケジュールを築 えこは柔軟性を持ってスケジュールを構築 できなければいけないということでえこれ はチームが大きいとその調整ですとか マネージが大変になりますが色々な オプションがありますこれうまくいく そしてユーザーに使いやすいような方法が ありますですのでウェブインターフェイス に入ってもいいしグルグルカレンダー あるいはアウトルックのカレンダーに入っ てそしてスケジュールをここでマネージも でき ますということ でえスケジュールかなり 大変ですがもうすでに午前中これはコール スケジュールとしては良くないギャップが あるからという風になって ます他にも色々とできます例えば我の ユーザーにとっての特徴としてはシフト スワップをリクエストするここでクリック してえここはちょっと出張なの ででリクエストを入れるとそうすると チームに通知がってこの時間はないと どなたかこれはカバーして欲しいという ことが分かりますそして誰かが他の人が これをカバーしてくれるとえ誰かその説明 をする人を誰か捕まえてこなくてもいいと いうこと ですそこでオン コールコンピューターではなくて例えば寝 てるかもしれないえその場合にはえこの モバイルアプリとして受信ができるように しなければなりませ んですのでグラファな コアンダアイデムオンコールということが ありましてこれもデモをしてみたいと思い ます私の電話 ですアプリをローディングしてえ インシデントがえ全部出ていますえでオン コールのスケジュールもありますしえオン コールの時誰が他に入ってるか スケジュールどういうかて言ったか簡単に 見れます あとアラート もファイアーされたアラートえ現在 アクティブなものななど見れますそれから インシデントがあればこれらも見られ ますアラトこれもう解決してます ね40分45分 前えこれが電話に届くと通知が出て見ます でえ何が影響を受けているかサービス影響 を受けてるところそれからメタデータも出 てますえどのクラスターに入ってるかそれ からネームスペックななど全部見られ ますでこれを使いやすくしているん ですこれはかなり深刻な問題だていうこと であればこれはインシデントクリアします 名前をつけ [音楽] て問題があると ペイメントで問題が あるちゃんと綴れてないです ねそれからの重徳度これはメイジャーです ペイメント決済なのでこれは我々の サービスの重要な部分でありますので深刻 だとえそしてラベル付けをします ペイメントラベルトラッキングできるよう にしてきますそこ からクエリこんなに簡単なんですこれで エンジニアがこのインシデントをクリアし ますそして次にこのえ何が起きてるの かっていうことしっかり理解できますそれ からグラファなクラウドに戻りますえ インシデント一覧を見ますこの新しい壱が ここにも出てきましたここに入ってこの 問題にすぐ取りかかれますこの インシデントがクリアされるとハで は様々なこの自動タスクが設定されてい ますのでこのポストインシデントレポート Pを作ってそして情報をに入れてそして Googlemeetを作っ てそしてオンラインコールを開催して そして必要な方に参加してもらって問題に ついて話し合いますスラックチャネルでも Microsoftチームでもしかり ですまたラブックがあればそれらもここに 出てきますのでチームがすぐにその問題に 取り掛かれるようになっていますそれから 情報もこちらに掲載してそして記録として トラッキングできるようにしてきますそれ からスラックMicrosoftチームで えこのチャンネルにシンクされますので 同期されますのでえスラックでもえども そうですスラックに入ってそして メッセージが掲載されたらこれこちらの ページにも同期しますですので何が起き てるのかのログがしっかり掲載されてえ 状況は理解でき ますこれは今力入れているものです インシデントレスポンスにおいてしっかり とこの定され役割があるということ誰かが この調整を行うということそれをきちんと 担保しています誰かが手掛けているお互い にその重ならないように邪魔にならない ようにていうことでトムさんがここに指定 されました彼は得意ですのでそして調査 トラブルシューティングえこれは私にし ます他にえこの中に取り込みたいという方 がいれば例えばこのシステムの専門家が いればその方もしっかりとユーザー得意的 にここに入れてそして通知を受けるように しますまた別のチーム例えばインフラ チームと話をしたいっていうことであれば インフラ側の問題でもあり得るということ であればそのチームを入れてそして通知を してそして関与してもらい ます またえタスクも作れますえこれはフォロー アップしたいというものしっかりと終え なきゃいけないようなものこれもしっかり トラッキングをしてえこれについ てこのような問題の再発を防止するという ことも検討していきます からもう1つログそれからケイパビリティ の話をしましたがimlでも話をしました がシフトですこれも素晴らしくてこの インシデントのコンテクスト内容ですね クラスターネームスペースを知ってる方 あるいはタイムレンジいつ時間起きたか 分かってる方がいるのでそのデータを見て グラフなクラウドの中で見ていくえそこで 関連のあるものをここに入れます例えば ログ パターンここで見ていくとこ にエラーが起き てそしてロボリュームが変化したそれを ちょっと見 てあるいは生成の能力を見てログ メッセージの意味はということも聞けます あるいはえこの後その修正には何すれば いいと言ったようなことも分かりますこれ とてもパワフルです他にも最近の デプロイメントあるいは最近インフラ側で 何かあったとかノーリースタートとかえ そういのものもこののバブラップして もらいますえこれ満足してい ますこれは解決しましたえこれは学習 フェーズですこのチームでも今うまくいっ てそしてプラ問題も解決しましたが今度は サマリーを書かなきゃいけないということ でこれも重要ですこれは必要と思われるか もしれませんもう解決したんだからでも しっかりと我々何やった何を学んだか教訓 などしっかり記録化して文書化してチーム と共有してえそして様々なトレンドなどを 理解するのに重要ですこれはもう1つ ボタンを押せばこれを私のため作ってく れって言えばえ生成愛て本当に素晴らしい ですよねこの情報が出てますこれはとても いいサマリになりましたこの ケイパビリティでよりえこのエンジニアが 書くよりも良いPができるんですこの報告 書が出来上がりましたこれ十分いい ですここからはタスクです ねこのタスクを作ったらそれをフォロー アップしてそしてそれを後でマネージ できるようにしますそしてこのタスクを 作ると例えばギットハブなどでえこれを一 周として作るそしてチームがそれを フォローアップできるようにしてきます そしてあるいは他のタスクもあるこのよう な形でトラッキングをしてそういった タスクがしっかりと設定されるそして誰か がフォローアップをしていることを担保し ていき ますでは他にも何が学べるでしょうか色々 とインシデントが起こっていて そして学ばないといけないわけです前回の サイクルからトレンドを確認ませirmの 能力だったりアウトオブボックスえ ダッシュボードだったりを使うことでイン サイトを得られないといけません ボリュームの動きがどうだったのかだっ たりえ解決のための時間はどれくらいだっ たのかチームごとの状況サービスごとの 状況を確認をして前回のサイクルにおける インシデントリスポンスの状況を確認でき ないといけません全体的に組織内でどう なっているのかどのサービスがうまくいっ ていてどのサービスが期待値に達ししてい ないので追加的なリソースが必要なのか だったりそういった全体図を把握できない といけませ んそれではまでもは以上 ですこれまで に話をした内容がライフサイクルのえ検出 を始めいろんなステップについて話をし ました最初が検出ですsloがアプローチ をするにあたっては1番のアプローチだと 思っていますアラートのノイズに困ってる チームには大切な1番重要なところに フォーカスを行うことが必要ですサービス においてデリバリの観点で1番重要なのは ど何なのかその成功率をモニターする必要 があります期待値に 達成達しているのかどうかえを基準として 設定をしスローバンファストバンもそう ですけれどもえそのユースケースがあり ますので大切な基準の測定を行えることが 必要ですアラートのこちらは進化プロセス ですま何百何千というアラートが今出てき ているという状況においてヨーロッパのお 客様の場合にはえその数千件のアラートが ありましたなのでこれは多くのノイズに なりますアラートが多いとシグナルが 埋もれてしまいましてユーザーとしてもで はどういった時にどう対応すればいいのか が分からなくなってしまい 予算ベースのアラートSLアラートだっ たり色々アアラートが出るとお客様に 対するインパクトがあるということは 分かるので何かしないといけないという ことは分かるわけですがもっとそこから先 に行って潜在的な問題が実際問題になる前 に予測を行って対応できないといけないと 思ってい ます今後進化するにあたっ てリースだっも増やしながらえその対応力 も対応し潜在的な問題にも対応できるよう にしておくということが必要であると考え ています次に我々の対応について考えて いきたいと思います必要なの はリクエストを適切な方々にえ届けること が必要です適切なチーム人材が適切な タイミングで対応するとことが必要であり データのアベイラビリティをるということ が必要ですプロセス全体においてデータへ のアクセスそして必要な情報を見つけ られるようにするということそして原因が 特定できるようなえ仕組みにしておくと いうことが必要でそれを行うためのチーム との調整体制も必要になりますそして学習 段階になりますと必要なのはこのステップ を よく飛ばしてしまいがちですが問題を フィックスした終わりではありません学び をドキュメントに記載するということが 必要でトレンドは何だったのかどういう トレンドがあってサービスにどういう影響 が出たのかその学習学びをまとめる必要が ありますインシデントポスト インシデントドキュメントの中に知識を 共有する目的で学習内容をまとめる必要が あります主要なインサイトは何だったのか 測定の結果パフォーマンスの測定定結果得 られたインシデントをまとめることが必要 ですグラファなクラウドはsloのレベル で検出のステップから始めデータを揃え データを確認をし問題に対する認知度を 高めます問題が見つかりますと適切な チームに問題をつなぎまして彼らに関与し てもらい彼らが適切なデータだったりに アクセスできるようにしないといけません そして対応者適切な人たちが対応できる ようなえ仕組みをまプロセスとして作って いくということが全体的なマネージメント を用意にしますそして最後学びが必要です 学びが必要な理由は改善をライフサイクル を完全改善するということもあります けれどもそれ以外に もえその信頼性を高める にえそのプロダクトサービスの問題点血管 に対応できるというえ機会を生んでくれ ますので学習の段階も必要ですこれが全体 的なライフサイクルでそれを1つの アプリケーションにまとめ上げることで 大きな価値を提供し成功の可能性も高まる と考えてい ます色々と話をしました文化的な課題は 簡単ではないとです が取り組むべき課題であるとそしてで ツールをしっかり使ってえツールによって 導いてもらうということそして プロセスを継続的にしえ習慣的にすると いうことが必要です我々としては オピニオンてィとまこだわりの強い ソリューションを構築しその人数に対応 しようと取り組んでいますま文化的な点が やはり背後にありますのでまずは エンジニアと話を今うまくいってるのは何 なのかエンジニアの観点から何が望ましい 望ましくないことは何なのかしっかりと 会話をすることが必要だと思っていますえ チームとしてしっかりデータにアクセスが できているのかインシデントからどういっ た学びがあったのかしっかり コミュニケーションを通じて学習の サイクルも寝付かせてくださいよくあるの がままずはフォーカスしたい エリアにフォーカスを置いていくという ことそしてえ負担を軽減していくことが 必要 ですまず全てのチームからではなくてまず はどこかのチームを選んでまずそのチーム に集中的に取り組んでみてくださいそこ から色々なフィードバックをもらってその チームに関連した改善を行ってきます色々 と学習をしてそしてその得られた教訓を他 のえチームにも広げていくという アプローチが良いのではないかと思います

