# Best Practices and Demo: Grafana Cloud&#39;s End-to-End IRM Solution | Grafana Labs

Grafana Cloud's Incident Response and Management solution provides workflows that span creating alerts and SLOs, managing ...

Published on 2025-04-04T18:28:00Z

URL: https://www.youtube.com/watch?v=y_Ewv9azoE4

Transcript: Hey everyone. Uh, welcome to users's guide to Grafana, the cloud's endtoend IRM solution. I'm Will and I'm Joey and we're part of the IRM team at Grafana. Today we're going to talk about IRM and Grafana's approach to how we run instance and responding to them. We'll look at in small part some of the cultural aspects to think about when you're looking at your own instant response and we're going to show a demo of some of the uh showcasing some of the ways that Graphana's approach can help you and your teams. As always, we have a QR code to the slido link. Feel free to hop on there, ask us some questions. We can answer them at the end of the talk. Feeling a little shy, we'll be at the Ask Experts booth outside. Feel free to grab us there, too. Love talking about incidents. So, please come towards us. Uh, and to start, I'm going to ask for a little more audience participation. Quick thumbs up from folks. Who here feels good that they're able to open up a PR when they need to make a code change? Thumbs up. Seen some good thumbs. Yeah, great. Keep those thumbs up. Keep them up. If you feel equally confident that every time you get it right when you declare an incident, it's not a bug, it's not a yuck, it's definitely an incident and we definitely need to page people and potentially wake them up. Feeling a little shakier about that maybe a couple people and that's okay because it gets hard doing this. Uh that's a challenging part of what we do. What is the nuance that exists in there? At Grafana, we want to enable you to feel confident in that decision that you're comfortable declaring the incident just you would open a PR and that the incident management process can be empowered by those tools. So, how do you do this? How do you make this big change to your org so you feel confident and it's hard? First and foremost, you need buyin from the teams that are going to respond. They're your experts. They need to have confidence in your system. They need confidence they're making the right decisions and they need to have clear understanding of what's going on. When you're on call and something goes wrong, do you think, is this really worth paging? Is this really an incident? Do I need to start the whole process for an incident? Do I need to potentially pull people in from their day-to-day work? They're busy doing things. Do I need to think about if I open an incident, am I going to get in trouble? Am I going to potentially have a black mark on our perfect record of no incidents? Is it just too much work to do all the process and explain to folks and share expertise? Yes, this is why we think this is an incident. Yes, we understand what's going on and no, it's not impactful to customers and try and talk to customer support. That's going to be challenging all the ways you need to bring this out to other people, share this expertise. So it's cultural change and it goes across tooling, people and processes. You need to be intentional about the tooling you're using because you want to enable frictionless, blameless incidents, lower the anxiety as part of them. I like to think about incidents and breaking down two aspects. First, the diagnostics part trying to figure out just what's going wrong. Is this actually an incident? How impactful is it to our customers, to the operators behind the system? Do we have a quick solution for this or is it going to be a multi-hour incident? That sort of thing. And the second part is the communications. Who needs to know about everything that's going on within your incident where the stakeholders need to inform customer support? Perhaps a senior leadership needs to be informed about major impacts, high paying customers that need to be told about what's going on, maybe even security vulnerabilities, that sort of thing. As part of this, you want smooth processes that people understand they can communicate across channels. You want tools that make it fast and seamless. And you need people who are going to respond to it because people are the adaptive part of your systems that are going to approach these incidents which are surprises and understand their systems, be able to respond to them in novel and new ways. We believe this is best done with observably native IRM. For us, that means putting your incident management tooling close to your telemetry data. You don't want your engineers looking at telemetry data and saying, "Great, I think what's going on here." And dragging it all the way to a new system and trying to update information and notes and such. And then someone says, "Actually, I have a question about this note here. Can you dig deeper into this dashboard?" Okay. You go back to your dashboards and dig in further and try to figure out, "Okay, I think this is a dashboard. Actually, I wanted a time slice that was two minutes earlier. Okay, going back over here now and jumping back and trying to think. That's a lot of thrash back and forth. You want to try to avoid that sort of thing. We've been building observability native iron tooling for instant response. We understand the problem space and we're working on this to make this flow seamless. Now, there's a saying that all models are wrong, but some are useful. And I'm going to try to apply that heristic here to this here broken down. Now, we kind of think of an instant as the tech phase, the respond phase, and the learn phase. Now, in reality, there's actually small little eddies of each throughout the whole entire process. When you're detecting, you're doing a little bit of learning, trying to understand your system. When you're responding, you have to go back and detect again. Was that actually right? And jump back and forth with questions, that sort of thing. But breaking it down, have a macro level of it. Understand basically the flow. This is a good place to be in because you want to detect issues early and you respond to them quickly because that's where you want the data to be. And then you can capture all this context in the learning phase after the fact. Prove your response, figure out how you can respond faster, detect faster, and the cycle continues. Now, what happens when that cycle breaks down? What are some ways it could? Well, for example, maybe your customers are telling you the problem before your alerting is, or it's hard to pull in the right people when you need to, the experts in any given situation. Maybe it's hard to figure out which of the dashboards you need to use or how to get the data you need to. Maybe you're doing post instant response and in your reviews, well, you throw like a list of 20 action items and you don't get any of them done. or maybe worse, you do the action items and you repair one bug and open three new ones later on. And if you're in then situation kind of this reactive state figuring out what to do. Now, here's where I maybe lose a few of you. I'm suggesting today not to have fewer incidents, we have more incidents. Now, Will, you say I'm no incident expert, but I'm pretty sure incidents are bad for our systems, whether the customers complaining about not using our service and the downtime and losing money and all that. And that's fair. It's true. I'm not suggesting we create new incidents. I'm saying more often than not, we declare more incidents. For example, how many people here might have had a near miss recently? That makes sense. Things didn't actually go wrong. you missed the incident, but that's still a fantastic place to learn. There's expertise there that we can share. Experts figured out something before it became a problem and solved it. That's a great place to dig into for your instant response. We also can't prepare and learn from anything, you know, all the other situations we come across as well as we can for novel incidents. Sure, there's game days and there's stress testing and they're wonderful tools and wonderful processes for our systems. But instance by their nature are surprising and there's nothing that teaches more than a surprise. Declaring incident should be worthy of a practice in our system. Our observability based IRM tooling will empower your teams to do that and show you how to do that. Joey's going to run through a demo of some of the IRM tools. Cool. Thanks, Will. Uh yeah. So I'm gonna run through a demo uh and I'm going to discuss the three phases that uh Will was just talking about. So first we're going to go through the detect phase. I'm going to show you how uh we've made it really easy to create and manage your SLOs's in Graphana Cloud. Then on the respond phase, I'm going to show you responding to an incident. So this is going to be an alert that's firing a fast burn alert on an SLO. We'll declare an incident on that and we'll go through the incident workflow in Grafon IRM. Uh and then lastly on the learn phase, I'm going to show you some of the insights data that you get out of the box in Grafon IRM. Um and then something that was in a recent release in Graphana Cloud, what we're calling service center. I'll show you how you can use these tools to do things like uh operational excellence reviews with your team. So this is going to be based on a scenario. So, it's going to be a team that's going to be owning a payment service and like I said, they're going to set up their SLO. They're going to get alerted, declare an incident uh and then learn from all that. Um, and then building up uh on top of what Tom announced earlier this morning uh during the keynotes uh about Grafana IRM, I'll actually be doing the demo in the new Graphana IRM app today. I don't mean to tease everyone. It's not yet available for you in production. Uh we'll be rolling that out next week. Um, but look out, we just uh published some blog posts this morning that have like all the details about that. So take a look at that if you're interested. Um, yeah. So let's dive right in. Uh, so we'll start actually in Grafana SLO. So this is Graphfana. You can see my screen good. This is Graphfana SLO uh in in Graphfana clouds. Um, and one of the things that we we really focused on here, um, is making it as easy for you, um, uh, and opinionated during the create workflow. So, the first part of this is defining your SLI or service level indicator. Um, so I've gone ahead and pre-filled most of the information out uh already here, but um my success metric here is looking at um successful requests uh under 200 milliseconds to an endpoint that I have called payments over the total number of requests. I'm going to group this by a cluster label. Um, and that might be helpful for in the context of an incident, giving me some information about if this is regional or not. Um, and then I can come down here and I can just kind of see over the last couple of weeks how that uh query is performing. Um, so next we're going to go ahead and set our targets and error budgets. Um, so one of the things that we actually released in the last couple months here is uh what we're calling an ML enhanced SLI targeting. So, this is just a pretty cool way of when you're setting up your SLOs's, uh, being able to see based on your historical data, what's the probability that you're going to, uh, meet your target. So, my team, they're pretty new to SLOs's. I want to give them a good chance of actually meeting their objective um, or meeting their targets. So, I want to give them about a 90% probability uh, of making their their SLO. So, that gives me about 78. Uh, 79 looks good. Next, you're obviously going to want to give your your SLO a name. So, in this case, it's the payment service uh response time. Um, I've gone ahead and put that in the Graphana SLO folder. Um, labels are obviously a really good way of organizing things, but they're also going to be useful for some of the IRM workflows that we're going to go through later. Um, and then you're probably wondering, so how do I create uh alert rules uh for my SLOs's? So, it's actually super um super easy. One click and you get a fast burn and a slow burn alert rule created automatically for you. And then of course you can come in here and you can review everything. So I have my target and error budgets. My alert rules are created for me. Cool. That looks good. Um next we'll just jump over to a dashboard. So when you go ahead and create an SLO, you get a dashboard created for you. Um and you can just quickly glance here and you can kind of see the historical performance. So over the last 30 days, how is my SLI performing? Um because we grouped by cluster, I can come down here and I can kind of see how is my SLI performing across my different clusters. Cool. So that's everything on the detect side. Uh now we'll move into the respond phase. Um so yeah, so this is the new IRM app. Uh the schedules editor probably looks pretty familiar with what you're you're used to today. Um but one of the obviously one of the important things uh in IRM is making sure that you get the right alert to the right person at the right time and a big part of that is your on call schedules. Um so we've tried to make it really easy in the UI in the the UI here to give you some really useful tools when setting up your schedules. So for example up here at the top I can just quickly uh glance and see all of the people on my team and see their time zone. Um, and this might be useful when you're setting up your rotations to give you an idea of is this person going to be awake, asleep. Um, there's some other stuff, uh, like you can see the quality of your schedule. So, if your schedule has any gaps or maybe there's some users that are overloaded or uncall more than others. Um, one of my favorite features in the schedules though, and I may be a little bit biased here because I actually built the feature myself, um, is the shift swap request feature. So, for example, I'm on call for this schedule next week on the 19th. But let's say that I have a dentist appointment. I can just come in here. I can create a shift swap request. This is going to send out uh notifications in in Slack, push notifications through the mobile app. Um and then anyone on my team can just accept that. Um and it kind of takes off some of the burden for me finding coverage for my my on call shift. Uh the last thing worth mentioning as well uh while I'm in the app is that obviously you can come in and you can click around the web UI to go ahead and configure everything. Uh but you can also do uh do all of this through infrastructures code. So we have Terraform and cross crossplane providers for uh most of the IRM stuff. Uh cool. Okay. So next I'm actually going to move over to the mobile app. I told my girlfriend not to text me while I was uh doing the demo. So, we'll see. Um, cool. Okay, so yeah, this is the mobile app. So, the Graphfana IRM mobile app. Uh, we have a mobile app in case you didn't know. It's pretty neat. You can come in here. you can get a glance of uh all your alert groups, um incidents, schedules, and you can do most of the actions you can do on the web via mobile as well. Um but I'm actually going to go ahead and um respond to an alert group here. So, I'll go ahead and filter by an escalation chain that I want to look at. Um and this was one that got created earlier. So yeah, so this is a fast burn alerts uh on my payment service for the SLO that we set up earlier. I can come in here and I can just s quickly see some context about this, some of the labels. You can see the timeline for the alert group as well. Um, but this looks pretty serious. So I'm actually going to go ahead and declare an incident. So we'll just say it looks like there's an issue with the payment service. And the payment service like this is this is customerf facing. It's a pretty serious pretty pretty critical service. I'm going to go ahead and mark this as critical. We'll go ahead and add some labels here. Oops. Cool. So, I can see that my incident is created. We'll go ahead and just copy this link and then we'll hop back over to the web for the rest of the incident triaging process. Um, yeah. So, the incident workflow. So there's a lot of things that we we automate for you to try to help you out uh with the incident uh triaging process. So for example um we'll go ahead and create things like Slack, MS Teams channels, uh a Google Meet room, even post incident report doc. Um so you know just trying to make it take some of that burden off of your team and make some of these things more routine. Another really important thing about uh uh running an incident is roles. Um so roles, these can really help uh clarify responsibilities. So you might have in your incident response guide, you know, if you're assigned a commander or you're assigned investigator, here's here's your responsibilities. Um we also allow custom roles, but in this case, I'm just going to go ahead and assign a commander and investigator. So, I'll assign myself as the commander and I'll assign Ryan as the investigator. Um, another cool thing is that you can actually uh page people and pull them in directly into the incident. Um, so I'm going to go ahead and do that. Go ahead and page in. Okay. Actually, yeah. So, it's telling me that in a I forgot that he lives in Singapore and it's 3:30 for him. So, I'm gonna page someone that's lives in the Eastern time zone. So, let's page Ryan actually again. Cool. And then we'll page a team as well. So, we'll page the app M team. Uh and and when you page a team, it's just going to uh pull the person that's on call uh for that team. And when it pages individuals, it's going to use their personal notification rules that they've gone ahead and set up uh within IRM for how they like to get uh notified. Um another cool thing. Okay, so it looks like Edward's come in and he started investigating things. This is pretty cool. So, um, in Slack and MS Teams, when you add a little robot emoji to, uh, to messages, it's going to go ahead and pull these into the timeline here. So, the timeline is really useful just for building a picture when people join and they can see what's going on. Um, you can post links, documents, pull requests. Um, so that's really helpful. Um, another thing I wanted to go over as well is Sift. So, SIFT is an AI ML uh integration that we have. Um, and this is just one way that we're leveraging our unique observability native IRM position. So, SIFT uses context from your incident to run a series of targeted checks across your telemetry data. So, your logs, metrics, and traces. And then it's going to surface unusual uh log patterns or metric spikes that might be relevant to the incident. Um, and this is really going to help your engineers pinpoint potential causes faster, reducing things like meantime to resolve. Um, so in this case, I can see there's some resource contentions. Um, I'm going to click on the the logs. So there's five patterns uh with an increase found in error logs. I can even come down here and for example uh there's another LLM tool that we have where it'll try and summarize what's going on here in the log. So for this example, for this log, it looks like there's maybe an authentication issue uh with Postgres. Cool. Okay. So it looks like um this incident's all good. I'm going to go ahead and resolve things. Um and then another really neat feature is the auto summary uh feature. So, this uh it has an OpenAI integration and it'll basically take all of the context and the timeline that you've built and try to give you a summary uh of what's happened. Um, so this looks pretty good. You can edit this of course, but I'm going to go with that for now. Cool. So, we've resolved our incident. So, now let's actually move on to the learn phase. Um so this is our insights dashboards that you get out of the box within Graphana IRM. Um so these break down uh and give you kind of an overview of your total number of alerts. Um so for example I've drilled down here into my payment service and I can see that they've gotten about 370 alerts over the last 30 days. Our meanantime to respond uh was about 15 minutes. I can come down here and I can see uh things like who what teams and what users were alerted. And then on the incident side similarly I can see for example how many alert how many excuse me incidents did I have uh how many were were there by different severity. So kind of your your data sliced and diceed by different dimensions. Um and then so this is um service center. This is something that came out in a release recent release from graphana cloud. Um, and it kind of gives you a single view of a services incidents, SLO health, and dashboards. Um, and this in combination with the insights dashboards that I was just showing you, this can be really useful for doing things like uh operational excellence reviews with your team. Um, and this the service center is one area where we really want to invest more um in the near future. Um, so keep an eye on this. Uh, yeah. So that's that's just a couple examples of how we're helping you in your FA cloud to detect, respond, and learn. Um it was a lot. It's quick, but that's the demo. So yeah, now I'm going to pass it off to Will and he's going to guide you through some of our best practices. Great. So jumping back to cultural aspects and applying it from the demo here. Uh we want to think about the detection phase. How are our tools applying to our day-to-day work? and let's see how we can get a strong signal to noise ratio, which is important for any kind of alerts come in. Making sure that we're actually responding to things that are worthwhile responding. They're painful to you or your customers, that's worthwhile. If they're not, they're just annoying and toilsome. Probably need to skip. Alerting should feel like a journey. Your system is actually moving underneath you. It's constantly changing. It's dynamic. Your alerts should be as well. They are not static. Sometimes you need to move away from this reactive phase where every alert is a fire. probably need to get ahead of the game a little bit. If you're doing too much of that though, too much predictive stuff and there's a lot of noise that are things that are not burning down the house today or tomorrow, maybe you need to pull back on that too and try to tune your SLI to the important customer impacting metrics. Uh declaring the incident should be straightforward with folks feeling confident in their approach and there should be no stigma involved. If you get it wrong, that's okay. Even if it's off hours, no one likes get paged, but it's better than the alternative. Your practitioners on the sharp end are your experts. They should be there to understand the problem. We should trust them. Likewise, when you're running incidents, make sure to pass the role around so that expertise can be shared. Other people can be can level up in those incidents. Questions to ask are are people feeling apprehensive about declaring incidents? Do they know what it looks like when they're paged and where to go in their systems to find data? Could they escalate if need be? What's the story in the learning phase that you're telling your teams? It should be our teams made choices that were in the best interest of the stability of the service at the time of the information they had. Surprises along way are the interesting bits to tease out. Dig into those. Those are places for us to update our mental models. All the tools as part of graphana IRM help in each phase of the life cycle and we should look into that observability native IRM want tools that are tightly coupled and to avoid unnecessary thrash both the diagnostics and the communication side as part of that important takeaways effective IRM is a cultural problem it's nuanced it takes time you can't mandate it your IC has to be involved you want to reduce friction when pulling in data Otherwise, you're making it harder to diagnose and communicate ideas. And has to be a constant practice. You can't just do it during the incident. Can't just do it during the review. Incidents are a daily part of our work. How do we know that things are working? How do we know we're evolving our maturity? What are your near misses look like? What's scaring your teams? Dig into those places. The dark corners are places to learn. Sing leadership is encouraged, but I say should be there to run with it. And are your repair items meaningful? Are they doing anything worthwhile for your systems or making it harder? And start with the beach head team. People that are willing to invest and willing to hear feedback and iterate on things.

