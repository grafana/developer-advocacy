# Grafana Alloy Community Call (August 2025)

Published on 2025-09-03T19:43:55Z

## Description

Join the next Grafana Alloy Community Call: ...

URL: https://www.youtube.com/watch?v=EpNUHXUVD6Y

## Summary

In the August Alloy community call, participants engaged in a Q&A session with no formal agenda. Key speakers included Greg and Paulin, who discussed upcoming software updates planned for early September, such as the new Windows exporter and improvements in Prometheus dependencies. A significant topic was the transition from CS host to OS host metrics, where participants raised concerns about alerting issues during the switch. The conversation also covered tools for improving query writing, sampling strategies for traces, and the ongoing development of features for Grafana Cloud. The session highlighted the complexity of managing alerts and metrics, particularly for legacy systems, as well as the evolving capabilities of the Alloy platform and related tools like Grafana. The call concluded with a commitment to provide additional resources and support via Slack.

# August Alloy Community Call Transcript

Welcome to the August Alloy community call! We don't have a set agenda today, just open community Q&A. We may discuss a couple of internal topics, including what's planned for the upcoming release in early September. For now, let’s open the floor to community questions. If anyone has a question, feel free to unmute, raise your hand, or send a message in the chat. 

### Upcoming Release

Regarding the upcoming release, we’re introducing a new Windows exporter that will replace the CS host with the Windows OS host. I still have some older Server 2012 instances running Grafana version 1.8, which supports the old metrics. Is there a way to copy the Windows CS host metrics to the new Windows OS host metrics? I haven't seen a method to copy metrics; I know I can rename them, but I have concerns about the timing of when to do that.

Renaming is the primary method to shift those metrics if you want them to follow the existing series in your destination. If you have a specific pipeline you want to share, either during this call or later in Slack, we can discuss what those renames would look like. 

The issue is that the alerts are currently configured to reference the CS host. When I push the changes and start renaming, I will still have the CS host metrics for the older instances but not for the newer ones, which complicates my alerting.

I'm not sure if anyone else on the call is an expert in writing alerting PromQL queries that could help with joining metrics. I know it’s possible, but I don’t have all the resources handy right now. If you want to start a conversation in Slack after this call, we can try to figure out the right resources.

### Grafana Assistant

Just to add, there’s a tool called Grafana Assistant, which is an AI helper that might be useful for writing PromQL queries. It shows up as a button in the top right of Grafana. You can try asking it for help setting up a query.

I'm currently waiting for approval from our legal department to use it. I have also asked Grot AI for assistance.

### Alerting and Metrics

For my own understanding, since I’m still fairly new to the Alloy team, is this alerting issue a problem only for the past 24 hours, or will it persist after that window? 

It would only be a problem during that 24-hour window, and then I would have to adjust the alert to reference the new metrics.

I was really hoping to find a way to copy the metrics since I still need to support those Server 2012 instances, and there will be more issues in the future.

You could create a relabeling rule, which might give you more time. However, depending on how many alerts you need to update, one method might be easier than the other.

### Release Highlights

Is there anything else from the Alloy squad regarding the upcoming release in September that you’d like to share? 

One key update is that Poner has been working on updating Alloy's Prometheus dependencies to Prometheus 3. This update includes support for Prometheus remote write v2, which we’re still adding to the write queue. It will also introduce UTF-8 support for labels and metric names. However, if you're using Grafana Cloud, be aware that Mimir is not yet ready to handle UTF-8 values in labels.

The default values will remain the same as before, so it will continue to work for now. This update will also enable the propagation of metric metadata using the remote write component, which will be beneficial for many users.

### Questions from Last Month

I have a question from last month that I wasn’t able to ask previously. I heard about Appali and Infraali, and I was under the impression that Apali was based on OpenTelemetry. What does Infraali refer to, since these metrics primarily go through the Prometheus pipelines?

You’re correct; Apali is based on OpenTelemetry, and Infraali is based on Prometheus. Prometheus is currently the industry standard for infrastructure observability, which is why it makes sense to meet customers where they are. The official guidance is to observe applications using OpenTelemetry and infrastructure using Prometheus-native components to ensure matching label names and values.

### Filtering Noisy Logs and Traces

There was also a mention of using Alloy to filter out noisy logs and trace data, like health checks. Are there specific suggestions for that?

For traces, it’s best to avoid tail sampling if possible. You can filter out spans from health check services. While you still have a trace, you may want to remove the problematic span instead of the entire trace. 

If a log or trace is particularly noisy, a filtering rule can help. If you want to sample parts of the noisy spans, you can use probabilistic sampling, but be aware that it applies to the entire trace.

### Conclusion

It sounds like we have a lot of complexity with sampling, especially outside of Kubernetes. The configurations can get quite intricate. Fortunately, for those using Grafana Cloud, adaptive tracing is in private preview and will simplify the process of generating rules to drop health checks at ingest.

If you have any other questions or need to discuss further, feel free to reach out on Slack. 

Thank you for joining today! I will capture the links we discussed and post them in the Slack thread for this community call. I will also post the recording shortly. 

I'm stopping the recording now.

## Raw YouTube Transcript

All right. Well, welcome to the August uh Alloy community call. Um we don't really have an agenda coming into this, just community Q&A. Um we have maybe a couple topics that we can discuss uh from internal conversations as well. Depending on what folks are interested in, we can talk about maybe what's coming in uh what's planned and what's already ready for the next release coming in early September. But, uh, for right now, we're going to open it up for community questions. Got a couple couple folks here. If anybody has a question, feel free to unmute and and talk or raise your hand or send a message in the chat. Just pause for a second and let folks do that. On the upcoming release, we're getting the new Windows exporter that is getting rid of the CS host, Windows CS host. I've still got sadly uh server 2012 instances that I'm still running the old um the graphana that I guess is 1.8 that still supported the old stuff. Um is there any way to like go up? I can go update those and put it something in alloy to copy the Windows CS host to Windows OS host metrics. I I haven't seen a way to copy metrics. I know I can rename them, but then I still have an issue with timing of when to do it. Yeah. Um so renaming would be the the primary way um to to shift those metrics if you want them to basically follow the existing um series in your your destination. Um but um you know if you if you have a a specific pipeline that you want to share um in either on the call here or in Slack at a later time we can talk about what those renames would look like. Um, the release notes talk a little bit about the It's not the renaming that I have a problem with. It's that the alerts right now they're I don't know the right term, but joining to the CS host to get the name when I go push it out. And so it's looking for ones that were there that aren't there now. the up is not there and it has been in the last 24 hours. So whenever I push this change and start renaming that suddenly my I'm going to have the CS host there for the older ones and not for the newer ones and it just makes my alert problem weird. So I don't know of a way that you can join to either CS host or OS host whichever one is there. Uh I'm not sure if anyone on the call myself uh at least is an expert on on writing those alerting prompql queries that can support any uh you know joining between whichever metrics are available. I know that is possible. Um I've seen conversations about how to do it, but I don't have it loaded into my mind at the moment. U so there are definitely ways to say alert on this metric or this metric depending on on which one is providing a value but I don't have handy any any resources to point you to for it. Um okay. So if you want to maybe after this call start a conversation uh in Slack about that we can try to try to figure out the right resources or Paulin might be ready to come in. You are muted. Sorry. Yeah. Just want to say I'm super bad at writing proql. I have to do a lot of googling. I have to read the committee's book and then I also have to go to some AI tools to ask a bunch of questions. But the good news is that now there's this thing called graphana assistant and it's basically like an AI helper. I think it might be really good just for that kind of use case especially with like really weird punk things that you have to do very rarely. Uh maybe you can try asking it. I think it's just like some um just shows up as a button on the top right in your funnana. Maybe you can ask it about how to set up a query. Okay. Yeah, I'm trying to get that approved for from our legal department. So, wait a little bit on that. And I did well, I have asked Grot the Grot AI. I should try that. Yeah. Thank you. Hey, hey, Greg. Just for my own um knowledge, cuz I'm still fairly new as a team member on on Alloy, are you saying that this would be a problem uh if you're looking for like past 24 hours? Is this something that would only be a problem as soon as you deploy it for that 24-hour window or would this persist after that and continue to be a problem after that window has lapsed? It'd only be for that 24hour window. Um, and then obviously I'd have to change the alert to look at the new thing, right? Okay. Okay. I was just hoping there was a way to copy it because I still have to support those server 2012 things and there's going to be more things coming in the future that you know when you Sorry, Sam. I keep uh I don't know if they're functionally different, but you could create a re label rule. Um so that you know things from the new version are copied to the old which would give you some more time. U but at the same time it might be a little bit e yeah depending on how many alerts you have to update. One might be easier than the other. All right. Yeah, I had forgotten about that too. So that may be my solution for now. Thank you. Any other questions? Anyone on the Alloy squad uh have anything coming in next release, which is again early September, that they're particularly interested in in chatting about or letting folks know about. One thing that uh Poner worked heavily on is uh internally updating alloys Prometheus dependencies to Prometheus 3. Um that comes with a few interesting things. Um support for Prometheus remote write v2 will be uh in at least the remote rights component. We're still adding it to the right queue. That should get in before uh before time for release. Um but it also comes with things like UTF8 support for labels and metric names. Now that said uh if you use graphana cloud for example uh mamir is not yet ready for UTF8 values I believe in uh in labels and things like that. So alloy will have uh the functionality a little bit ahead of when it's ready in the mirror. Yes. Um the default value will still uh remain the same as previously. So um it will continue working here. Good point. Yeah, thanks for thanks for uh making sure that was clear. The other thing it does is it'll unblock propagating metric metadata uh using the remote write component um which will be really nice for for some people. One other thing, never mind that was in 1.10 was uh was fixing some some exemplar support in the the new experimental write Q component. We're still rolling it out internally. So I got mixed up on on what versions uh it's available in. If there's not other things I have questions from last month that I wasn't able to be on it, wasn't recorded. Um mentioned Appali and Infraali. Um, one of the things I thought apply was based on open telemetry. So I was wondering what infra meant um since those things mostly go through the Prometheus pipelines. I was wondering if I was missing something. Yeah, you're not missing anything. It's indeed based on Prometheus and Apple is indeed based on the open term. It's just because Prometheus is the industry standard for infrastructure observability at the moment. So just made more sense to meet the customers where they are right now. Uh that's why Apol is based on different semantics with with open telemetry. So the official guidance for users using Alo is also to um observe their applications using open telemetry components and observe their infra infrastructure using Prometheus native and locative components. So that way uh for your infrastructure you can get like matching label names and values for let's say like Kubernetes namespace. Um it's kind of easier to correlate. Okay. Um there was also some mentioning somebody mentioned using alloy to filter out noisy noisy logs and trace data like health checks. I was wondering if there were any specific suggestions or examples of how to do that. I guess mostly in traces because I'm going to leave the logs there, but the traces aren't as easy to exclude in the searches. Yeah, that's true. For traces, um I would say just do your best to avoid tail sampling. If you can avoid the sampling, it's better. Like for example, you can filter out the spans from the health check services. You still have a trace though where you know that that span might have been but hopefully it's not um not a problem to just remove that span rather than have to have the whole trace removed. Yeah, I think that's what usually people do. They usually have a filter component to remove just that span for the help check. Um, yeah. So, yeah, that's usually the noisy thing. Usually checks. But yeah, if something's really noisy and you just want to completely get rid of it, um, a filtering rule is really nice. If you want to uh sample parts of the the noisy spans but not all of them uh then you can use probabilistic sampling just for the noisy spans. Um you could do that in two ways. If you don't use tell sampling then you could just set up a proistic sampler component. But if you already use the sampling then it's better just to add the probabilistic sampling rule to the sampling. So you can use pro probabilistic sampling on just certain spans or does that over everything? Oh yeah, that's a good point. Uh so h No, no, no, sorry. You're right. It's for the whole trace. Uh Yeah, let me see. I need to double check. But I think I think actually this for the whole trace. Okay. Well, I don't want everything. I just want it all. I want I want to be able to ignore what I don't want. Never mind. I just want too much. You definitely don't want too much. But uh the uh the configurations for you know getting tail sampling working really smoothly they can be very complex and very um specific to uh to each use case. So it can be hard to provide a lot of really generic advice there. Um we do have uh the Kubernetes monitoring Helm chart uh maintained by the a different team at Graphana, but it also includes the sampling Helm chart which can do a pretty decent job of rolling it out for you. Again, assuming Kubernetes is part of your infrastructure, but that's kind of a common assumption to make these days. I wish we were there, but we're not. And in that case, tail sampling can be uh can be quite complex to get right, especially if you need to uh scale based on your throughput because a lot of the uh the tail sampling components need to be stateful to basically capture all of the a trace before it gets sampled. Um and so to uh horizontally scale that you have to then make sure that each of the the traces ends up at the right instance which is doable uh but requires kind of chaining the other things like uh load balancing exporter in front of your your tail sampling um instances and it all gets it's messy but doable um in Kubernetes and doing it outside of Kubernetes gets gets a lot more complex. Okay. Yeah. I don't luckily we're at the point where we don't have to do any even think about sampling. Yeah. And uh I know again if you're if you're a graphana cloud user adaptive uh tracing is I think in private preview at the moment. I don't think it's reached public preview yet. Uh, but it's it's coming for, you know, uh, being able to have more easily generated rules for things like dropping your health checks and all of that. Um, and dropping it at ingest rather than having to deal with it at your, uh, your collective. Well, that'll be nice. I haven't heard about that. Like I said, I think still private preview, but um, kind of found a link. Yeah, the docs are probably available. They don't it doesn't say what where it's at like in our uh availability, but it says public preview in that little note at the top of the docu link. So um should it says not available in all stacks. So uh you'd have to have to speak with your account team about that. But yeah, that's certainly certainly a really nice lever. All the the adaptive tooling for helping keep costs under control are really great. so hard to keep up with all the new things Grafana is doing. It's even hard internally. You know, there's a there's a lot of different a lot of different teams producing a lot of great work. That's for sure. I just checked the ballistic sampler component and yes, you're right. It is for the whole trace. Um you can also use it for logs and for logs you can just choose an attribute that you want to base uh the sampling decision on. Um yeah by the way speaking of u you know graphine cloud and what you can configure there and sampling and all that complexity uh one way to reduce the complexity of the alloy deployments is also to use bor. So for example typically the um you could use alloy for generating span metrics and service graphs but at least for a spanx and service graph you can also get bor to generate them and then you can simplify your pipeline a little bit um and then also you don't need to send them to tempo so that temple can generate them which can be a bit expensive so um yeah so bail And B is an EVPF tool. It can um listen what HTTP calls are going on the uh OS and then uh it automatically kind of does some instrumentation and it can generate some traces and it can generate some metrics as well. Well, since we still have Server 2012, we're on .NET Framework and use BA yet. So, Oh, yeah. So, Windows, we're working on it, but um yeah, so actually there's Yeah, there's going to be EPF for Windows as well. There's a Microsoft repository for this. So I think there will be some work there to get BA working on Windows eventually. Good. But that will probably be on a new version of Windows. Sure. We're working on it. It's just slow going. Well, I think you're the last person here who's a community member. So, if you have any other questions to follow up with, feel free. Uh, I'm sure I'll think of some just as soon as this ends, but no, you guys are pretty responsive in the Slack and everything else. So, I think that's it. Thank you. All right. Well, thanks for joining. Um, I've captured a few of the links we mentioned and I'm going to post them in the Slack thread for the community call, but um, otherwise we will post the recording in a little bit. I'm going to stop the recording now so I don't forget.

