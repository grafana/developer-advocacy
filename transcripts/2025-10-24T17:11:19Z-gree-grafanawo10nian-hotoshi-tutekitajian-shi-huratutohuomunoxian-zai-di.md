# GREE | Grafanaを10年ほど使ってきた監視プラットフォームの現在地

Published on 2025-10-24T17:11:19Z

## Description

グリーでは2015年頃AWSへの移行をきっかけにメトリクス基盤としてOSSのGrafana/Promethuesの利用を開始しました。

URL: https://www.youtube.com/watch?v=Zu7vBw6ymLE

## Summary

このYouTube動画では、イホさんがグリー株式会社における監視基盤の構築と運用について詳細に説明しています。彼はシニアリードエンジニアとして、グラファナとプロメテウスを中心にした監視システムの利用状況や歴史を振り返り、グリーグループのモバイルゲームやVR漫画などの事業展開における監視要件について解説しています。主なポイントとして、グラファナの導入背景、オンプレミスとクラウド環境での運用方法、特にモバイルゲームの高いリクエスト数に対応するための監視体制の重要性が挙げられました。また、彼は今後の取り組みとしてオープンテレメトリーやシティックモニタリングの検証についても言及しています。全体を通して、グラファナの利用によって開発者に一貫した体験を提供することが強調されました。

## Chapters

00:00:00 自己紹介と役職の説明  
00:02:30 グリーグループの事業内容紹介  
00:04:00 本日のアジェンダの説明  
00:05:45 グラファナの利用状況について  
00:08:15 組織とワークロードの説明  
00:10:30 実際のダッシュボードの使用例  
00:12:00 モニタリングユニットの役割  
00:15:00 モバイルゲームのワークロードの特徴  
00:20:30 監視構成の説明と図示  
00:25:00 グラファナ導入の歴史と進化について  
00:30:00 未来の取り組みと最近の検証内容

# 自己紹介とグリーグループの概要

初めまして、イホと申します。株式会社グリーでシニアリードエンジニアをしています。私のチームは、全社向けの監視基盤を提供するリーダーを務めており、SRE関連やEBF関連のコミュニティ活動にも参加しています。

## グリーグループについて

グリーグループでは、モバイルゲームやVR漫画など、様々な事業を展開しています。国内だけでなく、グローバルに展開しているタイトルもあり、要件に応じてクラウドサービスを使い分けて提供しています。

## 本日のアジェンダ

1. グリーグループにおけるGrafanaの現在の利用状況
2. 弊社の組織構成と監視構成
3. Grafanaの利用の歴史

まず、グリーグループにおけるGrafanaの現在の利用状況について説明します。

## GrafanaとPrometheusの利用状況

現在、全体で約480万目のタイムシリーズデータを使用しています。OSSとクラウドの両方を活用しており、約6つのインスタンスを運用しています。環境は、オンプレミス、AWS、Google Cloudの3つで利用されています。

### 利用者

主にエンジニア組織のあらゆる人がGrafanaを利用しています。インフラの各専門チームやプロダクトの開発メンバーが、それぞれ必要な情報やダッシュボードを自分たちで作成し、運用しています。

### 実際のダッシュボード

ダッシュボードには、ホストレベルのビューやプロセスレベルのメトリクスが表示されており、トラブルシューティングに役立っています。プロダクトによっては、DynamoDBなども使用しており、それらのダッシュボードも運用されています。

## 組織とワークロード

インフラメンバーについてお話しします。弊社では、各プロダクトごとに開発チームがあり、それに対応するインフラチームが存在します。また、アラートなどの一時対応を行う運用チームも全横断的に存在しています。

### モニタリングユニット

モニタリングユニットは、オブザーバビリティを扱う専門チームです。多様なサービスを展開するため、プロダクトごとに監視基盤やアラートルールを提供し、必要に応じてカスタマイズを行っています。チームは12人未満で運用し、実際にプラウド環境を扱うメンバーは4～5人程度です。

## ワークロードの特徴

モバイルゲームのワークロードについてお話しします。モバイルゲームはリクエスト数が多く、サーバー間でデータをやり取りしています。イベントなどによってリクエストのピークが発生するため、監視要件が重要です。

### 監視要件

- 15秒間隔でメトリクスを取得する必要がある
- 長期リクスを保持する必要がある
- 各プロダクトごとに分離したアクセスコントロールを実現する必要がある

## 監視構成の概要

監視構成は、ノード各サーバーに1つのエージェントがあり、そのエージェントがPrometheusのエクスポートを動かしています。リモートライトでGrafanaクラウドのPrometheus互換にデータを送信する構成です。

### AWS環境での構成

AWSのEKS環境では、監視用のポッドが稼働し、PrometheusがEC2で運用されています。マネージドサービスからもメトリクスを取得しています。

### GKE環境での構成

GKEでも同様に監視用のポッドが配置され、Prometheusからリモートライトでメトリクスを送ります。各マネージドサービスに対してスクレープを行い、メトリクスを取得しています。

## 利用環境の違い

OSS環境ではデータを自分たちで管理し、1時間ごとのメトリックスをダウン samplingしています。一方、クラウド版は高制度メトリックスをサポートしています。

## Grafanaの歴史

2015年からGrafanaの利用を開始しました。当初はデータセンターをメインとしていたため、Gangliaをカスタマイズして使用していました。AWSへの移行が始まり、新たに監視基盤が必要となりました。

### GrafanaとPrometheusの導入

2015年にはGrafanaとPrometheusのスタックを採用し、動的なターゲットディスカバリー機能が期待されました。

### GKE環境の利用開始

2020年頃、GKE環境でも利用が始まりました。メトリックスの負荷が高くなるため、新たなクラウド基盤が必要になりました。

## まとめ

グリーでは10年前からOSSのGrafanaを利用してきました。ビジネスの変化に対応し、小人数でメンテナンスが可能な環境を整えてきました。今後もGrafanaクラウドの利用を進め、開発者体験を統一することを目指していきます。

ご清聴ありがとうございました。

## Raw YouTube Transcript

はいそれでは最初にえっと自己紹介からさ せていただきますえっとイホと申しますえ 株式会社グリーでシニアリードエンジニア をしていますえ仕事的には全車向けの監視 基盤をえ提供するチームのリーダーを行っ ていますまその他コミュニティの活動とし てまsre関連であったりえっとebf 関連のコミュニティで活動しており ますはいそれではえっとまず弊社グリー グループについてえ少し簡単にえご説明さ せていただきますえグリーグループでは モバイルゲームまVR漫画などのえっと 様々な授業を展開しております国内だけで はなくえグローバル展開しているタイトル もありましてま要件に合わせてその クラウドサービスなどを使い分けてえっと 提供しており ますこちらが本日のアジェンダになります えまずえグリーグループけるグラファのの 現在ということで現在の我々のグラファの の利用状況等について説明させていただき ますそれからま弊社のえっと組織とかま 実際の監視構成みたいな背景の辺りをお 話しさせていただきますでメインとしては そのグラファなの利用の歴史ということで えっと弊社結構長くグラファのを使ってき ておりますがまそのなぜあの発生した課題 であったりとかまどういった意識決定を 行ってきたのかみたいなとこについてま自 系列に沿ってえっと追体験していただく ような形でお話しさせていただければと 思い ますではまずえっとグリーグループにおけ るグラファのの現在のについてお話しさせ ていただき ますこちらがま数になるんですけどもえ グラファなプロメテウスの利用状況となり ますま全体でえっとま4.8mって書いて あるんですけど480万目タイムシリーズ ぐらいのデータを使っておりますで えOSSとえそれからクラウド両方使って いるんですけども合わせて大体6ぐらいの インスタンスを運用しておりますでえ3つ の環境ということでえ現在えオンプレート それからAWSGoogleクラウの3 環境で利用しており ますでまそれらを誰が使っているかという ところなんですけどもまエンジニア組織の 基本的にはあらゆる人間がグラファナを 利用していますまインフラの組織のえっと 各専門チームえインフラプロチームの プロダクトの向き合いチームその各 プロダクトの担当のインフラチームですね それからえっとプロダクトの開発メンバー これあのデベロッパーの方になりますえ それぞれのロールでこう必要となるあの 情報であったりダッシュボードドっていう のは違うんですけどもそれらをま自分たち で作成し運用しており ますじゃ実際のダッシュボードどんなのを 使ってるのかっていうのをえっと見て まいりますま左は単純なあのホストレベル のビューでこちらまノードエクスポート などをえっとベースにしたものですね えっとインフラのメンバーが主に使って おりますでそこそれとえっと同じく プロセスレベルのえっとメトリクスも取っ ておりましてまこういったあのトラブル シューティングに使うこのメモリの使用量 であったりえっとプロセスに関する メトリックみたいなものを取ってえ表示し ており ますまたそのプロダクトによってはあの マスールなどのRSだけはなくダイナモ DBなども使っているのでま使っている プロダクト自身がえっとダッシュボードを 使って運用しておりますでえGKの プロダクトもいつかありますのでそれらも そのGK用にあのえベッド作り込んだダシ ボドそれのプロダクトで運用しております でちょっと変わったところとしてえっと これはデモになるんですけど スクリプティングインフラ主にインフラの メンバーがあのダッシュボードを利用して おり ますえ続きまして組織とワークロードと いうことでえ組織的な部分を少しお話しさ せていただきますまインフラメンバーって いうお話をさせていただいたんですけもえ 弊社各プロダクトごとにその開発チームが ま子会社などでありましてABCというま それぞれのプロダクトがえありますで対応 するインフラチームはこう1個の大きな インフラ組織なんですですけどもま プロダクトごとの担当メンバーというもの がそれぞれついていてま構築化運用を行っ ているまそれとは別にそのアラートなどの 一時体を行う運用チームというのがいて これは全横断的にえっと一時対応などを 行っていますで最後にその1番下にこう 色々書いてあるrdvsとかえkvsとか 書いてあるとこがあるんですけどもこれら はあのユニットという単位でまそれぞれ あの技術に特化したえ専門のチームを置い ていますでその中にモニタリングユニット というのがありましてまこれが私のいる チームなんですけれどもこれがえっと オブザーバビリティの扱う専門のチームと なっており ますえモニタリングニトは何をしてるのか と言うとまあのグリーではこう多様な サービス展開のためまプロダクトごとにま スではアカウントGoogleクラドでは プロジェクト単位でまそれぞれ分離する ような構成を取っていますえそれぞれの 環境ごとに基本的な監視基板アラート ルールなどを提供しつつまプロダクトの 要件におじてはカスタマイズなど行って 提供しておりますえ12未満のチームで 運用していてま実際にプラウド環境を扱う メンバーは4人45人程度でえっと運用を 行っております結構小さいチームで運用し てい ますえモニタリングユニットが提供する コンポーネントとしてはもちろんえ グラファなーとそれからプロメテウスを 中心としたスタックそれからそれらと連携 するま独自のコンポーネントですね自分 たちで内省したものをえっと提供しており ますえオンプレVMベースコンテナー ベースそれぞれの環境に提供していますえ まログ基盤なんかも合わせてサス利して 提供したりもしているんですけどもま歴史 的に弊社メトリックスに結構重きを置いて きてるというとがありまして結構 メトリックスに関するコンポーネントの 提供が手厚くなっていますま最近は トレースなども少しずつサポートをし始め ており ますえ続きましてワークロードのの特徴に ついてですねま弊社あの最初に事業紹介し たんですけもまモバイルゲームが結構 大きいのでモバイルゲームのワークロード についてお話ししますまモバイルゲームの 特徴としてまrpsが多いというのが1つ ありますまリクエストの数が非常に多いっ ていうとこですねで基本的にゲームデータ をこのサーバーがデータあのやり取りして ますのでまそのAPIを提供するこう ウェブベースのサーバーというのがメイン になりますえ一部でリアルタイム通信を 行うゲームもありますのでそうサーバーも ありますまたえっとグローバル展開があり ますま日本国内だけではなくっていうとこ ですねでえまローンチタイミングが1番 アクセスが来るえのですがまそれ以外にも 熱量の高い施策まあのイベントなどによっ てリクエストのピークがえ発生しますえ1 日の中ではま例えばログインボーナスの 受け取るタイミングとかですねなどに えっと瞬間的なピークが発生しますまこれ らのワークローに対してえっと監視要件と してこう以下のようなものがありましてえ 高制度のま15秒間隔で取得する メトリックスをえ扱うこれはあのスパイク を捉えたりするのにま1分制度なのでは ちょっと追いつかないので15秒制度の メトリックスを保持するようにしています えとそれと同時にえっと施策の部分ですね ま1年前にどんなキャンペーンやって どんなトラフィックが来たかみたいな ところをえっと比較したいというのがあり ましてまプロダクトの要件的に長期リクス を保持す必要がありますでもう1点は最後 ちょっとあの監視要件ではないんですけど もあの各プロダクト分離した環境で扱って おりますのでま開発者の方もいろんな ロールの方がいらっしゃるのでプロダクト ごとにその分離したアクセスコントロール を実現するという必要がありまし たはいではその監視構成といったところを 実際にちょっと図見ながらご説明して まいり ますはいこちらがちょっとプで非常に簡単 なんですけどもこれ移行移行システム移行 した時の図になるので下の方が現在の状況 なんですけどもま基本的にはえっとノード 各サーバーですねま物理サーバーにたに1 つのエージェントがいてまそのロールに 応じたえっとプロメテウスのエクスポート が動いていてまでローカルからグラファな エージェントがスクレープを行って リモートライトで直接グラファなクラウド のえプロテス互換にえっとデータを送ると いう構成になっておりますこれは非常に シンプル です続きましてAWSのeeks環境に なりますちょっとコンポーネントが増えて いるんですけれども えこちらのまメトリックスの部分に フォーカスしてお話ししてまいりますまず えっとクネスクラスター内に監視用の ポットが稼働していますえクラスター内の プロメテウスは基本的にまエージェント的 な位置付けでまこれ自身は状態を持たずに えEC2で運用しているプロメテウスが別 に立っていましてえそこに対してリモート ライトを行っていき ますでマネージドサービスですねえっとま rdsでやったりエスティキャッシュで やったりっていうところはえプロメテウス のエクスポートを経由してあのそれぞれえ 専用の監視用のインスタンスから メトリクスを取得してい ますえ続きましてgke側の構成になり ますちょっとまたコンポーネントが増えて 大きいんですけどもこれをえ分解していき ますえgkeでも同様にクリアクラスター 内ですねにえっと監視用のポットが配置さ れましてまプロメテウスからリモート ライトでえメトリクスが行れますまエウス と異なるのはえっとグラファなクラウドを 利用してるので直接あのグラファナ クラウドのプロメテウスに対してイえ メトリックスを送りますえマネージド サービスはえAの場合と違いましてあの 専用のインスタンスはいないのでこの プラスターにいるえっとこコンテナのえ サビコンテナからえっとエスポーたーが 稼働していてま各マネージ動サービスに 対してスクレープを行いえメトリックスを 取るそしてま同じようにリモートライトで えグラファなクラウドに書き込むという 構成になっており ますはいえOS版利用環境とえっと クラウド版の違いなんですけどもま基本的 にはこう同じように構成してるんですけど もosssの方はあの自分たちでデータを 管理していますので1時間制度の メトリックスをにえっとダウン サンプリングを行ってその1年間のデータ というものを保持しておりますま15秒の まはちょっと持てないのでダウン サンプリングをしていますえでさらにあの 車内のそのアクセス制限の話があったん ですけれどもあの車内のADの認証基盤が ありますのでまサルで連携するんですが AWSALBと国にとを返して度連携して おりますでグラファなクラウドの方は えっとそれ自体があの高成度メトリックス をサポートしていてえサムの機能でこちら あの自身の機能で連携できますのであの そちらで連携しておりますまこのように 構成の違いはあるんですけども ディベロッパー的には同じ体験を提供する ようにしてい ますはいそれではえっと歴史の部分につい て詳しくお話ししていきますこちらは時系 列に沿ってお話ししますえタイムライン ですねすごくざっくりしてるんですけどま あの古い拡大ぐらいの時代からガングリア の時代まグラファの性使い始めてえ クラウドを利用しオンプレをマイグレし 最終的にハイブリッド構成になるみたいな 流れでえっと進んでまいり ますまずえっと2015年のグラファなを 利用する前の状況ですねえっとこの頃は まだデータセンターがメインだったのでえ ガングリアをガリアとという緑があるん ですけどそちらをカスタマイズして利用し ていましたこういったグラフですねえ対象 はデータセンターで稼働するサーバーです でガングリアは内部でrrdツールって いうえっと自系列データを扱いつつあの 画像病がするえっとツールがあるんです けれどもまこれによってこう性的に画像を 出力するのでま見る側としては非常に安定 したシステムとしてえっと稼働していまし たただまそのグラフの追加をするこういっ た複合グラフを1個1個追加するのは結構 大変であのツールに対するオプションを こうコードで動的に生成してえインフラの 専用のメンバーがこう作るみたいなことを 行っていまし たそこでえっと2015年ですねえAWS の移行が始まりまし たはいま背景としてはま全車的にAWSを 利用開始してま新規のプロダクトは基本的 にAWSを利用するようになりオプレで 稼働している既存プロダクトも マイグレーションをを開始し始めましたま これに合わせてオンプレの今のガングリア とは別の監視基盤を新たに整備する必要が 発生しまし たで要件なんですけもま課題となるのは あのエウスに移行するんで今までその性的 なえっと物理サーバーだったんですけど同 的に増減するのでえ監視ターゲットをこう いちいち対応する必要があるというところ でま今の従来のガング構成ではここが ちょっと難しいまサービスの実割りのよう なが必要になったということになりますで さらにそのプロダクトごとに完結する環境 ということでえっとまオプレの環境って1 つの大きなネットワーク内に巨大なこう えっとサービスが稼働するみたいな状態 だったんですけども例えばそのビジネス 要件として事業の買収や売却あと運用遺憾 などが起きた時に環境が分離できないと 困るということでまエダスは基本的に アカウント細かく分プロダクトごとに分け るっていう構成になったのでまこういった も構成に対応する必要がありまし たでそこで2015年当時のアプローチと してま当時はまだ新しかったあのグラファ なプロメテウスのスタックを採用しました え期待としてはプロメテウスの同的な ターゲットディスカバリーですねあの 組み込みの機能によってECツイタースを 自動でえっとディスカバリーしてあの監視 対象に加えてくえるそしてあのシンプルな アーキテクチャーになっているという ところが採用の決め手となりましたえ プロジェクトメンバーの1人があの プロメテウスのプラグイン当時まだ グラファナってプロメテウスに対応して なかったんですけど初期の コントリビューターとしてそのプラグイン の実装を行いましたまこれについてはあの ミートアップのえっと初回でもお話してる のでよかったらこちらあのご参照ください えでメトリクスの取得はそうはいても結構 大変なので既存のガングリアのプラグイン ですねえっとPythonで抱えたものを 利用できるようにえっとプロメテウス形式 に変換するプロクシーを内生しました下が そのインテグレーション作った頃の記録に なり ます はいでこれによりAWSのえっと上で稼働 するえっとクラウド環境に適用した メトリックス基盤を作成できましたで最初 にお話えっとご紹介したあの スクリプティング 大きなグラファの特徴であるあの何回か セッションで登場しているあの デモクラシードあるいは オブザーバビリティみたいなところがある んですけもま利用者自身が自分でダッシュ ボードを使ってえっと課題解決を行うって いうことの下地がこのここからだんだん できてきましたえさらにまこれEC2で 運用してるのでえ結構運用が大変でしたの でま徐々にですけど監視インスタンスの 自動復旧やその設定各環境ごとに断る設定 を自動で更新するなどの内線の仕組みを 整備していきまし たえ続きまして2020年頃ですね ちょっとじっと気が進みましてGK環境で もえっと利用が始まりまし たま背としてはグローバル展開する タイトルが結構エベスではなくgkeを 積極的に利用するようになってきましたの でまグローバル展開を一環境でえっとま 叶うという構成が増えてきましたあるいは まスパナ扱いたいとかえっとマルチ クラウドを推進したいていうその背景が ありえそういったこうインフラの変化が また訪れましたでさオンプレエダベストは また別の監視基盤が必要になってきたと いうのがあり ますでえ今回gkeなんでえっとクネテス の利用により発生する効果デナてな メトリックスをクエリする負荷っていうの があるんですけどもまこれは何かというと あの区説の世界はそのVMにで運用してる 時と比べてメトリックスのそのタイム シリーズの管ナテが非常に上がっ高くなる んでま物によっては10倍とかになるので まそういったものに対してその長期間のり などを発行するとしばしばこうえっと プロメテウス側が後負荷になってしまうと いう問題がありましたでまエベスの方は 復旧なども作り込んでいたんですけどま 10分から15分程度結構人力でえっと 復旧作業するみたいなことが発生してで 利用者からしてもちょっとなんか長い タイムレンジでクエリーをしてしまったら 監視のインが落ちてしまうみたいなことが 実際に発生ししまったりしていてま体験が 悪いという課題がありましたでクネの環境 においてはこれがさらに起きやすくなると いうことがえっと課題として予想されまし たまたその新たなクラド基盤を構築運用 するということでまあの小人数で運用して おりますので単純にこの運用負荷が非常に 高いというところでこれもチャレンジに なる部分でし たはいそこでえっと2020年時点で えっとグラファなークラウドの利用を開始 しましたえワークロドへの監視導入はま ヘルムチャートがありましてQ プロメテウススタックというのがあるん ですけどもまこれを利用してえっと プロメテウスの導入やあの周編 コンポーネントの導入を行っていますまた あのえっとGCPのクラウドモニタリング ですねえっとこちらのメトリクスはあの 専用のエクスポートがあるんでそれを使い つつパターンとあとデータソースあの もちろんグラファたようなデータソースが 使ますのでそちらを併用する形でえっと グラファなに取り込みましたでグラファな クラウドのスタックですねあのそれぞれの こうグラファなインスタンスそれから意識 のこうプロメテウスなどのインスタンスは えプロダクトごとに分離する構成にしまし たこれはちょっと色々な理由があるんです けどもま単純にコストの配分の問題であっ たりあのアクセス制限の問題であったりと いうことでまちょっと繁雑ではあるんです けども個別にスタックを利用してえっと 運用しておりますでであの先ほど説明した ようにあのサルでえっと車内の認証基盤と 連携するんですけどもま導入投資はまだ そのUI上でサブの設定などもできなかっ たのでこうサポートでチケットを切ってえ 構築を行ったりまトラブルシューティング などしたりしていたんですけど最近ではま あのグラファの側でもUIが実装され だいぶ安定して使えるようになりまし たはいまこれによってマネージの環境で えっと長期間ですねま13ヶ月保有される メトリックス基盤がクラウド上に手に入り ましたで公開ナテな環境にもえっとタル マネージの環境ということでグラファなの そのえグラファナクラウドで利用している あのミアのアーキテクチャーあの分散 アーキテクチャーになっているのでま結構 こう買手の高いクエリーにも耐えることが できるということであの非常に良いも状態 になりましたえまた運用楽にするためにま osssのリソース先ほどのヘルム チャートであったりまあとあのテラホーム などを使ってえっとコード管理を行うこと でま小人数で運用可能な環境を手に入れ まし たはいえ続きましてオンプレですね 2021年にオンプレも監視基板の印行が ありまし たでこれはまあなぜかというとこなんです けどあのPythonの2系のeolと いうのがありましてまガングのプラグは Pythonを使っているんでこれの継続 が難しくなりましたまちょっと先行きが不 透明になったというのがあったかなと思い ますでオンプレはオンプレでそのクラウド の移行を引き続き実施していたんです けれどもまちょっとそのえオンプレ撤退 までにあの新しい監視基盤ないとまずいね というところでリプレースを決定しまし たはいでま課題なんですけどもそのえ移行 を決めた当時まだ1500ぐらいオンプレ のサーバーがありましてまこれらを サービスレベルを落とさずにあの継続して 運用するてところがま結構課題でしたえま ダスGoogleクウなどにあの移行を 進めていたんですけれども結構移行し づらいものが残っていてまSNSなどの コアなサービスや困難なサーバーってもの が残っていたという状況ですで少なくとも まオンプレが継続する数年間え維持し なければならないということでま新たな オンプレ機材の依存を増やさせずにえ監視 基盤を再構築するというのがミッションと なりましたえそこでまあのこちこらもグラ ファナクラウドを利用する構成にえしまし たでま先ほどのGKなどに比べてもう ちょっとシンプルな構成で各サーバーから 直接リモートライトでまエージェントを 使ってあのプッシュ型のように メトリックスを送る構成になっています ただまその既存のガングリアも10年 ぐらい運用していてすごい作り込んでいた ものなのでえオプレとしてはこれ8ヶ月 ぐらいで移行したんですけどまメトリクス を収集するあのつになるプロメテウスの エクスポートのパッケージを用意してで あのただ自分たちで作り込んでいて足り ないものについてはあのスクリプト エスポーたーやテクストファイル コレクターみたいなものを活用しながらま 自分たちでちょっと追加のメトリックスを 取るという対応を行いましたえさらにま サーバーロールごとにダッシュボードその ガングリアを使ってるのであのプロえ ブラフのダッシュボードはないのでそれら をえい個1個作っていきましたえ詳しくは ちょっとブログ記事がありますのでこちら も興味があれば見てみてください で最終的にえ結構手間はかかったんです けどもオンプレ環境が運用するまでの メトリックス基盤ができましたでまオプレ 環境はガングを長く使ってきたんですけど もまこうクラウドにサーバーが移行して いくとあの利用者の方はグラファのそちら はグラファナベースになっているのでま あらかじめグラファナを使った上でこう 移行してそちらでもまたグラファナが 使えるということでま利用者側としては 基盤に慣れ親しんだ状態で移行できると いうことでまそこはえっと良かったのじゃ ないかと思います はいそしてえ2025年今ですねえ ようやくオンプレの移行がほぼ完了しまし てえまイコンコーナのサーバーをちょっと 残すところわずかというとこで移行する 対応するフェーズになりましたま2021 年の投資によりえグラファナを利用した 一貫した体験をエンジニアに提供できたん ですけどもまオンプレがなくなって いよいよハイブリッドクラウド環境の時代 になりそうかなというのが現在の状態に なっておりりますはいということでこの 10年間の戦略をえ少し振り返ってまいり ます あえまインフラ環境の変化に合わせて osssクラウドをそれぞれまえっとま 10年間グラファなを利用してきましたえ まキーとしてはエオプレエベス Googleクそれぞれあるんですけども 開発者利用者の体験を統一する同じように するということをフォーカスしてきました またそのグラファのの強みであるその自分 たちダシボードを利用してま作って利用 するっていうところがまこれが最大限こう 全ての環境で使えることでえ最大化できる ていう状態になりましたまたあの初期の頃 はですねそれこそプロメテウスのプラグ インをえ作り始めた頃にはまだあの エコシステムをあまり整っていなかったん ですけどプロメテウスはクバネテの環境に おいてデファクトスタンダードになったの でま周辺のこうソフトウェアなどがかなり 充実してえま10年間でかなりいい状況に なったというのがありますでも コミュニティ標準のスタックとしてあの すぐにリッチなメトリックス基盤が作れる ようになったと思い ますはいで最後にえっと未来の話という ことで最近の取り組みについて少しだけご 紹介させていただきますま何回か今日登場 してるんですけどもオープンテレメトリー の対応というとこで主にトレーシングの ところを最近検証していますま店舗による トレーシングというなオープン テレメトリーのエージェントを使って えっとアプリケーションのトレーシングの 方を検証進めておりますそれから親戚ク モニタリングですねこちらもえっとあり ましたがまだんだんそのあの弊社ここの分 がちょっと弱い分でもあって外から見た えっとサービスの振る舞いというものをえ こう可視化してあの実際にどういった インパクトがあるのかっていうのを可視化 するためにシティックモニタリング非常に いい総称なのでこちらの検証をしています またえっとグラファなシーンズというのが ありましてあのスクリプトクリプテッド ダッシュボードがえっと何回かご紹介させ ていただいたんですけれどもこの機能実は もうデュプリケートになっていてそろそろ あの将来的に廃止されることがえ決定して いますで新たにその同的にダッシュボード を作るソリューションとしてあのグラ ファナシーンズっていうフレームワークが えっとグラファナから今 あの提供されているというかあの公開され ているのでそちらを使ったあの ダイナミックダッシュボードのえっと作 検証というものを行っていますまたあの ebpfなど利したアプリケーション内部 の可視化検証みたいなものもえっと行って おります はい最後まとめになりますえグラグリーで は10年前からosss1のグラファナを 利用してきましたま様々な変化特に ビジネス的な変化であったりそのマルチ クラウドのの対応みたいなところでえっと 大きな変化があったんですけどもまそれら に対応してま小人数でメンテナンスを可能 にするためにグラファナクラウドをえ積極 的に採用してきましたまこれによってま osssとマネージドサービス両方を利用 しながらあの開発者的体験としてはえっと 同じもていうものをフォーカスすることで ま結果的にこう価値の高いものをえ提供 できる状態にたどりついたんではないかと 思い ますはい以上となりますご清聴ありがとう ございました

