# Grafana Mimir Community Call 2024-04-25

Grafana Mimir Community Call from 25th of April 2024. We have discussed how to contribute to Mimir project, release of Mimir ...

Published on 2024-04-26T14:24:08Z

URL: https://www.youtube.com/watch?v=ORS4W1vEoCQ

Transcript: all right so welcome to uh this monthly graphon Community call uh on the 25th of April 2024 uh we have a couple of items on the agenda uh we'll go through them as usual and then you know open the floor for questions so let's get to it um the agenda is linked uh into the meeting invite as a dock okay so the first things that first thing that we want to cover is a question that came up on the last uh comment call about uh yep oh yeah thank you deitar um right so the first thing that we want to cover is that something that came up on the previous uh Community call which was about which was a question of um what to contribute to and and how to contribute and uh basically there are four main areas of of issues uh one is the actual mimir Golan code uh where you can find some issues marked with good first issue label in GitHub uh that you that is a pretty good place to start or you can start in any any place really um then there are the mixings uh these are written in Json net and these are used to generate the dashboards and the alerts and recording rules that we uh publish with mimir uh you can use this to monitor your system and then get started on that um and the last area is the is the deployment tooling uh so kind of packaging and there we have again Jon net and Helm um Helm is according to our M Helm is the most widely used so that's actually a good place to uh contribute uh but other than that so that's you know I guess super generic but more importantly um basically we advise anyone to just contribute to what they have a problem with or they want to improve uh and uh so we can start helping you on those PRS basically so we don't really want to dictate like you should do this or that but please feel free to uh contribute on on things that concern you um I don't know Dimitar if you want to add anything I know you not really um I think the most part part is to do stuff which is relevant to you which would help your work and how you been here um obviously we're open to to contribution just contribution just for the sake of contributions but if it helps it's it's better you already said that yeah Y and then about how to contribute uh we do have uh like documentation uh on the uh on how to contribute like what things to to consider uh this includes the go code as well as the uh ham chart um so the ham ham stuff has its separate how to contribute documentation uh but feel free to open uh PR with the proposal at at any time um and uh if you're not sure if it fits into mimir open issue with the problem and your proposal uh uh for discussion I would also say that uh as as a bit of an expansion on this is that a lot of time we see that PRS got get a little bit stuck on formalities uh like change log and and updating unit tests that are getting affected um so I hope that like we are thinking of making this things easier but uh unfortunately at this time these are kind of required uh because even this code that we are working on you have to know that it gets into uh graphon Cloud so what you get as a service on graana.com uh that pretty much runs this so if you are running you know open source M you're running almost the same thing that runs in in production for us so we have to be you know have to keep a tight ship basically and really can't can't really relax uh those those requirements um all right uh any questions on this this part actually I'll stop here for a second if no then uh okay about the ham 6.0 so uh I just wanted to bring this up because the last major release of H was a year ago I think it was last July uh for mimir and uh we have discussed a little bit the next major release at the at the at a meeting that we had uh team meeting and uh yeah the the scope is kind of shaping up um I would expect that we will have a major release maybe alongside the next MIM release or the one after that um and uh the one thing that is kind of new uh um or at least was new to me that we had like Community contribution uh into the into the helm charts repository that we have and now a bunch of graphon a Helm charts are being released into the oci repositories as well um and we unsure like we haven't implemented this for mimir uh yet but that might be something that uh we will do um I don't know if the people present on this SC have any preference if they actually heard about oci repositories or uh if that's you know something completely new to you it's so quite a few thumbs up on the issue in h charts popular yeah I think so yeah the the I I think the a little bit of the issue is that we have the like the Legacy Way documented as well uh in every place so uh it takes some work to actually uh get this working and and document it and and have everybody on board for It Anyway this might be coming in in 6.0 uh but I what I can for sure say is that we will not stop and remove the old Legacy Way of the doing things uh for a while so it shouldn't affect everybody like negatively um all right so next topic is that uh we' started working on migrating mimir to use native histograms for latency metrics uh this is progressing very slowly uh but this would mean that uh when you install mimir you would be able to to see much better resolution uh latency histograms latency information about your uh uh about the request being handled by mimir and also um this will bring in updates to the dashboards and alerts and recording rules uh but again uh this doesn't mean that the OD like you don't have to change to Native histograms uh but this this would be an improvement in in in many cases all right um let me hand over the r to diit actually to talk about um a very interesting topic I think great um I'm going to talk about that congestion so internally I mean not internally but final apps uh me Marco Peter Vladimir um and Jonathan and Nick have been working quite a few people working on ingestion VI Kafka um so we the gist of this architecture is that we're introducing Kafka in between the inors and the Distributors um in so with this with this architecture change the ingestor moves to being Solly a read path component um the goal like the there are a few primary goals with this um first is that we decompo the read in the rack path so whenever investors have an outage that there's no longer a full outage on the right path as well um it's only queries which are affected um and right now well I mean with Kafka if Kafka has an outage uh then reads are not that affected like you might get slightly stale data uh because you cannot read from C but uh whatever has been consumed from kfka is uh is available um a second goal is to make multi a deployments a bit more feasible and a bit cheaper so today because of how Corum works because of uh how each query needs to access data from two ingestor zones uh the cost of deploying m in MTI Cloud environment is uh is it's quite big I think some folks have said that networking costs are higher than compute costs um when introducing gafka this should this should reduce because then uh what's copyed is only the ingestion traffic and then per per Zone um queries queries will only need to access the Zone local data in inors um and then a final something which comes as a side effect like was not a primary goal uh is to reduce replication Factor so partitioning data changes um in this new architecture uh series are shed to partitions they're not sharted specifically to ingestor so um this means that we can drop the replication factor from 3 to two and then queries only need quum of one don't need quum of two so one query only needs to read from every C competition um once every C competition from one investor who's who's reading from that c competition and then the query can succeed um so this hopefully accounts for the cost of Kafka um and maybe even offsets it or or sort of drives further cost forther down but um we're still not sure uh it's still experimental um and very much a work in progress we've been merging code into mere with pool requests so if you I don't think we have a label but perhaps if you just search for C in PRS yeah find 52 open PR um mer PRS um we haven't documented this so configuration parameters are are hidden the reason is that this is like we still weren't 100% sure how to run and configure this uh we didn't want Fox to get confused by a bunch of configuration parameters um there's also no issue tracking the work uh which I think will will change soon we have some Cal place which describes the new architecture U and how work has been going and how it's planted um that's about it so that we've been working sort of quietly on this only through PRS PR should have good descriptions but they're not exactly the way to communicate with uh the community so yeah I decided to put this up in a Comm on a community call um I think that's all I have I do have one question which is uh um which is that can you hear me yeah I hear you now okay so I do have a question on behalf of you know everybody that probably wants to use this is uh do we know already uh because I'm not in that project actually so do we know already if we will we would deploy kofka uh from our Helm or Json net or would that be something that the user would have to do um I think it's not something we've explicitly discussed um my take is that H CH should be sufficient to deploy maybe so maybe we have some some Kafka deployment I I don't know how to deploy Kafka with Helm so maybe that's too complicated and becomes out the scope uh yeah Peter well in our case we already had cfas in in our Sal where we tested this because of the other product but there is a Kafka home chart so it's easy to deploy Kafka to your kubernetes I wouldn't necessarily bundle Kafka home chart with MIM and just make it a prerequisite if that's something that's possible in home world but prob you know better yeah you can bundle like you can uh depend on on other hchs but they they get bundled then like that's how we do Mino and mcast um and if you want to keep you know the philosophy of the getting started guide that you just need to install this one thing and get started then we will have to do it or do something else like have something that can replace kaar like some IM memory queue or something that is just for the purpose of demo but anyway uh that's some ways of yeah we are very far from having home chart for this yet but it sounds like we should be able to depend on K chart from what you are seeing so I think that would be the way forward once we get there in a year or two yeah uh Andre yeah uh wouldn't this be just like moving the the pro from one one part of the architecture to to Kafka I mean how how does it uh remove the um outages on the on the ingestion path for example I mean if kaf Kafka is down would be the same as the ingestor being down no um if kfka is down then the ingester cannot receive new data but it can keep serving whatever it's uh it's res it's received so far yeah but I mean for the for the right path would be for the right path uh the distributor writes to Kafka and then the request is done the distributor doesn't wait for the inor to to receive that so as long as it's in kfka the right request is s yeah no I'm saying if kfka is down then if Will down down the right path is down yes yeah but is that is that very is that common that the inest is down I mean wouldn't like the same type of issues that are affecting the inest now affecting Kafka as well because managing a big for small clusters is is is good but Kafka for very big cluster is quite it's quite challenging sometimes well at least historically when I when I work with CF in the in the past not I haven't worked in a few years now but the in our experience a lot of the issues with investors happen because of queries so they're usually fine just ingesting uh a fairly robust ingesting whatever you throw at them uh whenever complex queries come in um or I don't know wide label values requests uh you end up with high CPU and then this slows down the the right path so we think that you don't just move the problem you sort of separate two problems and then the right path should become more resident um kfka is also a bit more uh robust than than mimir it's been around for longer more companies investing there uh so it should be able to handle more right surges than than beir um yeah it is complicated to run I so we don't have to run Kafka I think there are Kafka replacements uh we only need like it would only need the Kafka protocol uh I think you can pay Cloud providers to run Kafka for you or just some Kafka like implementations I think we only use very basic protocols so nothing really complicated okay and and doesn't the uh I see that in the mimir there is a um config option that basically the amount of CPU CPU that is used for the quy path um does that uh does that mean that it is not doesn't really well or I think you know maybe others have more experience with this but I think it didn't quite works so well for us um it's a bit slow in responding um and I don't Peter well ARA is on who implemented this but the way it works is it basically stops the query from working so it will keep alive your right path but querying will go down if you configure the limit and CPUs is is over the limit so that's not so great with this new architecture iners they are still patching the data from cka but using only single go routine and as as long as they are up to date like I mean they will always unless that one single go routine consumes your entire CPU course or limit that you give it which should not be the case then your query should be still working so sorry I didn't can you say it again how how how does that work so like if I if I limit a CPU to I don't know three CPUs how does how do you make sure that it never goes above that you mean in the current codee uh yeah in the CPU limit um so right now well ARA can tell us but if I remember correctly it's basically checking some slpr files to see what's the CP usage of the process oh okay and as soon as it goes about limit it will basically stop the read path is that correct AR basically it do um yeah it reads the the CPU utilization from uh the proc file system um and it does a a weighted moving average so it's not it's not a raw utilization but it's on average uh and then if it's above the configured threshold it it starts rejecting reads so that's how it works and but how how do you know that is the the process is in the read paath um I mean the inor is is uh both a read and WR path components yeah this limit only exists on the inors yes can only configure in let's say that inor has 10 cores and how how does the ingestor process knows that which like if you if you look the the utilization like how do you know like so let's say that goak Pro is set to 10 uh the go the Gan time will spin up 10 West thread or more if there is someone waiting for a y or something how do you know which one of those is actually using so you can see the the utilization per it's only per full process yeah it's only for the whole process exactly yeah but how do you know which so the the the pro the all the go routine in the investor will be a little bit uh you know you will have I don't know 10 of thousand for the the rad 10 of thousand for the vad how do you know that they are distributed and used using uh you know certain amount of core just for the read paath well we we don't take that into account it's just that we we just see whether the process CPU usage is above the threshold if if it's above the threshold uh the read PA is dis disabled or it mean or it means it means it it stops accepting read requests oh I see so the the CPU utilization is for the entire investor not yes correct yes okay okay it is it is quite simp oh I didn't get that okay because we were going I I was going today to to actually try in one of our uh production cluster but a small one uh if that uh if that worked and then move to to the big one because we have this problem in that the the read path is taking more than half of the CPU time uh and is affecting the in and okay so I didn't get this at all actually so it's just it's kind like an idea to to to prevent the um the inors from dying and then we we we chose you know by Design to protect the right path so it's more like you it's more like you set like a threshold like above this Above This threshold uh we're just going to protect the right path and and uh ignore reads if I mean oh okay okay okay okay I didn't yeah I was not clear to be that so it was kind of motivated by incidents we had where where investors will just like uh get really backlogged and uh and the memory usage with balloon and uh because of heavy ques so that's kind of like the the the idea okay okay okay ah damn it then might not it could be the righted path is pretty in our experience constant in usage uh so if you if you see them at the stable State that's probably how much the right path consumes so you can put some limit and then assume that whatever that the difference is is the read path um and then and then sh that uh but I think in go along figuring out what process does what is hard um so we don't do that that's why the new architecture where the jest is purely for for qu okay I see yeah and to pigy piggy B on that actually we're thinking of further splitting the inor uh in two parts one would serve serve quiries the other would be responsible for uploading blocks to because what deitar said like goong is not really a soft real time or any time system and it's just very hard to influence the scheduling and then do those measurements by AR um so yeah so basically the only uh weapon we have in this is that is to is to split frings and then just look at that one schedule that one thing okay so your suggestion is to look at how much of the CPU time is spent on the uh ingestion and then basically get a a limit on you would look at how much CPU and memory the inors use in a stable State um they would still be serving some queries uh but maybe half of that is is the right path um you would set the limit to a bit lower than the physical limit so if you're setting go Max Pro to 10 uh then maybe the the limiter limit would be nine um I forgot what the config option is um same with memory and that would whenever the limit is reached that would cut off all queries it would let existing queries finish and then stop new queries I see in our case I don't uh so we have a I don't know I think 10 CPU per uh investor but investor is not using the CPU is using 7 7 7.5 um but uh so still around 55% is used by the the quy pa uh so there is a lot there is still CPU to to use but it seems that uh it's not able to there is some sort of botton because we reject a request uh in the in the investors even if there is a um if the the bottleneck is not is not the disc um and uh and uh we can't figure out uh yet what is the bottleneck there um so I was thinking about using the limitation on the read path because I well with my previous understanding now now I get that it's not possible but to to to say like okay just use less CPU for that so at least all the rest remains uh for the for the ingestion but which means that in this case uh even setting the this this the re uh limit wouldn't work at all because the CPU is not even use up to the to that point we have a 10 CPU uh request in kubernetes and go Max Pro Set to 10 um and still the and and we reject request and I don't know I don't know why the error mess oh we reach the um the maxim flight push request how many series are you having in those inors uh there's a there's a metric and on the rights dashboard or the rights resources dashboard you can see you know series per inors um I think we have a five or six million um serious I might have it here uh in memory serious no this one is I'm I've got resources yeah uh resources uh I'm just asking because if you're running into that inflight request limit um it could be related to to that at least that my feeling that uh if you uh distribute more series per investor that yeah we have 6.6 million per inor um uh I think that the the limit is uh we we double it is like I think it's like around 60,000 uh reest per second yeah uh but we have less it's just like sometimes they just Spike and they they are they rejected uh but from from many from many inest some some investor Sometimes some other investor like a you know like 30 seconds later and so on um and uh yeah the number of request rejected are like a few thousand maybe 10,000 a second per second across all the fleet uh we have no idea why we we The Bu like we constantly check what's uh um yeah like the frame graph to to try to understand if there is something well nothing seems to be wrong but so I the the the next the next step was the um yeah was that which now not next step anymore well J close I suppose very quickly yeah and by the way if I may ask how many iners do you have I think you we have don't know around 500 and something yeah right between five and 600 right right right yeah like uh after this it's uh we even changed e two instances to get like some faster one uh we see we saw like a decrease in CPU utilization on the were less contention resources but still that's not great so no idea how to how to to know this I assume you've looked at traces uh slow iners like Jagger traces uh we don't have traces there because we we we used uh Jagger in in the small clusters and we can and but there we don't have any problem at all it seems that the the the behavior drastically changed with this massive cluster compared to the other one uh so it's not linear at all it's just like something like completely different with many other components as well uh and could be also you know I don't know noisy neighbors or there is nothing else in those uh in those um um two instances uh because we dedicate stuff just only for grafana and that and all our microservices but they are not really very uh they don't use a lot of CPU memory IO or anything um so uh but but it it behaves very differently so in that cluster though we don't have yet all the automation together like we don't we would like to use Tempo because Jager doesn't scale with that uh with that many traces um with one instance so it takes time to redeploy uh or grafana agent or well now ay and stuff like that so we don't we don't have traces unfortun unfortunately and the even profiling we we when I get the profile I I go manually and and get it so it's not uh we don't have continuous profiling to so we we we lacking a lot of automation there to uh to get all the data historical data so that limits us a bit with the the bugging as well uh one problem we had inws is with network through puts on the notes uh sometimes limits um um they they they appear weirdly like they would appear as maybe slightly increased CPU usage but like fastly increased latency on only some inors um and then we found this to be either noisy neighbors or just heavy queries that transfer a lot of data out of the ingestor and then this also affects this affects like the whole networking on the Note um and if if if you're using network attached diss then that adds to to that uh overhead um I think you can find you should be able to find network throughput from the E2 instance somewhere in the console I think it's also I don't know what exporter you use for those metrics but um you can see whether like how close it is to the instance limit um yeah we I see that the packages are dropped uh in the in in many instances we change instance type to increase the the limit and we see still a lot of rejection so we enabled the um we we we we are trying to reduce also that problem because that that surely is we don't know if that is causing uh in the ingestion issues but surely it doesn't help with the debuging because you always had a doubt that okay is it that or not are those like rejected packages enough to disrupt all the rest um and that is uh yeah we need we need we need to fix that so so we we change all the E2 instances with a new ones but that didn't increase so the the only way that they have to reduce traffic now is to enable compression gzip compression everywhere uh but then the the the distributor um um latency you know gets higher so we don't want that to be too high um uh so yeah we are trying to to see that there's also Snappy there's yeah but we already have Snappy but it doesn't uh it doesn't decrease as uh enough to to to to basically not hit the networking limit because the the cluster is very big so uh it's uh there are thousands of PODS and all of them like those ingestor and the queries as well we have like thousands of queries a second and the the the traffic is just massive and we we can't we don't know how to reduce it more so that we we tried gzip and still uh we have less rejection now but it's not enough so no sorry Snappy and now we need to move everything to gzip to decrease even even more Thinking Out Loud bigger requests would compress better um so if you send bigger request from Prometheus or from whatever your AG alloy the agent uh maybe there'll be sh it into like the request that sent to the iner would contain more series so it compresses better um I think when you have that many inors if your talent is on no inors then you know a request with a th000 series get charted like each inor gets two Series so it's it's not that efficient um yeah that's a that's a that's a good one H we actually have a something in the backload to do that but for the wrong not for the wrong reason but for a different reason because we saw that the CP utilization on the distributor uh gets a lot better when the requests are bigger uh because the some sometime in the the frame graph you can see that the CPU is used to to um to to open the package and pass it and so on and and that part is uh is pent on yeah opening it instead of really doing something with the it so the bigger they are the better the CP utilization uh but mention it yeah we should maybe do that sooner and see how that that would help for this as well so even better yeah the inor CPU is also a bit more efficient when you send bigger requests because it involves like it does Cisco for more data uh like one Cisco purists more data to dis so it's it's a bit more efficient also in your case I bet you have a lot of histogram so native histograms will be uh something that will help you but uh migrating to Native histograms isn't trivial at this point so like um that's that's in itself a lot of work as we know ourselves um there is a by the way um a new kind of uh project in pritus where we will be able to compress C histograms into native histograms on on scrape so it wouldn't affect your application your instrumentation just the this scrape uh it will still produce a kind of native histogram which is much more efficient so that will help you but again on the quiry side you need to rewrite your quaries um and uh and and dashboards and depending on what kind of things you're doing the migration can be more or less complicated like if you can just switch to the new type and and uh you know you don't want to see your data in the same dashboard you're fine with just switching out the dashboard and it's fairly simple like we have documentation in the mimir do on uh if you go into the mimir do there's visualization SL native stogram and you can see like how you need to rewrite your quiries but um yeah yeah um my guesstimate for that work in pritus and getting into mimir was like six months because uh like everything we need histogram is just complicated because it's uh it's it's in it's in right in the like the base base line of uh uh right in the middle on the tsdb of the pritus implantation but I mean that's coming also we are working and I hope that for the next Community call uh uh we can talk about uh more about uh an optimization that we want to do the quiry engine as well that would uh reduce your uh resource utilization and speed up quaries that doesn't help you much on the right path but you know overall if you have a huge system that will like uh help you so hope to hope to speak about that in the next Community call the streaming one yeah yeah yeah uh so if you see the prom con presentation I did uh yeah that's the one that now now we actually started to like we planned it and it's a project internal it's not just a hackaton anymore it's like actually at least two people working it I don't know maybe more um so it's that's coming along as well sounds good thanks yeah all right um any other question I think well um thank you for coming and I thought you were asking about that specific thing I would have one probably is quick do you have any reason why you don't set gox goax procs for everything that you have in a yeah or in well I'm using the jonet um I am uh I'm I'm trying to so I I see that there is nothing for the ingestor but when we set go Max proc for the invester it drastically reduced the the CPU usage and the the even the amount of like rejection that we had and so on um so it seems that the the schedular was doing a much a better job to to actually using the CPU instead of wasting doing you know juggling go routines and so on and uh uh I see that it's uh is set for Distributors and other components but then there is a a logic that I don't know where it comes from I don't understand it that says basically choose the max between eight and the actual CPU request uh sometimes is I don't know if it's multiplied by two or not uh I don't remember uh but anyway the uh sometimes you mention that 25% of the CPUs uh is dedicated then to it can be used by the garbage ction um I don't know where you know what the logic behind it why so shouldn't in theory I know that the practice is different in theory but should in theory the runtime and the and the the all the the the go routine that you spin up work better when you actually when the when the runtime knows exactly how many CPUs there are to be to be used so and is the runtime that knows how many what's uh how to best manage the the the O thread when the garbage collection needs to happen um I don't know I don't know maybe the others I only know that if you don't set it then it's set automatically to the number of CPUs that have you have right yeah so automatic do however many CPUs the machine has um detected detected somehow with SL broke uh um but that doesn't work in a in a in a container environment though because yeah it just uses the number of cores on the Noe yeah yeah we saw that like the number of CPUs that we have for example are don't know like 32 64 and the and the investor when the investor has that amount of uh when the investor when when the gantine thinks that there are that many uh cores it works a lot worse than when we set it to the actual CPU request of the of the container and that helps a lot but that is not in the your code so I don't know if it can help to have it for for the community or maybe not I think our thinking is that we want to allow spikes to go over the limit and if we set the gox procs that would not be possible however we were recently discussed what setting that for Distributors the small issue is then we have Autos scaling based on CPU and if they don't go over the Quest they would never out of scale so we need to figure that out uh but yeah for this if if you want to use that feel free to do that I mean there on Distributors we don't think there is any problem in our case on investors we want them to use more when they when they have to usually not all so we have many workloads in our clusters and not all of them need to spike CP usage at the same time so this is this works for us but it may not work for you yeah yeah yeah now okay now now I understand it yeah yeah make other I I didn't think about the the spikes and if yeah if you want that to to spike yeah we haven't tried this for other components I think we just never tried maybe we should we just save CPU we have to try it and find out okay but like to your question if you if you do try it and like it shows results I think U it's okay to include in the chart or help J sorry okay okay sounds good thank you uh all right uh anything else to this topic or to the topics uh before that another question comment native hisrs rule yeah native histograms rule but yeah um hopefully we can have a Prett like a good migration documentation as well but uh I don't want to write it before we actually do our own migration and like go through all the details and you know and and just do food it ourselves uh cuz I really liked how somebody mentioned that when you have slos based on specific times and buckets native histograms don't work that well because you wanted that exact uh boundary and that's not easy with the native histograms but apart from that I think just use them everywhere yeah yeah I'm actually working on the slos uh for ourselves to try to do them with with Native suum um yeah I just need more Json that reviewers never mind all right so thank you very much for for showing up and the questions and and listening and uh let's close the meeting and see you next month thank you thanks that you

