# TomTom’s Observability Governance Framework: Driven by OpenTelemetry &amp; Grafana

Published on 2025-07-03T20:49:50Z

## Description

Join TomTom as they share their Observability Governance Framework, built on OpenTelemetry and focused on standardising ...

URL: https://www.youtube.com/watch?v=19cdy3VZ-iA

## Summary

In this YouTube video, Nikl Gupta, the head of API and Reliability Platform at TomTom, discusses the company's observability governance framework and its journey towards improving software reliability and data integration. He highlights TomTom's evolution from navigation devices to a leader in mapping and traffic software, emphasizing the challenges faced with numerous disparate observability tools that hinder user experience and incident management. Key issues included traceability, alert fatigue, access control, and high observability unit costs. Gupta outlines their strategic response, which involved establishing an observability governance framework to standardize tools and processes, adopting open telemetry, and integrating Grafana for a unified observability platform. He also touches on future goals, including enhanced machine learning capabilities for anomaly detection and improved front-end observability in relation to their Kubernetes platform. The video concludes with an invitation for questions, further engaging the audience.

# TomTom's Observability Governance Framework

## Introduction
The topic for today is TomTom's observability governance framework. We'll discuss the journey TomTom has taken and our current standing. My name is Nikl Gupta, and I am the head of the API and reliability platform at TomTom. I have been a full-stack developer even before the term became popular, with experience in front-end, back-end, jQuery, and databases. I have been an engineering manager for many years and have been with TomTom for almost three years, leading observability since 2025.

## About TomTom
How many of you know about TomTom? (Audience response) Great! What is TomTom known for? Navigation, right? However, we no longer focus on devices as much. Instead, we are leaders in maps and traffic, as well as the software that surrounds these areas.

One of our new platforms, **Orbis**, integrates various data sources. We cover **235 countries** and utilize **5 billion data sources**. This means we integrate data from small, medium, and large sources. We gather data from the Oberture Foundation, OpenStreetMap, and various proprietary sources. Additionally, we now allow you to bring your own data to integrate with Orbis maps, providing a vast data source for our users.

The **Obert Maps** initiative, created with Amazon, Meta, Microsoft, and TomTom, is governed by the Linux Foundation, and TomTom is a founding member.

## The Problem
In the software world, we always start with the problem. TomTom faced several challenges, which were also highlighted by the previous speakers. We have a plethora of tools—self-hosted, cloud-native, and cloud-agnostic. However, from a user experience perspective, this is not ideal. Logs, metrics, and alerts are scattered across different platforms, making incident resolution cumbersome.

### Traceability Issues
Traceability has been a significant challenge. With a distributed data environment, connecting the dots is difficult. For example, various terms like trace ID, correlation ID, and request ID complicate tracking user requests through multiple backend services. When something goes wrong, it can take hours to perform root cause analysis and find mitigation strategies.

Inconsistencies in data formats add to the challenges. Dates are often formatted differently, complicating dashboard creation. Engineers spend too much time converting data formats and consolidating information.

### Alert Fatigue
We also encountered issues with alerts. Poorly defined alerts create fatigue among engineers. Alerts that lack clear severity or ownership can overwhelm teams, leading to burnout—no one wants to be woken up in the middle of the night for a non-critical alert.

### Access Control
Access control and management have been problematic as well. Sensitive data access needs to be managed carefully. Not every engineer can have access to data during an incident, and data residency requirements, especially regarding EU regulations, complicate matters further.

### Observability Unit Cost
Finally, we have the observability unit cost. This refers to the total cost of ownership for observability tools, including logs, metrics, and alerts. We struggled to identify the ownership of logs and the quality of our data. Rogue data—logs and metrics without clear ownership—complicated this further.

As the saying goes, "more tools do not mean more insights"; rather, they create more problems.

## Defining Goals
To address these challenges, we established clear goals. We aimed for a transparent observability spend mapped to service ownership. We wanted to know which teams were responsible for which services and the observability data generated by them. Reducing Mean Time to Recovery (MTR) and Mean Time to Detect (MTD) was crucial, as it is a key selling point for our customers.

We also sought to improve developer productivity and efficiency. Having multiple tools can severely hinder productivity, so standardizing observability practices became essential.

## Establishing the Observability Governance Framework
This led to the creation of our **observability governance framework**, aimed at establishing an observability golden path. This means identifying the tools that will be used for the foreseeable future, ensuring a clear return on investment.

We adopted **OpenTelemetry** as a standard for instrumentation. We're early adopters of this technology and plan to use **Loki** for logs and **Tempo** for metrics and profiling data. Establishing clear service level objectives (SLOs) is vital for our service delivery and customer satisfaction.

### Application Command Center
We are also developing an **application command center**. This centralized view will provide insights into all services at TomTom, including infrastructure, technical, and business services. It will display health checks, metrics, and overall data quality.

### Automated Incident Management
Incident management is another area of investment. We are working to streamline incident handling processes. By integrating **Grafana OnCall** with our incident management tool, **ServiceNow**, we aim to enhance visibility and control.

### Unified Observability Platform
Through Grafana, we are building a unified observability platform that consolidates logs, metrics, traces, and on-call schedules. This single platform enables engineers to access all relevant data from one dashboard, improving incident response.

## Future Roadmap
Looking ahead, we aim to establish an **instrumentation score** for each service to quantify the quality of data delivered. This will help reduce waste and improve data quality.

We also plan to expand our application command center to connect services with ownership, providing a comprehensive view of logs, metrics, traces, and data quality.

### Centralized SLO and SLI Governance
Centralizing SLO and SLI governance is essential. Currently, different teams manage their own metrics, but we aim to offer a unified view to benefit both engineers and executives.

### Machine Learning Integration
We are exploring machine learning-driven anomaly detection to reduce manual efforts in analyzing observability data. This will streamline the process of converting data into actionable insights and post-incident analyses.

### Front-End and Kubernetes Monitoring
Lastly, we aim to enhance front-end observability and improve our Kubernetes monitoring. As TomTom builds its own Kubernetes platform, effective monitoring is crucial for maintaining reliability across our numerous clusters.

## Conclusion
This is where we currently stand in our journey towards improved observability. Thank you for your attention, and I am open for questions. 

![LinkedIn QR Code](link-to-qr-code)

## Raw YouTube Transcript

The topic for today is TomTom's observability governance framework and we'll talk about how the journey has been for TomTom and where do we stand today. So about me uh Nikl Gupta I am head of API and reliability platform in TomTom been fullstack developer before the term full stack came into existence I believe done front end back end jQuery database whatever you call so been there uh been engineering manager since many years now and with TomTom for almost 3 years or close to 3 years and leading observability from 2025 before. So now about TomTom. So how many of you know about TomTom? All of you. Okay. Do you know what TomTom is known for? Navigations. Yes. The device. No, we don't make devices anymore that much. Right. So now we are leading in the maps and in the traffic and uh also in the software around maps and traffic as well. As you can see uh we have Orbis. Orbis is one of our new maps platform. What we do with Orbasis? We integrate different data sources. As you can see we cover 235 countries. We have 5 billion eyes or data sources. What does that mean? That we integrate data from various sources. It can be small, big or medium size. Uh we have data coming from Oberure Foundation. We have data coming from Open Street Maps. We have data coming from various proprietary uh data sources that TomTom has. But now we also have the possibility to bring in your own data. So you can bring in your own data, integrate with Orvis maps and you have a big data source available for you. uh if you know what is Obert Maps uh if not Obert Maps is a foundation created with Amazon, Meta, Microsoft and TomTom and it is governed by the Linux Foundation. Uh and TomTom has been the founding member of the uh organization. That's about TomTom. That's about me. And now about the problem, right? We in software world, we always start with the problem. And the problem that TomTom had and that has been shared by the two speakers before me. We have a lot of tools, right? And when I say a lot of tools, the count does not matter, but at least the categories matter. We have tools which are self-hosted. We have observability tools which are cloudnative. We have observability tools which are cloud agnostic. So what do you do then? Right? From a user experience point of view, it's not a great user experience. We have logs somewhere else, we have metrics somewhere else, we have alerts somewhere else. And when there is an incident, yeah, you know how it goes, right? Uh along with that, traceability. Traceability was a big problem at some point in time. When I say traceability again since the data is so distributed it's extremely difficult to connect. Today in one of the presentations we saw there was a term trace ID but imagine if this is not the only term used in your company. You have correlation ID, you have trace ID, you have request ID, you have I don't know whatever else exists in the world and it is extremely difficult during RCS. It is extremely difficult to navigate through one request that comes from a user application goes to an API gateway and then goes to 10 different backend services to let's say gather the data and if something breaks it takes hours to come to an a root cause analysis and conclude and find the mitigation around it as well. Inconsistency of the data format dates are usually the biggest problems. So dates are in all different possible format that exists in the world. And when you consolidate or trying to consolidate a dashboard or build a dashboard using all of this data and data sources, imagine how much effort it takes for an engineer to build such a dashboard, right? Convert dates, convert uh JSON format into a different format uh unstructured data into structured data as well. Uh we also had alert problem with alerts as well. And what is the problem with alerts? The alerts create alert fatigue. Right? So since there are alerts which do not clearly define the severity or define the owners, sometimes the alerts are going to different teams, right? If there is an alert on the product level, it goes to 10 different technical services. That creates alert fatigues for our engineers. Nobody wants to wake up in the middle of the night, right? Nobody likes that. Another set of problems we talk about uh access control and uh access management. So sensitivity of the data right who gets access to the data who can see the data during an incident does everybody has access to the data is it the on call engineer who has access to the data. So that was also a big problem. How do we define access and also how do we define the data residency right? So TomTom help global uh users and some of our users or customers have requirements to keep our data in a specific region. These days the EU data residency is a big topic. So you might have heard about like various companies AWS, Microsoft, they're talking about bringing their data in the EU and not in the US or also having challenges in terms of defining how do they comply with data residency in EU. Uh last not the least observability unit cost. We talked about today also the cost or the total cost of ownership. For us we call it observability unit cost. What does it mean by observability unit cost? You have logs, you have metrics, you have alerts and then you pay for infrastructure or license cost and stuff and then you try to identify how much is the unit cost for it because in the end you want to get the maximum value out of it. So for that it was extremely high because for one thing we did not know who is sending logs right. So there was no clear ownership of the logs. There were logs, there were metrics, there were alerts but some of them were really rogue. So maybe 20 30% of the data was rogue. We didn't know who was sending it and we could not categorize it as well. So when you have so many problems as the title says more tools does not mean more insights. it creates more problems for us. So what we did we went ahead and we said okay let's let's solve the problem by defining certain goals right in the first slide to Tom explained the uh using chess the analogy how you define your business goals and then how you define the project you buy get the buy in from stakeholders and other things as well. So we did the same as well. We defined certain goals for ourself. We said we want to have a transparent observability uh spend mapped to the service ownership as well. So we want to know exactly which team owes which services and which services generate what kind of observability data. Of course we wanted to reduce our MTR and MTD as well because in the end that is what is important when we sell our products to our customers. One of the selling point is we are better than Google in terms of better MTR and MTD. Uh developer productivity and efficiency in the end if you have 6 7 10 different tools uh I think in one of the slide it said 101. So not all companies have 101 but still many the uh developer efficiency and productivity is extremely low because during your day debugging or during incident you have to navigate through many tools. And last but not the least is standardizing observability. Like I said, data formats were a mess. Traceability was a mess. So we said we need to have a standard way how we handle observability across TomTom. So what did this led to? This led to an observability governance framework where we are trying to establish a observability golden path. And what does observability golden path means? It means that among all the tools we identify what is the tool for the next n number of years because we can define clearly the return of investment by investing in that tool as well. Uh we talked about uh instrumentation via uh open telemetry because open telemetry is now the new standard uh in the industry and uh TomTom doesn't want to stay uh behind. We are I think we are one of the early adopters of open telemetry. If you look into the open telemetry website uh we also uh wanted to use Loki for logs and meir and tempo for metrics and uh profiling data as well. But why do we wanted to use it? Because it's quite an open standard as well. Like uh some of the other slides mentioned they have graphana has three different models. you have open source and you have enterprise and you can manage your own as well. Uh along with that all of this is important but if you do not define the correct service level objectives and indicators it's not going to work right. So we sell our products our our customers come to us with an SLA demand and unless we start monitoring our our uh SLI and SLOs's it's not going to work for us. So this is one of the things where we want to invest. The next is application command center. What does it actually mean? It's a big word but what it actually means is a single pane where we have all the services defined that exist in TomTom. be it a infrastructure service, be it a technical service, be it a business service that we owe and in one view people can see what are the different SLOs's health checks and uh uh metrics and data and quality around all of them as well. And the last is automated incident management. So as I said incident handling was quite a pain. So we definitely want to invest in incident uh management as well. Uh going back to the establishing observability golden path uh what we did was we uh we we defined what are the what are the pre-approved uh tools available in TomTom and we said out of these pre-approved tools which tools are in a state where you can adapt which tools are in a state where you they are on hold which tools are in a state where they are should be decommissioned as well. What this does did was this clearly defined which tools are the tools where we are investing in future uh with the with the SLOs's and SLI what we did was uh we wanted to make sure that we have a clear ownership today uh in the breakout we were talking about we have service now as our uh let's say incident management uh tool but we are trying to now use graphana on call to integrate both of these So we use graphana on call to handle our uh schedules and on call and we are integrating with service now where we handle our incident. So this tight coupling between the both tools will help us have a streamlined view of how our incidents are like and what are the uh what are what is the base data behind these incidents as well. And how graphana helps us in powering this journey is we with graphana we reached a unified observability platform. Uh it's a single platform where we have logs where we have matrix where we have traces where we have on call. So as an engineer when you are on call you can go to into a single tool and you can look into the single uh let's say dashboard and you can get all the view from it as well. uh incident response management like I explained uh we are working with Grafana on IRM uh which is Grafana on call in the past and we are integrating it with our service now capabilities. How it helps is that the service now uh the incident management team has more control on how they want to represent the incidents uh for the company and we have more control in terms of how the schedules are like how they are connected with the incidents how they are connected with the data telemetry data in the background as well. Last but not the least the most important one from my perspective is the observability unit costs. So in graphana we uh we also saw in some of the slides the adaptive metrics adaptive logs and adaptive tasting. So with graphana we are constantly working together to optimize our log usage our metrics usage and our traces as well. So I'm quite happy to say that last month we actually reduced 20% of our uh metrics which reduced 20% of our cost as well on metrics. How does the road ahead looks like for us? So we want to establish an instrumentation score. What does it actually mean? It means each service will be able to quantify the quality that they deliver. When I say instrumentation score, it also means all the attributes that are in your logs and in metrics. Are they necessary to be there? Are they important to be there? And what we define as observability team the most important attributes are they in your instrumentation uh or not. What it will help us is it reduce waste on one side but it also increases the quality of the data that we have. Uh we want to expand our application command center. Uh so we want to connect these services with ownership and we want to make sure that when a person is looking at a service they are able to see the logs the metrics the traces and the quality of the data all together in one view as well. Uh like I said centralized SLO and SLO SLI governance this is most important for us because it also is connected with external customers. We want to centralize it today. It is not centralized, decentralized. Different teams manage their own SLOs's SLI. But we want to offer a single view of it because it helps not just the engineers but also the executives to understand the quality of the services they are accountable for. The next is machine learning driven uh anomaly detection. We learned uh today in some of the slides as well Gfana is also investing in AI. So that is something that is also on our radar. Uh where we want to invest. How we want to invest is today there is a lot of manual effort needed in terms of analyzing the observability data converting it into incidents and then converting into post incident activities as well. Right? So creating RCA creating postmortems as well. So we would like to invest in AI and machine learning because this is the next big thing. And then we want to connect or extract the data that is available in graphana and use it to build our RCAs and postmortems afterwards as well. The last but not the least is the front-end observability and the monitoring uh kubernetes monitoring. So in the last one year what we have done is we have tried to consolidate our observability journey towards graphana. That means our logs and metrics are already in graphana. So let's say 60 70% of our backends are already covered using open telemetry and graphana integrated into graphana's uh platform as well but we are still lagging the end to end user experience that we were talking about in the earlier as well. So we have front end application so a user makes some requests but today we don't have a connection how things are happening in the back end. If somebody is calling our APIs, it's if even if there are 10 APIs connected together, we know today how they are connected, what is breaking, what is not breaking. But now we want to uplift it and we want to connect it with the end user experience as well. Why is Kubernetes monitoring important for us? Because at TomTom, we are building our own Kubernetes platform. Uh many companies do it for the cost purposes as well. Uh and we talked about fleet management. We we looked into some slides and it was about fleet management. Our fleet management in TomTom is 1,200,300 Kubernetes clusters and it's a nightmare for our engineering teams to understand which cluster is breaking which cluster is not breaking what is going wrong with one cluster and other cluster. So we want to invest in Kubernetes monitoring specifically to ensure that we offer a reliable Kubernetes platform all across. That's pretty much it from my side. So, this is my LinkedIn QR code and I'm open for questions now. Thanks.

