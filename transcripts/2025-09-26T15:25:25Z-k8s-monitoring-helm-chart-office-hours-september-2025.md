# k8s-monitoring-helm Chart Office Hours (September 2025)

Published on 2025-09-26T15:25:25Z

## Description

In the September edition of the Kubernetes Monitoring Helm chart office hours, we discuss the version 3.4 and 3.5 releases as ...

URL: https://www.youtube.com/watch?v=7V7NKY1NAqE

## Summary

In the September edition of the Kubernetes monitoring Helm chart office hours, Pete Wall discussed recent updates and upcoming features of the chart, specifically focusing on releases 3.4 and 3.5. Notable changes in version 3.4 included the removal of a post-install hook, the introduction of a new Loki standard output destination for logging, and enhancements in TLS configurations. Release 3.5 brought about a separation of the pod logs feature into distinct components, allowing for more flexible log gathering, alongside the introduction of custom destinations for more complex data routing and processing. Pete also highlighted upcoming features such as front-end and database observability, enhancements to cluster role permissions, and cloud provider label enrichment. The session concluded with an invitation for community feedback and questions.

# Kubernetes Monitoring Helm Chart Office Hours - September Edition

Hi everybody, I'm Pete Wall, and welcome to the September edition of the Kubernetes Monitoring Helm Chart Office Hours. We've got some cool stuff to talk about today. We've been pretty busy since last month's office hours, having released two more feature versions: 3.4 and 3.5. 

## Overview of Recent Releases

### Version 3.4

**Release Date:** September 3rd  
**Notable Changes:**
- **Removal of One Post Install Hook:** In version 3.3, we added a hook to address potential race conditions between deploying the Alloy instances and having the Alloy operator up and running. This was done by moving the Alloy instances into a post-install hook. While this solved the initial problem, it led to some issues with installation methods like Argo CD, which skipped the Helm hooks. As a result, Alloy instances are now back in the main set of manifest files. However, upgrading from 3.3 to 3.4 may require a manual step to reclaim those Alloy instances. More details on this will follow.
  
- **New Destination - Loki Standard Out:** This allows you to take the Loki log data and print it to the Alloy logs output. This was introduced based on requests to log cluster events in standard out, similar to functionality in version 1 of the Helm chart.

- **TLS Configurations in Remote Config Section:** Added support for TLS configurations along with updates to upstream dependencies.

### Version 3.5

**Release Date:** September 15th  
**Notable Changes:**
- **Pod Logs Feature Split:** The pod logs feature has been split into a separate feature for Kubernetes API logs. This allows for more flexible log capturing in heterogeneous clusters.
  
- **Custom Destinations:** This new feature allows users to define custom destinations for metrics, logs, and traces, expanding the capabilities of the Helm chart significantly.

- **Removal of Internal Spans in Span Metrics Generation:** By default, we are now filtering out internal spans to reduce noise in the span metrics.

- **Scrape Timeouts:** Users can now set scrape timeouts for all features, enhancing configurability.

## Detailed Features

### Loki Standard Out Destination

You can define the Loki standard out destination using either the destinations map or array. For example, you can specify a log processing rule to filter data from the production namespace. This enables pod logs to be sent to both Loki and the Alloy logs.

### Upgrading from Version 3.3 to 3.4

If you are upgrading from version 3.3 to 3.4, run your Helm upgrade with the `--take-ownership` flag. This will reset all required annotations for the Alloy custom resources.

### Pod Logs via Kubernetes API

The pod logs feature now supports heterogeneous log capturing by separating the Kubernetes API logs from the main pod logs feature. This allows for volume-based log gathering on traditional Linux nodes while still accommodating managed nodes like EKS Fargate.

### Custom Destinations

Custom destinations allow you to supply extra configuration for arbitrary Alloy components or to create new destinations for different types of data. For example, if you wanted to send data to Kafka or AWS S3, you could define a custom destination for that purpose.

## Upcoming Features

We are actively working on the following features:
1. **Front-end Observability Feature:** Utilizing a specific Pharaoh destination.
2. **Database Observability Feature:** This will help with inspecting databases like MySQL or Postgres and performing query analysis.
3. **Running Without Cluster Roles:** Exploring the ability to operate with only role-based access controls.
4. **Cloud Provider Label Enrichment:** Adding tags and labels from EC2 instances into the metrics and logs.
5. **Agents.md:** Creating a useful markdown file for AI tooling related to the Helm chart.

Thank you for bearing with me through all that information! Now, I'd like to open the floor to questions and answers. If there are no questions, I want to express my gratitude to everyone using the Helm chart and providing feedback. Your input is invaluable to this community-driven project.

If you have any questions, comments, or feature requests, feel free to join us on the Grafana public Slack at grafana.slack.com in the #kubernetes channel or visit our GitHub repository at grafana/monitoring-helm.

Thank you for watching, and I hope you have a great month! See you in October!

## Raw YouTube Transcript

Hi everybody. I'm Pete Wall and welcome to the September edition of the Kubernetes monitoring helmchart office hours. Uh we've got some cool stuff to talk about today. So we actually been pretty busy since the last office hours last month. Uh we've reached we've released two more feature releases, uh 34 and 35. So I'm going to talk a little bit about what's going on in both of those uh as well as talk about some of the upcoming features and we'll leave time for questions and answers. So 3.4 3.4 was released in September 3rd. Uh and it's got a number of notable changes. Um the most uh probably visible to a lot of you might be the removal of one of the post install hooks. Uh in 33 we added a hook that uh attempted to address any potential race conditions between deploying the alloy instances and having the alloy oper operator be up and running. And it did this by moving the alloy instances into a post install hook so that we knew the alloy operator was going to be up and running with a running pod and then we'd put the alloy instances onto the cluster. Um it definitely solved that problem but the issue was that by moving the alloy instances into post install it kind of had some other uh ramifications and so we undid that alloy the alloy instances are back in the main set of manifest files. Um but what this does mean is that from 33 to 34 it might require a manual step to kind of reclaim those alloy instances. I'll go into some more detail about that in the next slide. Uh we added another destination called Loki standard out. Uh this one allows you to take the Loki log data and print it to the alloy logs output. Um and I'll explain a little bit more about why we want why we went that way. Uh and then uh we added support for TLS configurations in the remote config section uh along with along with plenty of other fixes and then updates to uh the upstream dependencies that we have in the Helm chart. Um let me talk about the Loki standard out destination first. Um that one uh oops let me go back. All right. So that one uh looks like this. So here's the Loki standard out destination and how you define it. So in the your destinations uh here I'm using the destinations map but you can use the destinations array. Uh I'm defining a Loki destination as normal but I'm also defining this new Loki standard out destination with a log processing rule to say I'm only going to keep data that comes from the production namespace. Uh and so by enabling pod logs uh since pod logs creates Loki type logs it's automatically sent to both of these destinations at the same time. So if you were to inspect the alloy logs own Kubernetes pod logs, you will see the logs that are coming off from this name space. Why would you want to do that? Well, what happened was in the uh in version one uh of the Helm chart, let me go back to slideshow here. Oh, I actually have a slide about this. Cool. Um in version one of the Helm chart uh the cluster events feature had an option to log to standard out. So as the in back in version one this was alloy events as alloy events would get cluster events from the Kubernetes cluster it could also log it to the standard out of Kubernetes or of the um alloy events pods. Uh and we had a request to bring that over into into the the latest versions of the Helm chart. I thought about adding cluster events and log to standard out there. But what I opted to do instead was to create this Loki standard out destination type. So you can get the cluster events from the normal cluster events feature and then have it sent to the Loki standard out destination. And this would achieve the same thing, but it makes it much more flexible and generic so that other people can use it if they wanted to to set something up. Um there's someone in the community forums who've been talking about kind of an interesting setup. they want to be able to uh capture logs and then save them and print them and and I'm thinking the Loki standard out might be a good use case for that sort of thing too. Um so there's a there's there should be multiple uses for something like this. Um all right so the upgrade from 33 when I talked about one of the things that we did in 34 was removing one of the post install hooks. So like I said in 33 we put the alloy custom resources inside of a post install Helm hook uh with the goal is that it would stage those uh after the alloy operator was set up. Uh but the side effects that we saw after that's in the field for a little bit was that in certain install methods like Argo CD uh it would see that these are Helm hooks so I'm not going to actually render those manifests and so they would actually just get skipped. Uh well that was the a really bad part. Um in effect it ended up meaning that those alloy instances didn't feel like part of the main deployment. Uh and so they kind of got uh got sidelineed sometimes. So in 34 we put them back into the main manifest set. Um the issue is that in 33 if you're upgrading to 34 or later uh you need to tell Helm to read those alloy CRs. So, uh, simply just run your helm upgrade, but run helm upgrade with the d-take ownership flag, and that will reset all of the required annotations onto those things. That's just a one-time flag. You only need to run it when you're upgrading from 33 to something later. All right, we talked about the Loki standard app destination. So, let's talk about Kate's monitoring helch chart 3.5. Um, like I said, we were busy. We came up with two feature releases this month. Um and we had some really fun stuff and exciting stuff to bring into this too. So 35 was released about two weeks later uh on September 15th. And the notable changes there is that uh I have started the process of splitting up the pod logs feature. So for those of you who know since 2.0 the pod logs feature has had a lot of options. It was able to read pod logs from the file system which is its standard way. It was able to read pod logs from the Kubernetes API. Um, and then I had stubbed in methods in there for doing things like a Loki receiver or using the Open Shift cluster log forwarder, those sorts of things. Uh, that never got fully uh fully written out. Um, and as that feature got more and more complicated, uh, it became something that I wanted to do to kind of actually split it up and make it uh, you know, more targeted towards the individual tasks that it's supposed to do. So, the first thing to happen now is that the pod logs via Kubernetes API is now its own separate feature. Um you can then uh create a pod logs feature. You can use the pod logs feature the original one using the file log or the volume based uh log capturing in parallel with the pod logs for the Kubernetes API. Why would you want to do that? It allows for like a heterogeneous capturing of pod logs. Uh that really makes it much more flexible for um more complicated clusters uh that you might actually have. Um, other notable changes, custom destinations. This one's super cool. Um, I'm I'm excited to share more about this in an upcoming slide here. Uh, also for the span metrics generation in the application observability feature, we are now by default removing internal spans. We found that these were just not helpful and just adding a lot of noise uh to the span metrics that are getting generated. So, by default, we'll filter those things out. Uh in 3.5 we allowed you to set scrape timeouts everywhere. Uh this was uh something that was requested and you could set them individually for some features. Now we're letting uh the scrape timeouts be set everywhere. Uh and just like the last one, we've incorporated another another set of good fixes. All right. So pods via Kubernetes API. Like I said, this is this splits out from the main pod logs feature. It replaces what in the main podlogs feature is the gather method equals Kubernetes API. That's still in there. So you don't need if you're using that and it's working fine, you don't need to change anything right now. Um but we are we have marked that deprecated uh and we won't touch it until 4.0. Uh but if uh if this if the rest of what I'm going to tell you about this podlogs feature, the split out Kubernetes API podlocks feature seems interesting, you might want to look into it more. Um, now that the two features are split, like I said, it allows for heterogeneous log gathering. So, if you have Linux nodes on your cluster, that's kind of the most common. We can use the volume based log gathering. Um, there's a lot of advantages to using volume based log gathering. It's my experience, it's more stable. uh it's persistent to restarts both of the alloy pods but also of the the individual pods themselves uh because those log files are persisted onto the nodes file system. But if that's not feasible because you're running uh nodes that are managed nodes like on EKS Fargate or GKE autopilot um then you can use the API log gathering. uh if you're running Windows nodes and uh we can't get to the file system on that um we can use API based log gathering um or for any other system if you have a cluster where the host path volume out is just disallowed and you can't get to that you can use API based log gathering for those um Windows I would love for that to have volume based log gathering that is something I'm thinking about um and I'm trying to figure out the best way to do that uh in the future but for now uh the API based log gathering is probably the option. Um, so how did this look in practice? So, uh, if this is kind of what we considered like we have a a cluster here and pretend this maybe is an EKS cluster with two traditional Linux nodes and two EKS Fargate nodes where they're nodes but they are fully managed nodes by uh by the EC2 or the EKS system, right? Um, in this example, the alloy logs damon set would be deployed to uh the the more traditional nodes and it's able to read the log files by going to the host path volume mount and reading var log pods. Uh, but unfortunately we would skip any pods that are running on those ephemeral nodes because var log pods doesn't exist. You can't get to that log file path. Um so this these arrows kind of show the traditional based volume based log gathering. And so if you had these ephemeral nodes in the past the guidance was switch to API based log gathering. So uh all of the alloy instances would talk to the API server. You would stream the pod log data from all of the pods straight from the API server. And this works. It's it's nice. It's flexible. uh but the the the downside here is it adds a lot of network traffic internally to the cluster and it you know you're adding a lot of load onto the API server. So by having two features one pod logs feature for the volume based monitoring or volume based log gathering uh and a now pod logs via Kubernetes API feature for API serverbased log gathering. uh we can do both. Uh and so you enable both and you tell the API server based log gathering uh I'm only interested with pods that exist on these nodes or something like that. Um and so then you can get uh all of your pod logs in the most efficient way possible. All right. 35 also brought in custom definition custom destinations. So since forever we've always had the ability to supply extra config. Um this is a little bit of background. So in the alloy definitions you can supply extra config. Uh and this allowed for you to uh add arbitrary alloy configuration um for anything that you want to do. This was really great for adding new data sources that we didn't have features for. So uh stats dex exporters or like the MongoDB Atlas connection. Um we don't have built-in features for some of these things. And so the guidance was you know hey write it up in extra config and then you can wire it into the existing destinations for sending metrics logs traces profiles. Um this was great for that. It was fine for defining whole pipelines. If you wanted to to add just basically arbitrary config to gather data process data and then deliver it somewhere. Um, you could do that through all of extra config, but it was not something that you could use if you wanted to gather, say, like the pod log data uh or cluster metrics from our built-in features and then send that to a custom destination. uh because of the way that the helmchart works, we kind of do some complicated routing of the data from data source to data destination and um you couldn't use extra config to create new destinations until we added the custom destinations. So custom destinations is a new destination type called custom uh and it has a few required fields. First is config. I'll show a demo of this or show some examples of this in just a second. Um but the required fields are the config which is just like extra config. It's the raw alloy config components that you want to use for that. Uh you define the ecosystem which says what's the native type this config is supposed to accept. Um I'll explain a little bit of that when we look at the details. Uh and then you need to enable at least one of the data types. So, is this config meant for metrics or for logs or traces or profiles? Or perhaps it's meant for multiple like on some of the open telemetry type components. Um, and then for all of the data types that you enable, you need to to specify an alloy input target. So, where like how does alloy know where to send this data? What's the component that you need to send? Um, so let's uh here. Yeah. All right. Great. Here's an example um of us making like a Kafka custom destination type. Uh so in the destinations list I have cut type is custom and I can name it Kafka. And then in the config block this is the alloy components that I want to add for this destination. And so we're doing two actually. We have theel call.exporter.cafka for the actual Kafka exporter. But we're going to run it through a batch processor first. Uh so the first input should be the batch processor which then batches things and then sends the log data out to the Kafka exporter which then defines how we're going to send it out. Um and so the second required field is the ecosystem. This is an OTLP type uh because it's using the hotel coal.exporter. If it was something like you know Loki something then it would probably be the Loki type. Um logs are enabled. All of the others are are missing from this definition. So they're disabled by default. And then the target for logs is this. And so this hotel.processor.batch.cafka.input says this is the component where I want you to send log data to to deliver to this destination. Um, so custom destinations is a great way to add more destinations if you have uh something where you want to send your data to that's not in the built-in official destination types. Um, you know, we have Loki, Prometheus, Pyroscope, uh, Open Telemetry, uh, and now Loki standard out. Uh, but if you wanted something different, like you want to export data to AWS S3 and we don't have that built in, it's a great way to add that sort of functionality in there. And honestly, it it opens up the idea for building pretty complex pipelines because this is it's in the destinations, but there's no reason why you couldn't accept data and then you can use all of alloy to do anything that you want with that. So, processing it, redelivering it to a different, you know, alloy instances or something like that. Um, there's a lot of potential here. Let me switch back over to the to my examples here. So this is the same custom definition for Kafka uh that we just saw a second ago. Um here's another simple one just a debug destination. So it's using the hotel exporter.debug uh which prints into your standard outstream for the alloy in this case. uh it prints you know here's how many traces arrived here's how many logs arrived for metrics or things like that and so in this one ecosystem is OTLP again but metrics logs traces are all enabled and so this uh destination is available for anything that generates metrics logs or traces to be sent to this destination so in this example I'm using annotation auto discovery to gather some metrics and I'm forcing it to say the destination for this is this new debug destination Uh, and so as those uh, metrics are scraped, it's going to go into here and it's going to run into this component which will then print the metrics that it's gathering into the alloy metrics debug stream. It's really cool once you try it out. Um, I'd love to hear what sort of custom def destinations that you're building. Uh, so if you try this out and you build a custom destination, let me know. So that's the recap of 34 and 35. Uh in the upcoming features, uh we are actively working on both of these top two. The front-end observability feature, uh we'll have we'll be using the very specific uh Pharaoh destination. Um we're working through some some final changes on that, but we're excited to get that in your hands. Uh the database observability feature. So this one is using the database observability components in alloy uh to do some inspection on uh not just your the databases that you're running at MySQL or Postgres uh but also some query analysis into that. So there's some really interesting stuff that's going to be coming there. Uh and having a database observability feature will make it a lot easier to get that up and running. Um I'm still looking into the ability of running without cluster roles uh and using only ROS and RO bindings. Um after doing some testing with this uh there's a few caveats that I want to put under that. Some cluster roles are required for the cluster metrics feature because some non non non-namespaced things like nodes uh can only be given permission for access with cluster roles. And so things like getting the Kubllet metrics are aren't really possible because you need to first get all of the nodes. Um and so some of the features will not be compatible if you have no cluster roles. Uh but uh but I'd love to when we get further along and we have this feature in there, the actual change for alloy is still in review. Um but I'd love to uh uh I'll have proper documentation on how you can minimize the amount of cluster roles that you need to make. Finally, cloud provider label enrichment. Uh, if we detect that you're running on EC2 nodes, um, can we use that information to add tags and labels from your EC2 instances uh into the metrics that you're generating or the um the logs that you're gathering. Uh, that's some interesting stuff. Uh, and then finally, uh, agents.md. So, I've been working more and more with some of the AI tooling, and I'd love to make a really useful and effective agents.mmarkdown file uh that can live in the repository. So, if you have questions about how the repository or how the Helm chart is built, um or hopefully even just questions about how to enable features, um there will be some affordances into the repository itself to make it easy for those AI agents to actually get the accurate information for you. Um, so I've been working with that and uh I'll be excited to put that in there, but I don't want to put it in there until I know that it's going to be um useful for actually answering real questions. All right, I know that was a ton of information. Thank you for bearing with me on all that. Now we've got time for questions and answers. and I don't think there's going to be any. So, thank you so much for watching. I appreciate uh everyone who uses the Helm chart and gives feedback. Um it means a lot. Like I say, every month um this is a heavily communitydriven project. Uh and so your feedback means a lot to me. Uh so please uh if you have questions, if you have comments, uh you can join us on the Graphfana public slack, graphfana.sflack.com. Uh I hang out in the hashtag Kubernetes channel there. Uh if you have issues or feature requests or anything, uh you can find us on the GitHub repository graphana monitoringhelm. Uh and again, thank you so much for watching and I hope you have a great month. We'll see you in October.

