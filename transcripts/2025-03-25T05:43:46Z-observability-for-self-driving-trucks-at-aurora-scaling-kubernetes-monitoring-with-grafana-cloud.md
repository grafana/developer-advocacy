# Observability for Self-Driving Trucks at Aurora | Scaling Kubernetes Monitoring with Grafana Cloud

Aurora's observability journey has evolved over the past five years, shaped by the challenges of self-driving technology and the ...

Published on 2025-03-25T05:43:46Z

URL: https://www.youtube.com/watch?v=l7s3u4bBA88

Transcript: so today I'm going to talk a little bit about our uh the evolution of our observability stack for the five years I've been in Aurora so first uh who or what is Aurora Aurora's mission is to deliver the benefits of self-driving technology safely quickly and broadly so what does that mean we do self-driving Vehicles as you can see with the picture we are focused on trucking at the moment we are based in Pittsburgh but we have offices in several States and our primary Trucking routes right now are in Texas this will become important on the next slide one the other important uh piece of information related to our observability stack in the observability evolution was an acquisition in early 2021 we acquired a division of uber called atg uh and this will play into how our stack evolves so this Tech stack at Aurora is relatively complicated uh one of the big pieces of complication is we essentially have a mobile data center so there's computer moving down the road at 70 mph which has a bunch of issues one of which as I mentioned our main routes are in Texas is anybody has ever been in West Texas trying to get a cell signal in West Texas is not easy uh also you have other issues like vibration dirt dust Grime things like that we also have a bunch of sensors uh cameras lar all that stuff around the vehicle you can think of it as a very complicated robot and obviously we have cloud services to kind kind of tie it all together what this all amounts to is a relatively large ecosystem means that we have a very broad set of Engineers we have a set of Engineers with a very broad set of skills our Cloud infro we are primarily in AWS shop uh we have several uh we are mostly a kubernetes shop we have some things that are not kubernetes as you naturally do and our kubet clusters are broken up into more or less three main types of clusters obviously we have clusters that are dedicated to services that were are focused on customer visible things how do I book a truck where is my current um uh truck uh moving Etc we have internal and R&D services so how do we uh train all the AI and the ml Etc uh these are all in R&D clusters and then lastly we have uh clusters that do asynchronous jobs uh batch jobs this is things like simulations uh crunching all the data from the vehicles stuff like that the important part about these batch clusters is they can scale up and down by two orders of magnitude roughly uh in a given day and so they scale up and down which means things like cardinality become a big big problem big challenge for us we're a big terraform shop I believe very heavily in inferis code um and this plays in a little bit into our decisions later we have many layers of abstractions uh you have people who write code uh they write a they that calls B that calls C that calls D they have no visibility into how D works but D might be emitting metrics it might be logs they have no control over that and again they have no idea that D is uh they have no idea that what D is actually doing we have a monor repo this doesn't play too much into it it did make some things easier seeing all the code was in one place I've been in Roar for roughly five years uh when I joined we were all open source so we were Prometheus and thano stack with grafana for visualization at some point we had a logging vendor uh that didn't go well for a variety of reasons uh partially because we didn't spend the effort to actually educate people on how to use it we hope that uh uh to quote a movie if you build it they will come they didn't come even though we built it obviously open source projects take more headcount to manage uh so you're moving that money from uh paying a vendor to headcount uh this becomes important in a minute some pieces of it were cumbersome and clunky uh as an example alert manager alert manager is solid it works however uh I mentioned that we have a broad set of we have Engineers with a broad set of skills and so taking somebody who knows lar for example and have them understand how alert manager Works was simply asking too much so I mentioned that we had this acquisition in early 2021 uh with a division from Uber this acquisition was very interesting because we Aurora acquired a a division of a company that was twice as large as Aurora was so not only did it bring over Engineers with their own set of skills but they had their own existing solutions to some of these problems so we had to integrate all these things together one of the big decisions we had to make at that point was whether we wanted to continue with open source or move things to a vendor since we had just acquired so many people getting more headcount as I mentioned open source usually requires more headcount to manage getting more headcount at that point was simply not going to happen therefore we move to a vendor specifically we chose chronosphere uh as our metrics vendor this is again 2021 if you know anything about chronosphere this is very early days of chronosphere we were one of their largest early customers uh so we really saw chronosphere grow and evolve over the next couple of years as I mentioned uh atg had a bunch of existing Solutions one of which was honeycomb they used honeycomb to manage their logs and traces this was uh good from the developers perspective one of the uh key things that we wanted to um optimize for was a simple stack and for honeycomb uh you'll see a slide later you'll see a picture later of the query interface for honeycomb if you're not familiar Honeycombs interface is very very simple which is great for onboarding uh and a very very shallow learning curve but it makes it harder to do more complex queries and then we have a bunch of other data sources we have a large data lake with a bunch of data in S3 red shift various RDS instances etc etc we have some services that uh put their logs directly into S3 and then we obviously have Cloud watch things like Lambda for example which is natively uh send their logs to uh to cloudwatch one of the problems in having multiple vendors is kind of swapping between the different uis of these vendors the swivel shair problem the browser tabs Hell whatever you want to call it as an example of a problem if you have vendor a up on you know one tab or one screen vendor B up on another one defaults to UTC and the other one of defaults to say local time in our case specific time and you're swapping back and forth while time zone arithmetic is not super hard doing it at 2:00 in the morning can be a little bit error prone there are small Nuance differences in how dashboards uh represent the data and swapping between these vendors um especially if you're used to one vendor versus another that can be uh especially difficult again at 2 o'clock 3 o'clock in the morning so our criteria for kind of uh fixing these challenges was one we one it's simple I mentioned uh a large set of developers with a broad set of skills an expert in lar isn't necessarily going to understand anything about how the cloud works but they still need to look at data they need to be able to troubleshoot services that they are kind of tangentially uh aware of uh need single paint of glass or first paint of glass whatever you want to call it was also a major goal a smaller goal but one that did kind of play in was Finance wants uh as few vendors as possible obviously having you know fewer companies to write checks to fewer uh contracts to negotiate um things like that Finance really wanted a single vendor this was not a huge uh piece but it was an important piece Prometheus compatible we had spent since we were a large kubernetes shop we had spent a lot of years uh with promql we had a lot of dashboards moving away from Prometheus wasn't exactly a non-starter but it was very very close I mentioned these big asynchronous jobs these big batch clusters that run all these jobs like simulations and processing data it was it's common for a job to spin up it for for it to spawn a thousand other jobs each one of those to spawn another thousand jobs Etc so a small performance change in one of those can have an exponential uh impact on the entire infrastructure thus having some visibility into where you can shave off seconds of CPU or gigabytes of memory is extremely important therefore profiling was a longer term this not a short-term goal this is a very very important long-term goal lastly I mentioned these different clusters we have we want to prioritize customer visible clusters obviously customer visible services are going to be more directly tied to revenue so those are more important but I will mention that we did prioritize these but these are far from our biggest clusters uh they were probably I wouldn't say two orders of magnitude but at least um an order of magnitude smaller um than any of our other clusters so they're relatively small but they're extremely important so why do we choose grafana obviously they have the three three main pillars that you hear about constantly in the observability space metrics logs and traces all in one vendor which made Finance happy not a huge part of our evaluation but it was factored in prom and Prometheus uh Prometheus and promql supports um I mentioned honeycomb uh we did talk to Honeycomb about expanding our use of honeycomb um the fact that they don't support uh Prometheus and promql was uh that was a major roadblock and one of the reasons why we did not move forward with honeycom they have a well supported uh Helm chart this Helm chart is great uh but it is a little bit difficult to use when you're first getting used to it uh if you're not familiar the main chart that they recommend this K us monitoring chart which essentially a wrapper chart for a bunch of other charts so to understand how you actually make changes to say you know the alloy agent you have to understand how the rapper chart then calls the subchart it can be a little bit overwhelming but over time we have found that their Helm chart has really good support uh and is extremely powerful obviously pyroscope was a big piece of the evaluation and profiling um is an important piece again of longer term not shorter term strategy I will mention really quick uh later uh about uh Baya um to kind of fast forward for Shad it a little bit uh Baya didn't work for us uh but we have a huge interest again the Simplicity part of it to have a low code or no code option the first thing we did was migrate our metrics this was not a deliberate Choice it just happened to be that the contract for chronosphere expired before the contract for honeycomb it was just by chance as it turned out that was a a pretty lucky chance because we were migrating from a prom Prometheus vendor to another Prometheus vendor this migration was relatively straightforward that said there are still a bunch of problems that we ran into alerting uh both grafana and chronosphere are roughly based on alert manager but there are tons of differences relatively small but important differences in exactly how alerts are defined and how they're visualized and so we had to tackle that recording rules if you're not familiar recording rules essentially make some kind of computation against uh metric data when you have large clusters especially when they scale up and down in a given day it can be very challenging to have the recording rules finish in the allotted amount of time what chronosphere did is they had a custom solution to try and address this when we migrated away from chronosphere we had to deal with their customization which is obviously very Chronos Ser spe specific uh and migrate those back to recording rules the standards even though these companies are both Prometheus compatible there are small differences in these standards uh specifically how they handle service monitors or pod monitors there are relatively minor things but at uh Aurora's roughly 2,000 employees at that many employees you're going to have people that do things in a interesting way um and so you need to figure out how to migrate them to the the new supported way of uh for example service monitors billing uh every vendor does billing a little bit different um no one in my opinion no one is good or bad the differences are the pain so where you're migrating from one vendor to another dealing with those differences in Billing um can be a bit of a challenge in this specific case chronosphere uh build mostly on the the number of metrics whereas graphon builds primarily on cardinality it's not cardinality was not part of chronosphere it was just a relatively small part so we had to deal with that um one of the big Advantage one of the nice things about when we enabled a tenant in grafana is we got a bunch of dashboards by default when I mentioned chronosphere we were relatively early customer they grew a lot um but when we first started there were a lot of things that they didn't have when we enabled graan we had a bunch of dashboards just by default and that was a very pleasant surprise one of things in retrospect I would have done differently is spend a lot more time on the cost dashboards if you don't know there are a bunch of cost dashboards that just come by default there's one main one with a bunch of sub ones where you can drill into more issues dealing with the the cost issues uh has been uh a source of I wouldn't say pain but uh a source of uh time that I needed to spend and I would have spent a lot more time drilling into those early on next was a logging tracing migration I will mention that this is not done while we have done the bulk of the work there is some longtail stuff that we're still still dealing with one of the big challenges we had here is there are fewer standards which meant things were a little bit more difficult when I mentioned this I will say that this is for logging primarily not for traces traces were all open Telemetry Hotel so that wasn't as big of a problem uh logging was a much bigger problem with this simple interface here you can see a simple query interface for honeycomb where you have a relative simple kind of function where they want to account some uh an average that kind of thing um for some subset of data and then potentially grouping those the three three main things you can normally search for as opposed to a log ql query which is a lot more daunting for somebody new to the system so this learning curve was a uh was a bit of a um was something that we had to deal with but it does mean that there are more advanced queries that are much more possible with log ql that simply were not possible with honeycomb we had a lot of different client implementations because of the lack of Standards we had people the majority of things come from kubernetes and we have a custom agent that will look at the container logs standard out standard error and kind of send them Upstream we had to adjust that agent to send to grafana instead of sending to honey well a double right to grafana and honeycomb but then there were a variety of other things so for example our build system is via SAS vender and so we want to take those logs and ingest them into grafana there were tons of different implementations of how people got log data into the Upstream system and so we had to deal with all of that lastly was a front-end observability essentially what the front-end team did is they embedded a honeycomb client directly within the JavaScript so they could send directly to honey home from the various web browsers so that didn't come through any of our agents so this was all taken care of with the frontend observability application within grafana and that one is still under uh still in progress post migration what have we liked um adaptive metrics and logs is great while we can tune a lot of things in alloy such that we can drop labels or aggregate or whatever there are times where that's simply not possible and you want to do some kind of dropping or changing of data at ingestion and adaptive metrics and logs uh has helped uh with that I will say that adaptive metrics we do use quite a bit adaptive logs we do not uh we ran into a bug that kind of blocked us temporarily and we just haven't gotten back to it um but uh we are very excited about both uh adaptive metrics and logs you can see a screenshot of adaptive logs at the bottom next um oh graan wanted me to mention as you saw earlier today uh adaptive profiles is on the road map and coming soon um and more about profiles in just a second good API and terraform support so I mentioned that we're a big terraform shop their terraform provider is solid uh we had some challenges with it but it has worked very well for us and the API has provided options for when terraform doesn't work for us there are cases where it's not great uh and so we are able to write custom scripts to do whatever we want through to the API log ql and Trace ql what I keep mentioning uh honeycomb simple query interface was great for getting people onboarded and so the migration to log ql and Trace ql has been a challenge again broads Set uh sorry a large set of engineer with a broad set of skills a lar engineer is going not going to want to spend days trying to understand log ql to that end graan has helped us with training uh on both log ql and Trace ql that is ongoing we've had a couple of sessions and we have a couple more planned events uh one of the things we really want to do is augment the data you see in grafana with other things for example somebody deployed a new version somebody enabled or disabled a feature flag that kind of thing we initially looked at annotations from grafana but that didn't work for us fortunately Lo Loki is a relatively open platform so what we're able to do is send a log something that looks like a log and it's essentially an annotation um saying that this service was deployed or this feature flag was enabled that kind of thing and again this was all enabled uh all made possible because of the openness of Loki baa we did a little bit of work with baa the tldr is it didn't work for us um we are still very excited about the future of Baya as of right now Baya doesn't work for us um but we are very very interested in Eep ebpf in general and and any kind of no code low code solution solution to deal with the broad set of skills that our Engineers have have what's next esos as a first class citizen if you're familiar with honeycomb honeycomb has esos as a first class citizen chronosphere does not with chronosphere we use a combination of sloth uh sorry we use sloth which is a combination of recording rules and alerts essentially this worked uh Big Air quotes worked um but it was clunky uh it was not ideal and so we're very excited about doing more with slos in the future we have some slos right now it's just not as widespread as we want and we haven't uh applied as much automation Al terraform to these slos as we'd like profiling when I first wrote this slide um profiling was on the road map and something we're very excited for as of yesterday it is actually enabled so we have profiling enabled in just a couple of test clusters uh it's really really small at the moment it is very very interesting data I was looking at it yesterday I really really like pyroscope the big challenge we have is managing costs especially these batch clusters can scale up and down and especially for data that people don't care about it's one thing to have something expensive that has a lot of value it's a different thing when you are spending a lot of money for something that people don't look at and that's these big asynchronous clusters that's our big concern so the the only thing slowing down profiling and our deployment of pyroscope is our concerns with cost it is not a technology problem early on I mentioned that when I first started at Aurora uh we were all open source and one of the challenges with open source I love open source open source is great but there are features that if you want to develop internally can be be can be a bit of a challenge we are an AI company and developing AI features for observability was simp not going to happen that is a whole different part of the engineering uh organization and we don't have those skills on our team so one of the other areas we're very excited about is the AI and ml features that we get from grafana uh namely sift investigations and Metric forecast um anything we can do to let the computers uh do the work for us we are definitely very interested in and lastly more data sources I mentioned that we have a bunch of other data sources uh raw logs and S3 Cloud watch uh our large data Lake all of that stuff we want to be better integrated into a single painted glass first painted glass whatever you want to call it and graffic calls this they big tent uh idea also with the data sources because they have an open SDK um we can extend that if we find some extra time somewhere uh we can extend that and write our own custom data source if if we really want to

