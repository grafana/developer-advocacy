# Grafana Alloy Community Call October 2024

In this discussion we talk about Grafana Alloy performance, OpenTelemetry and conversation around logs. ☁️ Grafana Cloud is ...

Published on 2024-10-17T13:47:19Z

URL: https://www.youtube.com/watch?v=SLHn0FPHw7w

Transcript: we're recording hello everybody and welcome to the October alloy Community call uh we switched I think since the last recording we were on more of a presentation format uh we have since switched to a more conversational office hours format so there's not going to be a big presentation or anything uh it's just going to be us chatting with people so I will open up the floor to any questions or comments or anything anyone wants to talk about you can raise your hand you can put it in chat you can interrupt me they're all valid and all good I think you might be muted I can see your Herve no I see a little muted icon in the corner of your screen oh yeah but I was not talking anyway oh okay sorry sorry I thought you look at R I thought were but I I I can okay if nobody wants to if you got topic bring it up yeah um do you have numbers like comparisons between uh um alloy and promise use agent uh for resource usage basically it seems that on my setup for like same kind of configuration um alloy uses like twice as much memory as promis use agent and I'm a bit surprised so let's talk about your configuration a little bit because I have maybe an idea here um are you using kubernetes Discovery yes yeah so especially if you're running a very large cluster so to give you some background on this is alloy to display in the UI kind of the arguments it caches those arguments in memory so if you're like discovering a thousand pods and each one of those have 50 60 labels which is not uncommon um that's probably high but a lot of labels a lot of labels um those are all stored in memory uh to be dis and the at the moment it's not a big cluster yet oh then I yeah so well how much memory are we talking about do we have a like are we talking gig with uh yeah with prise use agent is using 14 gigs and 20 I have 20 with with law oh then yeah that sounds sounds higher than I would expect for that [Music] um uh can you send me a heat profile yeah yeah I I I just I actually I did not want to fire straight right now with with with this kind of questions because I did not investigate yet it just numbers that that that landed yesterday for some for some our first test on one cluster and I wanted to to know a bit but if you had just numbers usually if that expected or not I that number is high yeah um I I would not expect that value um in yeah like I it's higher than I would expect from discovery kubernetes which is generally one of the kind of pain points um what else uh sometimes it can be the label store um doesn't handle really high cardinality sometimes so it could be that um yeah it's kind of hard to tell like if I have a heap I can identify it in you know at least identify the hot spots pretty instantly um but yeah I would say that's abnormal that amount of difference if you told me it was like you know Prometheus agent was 14 gigs and aloe was 1617 that yeah I would say sure that's that's probably expected this doesn't sound like it is um we do have oh go ahead yeah by the way the page showing how to profile Gano for resource consumption is great um I I'll I'll read it carefully and give it a try but I have yeah I have basic I I did not even start looking at it it's a colleague of mine just compiled the numbers quite high level from the Clusters Global clusters resource usage and so I really have to investigate before before going further into discussion but is a few other kind of leading questions that maybe you can research a little bit are the um like resource limits set the same both of them something to look into I don't think we set resource limit on it for now okay um that could be so so if you set a a memory limit it will automatically set the go MIM limit to that in alloy I don't think prus agent does it but an alloy it will which may Tamp that down some now it kind of depends on where that memory coming from I assume you're looking at memory that the like um C advisor or the container itself is reporting uh yeah yeah yeah okay yeah so there could be a pretty big gap there of go not returning m to the operating system okay so so basically it means that uh for promise use agent we have um G um what is it I don't it's not a m limit it's basically that it it does the the garbage collecting when it uses basically twice the twice the amount of memory uh so so it will do some garbage collecting more often that if you have a GM limit set to a high value and if if prise if alloy sets a g limit to the node size it's normal that it uses lots of memory then wait yeah so D normal like it P um it shouldn't use more than double so you have the the actual memory that like go is using actively generally it won't use more than double than that or it shouldn't unless some settings are out of whack um and then yeah uh probably coming to the limit of what we can talk about without a a heat yeah yeah yeah but um yeah in general um alloy will will not uh if if you don't set a go M limit it'll assume whatever the node is which is often not a great thing because you know you're sharing uh if you know if you have 128 gig node you probably don't have 128 gig memory when you're running a lot of PODS of container so setting a go M limit can definitely help that so yeah set setting a go M limit would help but okay I'm I'm trying to think about the side effects of of so if I set some memory limits on my on my pods it will set a go limit and it will try to to fit uh in below this limit but it also means if I have a vpa for instance it will always Reach the the limit and vpa will keep increasing it so I don't remember does does a VP does a vpa restart it or does it just change the limit um no it it restarts the P okay so then yeah it would it would keep bouncing it up then um so yeah that would be I I do think there's a problem here or there is something um less than ideal um based on the numbers you gave me of 14 to 27 so I I think the setting the memory limit will help but I think there's probably something underline here okay yeah I have to play with it but that's that's a that's an interesting point yeah um we added that the automatic setting of g m limit a few months ago so if you're running a very old version it it may not have it um but if you're running anything in the last two months you should have it in there no no we up okay uh and yeah feel free to if you get that Heap uh you can DM it to me in the uh private CH in the alloy Community slack just you know ping me send me a direct message and I'll take a look at it and uh give you some feedback okay nice thanks other questions well we can always prime the pump a little bit here um have you did did uh did you look at the do you all use otel any kind of your environment her or Solon nope not yet okay get can you say that again please uh we have some hotel but not native hotel from collection point of view I get okay let me rephrase I get the metrics bya OT collector in alloy and send it to promus and elastic search oh do you use the converter the exporter inside of alloy to convert that from open Telemetry to Prometheus style y oh receiving it via a collector does the transform and for the kubernetes uh attributes to add some attributes about the cluster and then sends it bya EXP to MIM okay cool and elastic search um maybe a leading question but uh was there anything that made you choose alloy versus otel collector for doing that uh it was a how to say conscious decision not to add another collector into the picture we are already having alloy for everything else and application started to generate Hotel metrics they ask for Native Hotel collector and I P back to get it into alloy so one collector one thing to manage or upgrade or maintain from our point of view a PO um yeah that makes total sense um I just have uh one question if I understand correctly you are ingesting open Telemetry data and you are converting it to prus Matrix um but actually M has a native open dmetry endpoint so I don't think it's really necessary to convert to prus format if you have open t metrics I think you could just straight it sorry you could send it straight to Meir um and it should be fine that's true uh but when we start to play with it yeah we have a um for nonproduction environment we have grafana open source L GTM running in our premises and it was not with that ability at that point so I stuck to uh pret way while asking that is it something if I push it directly to Hotel end point in mimir will I be able to carry via normal r or not you you will be able to query the thing I don't know if we can promise is that the labels will be the exact same um it possible that the the labels are diff there might be some slight differences in the label naming I'm not sure if our internal converter behaves the same as the OTP Gateway converter essentially um so I think that might be the only danger in switching over to it you it might break some dashboards but you can definitely query it with promql good because the if I don't use use the kubernetes attributes processor I get the name space like K8 sore name space uh pod name is K8 sore pod Etc if I use the transformation it gets the correct name but I don't know as you say I I can try do we prefer to directly send it to O endpoint or or the other way around any preference on that from other perspectives out of usage um yeah well now the open Elementary end point in nir table in the latest versions of nimir so we we usually recommend people to just send open Telemetry metric straight into m without converting them tends to be a little simpler I mean if you already have your pipeline working and it's working totally fine for you then definitely feel free to do this there's nothing wrong with it I'm just saying that if you're starting up now uh with the fresh pipeline it might be easier just to send the metric straight through or if you find any issues in your existing pipeline then instead of the buing the issues maybe you could just try sending the metrix straight to I see I see yeah I can go try of course yeah the main benefit of uh sending metric straight through to the open Telemetry end point is that over time the ability of mimir to ingest native oel data will improve for example if you ingest resource attributes in a better way uh and then you you'll be able to take advantage of those kinds of things if I understand correctly at the moment resource attributes from Mel metric they are not uh made into mere labels into metric labels because that would just increase the cardinality of the metrics and it would increase the costs so instead there is a target info metric which contains those kinds of things yeah that's true yeah pretty much agree with don't change it unless you got a reason uh but Greenfield yeah that is the general rule if it is working don't touch yeah hey Carl uh just FY we're recording right now so just give you a heads up as you joined in and we're at a kind of open Q&A or talk about stuff so if you have any topics you want to talk about uh feel free to Bubble them up or any issues all right thanks Matt um can't think of anything at the moment awesome I have one question about uh using uh sorry ingesting blocks whilea aloy I remember we discussed that in the slack but I couldn't find it back probably it's gone with the limit in Slack side the there was the uh demon set now we are using the demon set to collect logs where uh from the node vlock Y and there was the uh one uh running as a state food set or deployment collecting it via the API call at that point when we first start with gra on agent old version it was noted as experimental Etc now I don't see it so which one is the preferred which should be the preferred way in normal terms and conditions so in I so if when I get access I say start with the file Source kubernetes approach um it requires less permissions if doesn't create a Damon set on every pod or every node um and it um you you kind of get a lot of labels for free um which is generally good but it is less performant and it does put more pressure on the control plane [Music] so I would say generally start there and if you see especially your control plane starting to fall down or you don't have control over your control plane um and I think generally like the cloud providers depending on the number of nodes you have in a kubernetes cluster will bump up your control plane so it's possible you don't have control over that and you're not at a size that they bump it up um you may run into problems but I I would suggest starting with the kubernetes source and um if you can make it work it's better um it's more scalable it's less permission [Music] um and you get the labels for free now we are working on some features um Paul and and particular on being able to join more arbitrarily to pull in those kubernetes labels that if you're using a Damon set um which is pretty valuable because people generally want the kubernetes labels and the Damon set one really just gives you name space and I think the the container uid or the uid um from the file path Yeah by setting pressure on the control plane do we mean based on the number of PS uh is collecting data or size of the number of nodes and other size constraints on the cluster level I personally believe it is a number of the number of no um uh the number of connections I think is some sometimes the pain Point um although I have seen it just the volume of logs coming out of it feeding it fast enough because you are you are kind of accessing you know and it depends on the chattiness of the logs um you know if if you're just spewing out very unstructured huge uh gobs of logs um that's going to be different than you know like a more uh cut down application that's maybe not spitting out volumes of Vlogs yeah okay then I will stick to demon set our one of the applications is crazy generating looks yeah I I talked to um a lady at um obson who like her log line a single log line could be Megs of Link yeah and I was like No just stick to Damon sex I'm I I that feels painful to pull from the control plane yeah I have some uh from time to time I got uh long log record size uh issues from graan uh dashboards building dashboard I see that we are hitting the limit from time to time so yeah better to stuck with the old way yeah I want to get yeah yeah I really hope uh and there's also a few caveats with the kubernetes source logs um we found a bug or an issue with um um pulling logs from them at a certain kubernetes version that they have since fixed um and we have there's kind of a A workaround in the code to work around that um but it's you know it's a hack uh to make it work on the older version so if you're St on like an old version of kubernetes I don't know off the top of my head the version of U which one it was um that's possible if you're on this like a year ago um it's possible that you could run into that but if you're already running pulling from Damon sets yeah you know go that route and um if you have a brand new Green Field environment on a different cluster start with the The kuber Source pods I see okay thank you does anyone else have any other comments questions or anything they want to bring up all right then I will say thank you for everybody for showing up uh had a good conversation I will put this up on YouTube probably in a day or two and uh you'll be able to see it if you want to um feel free to jump on the community channel uh if anything pops up and uh we'll see yall next time then thank you byebye bye thank you bye everybody bye bye

