# Grafana Tempo Community Call 2025-01-09

Join our next Tempo Community Call: ...

Published on 2025-01-09T21:48:00Z

URL: https://www.youtube.com/watch?v=bG8iRmnuK6E

Transcript: cool welcome to the tempo Community call First of 20125 um I believe yeah the very first of the year January and our agenda's quite short I think um mainly because like a lot of folks we've been on break for the past few weeks I am myself have only been in two days I think in the past three weeks so I spent this morning um mainly catching got up on emails and all the normal stuff so I barely know what's happening um but we will today review the 2.7 rc0 release notes which should be exactly the same or very similar to the T Tempo 2.7 actual release notes um give people an idea of what's coming up I believe we intend to release this next week assuming we don't find anything too terrible um yeah and then after that we will get into like an ma style thing uh if you all are interested in um any questions about Tempo operating it or what our plans are or how to do something in gql or whatever comes to mind I suppose is all good um and apparently the first item of the agenda is to wish everyone they had a good break I do not know who wrote that but I will now wish you all a good break Marty did okay any good idea um 10. 2.7 so we were maybe a bit late on this um we do try to do it four a year so one a quarter every three months I think we're a couple weeks late on this but not terrible I think it just kind of turned into did we want to shove it in before the break uh at the last second or just wait till we got back and we all voted on just wait till we get back um so here we are we're back and it's time for the tempo 2.7 release uh so I'm going to go through this I was just dealing with an escalation right before it so didn't not have time to dig into this before so it might be a little rocky getting through here um if any team members have thoughts about things they want to highlight please jump in otherwise I am going to uh just kind of mention the things that I'm aware of that make the most sense to me and I think this is an important deprecation here and it's the first item on the release notes for a reason which is uh we are going to deprecate the serverless features um we have found internally they are more trouble than they are worth uh and we do not we would prefer spending all that time that we are keeping them up to date uh wrangling all the CI processes related to them we'd rather spend that time just making Tempo better and we think we can achieve a better outcome investing in queriers and just Trace Q performance uh in other ways so uh this was initially added back on the old block format with the old way of searching and uh we felt it was necessary then to achieve any kind of reasonable speeds but with the newer format with the parket file or par yeah block format U things are so much faster that we believe this is just unnecessary burden on the team so I believe in the next release we'll just have all this code deleted um and we're moving on from serverless Good Luck serverless your way better in batch processing and event processing breaking changes have quite a few listed here I don't think any or most of these are super concerning um but I I'll walk through these real fast um we added a maximum spans per span set a lot of these will not impact you Carlos did that our newest team member um and uh right now you can ask Tempo if you do a trace QQ query for 200,000 spans per spans set and if it finds a trace with that many spans that match it will do its best to return all of them which makes Tempo sad so this is mainly protection kind of setting uh that we use in Cloud to prevent people from doing exactly what I just said and overwhelming the query path by returning you know whatever me hundreds of megabytes or gigabytes of data in one query um I don't think this is really going to impact anybody it's related to remote right on our metrics generator I believe we are now just honoring honoring the config instead of um honoring the config instead of uh whatever's in the actual context hey Oscar uh awesome that you're using Gonic Cloud Tempo uh this we'll go over OSS Tempo stuff here uh everything you see here is probably already in cloud or close to it um and then at the end of this you are more than welcome to ask Cloud questions that's absolutely encouraged here so questions about Trace Q or how to use Tempo is all good um as well as what we you know as well as covering OSS Tempo here uh okay moving forward we thank you to Andreas who's on this call finally got away from open tracing which the old Jagger SDK which was at for years uh so now we're fully on the newer open Telemetry Tracer um read these uh issues or these PRS if you are concerned um you can still use the old style exporter um because open tter Sports Jagger you just need to do a little configuration uh so that's what you would do if you're actually tracing Tempo I think this is maybe one of the most important ones though uh yaa upgraded our open Telemetry collector uh dependency which changes the behavior to only listen on Local Host instead of the all interfaces 00000000 um so if you depended you might and it's quite likely you depend on this Behavior depending on how your configuration is done um and you will need to configure this uh perhaps directly if you want it to listen on all interfaces or maybe a specific interface if that's important to you uh so this is a pretty standard security change where uh you want to by default not listen to all interfaces um you want to be a to like specifically say which ones or configure which interfaces to listen on this affects the distributor receiver configurations please see this couple links here and does this affect the helm chart I think we determined this does not affect the helm chart is that right because the helm chart was already just putting zeros in um I think so yeah okay so if you're deploying with the helm chart I don't know if anything's going to change if you have your own deployment configuration I would look closely at this this will be called out in the blog post as well as um uh the release notes uh that Kim and Alex work on as well so we should have more details on this but I would keep an eyeball on this if you operate Tempo yeah I think the issue is mostly if you keep the endpoint like empty use a default value the value changed in autal Upstream if you specify like a specific host name it's not gonna change anything for you okay cool um we removed a config option that did nothing I can't imagine anybody's using it and then I think this one also will impact a lot of users this is already in Cloud we switch to use the Prometheus fa fast regex which has a lot of optimizations uh where it attempts to do matches before it even drops to the go regex um and so all of your regexes should be faster but the Behavior will change and the behavior will change in a way that's consistent with Loki mamir with Prometheus so we think overall it's a positive change we think it plays nicer with graphon dashboards as well uh because all of grafana dashboarding templating kind of behaves in a way that is consistent with Prometheus for obvious reasons um and so I I think this changes overall good but it will change Behavior so this query right here span Fu bar uh used to be a substring search and now it'll be an exact match search because all reg X's are treated as if they fully anchored so you can kind of see this example here about what how what the behavior change was from this to this um your rxes should be significantly faster they should behave more predictably based on like just using the other similar databases uh and hopefully you won't have to make major changes to the way you all use or operate Tempo I think this is overall a positive change uh even if there's a little bit of rocking us uh adopting it all right any other call outs there team oh uh I was wondering is anyone using Cloud I think this red exchange would have already rolled out to cloud and I was wondering if anybody noticed it cool a couple of our internal people noticed it because they asked why their dashboards changed uh we did announce it in the cloud uh what's new thing I wonder if we need a what's going to be new thing I don't know some way to say hey in two weeks something's going to change instead of that week that's maybe a little jarring for some folks but whatever we can work on that uh separately um changes uh I've already gone over the breaking changes here I don't know if there's anything in particular worth calling out um I think the changes are all pretty innocuous uh hopefully for the better um so features we do have a few Trace kill features uh jav did some good work there um fam also did some good work here so the instrumentation scope which is a specific scope in open Telemetry is Now supported um so you can it's a very strange scope it basically it's probably somewhat unused by you directly but it injects like the the uh instrumentation library and the version of that Library I think maybe the runtime version I'm not sure a couple of small details in there that might be useful to you uh so you can now query that directly in Trace ql which is cool um and then we added a couple of Trace ql metric um uh features so average overtime um yeah and then maybe some features related to uh operating Tempo uh anything worth calling out here on the features I think this is an interesting one although maybe difficult to parse uh if you operate Tempo you can use this to uh this is a very long PR description but to track uh the bytes per second is that right Marty bytes per second based on any configurable uh uh label right so so if you want to if you would like to understand your tracing volume better this would let you split it Down based on any attributes in the data um so what it means is you it's separate from the metrics generator span metrics which you probably would have already been using to do this before so there's the span size metric um but this allows you to cut it by different attributes so you don't have to like interrupt your spam metrics config or try to find one config that works for everything so it's kind of a specialized feature maybe if you really have like sub tenants or internal tenants and you really you know you want to understand what's what what your volume is coming from yep uh so we use this internally for what Marty's saying there like a cost attribution style thing right um where we can assign based on labels let's say you want to know which teams are sending how many so you have a specific label in your traces and then we break down byes per second per that label uh so this might be useful to operators did Papa have the question I think this is cool so can I like figure out which namespace is sending how much data for which you deployment if I have those labels yeah you could use resource set name space or whatever and so you could use that as a label and um you have to it's a separate scrape in point so you'd have to set up an an additional scrape on the distributor's after you enable it because um just to keep the operational metrics away from like the customer type style metrics like tenant style but yeah for sure so it would be more accurate than the other metric as well so the span size metric is not as accurate because it's just counting the spans themselves and it's missing all of the non-s span data like resources instrumentation scope so this one would would be more accurate also so definitely a feature operators might be interested in so they understand who's spending money basically in their Tempo cluster and then you can go ask them questions cool uh other than that I don't know we have a large number of performance improvements in fact I would call this release mostly a bug fix in performance Improvement release so there are a couple features sprinkled in there uh but there are a there's just an enormous number of improvements you can see um uh Marty's got a couple of trq queries with very solid descriptions here maybe we should improve those I'll go look at that um oh we should move these up to Features I thought we had more than one Trace metrics Min overtime Max overtime new Trace ql metric operators um which are cool so I should I'll put those up in features uh and then uh SJ spent a lot of time working on our tag endpoints our metadata endpoints so lots of work work uh improving the performance of those added caching on dis in the adjusters added all kinds of neat little performance improvements in the way we um do distinct collection of the strings um so a lot of a lot of nice ones we feel like uh ingestor we saw a 20 to 30% reduction in Heap size on our inors with some of the uh some of the changes here red improving this behavior of allocations um in our Proto ingestion and marshalling uh so yeah primarily we hope that you see better performance uh lower resource usage and then a couple of these nice um Trace ql features is the goal yeah I'll move these up here up to the top in a second here all right any other changes the team wants to call out are we good there oh I'd like to call out the block index change from Jason to Proto so you'll see a different file in the back end if you're familiar with what tempo is Flushing to the bucket and the Proto form you should see better CPU in memory across query front ends queriers compactors that's part of a long struggle to improve our polling resource usage is that right yeah so especially in larger clusters if you have a million several million blocks you should see good savings from that right I think our largest cluster right now is 3 million blocks across all tenants um and the improvements over the last couple releases have made that possible including this one so that's a good call a lot of query front end improvements I feel like a lot of Alex I spent some time I think in the early fall making the query front end more more performant cool all right um it looks like we do have a couple of AMA questions here uh so I can walk through those uh and then if anyone else wants to jump in please do and we will and then if anyone else has questions feel free to drop them in chat or feel free to unmute and ask whatever you're comfortable with it doesn't doesn't matter to us so I'll just walk our way down through these we are ingesting all traces from our service straight to gra Cloud's oel endpoint Tempo gener span metrics from those we' like to create a system where span metc gener from all traces all traces inj Tempo due to volume W un is here to talk about this perhaps only in just tail sample traces such as errors or hydration spany if I something like this up um so I'm am going to mention uh SP for alter notter okay so right I'm gonna mention uh potentially doing this in pipeline uh but I'll hand it off to unit 2 because they're doing a lot of work in this space as well so we do have this easy button option if you just want to generate span or metrics from the traces you send us but we understand that these are often heavily sampled and are only you know somewhat representative of what's actually occurring if you would like to generate metrics from a larger percentage of your traces uh my current set of advice would be to use um either alloy or the open Telemetry collector whatever you're using for your pipeline generate your metrics there push them to graphic cloud and then after that step do your tail sampling or down sampling um and then push whatever you want to cloud ches and then I'll hand off to you um yeah I think what Joel like describe kind of sums it up the issue is if you have down sampling before generating the metrix if it's like probalistic sampling you can reconstruct the sample rates so like if it's like a constant rate of sampling you can like reverse it I guess and like correct the metrics but um if you're doing tail sampling it's just not possible because you kind of skew the the ratios between spans so it's like not possible to correct it anymore and in like we recommend to do it to generate the metrix before you down sample in the The Collector this is a bit of an investment because you have to like run the the generator like the the thing generating metrics on your own environment um but yeah we're like aware that's an issue and like trying to fix it in Cloud but uh yeah it's like pending cool upcoming keep your eye on this space Etc yeah so right we definitely have some work in this area and we know this is a pain point for a lot of folks um second question for May about alerting from here so uh link to this maybe I should share my window some oh am I getting called am I getting paged uh yes I am getting paged somebody go check the pager thanks uh while I share this screen and tell our on call thing that I'm acknowledging if everything is fine there we go um okay uh is okay it is not possible to generate directly to directly generate traces from Tempo I'd say it's on our long list uh but we do have a lot of priorities before this um I would say right now the absolute best path is to generate a metric from spans and alert on that metric I think there's some question about what we would want to do um maybe we need to start an issue where we collect some ideas or perhaps I just add your ideas to here um recently for the first time in ever we have a dedicated PM for Tempo and answering these kind of questions uh is their purview um his name is Alex I don't think he's on this call but if you add any details you have any needs um to this discussion that would be a great place for us to collect you know the things you're interested in doing and then maybe try to prioritize a feature list for this we we want alerting to but it's hard to just add alerting right there's a lot of things that that might mean so if you add your ideas here it can help us uh right determine uh what to what to prioritize and and what to add first I suppose uh and then yes the current recommendation would definitely be to do spans sorry metrics from your spans and alert on right 2.7 released in Tempo Helm chart so uh yeah uh we will release 27 next week at which point generally someone from the community submits a PR before I get a chance to so it generally happens like a couple days after um it generally happens a couple days after uh sorry after we actually do the release so yeah I'd expect it some sometime next week uh or maybe early the week after in the helm sh Trace storage small smaller currently using sample rate and retention would be good uh would it it would be good to see a tutorial on how to get storage ingestion breakdown by span Trace name and sort by largest um so actually the feature we just mentioned would be really good for this where you can specify if a span and Trace name are important to you I'm not sure we support Trace name but span name was important to you uh you could see the bytes per span name uh can I switch to the doc again yeah I can um you could do the bytes per span name uh if you have you could also do it by team perhaps any attributes on your span or your resource like a team name or a uh application or a service or many many different ways a cluster a cell a region literally anything in your attributes so uh the that feature we just mentioned uh we can me we can talk about some of the blog posts since there seems to be some interest in it uh we will I I would recommend looking at that you can also right now get a sense of it perhaps not exact but get a sense of what's going on with oh my God I'm getting page CR constantly what's going on team uh you can get a sense of it with metrics uh so you can do like rate by I hope this works I assume this environment is set up correctly so this is rating all spans by resource service name um and then we can just visually see there's this is like demo data right uh there's a service called User it's creating the most spans and then this mythical server and so this is also an option and it won't be exact right you're not getting um bites which is different than uh you're not getting exact bytes but you're getting some information you could do let's say we want a home in on one service we could do by span name perhaps um so we can see you know for this service which spans are making or which are the most common so there's some options in Trace ql metrics right now that will help give you answers for very exact answers I would recommend looking at the new feature in 2.7 that includes like the bytes per whatever attribute name you want um is it expected to see error logs constantly in temp po pods well I don't know hope not sometimes query on gra just hangs it's flaky that would recover in a few minutes some much longer some days queries runs with no issue our traffic D in gestion speed should be similar quer front end queries about 10% of C allocated rate is great I you yeah so um if you're having issues with specific queries uh maybe do we have a good query optimization guide there's definitely things you can do maybe even kind of surprising things if you don't know the tricks per se that will make your queries run significantly faster uh if you have a lot of good data like um uh when it dies yeah that's that's for sure when it dies all queries die um if you are seeing some queries consistently slower than others and you can collect good data around this um I would recommend filing an issue like hey we run this query and it takes a long time and maybe give us some information about uh the number of blocks you have the amount of resources used by your different components and then we run this query and it's really fast and we feel like they're the same query we don't know why um if you're able to provide something like that in a GitHub issue or a discussion I think I could help you uh but uh you know just sometimes queries are slow is very hard for me to you know diagnose uh sitting here without you know without additional info so if you were if you did post something like that um myself or someone from the team would probably go back and forth ask some questions get some more data and maybe we'd be able to help you out cool um anything else any other thoughts concerns cares worries nothing thanks Oscar I appreciate it you're welcome I am paid to do it but it is fun also I think oh did pav another question I not sure if you answered I was looking at something else and you talked quite a lot yeah yeah so I have this use case uh that I'm working on we want to provide like um an arbu on kubernetes um um so that people will be able to query only kind of spans from Nam spaces where they have access to the way we want to implement it is through proxy like put a proxy in front of Temple that will uh do probably two things um for the get by Trace it's pretty simple we will get a trace and REM moove all attributes from spans originating from not allowed Nam spaces uh my question is like how should we handle the queries for Trace ql um yeah the place you'd run into problems right or a trace SK query that didn't mention a namespace so you wouldn't know right where Nam space it came from and whether or not to uh exclude it what something you could do uh is in the proxy append uh this yeah like a list like a list of Nam spaces the user has access to or something like that if you can select resource Nam space then you would get the name space back on every query um and you could in a trace skill query then omit you know whatever based on whatever your policies are uh the problem is mainly I think going to be Cal metric because you did a trace Q metrics query I don't think there's any good way you'd have to always add a buy name space or something and then un agregate the data you know because you'd be broken down by name space and then it would be a mess basically on Metric I think the right way to do this would be to do it in Tempo uh definitely and we've talked about arbac it's difficult for us to prioritize it but people are more and more folks are asking for it so we want it to um I think our question right now is what is the first thing we should Implement with arbac that meets needs and is you know somewhat doable in the short term this would obviously be a lot faster right if we push it all the way down to the query layer where we could um you know in park a just skip all rows with a set of name spaces right um so I would like if you are interested in that work maybe creating an issue that details what you want and this would be another good one to work with Alex on uh because he's he is right now I think uh noodling arback and what we want to do as a first step for our back so I think that's probably in our future in the next couple quarters uh and it would make more sense for us to work on that with you than you know you to make a proxy I think thanks I can yeah I will book tickets and we can take it from there I think it's the best way to start my guess is you know if we do some work work on our side um you all give you talk about what you need if Andreas or another red hat developer is interested in also helping with that um then uh I think we could get get it done even faster so if we work on it together I bet we can get something we both want perfect thanks we we already started working on this we wanted to have some MVP this quarter essentially um but like if we can create something more generic and have it directly in the back and I think that's better long-term goal cool agree one more hacky way I can think of again nobody please do this uh you know uh but uh Tempo now has cross tenant query uh so you can have each team in like a separate tenant you can do Cross stand query but again only do this if you have like five teams yeah good we want to have it like per name namespace based like align with like kues arbu model yeah yeah then this is not the way what I said forget what I said cool uh all right any other questions team I did kind of blow through those really fast maybe too fast perhaps I should slow down but I also want lunch I'm hungry all right it is great to see you all in the year 2025 um 27's coming out next week get excited for it and we are yeah we're moving forward with primarily our re architecture um so expect to see that next Community call maybe we'll give some more details on where we are on that and how much we've progressed we've done some good work there and I think we're quite excited about that it's going to improve performance reduce cost and uh give you some more more options on operating Tempo so everyone take care and I will see you in whatever the next month is February good okay bye bye folks e e e e e e e e e e e e e e e e e e e e e e e e for

