# Grafana &amp; Friends France - 29/02/2024: Talk Grafana - JB Marzolf

Assurer l'Observabilité des infrastructures informatiques supportant les données et automatiser la corrélations d'événements lors ...

Published on 2024-03-13T10:53:37Z

URL: https://www.youtube.com/watch?v=g2vyYfmh1Qo

Transcript: très bien nous allons parler de d'observabilité et de corrélation d'événements Vincent a parlé pas mal de gestion de la donnée nous on va faire un petit aparté pour l'instant et c'est Cyril à la fin qui fera la liaison vraiment entre les deux sur ce qu'on appelle l'observabilité et est ce qu'on pourrait dire un peu la data observabilité mais je vous pas dire trop pour l'instant donc voilà agenda une petite intro à grafana je pensecoup d'entre vous connaissent grafana c'est les logos orange la sticker compagnie voilà on va parler d' télémétrie on parler plein de choses et à la fin évidemment on à quoi ça qu sont les bénéfices que vous aurez en mettant en uvre cette cette plateforme on démarrer par un petit quiz vous ennuz un petit peu c'est untit peu le soir voilà qui peut me dire ce que c'est que ça le logo de quoi TR bien pas mal c'est pas mal celui ou en fait j'ai extrait parce que surt le logo il a marqué dou que j'enlève les cubes exaement cel ouais bonne bonne culture dans la salle c'est bien c'est bien ensuite vous êtes au taqu très très bien celui-là un petit peu moins célèbre très bien CNCF exactement et enfin l'acronyme est-ce que vous savez ce que ça veut dire observabilité exactement mais vous savez quoi je connaissais le terme observabilité depuis longtemps et c'est que quand je suis rentré chez gfana que j'ai trouvé ce acronyme je sais pas comment on appelle ça même pas un acronyme mais voilà je ne connaissais pas il y a quelques mois celui-là très bien open télémétrie parfait bon ben c'est un c'est un presque un S faut de la salle allez le dernier oh là là bon ben je peux plier ma présentation pas besoin de faire les slides hein voilà vous savez tout bref effectivement ce sont quelques logos dont on va parler quand on parle de grafana laabs grafana la solution grafana Labs l'entreprise derrière grafana et d'ailleurs ce que vous connaissez de gfana plupart d'entre vous en tout cas quand on entend parler de grafana la première fois en tout cas grafana laabs on entend parler de grafana de la partie visualisation qui est bien en fait la fondation de grafana au départ et c'est torkel qui est je crois dans un avion entre je ne sais plus d'où à d'où dans un grand avion un grand vol se dit que il va forquer le projet kibana parce que il veut exploiter des données qui sont dans graphite et il est pas satisfait des deux outils et il se dit j'aimerais bien avoir un outil qui soit beaucoup plus orienté time series voilà time series database il veut il veut faire de la visualisation de données mais dans un truc un peu plus moderne un peu plus récent là on est en 2014 si je me trompe pas voilà il est en 2014 et il fait un for de de kibana et il commence comme ça à faire un projet et ensuite donc le il est cofondateur de la société il s'allie avec rage et et j'ai oublié son prénom Anthony Anthony exactement et et voilà il décide de fonder Graf analabs qui d'pp d'ailleurs s'appelle rain tank au départ vous ver peut-être quelques par-ci par là des r tank qui traîne euh et il décide de CR créer grafana Labs évidemment au départ bah ce que les gens euh ont fait de grafana au départ il a relisé en projet open source donc c'était voilà c'était de l'OP source dès le début et les gens on commençaient à adopter grafana et je trouvais ça marrant moi je découvre un peu le monde de grafana je suis dans l'observabilité depuis très longtemps euh je suis toujours étonné de ce que les gens font avec leur avec grafana et là c'est l'exemple de de c'est un dashboard personnel c'est quelqu'un qui est chez lui a pris un dashboard public et qui a pris des des données de ses de sa consommation électrique et qui produit des données voilà qui produit un Graf temps réel de sa consommation électrique avec grafana il y a deux exemples que j'aime beaucoup parce que ça ça relie pas mal de choses dans ma vie qui est la la JAXA l'agence spatiale japonaise qui monitore l'arrivée malheureusement un petit incident au niveau du de l'atterrissage lunaire de la dernière sonde qui atterri je crois qu' a une semaine ou deux une dizaine de jours sur la lune et bien en fait le monitoring est fait en temps réel avec grafal donc ça a rien à voir avec l' c'est un peu le l'intoduction que je voulais faire c'est que gfana est utilisé dans beaucoup de cas d'usage qui ont rien à voir avec l'IT et je suis toujours étonné de la capacité des gens à faire des dashboards plutôt sympas avec avec gfana je vous j'ai j'ai vous regarderez mon LinkedIn j'ai fait pas mal d'autres entreprises au paravant le capacité dashboarding c'était toujours assez laborieux et c'était jamais très joli bref ce qui est devenu grafana la solution grafana et je dirait plus l'entreprise grafinalab c'est ce qu'on va appeler aujourd'hui le First pain of glass la première euh le premier outil que vous allez ouvrir pour avoir une vision sur ce qui se passe plutôt temps réel mais on fait aussi un peu d'htorique bien sûr ce qui se passe dans votre entreprise et et un petit peu un parallèle avec ce que fait denodo on va pas forcément aller chercher les données et les rapatrier dans grafana mais on va aller se connecter à différentes sources de données donc vous avez quelques logo ici que pris les les meilleurs d'entre vous reconnaîtront dans dans la seconde pour aller chercher la donnée là où elle est la donnée évidemment on parlera d'observabilité de monitoring mais le monitoring ça peut être effectivement d'infrastructure technique d'infrastructure informatique de n'importe quoi de chez vous la quantité de de ya dans le frigo pour restituer un tableau de bord temps réel qui me donne la visibilité sur tout ce qui se passe dans mon système informatique ok évidemment qui va me permettre de correr de faire plein de choses donc au départ grafana c'est ça c'est la capacité avec des plugins vin nous a fait une belle introduction de se connecter à plein de sources de données plutôt au temps réel et plutôt pour visualiser gra faire la visualisation sur ces données en temps réel ça vraiment ça au départ graphana sauf que depuis 2014 les développeurs et je nen suis pasis je suis très très loin d'être un développeur se sont amusés à développer un certain nombre de composants donc vous voyez Loki ici apparaît et notre plateforme repose essentiellement sur de l'OP source et nous sommes partisans dans des projets comme loky tempo Mimir on viendra après sur ces acronymes là et voilà toute une plateforme pour collecter les données les signaux de de télémétrie ou de d'observabilité dont on a besoin euh et les couches intermédiaires pour gérer ces donné bien sûr et pour les restituer pour faire quoi pour faire de la gestion de performance peu d'entre vous j'imagine savent que nous avons un outil de test de performance VO qui s'appelle K6 KX h un outil d'observabité applicative un outil d'observalité d'infrastructure et une gestion des incidents on appelle l'IRM l'incident response management donc l'offre Graf analab aujourd'hui je suis sûr que certains d'entre vous sont surpris effectivement c'est très large et couvre un un un un grand spectre de l'observabilité ok ce qui est très important de retenir c'est que les sources que vous allez utiliser les technologies que vous AZ utilisé avec grafana sont Open Source donc si demain matin vous êtes plus content de grafana on reviendra peut-être sur la question sur la version open source et la version la version payante euh ben si demain matin vous voulez plus payer grafana bah vous pouvez continuer avec les solutions Open Source alors évidemment à quoi ça sert tout ça c'est bien c'est bien d'avir la technologie on est on est beaucoup de geek chez gfana mais il faut quand même que ça serve un peu le business sinon on va se faire taper dessus on vendra jamais rien et donc à quoi ça sert ça sert évidemment à corréler toutes ces sources de données que vous avez en temps réel qui sont dans multiples référentiels et je suis aussi coupable de ça vos entreprise ont fait l'acquisition de plein d'outils ont mis en place un grand nombre d'outils qu'il soit gratuit open source ou qui soit payant et des versions Enterprise de beaucoup de solutions et vous en rajoutez malgré vous et malgré nous en tout cas à cause de nous vendeurs de logiciel de plus en plus on pourra voilà blâmer les commerciaux qui sont quelque part par là le souci de tout ça c'est que bah ça fait beaucoup de sources de données et que vous retrouvez avec une myriade d'outils quand vous avez un incident de production et bien vous passez des heures à rechercher l'origine des problèmes et vous ouvrez des tas d'écran et encore aujourd'hui on est dans un monde web pour les plus vieux d'entre nous on se rappelle qu'à une époque on avait aussi des outils en voilà des clients lourds qu'il fallait charger sur le poste de travail pour rechercher tout ça donc le but du jeu c'est d'avoir une plateforme qui permet de visualiser en temps réel l'ensemble des signaux de mon système d'information qui me permett de réduire le temps d'investigation et donc d'augmenter la qualité de service de mes applications augmenter la satisfaction des Chatur et si vous avez des sites commerçants ou qui génèent du business tangible bien d'éviter de se faire taper par les patrons les patrones donc on parle de meantime to innocence ou meantime to isolation le temps qu'il nous faut pour rechercher l'origine de la panne et vous savez très bien que fixer la panne d'origine n jamais le problème en tout cas souvent c'est facile de régler le problème le tout c'est de trouver d'où vient le problème donc on passe beaucoup plus de temps à isoler le problème qui a vraiment à résoudre le problème lui-même souvent quand vous dites à un développeur ou une développeuse le problème est là trouver la solution enfin voilà trouver le fix c'est rapide mais c'est plutôt de trouver pourquoi donc on va parler un peu plus d'observabilité c'est un mot qui est apparu je sais pas il y a 3 4 ans maintenant euh un petit peu plus on parlait une monitoring une époque moi je suis né avec SNMP on a parl avec des collègues voilà ça ouais ça ouais je sais je suis très très vieux euh on parle de cnmp on parle de plein de choses mais l'observabilité aujourd'hui c'est quoi et bien en fait ça repose sur trois grandes sources de signaux on va parler de signal on va parler de capturer des signaux qui peuvent être de trois natures qui sont les métriques classiques hein vous avez une CPU qui est qui voilà qui à 90 % puis à 20 % puis à 30 % la mémoire euh des entrées sorties donc voilà des métriques des de donné qui évolue au cours du temps mais aussi des logs et souvent voilà historiquement les applications les systèmes les infrastructures crachaient des des gigas de de log partout et donc ça reste une source de données incroyable dans nos entreprises que soit des systèmes Legacy ou même des systèmes un peu plus modernes il y a toujours un fichier de log quelque part qui est intéressant à analyser et enfin les traces peut-être un concept un petit peu plus moderne qui a été probablement qui est apparu autour des années 2005 2010 avec les premiers outils qu'on appelait APM à l'époque pour les encore une fois pour les plus viux d'entre nous l'APM l'application performance management qui disait ben on va tracer les requêtes qui s'écoulent dans les système d'information al à l'époque on était sur du 3 tiers voilà et puis on a évolué par du multitiers et aujourd'hui quand vous allez sur la plupart des sites modernes et j'imagine sur les applications d'entreprise que vous délivrez aux ateurs un seul clic d'un bouton ça génère des dizaines et dizaines d'appel si vous n'êtes pas capable de les tracer de savoir d'où part ce clic voir même de l'utilisateur dans son navigateur web jusqu'à la base de données mais vous n'avez pas de visibilité et comme il y a une interdépendance entre tous ces services là si vous n'arrivez pas à tracer ces appels là vous êtes aveugle dans votre système d'information on parle aussi de dire bah où est-ce que le problème avec les métriques donc souvent on met un seuil et puis ça alerte et puis on finit par avoir un sapin de Noël parce que ça clinotte dans tous les sens et puis on veut savoir où est le problème dans les traces et la log va nous permettre de savoir quel était vraiment le problème et bien aujourd'hui pour capturer tout ça il y a un standard qui s'impose autant prometous c'est un standard qui s'est imposé dans le monde kubernettis et globalement dans le monde du monitoring de l'infrastructure technologique mais open télémétrie est en train de s'imposer comme un comme un comme un euh comme un référentiel comme une comme une comme un standard de facto pour le monitoring des applications et pour être capable de récupérer des traces des métriques des logs dans les applications on eneviendra un petit peu juste après évidemment bah la couche pour visualiser tout ça la couche grafana où on est on reste agnostique sur la source de données on est ravi de vous accompagner sur la capture des traces des métriques et des logs on y reviendra tout à l'heure avec loky Mimir et et tempo mais vous pouvez tout à fait également utiliser les systèmes que vous avez aujourd'hui qui capturent déjà des métriques qui capturent déjà des logs et les intégrer à grafana encore une fois pour avoir C de ce qu'on appelle aussi big tent encore une fois c'est un concept que je connaissais pas il y a 2 mois donc voilà je suis encore tout nouveau et l'idée de dire j'ai une plateforme qui me permet d'avoir l'ensemble de mes signaux qui arrivent et qui me permett de faire du la résolution du trouble shooting beaucoup plus rapidement que si je suis obligé de correler manuellement d'aller chercher dans différents dans différentes interfaces graphiques dans différents endroits les données qui sont importantes pour moi et qui me permettent de rechercher la cause des pannes que j'ai dans mon système d'information ou des ralentissements que j'ai dans mon système d'information alors pourquoi on on s'est intéressé à open télémétrye bien comme je disais tout à l'heure c'est devenu un peu instant standard il se trouve que plusieurs acteurs en même temps du marché de de l'observabilité se sont dit bon voilà on a des agents propriétaires les entreprises aiment de moins en moins les agents propriétaires celui que vous devez déployer partout et que si vous le déployez pas partout bah vous avez pas de visibilité de bout en bout et et puis les développeurs je pense que dans la salle peut-être un mix de dev de HS de DevOps mais c'est vrai que les outils d'observabilité était souvent acheté par les opérations les gens qui gérent la production et en fait vous avez de l'autre côté les développeurs et puis tous ces gens-là on fait pareil en interne on se parle pas tous ne se parlait pas forcément dans dans les entreprises et du coup on se retrouvait avec des développeurs qui adoptaient des technologies qui aimaient bien instrumenter less applications et des opérations qui venaient voir les développeurs en disant tiens ce serait bien que tu mettes cet agent dans mon dans notre application Java et qui me disait non mais pourquoi je vouis faire ça dans mon application c'est mon application et Open télémétri aujourd'hui est un standard qui s'impose pour aller instrumenter les applications dans les langages que vous connaissez tous Java.net PHP notjs go et d'autres j'en ai sûrement oublié et c'est un des projets d'où le logo CNCF qui est un des projets les plus populaires euh de la Fondation CNCF donc ça ça veut dire aussi qu'il y a un attrait un appétit pour une technologie Open Source ouverte c'est un ensemble d'API de SDK qui va me permettre d'instrumenter les applications encore une fois dans les langages modernes voire même dans les anciens langages grâce à ibpf euh voilà qui permettent d'instrumenter un grand nombre d'applications de manière transparente et surtout qui n'est pas lié à un fournisseur de d'infrastructure de solution donc décorréer la partie collecte de données de la partie visualisation du finalement pour construire une observabilité de bout en bout et quand on a besoin d'observer les données faire faire la data observability bah on commence par observer chacun des systèmes qui est sur le chemin critique de cette fameuse donnée elle part d'un système voilà parler beaucoup de feuille Excel alors pe pas les monitorer une feuille excel mais on va les monitorer les sources de données les bases de données les systèmes informatiques qui portent les bases de données on va aller monitorer les applications SAS par des moyens modernes on va aller regarder tous les applications qui consomment ou qui produisent de la donnée les observer chaque fois avec une couche différente avec Open téléméri pour effectivement les applications je disais tout à l'heure auxquelles vous avez accèsquel vous avez accès au code et encore on peut faire l'instrumentation automatique collecter ces C données là la renvoyer der des backend qui sont scalable qui supportent de la volumétrie Vincent encore une fois nous disait bien la volumétrie explose B elle explose aussi dans la partie observabilité et plus vous mettez de cubernétis en place et plus vous avez une explosion des microservices et plus vous avez une explosion de la volumétrie des métriques des traces et des log que vous devez gérer et finalement une interface graphique et je reste enfin voilà après 2 mois je suis toujours en extase devant les dashbard qu'on est capable de faire avec cfana alors regardez maintenant c'est très bien tout ça on met en place de l'observabilité on observe énormément de systèm j'ai les couches basses les couches hautes les applications les midleware les infrast système voir les réseaux voir les IOT et ce matin alin chez un chez un client parlait de de cas d'usage effectivement dans l'industrie dans le transport et cetera on se retrouve à gérer un énorme une énorme quantité de données que ce soit des métriques des loges des traces encore une fois et malheureusement on va se retrouver avec un effet de sapin de Noël c'est imparable et back in the days je vendais un outil qui s'appelait BMC Patrol voilà CIT une petit formidable vous l'allumiez vous aviez des vous aviez des vous l'allumiez et ça ça clignotait de partout c'est c'est vraiment et on rigolait tout le temps c'était vraiment sapin de Noël c'était en orange en rouge et voilà c'était vraiment l'enfer et à l'époque on avait pas de moyen moderne de gérer cette quantité de de données et donc du coup bah on perd du temps dans dans l'informatique on perd du temps aller rechercher voilà de corréler essayer de regarder quelle est la quelle est la cause et la corrélation n'est pas causalité et du coup on perd du temps voilà rechercher dans des dizaines des centaines des milliers de métriques de traces de log quelle est l'information qui est importante du coup les équipes informatiques j'imagine que parmi vous il y en a beaucoup qui ont passé des heures à voilà à faire ce genre de chose à débugger à appeler les différentes personnes alors comme d'habitude c'est pas le réseau c'est pas l'application euh et puis à maintenir aussi ces outils là à se dire bah ok est-ce que je mets seuil de la CPU à 90 % ou à 95 %. est-ce que je réduis mon nombre d'alertes en changeant mes seuils ou est-ce qu'en faisant ça bah je vais du coup rater des choses et du coup il y aura des problèmes que je verrai pas et je vais me faire taper dessus parce que les utilisateurs détecteront le problème avant nous et peut-être au moment où les problèmes arriveront ce sera déjà trop tard pour essayer de voilà de de pas avoir d'impact majeur voilà on est un peu dans ce challenge là aujourd'hui autour de l'observabilité on a mis en place voilà aujourd'hui j'imagine que tous vous avez peu prou bien couvert vos vos infrastruures informatiques avec de l'observabilité que ce soit peut-être avec des des outils un peu plus anciens des outils moderne voilà du du prometteus ou euh voilà des tas d'outils euh du marché et on se retrouve avec beaucoup de ces signaux et on a du mal à faire sens avec tous ces signaux là et du coup grafana aujourd'hui arrive avec une solution et un modèle qu'on appelle le le al je sais pas comment on le prononce safe ou SAF ça me fait penser à çaas fait mais parce que je suis skieur euh ça fait le modèle ça fait c'est un modèle où on s'est dit peut-être vous a entendeu parler de la méthode red qui est aussi une une une une une méthode pour se dire comment j'analyse l'ensemble de ces signaux qui arrivent et quels sont les différents types de signaux que je peux avoir et qu'est-ce que je peux faire avec tous mes signaux donc il peut y avoir d'ors des phénomènes de saturation donc j'ai des métriques qui voilà qui me montrent que j'ai une consommation excessive de mes ressources informatiques je peux avoir des événements je peux avoir voilà dans un cubernétis pour avoir des événements tiens on a mis en place une nouvelle version de de laapi j'ai une nouvelle version du container qui est livré euh les développeurs ont fait une release hier soir j'ai eu un changement quelque part voilà j'ai un patch j'ai j'ai plein de d'événements on va les appeler des amendes quelque chose a été déployé quelque chose a changé un changement de configuration qui est pas forcément une erreur en tant que toi en tant que tel c'est un événement voilà et et en dehors de tout contexte bah c'est juste un événement et voilà en tant que tel c'est pas un problème mais on a aussi des anomalies et la difficulté aujourd'hui c'est de se dire bah est-ce que je mets le seuil à 90 % ou à 95 %. est-ce que euh je mets des seuils quand rien ne se passe euh on voilà on parlait avec un client avant-tiier qui n disait bah oui moi la nuit euh j'ai quand même de l'activité mais je voudrais quand même mettre un seuil euh alors je sais pas si je le mets à zéro ou à 1 euh mais j'ai de l'activité tout le temps et même à 3h du matin j'ai de l'activité donc je sais pas où mettre le curseur je me dis si jamais il y a pas du tout d'activité c'est qu'il y a un problème et c'est peut-être qu'il se passe quelque chose sur mon site web qui fait qu'il n'y a pas d'activité possible du tout mais du coup je sais pas si je mets à 3 à 5 à 10 voilà l'anomalie le principe de l'anomalie c'est de se dire utiliser des technologies modernes pour se dire bah est-ce que je peux pas essayer d'analyser le comportement normal classique normal classique de nos applications systè d'information observer ce qui se passe sur une période de temps un petit peu plus longue se dire bah j'apprends les patterns de mes de mes métriques de mes événements pour essayer de comprendre et de par forç préd mais en tout cas d'anticiper ce qui peut se passer ou une déviation à cette fameuse normale ce qu'on pour appeler une baseline aussi et d'essayer d'observer les phénomènes vous savez classique j'imagine vous avz tous des applications qui sont plus actives en pleine journée que la nuit voilà il a il y a peu d'activités aujourd'hui enfin il y a peu d'activité régulière il a toujours des pics et des et des creux dans vos dans vos applications on a aussi des failur VO des erreurs des événements qui fait que bah voilà j'ai un un pod qui crache qui qui qui continue à cracher voilà j'ai des év des erreurs au sens vraiment des crashes des des panes mais je peux aussi avoir des erreurs au sens voilà j'ai j'observe une log HTTP et puis j'ai trop d'erreurs HTTP mais ok super sur Wat voilà j'ai des erreurs est-ce que c'est important à quoi je peux le relier et comment je peux faire la corrélation donc on se dit avec tout ce modèle là avec avec ce modèle là on se dit bah voilà j'observe l'ensemble de ces événements là des métriques des des des logs des événements et je vais essayer de faire sens évidemment aujourd'hui l'humain ne peut plus tout seul faire ce sens là il nous faut des algorithmes pour réussir à faire sens de tout ça et donc grafana a introduit ce qu'on appelle acers avec un engin qui va nous permettre d'analyser plus automatiquement possible bien sûr l'ensemble de ces signaux qui arrivent et essayer de faire sens de faire des assertions c'est de vous faire je vais pas appeler ça des préconisations mais vous donner des informations sur qu'est-ce qui s'est passé quel est l'enchaînement de ce qui s'est passé quelle était la roue cause qu'est-ce qui s passé quelle était la cause première d'un événement encore une fois je j'imagine que vous avez tous le souci votre ordinateur je vais pas dire PC j'ai un mag je vais pas dire PC je dire votre ordinateur se met à ramer et vous dites tiens je vais rebooter l'ordinateur il marchera mieux vous le rebootz et comme par hasard il marche mieux bon sauf que vous n'avez pas réglé la roue de cause vous avez réglé le symptôme mais vous n'avez pas réglé la roue de cause et j'imagine que il y a un certain nombre de foisù vous avez réglé des incidents de production et finalement vous avez réglé ou vous avez palié le problème mais vous allez pas régler la cause d'origine pourquoi parce que c'est très difficile sans automatisation et dans un système d'information très complexe de rechercher l'origine la vraie route cause des problèmes et encore une fois quand on parle de corrélation j'adore cette phrase surtout dans dans l'esprit critique et et le scepticisme corrélation n'est pas causalité donc on essaie de faire sens de ces milliers de métriques de ces tbyt de Data de ces millions de traces qui nous arrivent on va les injecter dans notre système les observer d'ailleurs encore une fois qu'ell viennent de que ces informations viennent de grafana de notre agent grafana que ça vienne d'un open téléméry ou que ça viennne d'autres outils du marché que vous avez mis en place ou même une base SQL qui aura été connectée via des nodo premier principe c'est essayer de découvrir la topologie de découvrir les interdépendance entre les composants c'est bah oui j'ai une log cette log elle appartient à un serveur ce serveur héberge une application cette application fait tourner des API ces API sont connecté entre elles elles sont consomment et et elles produisent de l'information donc quelle est l'interdépendance entre C système essayer de faire sens manière automatisée on parle pas de mettre à la main des relations entre les composants bien sûr essayer d'automatiser de faire une map d'application et d'infrastructure et encore une fois de manière automatisée donc essay de détecter les pan parmi tous ces signaux là d'essayer d'identifier qu'est-ce qui pose problème est- problème de latence de trafic d'erreur et essayer de corréler tout ceci en temps réel pour à la fin faire desah essayer de de à l'aide de ce modèle safe d'essayer de sortir une assertion de dire effectivement le problème en tout cas l'événement d'origine ou le premier événement qui est arrivé c'est celui-là et il a entraîné une cascade de de sous-prèmes ou de conséquences qui ont impacté votre système d'information et essayer de retrouver effectivement l'origine du problème parce que c'était bien un problème de saturation parce qu'il y avait une erreur euh voilà on parle encore une fois ce matin euh c'est tout saut quelqu'un qui n dis disait bah hier il y a eu un patch sur une infrastructure de production évidemment l'hébergeur ne pensait pas que ça aurait pu faire un un problème et et évidemment le le login dans dans leur application est tombé ne marchait plus pendant pendant certain temps mais l'impact d'un patch sur un système voilà c'est très difficile de savoir à la fin il y a un des composants qui va être impacté donc on parle effectivement de d'operational intelligence de dire essayer de faire sens de tous ces signaux qui arrivent en temps réel vers la plateforme essayer de les corréler d'essayer d'analyser la roue de cause le plus automatiquement possible et vous fournir à la fin une assertion disant bah c'est probablement ça disclimer je pense qu'on arrivera pas dans les prochaines années je sais pas dans des dizaines d'années peut-être à à à être à 100 % vrai mais l'idée c'est vraiment de faciliter le travail des équipes et d'essayer de réduire ce temps d'investigation soit le plus facile possible pour les équipes et surtout on sait qu'il y a des équipes diverses et variées que tout le monde se parle pas tous les jours qu'il est facile aussi d'accuser les autres et de se dire ça vient pas de chez moi ça vient de chez l'autre la fameuse warro où tout le monde se se pointe son pointe du doigt mais l'idée c'est d'essayer de trouver par des mécanismes d'algorithme d'intelligence artificielle de machine learning on parlait tout à l'heure de la de la de la de la baseline de détecter des anomalies cette cette fameuse variation donc d'apprendre le comportement normal des applications et de vous restituer siib ible une information beaucoup plus qualifiée qui vous dit quelle est l'origine du problème en tout cas est potentiellement l'origine le vrai origine la vraie rout de cause du problème tout ceci évidemment dans le but bah de réduire ce fameux meantime to repair puisquon a dit tout à l'heure que le meantime to repair était principalement je crois que c'est 80 % je crois dans les chiffres du Gartner 80 % du temps est passé sur l'identification du problème donc finalement encore une fois quand vous avez trouvé l'origine du problème le régler bon c'est ça peut ner un petit dév mais mais finalement la c'est pas la difficulté de le régler le problème c'est souvent la difficulté d'identifier quelle est réellement la route de cause de nos problèmes donc tout ça pour faire de la résolution plus rapide des incidents et du coup de faire un retour à la normale quand vous avez un incident de production que ce soit une panne ou une dégradation de performance qui impacte vos utilateurs vos clients de faire un retour à la normale le plus rapide possible et puis du coup comme on si on fait une meilleure analyse de la rout de cause si comme tout à l'heure je reprends exemple de notre PC si on arrive à comprendre quel était le problème de cet ordinateur et que je règle ce problème là bah du coup j'aurais moins souvent rabouter mon ordinateur probablement du coup je vais améliorer la qualité de mon ordinateur si j'arrive réellement à à régler la rou de cause de mes problèmes dans mon système d'information là finalement j'aurais probablement moins de pannes parce que plutôt que d'essayer de régler les pannes d'une par une qui apparaissent parce qu'il y a un problème quelque part j'aurais réglé le problème d'origine facile on essaie de faire le mieux possible et puis encore une fois on s'est rendu compte que malgré la qualité des outils de monitoring d'observabilité qu'on qu'on fournissait il y a toujours un effort dans les équipes informatiques à confurer des seuils des alertes est-ce que j'en ai trop est-ce que j'en ai pas assez est-ce que je vais en rater est-ce que j'auraai de l'alerte fatigue est-ce que voilà je vais pas noyer mes mes équipes opérationnelles en pleine nuit avec des messages qui sont pas forcément les critique ou qui voilà on a du mal à juger la criticité de ces alarmes et donc on s'est rendu compte qu'il y a un effort important que les équipes informatiques passent à configurer ces alertes à faire des tableau de bord à essayer de faire sens de tout ça et donc on essaie de résoudre ce problème là avec de artificielle avec C solution qui s'appelle ACERT qui va s'appuyer en partie en grande partie sur les informations qui viennent de open télémétrie cette fameuse capture de trac de métrique et de log au niveau des applications on essayer de faire sens de tout ça et vous apporter de la valeur évidemment pour essayer que vos applications et vos assè d'information marchent le mieux possible voilà on est fini on est encore un petit peu en avance merci

