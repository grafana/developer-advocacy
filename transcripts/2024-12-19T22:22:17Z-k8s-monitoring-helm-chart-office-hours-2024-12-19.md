# k8s-monitoring Helm chart Office Hours 2024-12-19

In this session of the Kubernetes Monitoring Helm chart office hours, we discuss the upcoming version 2.0, the latest changes and ...

Published on 2024-12-19T22:22:17Z

URL: https://www.youtube.com/watch?v=zkhR_5v1i9g

Transcript: hi everybody I'm Pete wall uh this is December's edition of the kubernetes monitoring hel chart office hours um I'm got some really cool content uh to show so let's get started so today's agenda a little bit uh smaller just a quick version 2.0 update um a couple polls from the community that I'll ask here but I'll also put into the slack channel uh to get some feedback there uh acknowledging that it is before a lot of people's winter holiday breaks so um I'm I'm expecting not to get a ton of responses but I'm just kind of I wanted to open these ones up for for discussion and as always we'll end with some time for questions and answers so the quick uh version 2.0 update what's been happening since the last office hours there's been a lot of work for feature parody uh I've been working on the migration tool which is in the the last couple bullets there uh but by working on the migration tool you I've been able to find some of the places where there are features in V1 that aren't in V2 this time around it's mostly about authentication support So oaf 2 that was in V1 is now also in V2 for all destinations uh TL the ability to use files for the TLs certificate Authority as well as Bearer tokens that was available in V1 and now that's available in V2 one of the new things that we brought in uh about a week ago uh this was a side project of mine that I finally got some time to dedicate to to finishing and polishing is platform tests so now on specially tagged PRS as well as on commits in main we can run the the integration test the test that we actually use to test this Helm chart uh before we cut releases we can run them on specially made platforms from gke from eks the goal here is to test some of the cluster configurations that are a little bit different than just your vanilla kubernetes so specifically this is gke with autopilot and eks with Windows nodes as well so that's also executing the windows node exporter or Windows exporter uh a new feature that was added since the last the last office hours is node logs this is kind of a parallel to pod logs and node logs specifically its feature right now is looking at Journal D so it connects to journal D on your Linux nodes and gathers those things that can be great for people who are running their own kubernetes if they're using qbd Cube ADM to do that and so they could look at the cuit logs from the system level uh and capture those things now it says new service integration it's not quite merged yet hopefully in a little bit we'll get that merged in but uh this is Aaron's contribution uh but it's an integration to monitor Loki itself uh this is the first of a few Integrations that we're excited about because uh this enables the Kate's monitoring Helm chart to really monitor the monitoring system itself that we have a separate Helm chart that we've been using for a long time to do specifically that but by bringing that into here that makes The KES monitoring chart uh a much more frequently updated and maintained version of that uh and that should help out a lot of people who are not only wanting to monitor their kubernetes clusters but monitor the monitoring systems that they deploy as well uh and yeah go ahead oh one thing to add for that like this integration is specific for U meta monitoring so if you have your own on Prim Loki deployment this will gather all the you know the metrics logs Etc about that on Prim Loki deployment and send to you know another you meta monitoring cluster you may have locally or a metam monitoring cluster uh or GFF onic Cloud um you know to to host it there y thank you uh yeah so I'm really excited about this one coming in uh along doing this um it's also helped kind of refine and um make more consistent some of the internals uh as well so there's a lot of side benefits to this integration coming uh migration plan again the the documentation is there this that hasn't changed much since last time the migrator utility uh I've been working a lot on that this week to make bring that up to par uh and so there's it should handle uh V1 values files even better uh and then finally the timeline this is the one that I know a lot of people are excited to hear the current timeline for deploying 2.0 into GA is the first or second week of January uh so that is the plan for now um that I'm going my to do my best to stick to that plan uh the reason for that timing is it's a little bit of I wanted to give some breathing room so that if we released too early and a lot of people are on holiday breaks we're not interrupting people's holiday breaks um as many people who are on the call have tested uh we know that V2 is is a very solid Helm chart it's working well but there are little things that we are finding here and there so I want to give a little more time to to smoke out any particular issues so first or second week of January uh is the current timeline for 2.o being GA um all right so I wanted to ask people's opinions on two different design choices and if this were a few months ago I would have just gone ahead and made the change but we're getting so late into the development cycle close to ga uh that I wanted to give a little bit of space for the community to give some feedback on some of these things the first one is the subchart directory structure so over on the left side we show the current structure of how the different charts and the feature sub charts are organized inside of the repository we have the charts directory and then we have the Kate's monitoring chart itself uh as well as the feature charts and they're sitting parallel to that the intent for that design was to make the feature sub charts way more encapsula they didn't feel like it's part of this huge monolithic chart and the the eventual the optimistic goal for that was to encourage people from the community or other people from within graffo labs to contribute just to the feature that they care about and not feel like they have to be an expert on the entire chart um I still think that's a valuable goal I think that the side effect for this is that for changes into those feature sub charts to get included into the cases monitoring chart we have to update the uh the tgz file the tgz bundle of the feature chart uh and that causes some git dependency issues I think aerin you've seen that a couple times where if there's a small change to the feature chart you need to go into the main chart and tell it to repoll the feature chart um I can't do make file smarts well enough to know if the main chart needs to rebuild its sub charts because you'd have to go into the different directories and look specifically for change files it ends up being a little bit of a management mess um it's one that I've just kind of dealt with over the last several months but I'm wondering if uh the alternative where the feature charts live inside of the charts directory they end up being kind of like Anonymous sub charts uh it should have no effect on the actual usage of the helm chart it should be functionally ID identical to the way it is now it would just they would live inside of the katees monitoring chart I think a side benefit of that as well is that when people are exploring the git repository and they go into the charts directories they would see three charts rather than I don't know 15 or something like that is now which could be a little bit intimidating um but I wanted to make sure that this was clear I'm heavily leaning towards the alternative path but uh I'm willing to uh to listen to people's uh debate uh or arguments for either side so I'll give a few moments if you just wanted to just you know offer an idea or suggestion Marco go ahead well I I can say that I like the alternative uh best Because unless the chart the feature cluster match are independent charts completely independent you can use by themselves and maybe they they deserve their the spot in the sun under the charts uh the the the right side the alternative is is much more easy to understand and yeah and you don't I I don't like things that refer to outside of the base of chart so dot dot slash something like that and the second the second bit I don't like binary stuff in a g repo unless there is a good reason a targ that it's it's a it's it's not nice not nice to have um and and that's it well you could do much more in depth but these two things are already good enough to to lean towards the alternative so perfect thank you you're welcome I would say whatever is easier for you guys to maintain and I think the alternative is easier to maintain that's been my experience um you know we get regular updates from the other from from external dependent sub charts so there was just a recent one for node exporter itself uh the Prometheus operator crds is now there's a a PR for updating those when those update those then will modify these tgz files uh and so then it becomes a a PR issue or poll request issue rather than the other PR it becomes a poll request issue for anyone who has open PRS because then they have to always rebase rather than just purging file by file so dealing with that right now so I'm going to go for the alternative I know okay yeah I figured okay great um I am going to I'm going to create a branch I'm going to try out the alternative path and and make sure that there are no uh issues that come up with that way the the my biggest concerns is uh not necessarily the functionality of it but just making sure that all of the uh the behavior the current behavor Behavior works the same way which I feel like it should uh as well as all of our testing we do a lot of unit testing we do a lot of uh integration and platform testing um so making sure that those maintained um this is the next poll so this is one that actually Ain and I talked about and so this is specifically about labeling for service Integrations um and how do we discover and select specific instances of a service integration so the far left side this is how it is currently uh I'm just using alloy as an example but this also applies to ETD to CT manager to the other Integrations as well um where the goal here was to make it nice and Compact and not to have to be too verbose for things that you're going to be deploying several times what happens behind the scenes is by defining name is alloy metrics we actually already set a label matcher for or app. kubernetes or iame equals alloy metrics so for if you wanted to monitor the four or five different alloy instances you would make four or five instances of the alloy integration to Target those things we would automatically assume you're looking for Alloy metrics for this first integration for Alloy logs for the second integration and so forth one so Aon and I have been talking about this one of the things that this is kind of pushing against is one of the things I don't want the V2 version of this chart is to have too much magic for things to be happening that you don't expect um the V1 version of the chart has a lot of things enabled by default the V2 version of the chart has nearly nothing enabled by default and this when we started talking about this one this one highlighted that there is Magic going on you define a name but it does more than just name an instance it's actually setting Discovery rules so option one is just always be explicit you have to set some sort of label selector or a field selector or some sort of Discovery rule to actually call out what this one does um so this is effectively the same thing as option or as the current version but option one says um you need to give us some sort of Discovery rule We'll add in a validator that says that you know you can't you can't go with nothing otherwise it's going to get all pods or something but be explicit about what you want uh the the benefit to option one there's no more magic uh there's nothing that's happening behind the scenes that you don't expect and then option two is a little more interesting so it's the same thing as option one be explicit but instead of using a dictionary or you know an object to define the label selectors it's an array of strings which uses the actual kubernetes label selector language so it could be you know app. kubernetes iame equals alloy metrics so for an explicit match but using a string allows us to use more complex matching language so app kubernetes IO name in with the parentheses alloy metrics and alloy logs and the obvious benefit to this is now we're defining a single instance within the kubernetes monitoring Helm chart but it's matching all of the alloy in the alloy deployments that we care about so that cuts down on the amount of alloy config that actually gets uh instantiated which means it should help reduce resource consumption memory and CPU for the for the alloy metrics that's looking for all of these things the the biggest downside that I can see from this is that because it's an array of strings it's going to be much harder to validate and one of the big design principles for v2 is try to check as much as possible and try to give actionable feedback as much as possible if you if something that we if something is in the config that's incorrect try to detect that and tell you how to fix it and with it just being an array of strings it's going to be much harder to to check that syntax but I do like the flexibility of it so um Aaron this was your original idea to change so I'd love I'm just going to put you on the spot and ask for your opinion here um and then Marco and Carl if you've got ideas too I'd love to hear it and no opinion is fine as well if you don't if it's not big impact to you but Aon watch you go ahead yeah so the I'm definitely [Music] for I don't even know if I would say like the it's one of these options um I I I think in a lot of cases you would have app. kubernetes doio slame and that should be alloy right it actually shouldn't be alloy metrics app.net is.i instance in this case should be alloy Das metrics alloy Das logs right and so the the word instances is almost a little it's an it's an instance of an integration right yeah but it makes you think that it's an instance of alloy so so I I need to list every single one of them right and the in in the the current form the the the name is actually meaningful right yeah where the the name really should be the name of whatever you wanted to name the integration but we we actually take the name and we generate app. forname um where what I would like to see you know in this case is a I think the alloy chart needs to honor the name of just alloy and not override uh and set the instance you know appropriately that that's one thing um because that that would allow you to just have a for all of the different Integrations that we do because that's very common with many you know Technologies uh you open source Technologies where you're going to have you know app. D.O for/ mcash or app. kubernetes you name / mcash or MySQL or whatever it is because when you do a list here of multiple values what that what happens is it's doing service Discovery it's calling the kubernetes API and as a general best practice for like an integration I would tell you you should probably always specify the name space right and when you get into some of these you know massive cluster deployments and all you do is specify that selector that's more you know work um essentially that has to be done and I can't remember I I think that it does I can't remember if it applies that filtering to the kubernetes API or if it's doing at client side but you know all three of these options right now are at the discovery. kubernetes so it's doing it from the API request not what I'm saying is alloy doing that yeah yep alloy yeah when you add selectors into the discovery. kubernetes component that is affecting the request it sends to the API server yeah so anything that's in discovery. relabel is all done after the facts so yeah and and when you think about well I have a even this right say I have you know 10,000 pods in my cluster well that's still 10,000 that kubernetes has to Loop over and say does label selector match essentially you know and process it that way versus restricting it to a namespace and then each item in this list is another uh instantiation of your you know integration component more or less yeah and that's more service Discovery where if I could do a single you know most of the time like alloy I just know I want to collect all of these various alloy metrics the only time that I might have multiple instances is if I said uh my my logs pods are working fine uh and I'm just going to keep you know 10 metrics for example but my metrics pod is having issues and I I want to debug that further so I'm going to create another instance and I'm going to say for that one I want to keep all of the metrics where the the logs instances I only want to keep 10 Mets I I can intentionally have two there but that that's my uh thing there is to have as little you know extra service Discovery happening um same thing we don't have it for Alloy now but uh it it will be coming soon um you know with like what we did with Loki currently the logs processing is more or less generating a stage. match that says you know is the integration is you know Loki and the instances whatever you named it um do this processing right and most of the time you're probably not going to have multiple instances of Loki you know within same cluster but I me definitely going to have multiple is valid so that all of those same things within the the promil processing pipeline you're going to end up inadvertently just saying you know if the instances metrics do all these steps if it's logs do all these steps and they're the same exact steps yeah y sure so to to my ears that sounds like you're leaning more towards whatever reduces the discovery load whatever reduces the API server load MH so and and having option two wouldn't prevent you from declaring multiple instances if you wanted to like your example my alloy metrics instance I want to get all metrics but for all the rest of Alloys we'll just get you know a few we'll just get alloy build info or something like that to something that says it's online so I mean I guess I would say I'm more for like option two I guess you you could say um sure this one I will try to formulate some language around and I'll put it up on the slack Channel and then we can get wider discussion on there too so because I think this one is not as obvious of a a better option U Carl Mark Marco any uh any opinions from you or we can move on to the the real Q&A um if I may current is implicit is magic is if it's not documented it's it's it's really bouncing back as a problem I saw that multiple time outside of of this uh realm as well so I don't like that uh option one is what I'm normally using I like that very much there is an arrow I think in the last easy to spot here so alloy logs shouldn't be name alloy metrix but I guess but I guess it's easy to spot also an option option two is is quite nice and compact um I would say 1.5 if it existed would be my favorite but um I don't mind between option one and option two it's it's okay U it's implementation technically the current as it is today is option one for reference like that's what it essentially generates magically for you I wrote that in my values file I don't know if it's completely wrong right now but I wrote that in my f no it's it's complet completely accurate and valid to write option one is valid for today's Helm chart um it just replaces what it already does behind the scenes so okay anyway yeah yeah so I played around with the integration a little bit and then looked how many metrics it was sending to to the back end I'm like okay that's nice uh CU we're my company's very costc conscious so I think what I'm probably going to do is default allow list with alloy um well what I was going to do is um part of our our backup for like when the internet goes down graphic Cloud goes down what whatever destination we're sending to goes down um we're actually going to have like a in-cluster Loki Tempo Prometheus but with a very short retention period like 12 hours 24 hours so I was actually think about taking the alloy metrics and sending it there only so that if I did want to look at it I could look at those but I want to be collecting things like that somewh our company is more trying to really focus on just product team application code metrics that sort of thing and and the other stuff wrapped around it is as minimal as possible um sort of by method ology great cool so current option one two um I did current I kind of assumed in the back end I was doing something like option one right okay great I'll say to like I'm not really not yeah it's not about having to do like a um I I guess to to Really denote the difference between option one and option two is when you you say label selectors here um we could easily write the same thing on the leftand side that's like app. kubernetes doio forname equals alloy you know metrics sure yeah right yeah it's just exle showing the the in statement right the the difference between option one and option two is you're using a list instead of a dictionary right and option one is an implicit this equals this you can't say this is not equal to this right like I actually can't put a selector in there that says find all the alloy instances except for the one that says metrics yeah right and that's where I think the the string SEL gives you more flexibility it it's I mean yes you could write an in statement like like is is shown here um but yeah it's just using a string instead of a map yeah okay great this has been fantastic discussion on this this G me a lot to think about um I will put this one specifically up on the slack channel uh and put some more some more ideas behind this one as well um this does not sound like a thing that I'm going to be changing en coding in the next day and a half so I uh that will probably wait until the new year uh but that'll give it some time to to get some feedback um so now uh Erin if you wouldn't mind clearing out your uh your little comment there uh we've got uh time for Q&A if you have any other specific questions about the kubernetes monitoring Helm chart or about the project uh Now's the Time yeah so fire away uh do you want to formalize my question and your answer here in this case about the graph sampling chart or up to you the yeah the graphon sampling chart yes um the plan is to bring tail sampling into the Kates monitoring Helm chart I'm working very closely with the primary developer for that Helm chart uh on how do we merge these two things together it's a very common pattern that we hear it's you know I'm using the Kat monitoring Helm chart to monitor my application I'm getting traces I want to do tail sampling can I do that in the Kates monitoring chart we don't have any built-in option for that today uh it's not going to be added into 2.0 the plan is to to bring that into something like 2.1 uh and you know I don't have you know like I said at the beginning of the call I don't have a firm date on the 2.0 release timeline so I definitely don't have a timeline for 2.1 but uh but you know that's the major feature that's going to come into that one is the ability to add tail sampling um one of the reasons why this makes it not a a super clean deployment is that the to do tail sampling appropriately you need to sample all of the traces that come through and so all of the traces need to kind of filter through a single alloy instance and so looking at the topology on how to do that and how to do that in a smart way and a scalable way so that's we just want to be very careful that we're doing it in a way that's that works functionally at a small cluster level but also can scale up to large cluster levels too perfect thank you yeah thank you yeah definitely looking forward to the tail sampling great sweet cool all right unless we got other questions um I'm gonna wrap it up here uh as always you can find me on our public kubernetes slack channel uh and you know both everyone here has interacted with me quite a bit on that slack Channel um so if you have more questions about the helm chart find me there ask questions um the kuber monitoring Helm chart GitHub repo is there too so if you have issues feel free to file an issue or or file PR so thank you so much for joining and uh have a great rest of the month have a nice holiday break if you're going to be off uh we'll see you in the new year yeah thank thank for all your hard work thank you thank you take care bye

