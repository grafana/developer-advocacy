# Operations report: Running Loki on Different On-Prem Environments | GrafanaCON 2024 | Grafana

Over the past two years, Red Hat Senior Software Engineer Robert Jacob has designed, prepared, and launched observability ...

Published on 2024-04-25T22:58:44Z

URL: https://www.youtube.com/watch?v=RAJIUKiAyL8

Transcript: [Music] well hello everyone um my name is Robert I'm uh working for R redhead and I'm working uh together with the uh grafana Loki team on the uh Loki operator at rad I'm part of the open shift logging team and they in the log storage and we're currently working on migrating our existing customer base that uses elastic search to lowy and I'm here to talk a bit about we um what we learned from from running elastic search for a few years how we put this into uh in effect in low Kei and what we're doing to make these deploying your your Loki setup easy on your own cluster um for the customers um using an operator so first let's start with a few facts um I already said so I'm working in the open shift logging team open shift this is the conization product um from redhead it's based on kubernetes um it has a part that's called logging for Redhead open shift um it's split into two parts The Collection side and the storage side and like I said the storage side used to be based on elastic search and we're migrating that side to Loki now and um for um one of the things things that that that we do in in open shift is we try to use operators um to keep more complex software and workloads alive in your cluster and so our product uh logging for um redhead open shift it consists of two operators one for the collection side and one for the storage side and the storage one used to be the elastic search operator and it's now the lowkey operator so what has happened over the last years um we've been working together with grafana the Loki team on this since the early days of 2.1 which was around two 2020 and so with the newly released Loki 3.0 this makes nine or 10 releases depending how how much how you count um the operator has been released uh about one and a half two years ago now the first version and we've already reached adoption with over a thousand clusters uh over our customer base and this number is only bound to be uh to increase because there are still a lot of customers using elastic search um what we did for the operator is we focused on a single use case um we deploying kubernetes to we deploying Loki to kubernetes to open shift and it's an in-cluster workload it's only meant to Lo to to do logging in the cluster and um ship those logs somewhere or keep them in the cluster um and internally um for rad we we as the loog storage team we support two products logging that's I've already mentioned and the other one is Network observability who use Loki as a storage for their Network traces as well okay so why did we pick Loki this was mentioned before we're trying to Dem de democratize uh observability and Loki is the part that's de democratizing logging um we've seen with elastic search that it's very complex to run a log storage system at scale um a lot of people have a lot of logs to store usually they're surprised how much it is um but it needs to be as easy as possible to run your logging stack and Loki looked like a good replacement for elastic search because it does a few things different to elastic search our main uh thing there is the that the indexing doesn't uh doesn't happen on the ingestion anymore and you can just ship the logs and do the heavy processing just on the ones that you um actually want to read and so uh Loki looked like a V replacement for fastic search and the last years I think have shown that it that the choice was a good one to make so um what did we do with the operator like I already said running the the log agregation system it's a it's not a simple task um you have to uh account for for a lot of volume um you have to run it and then Loki itself it's also not not that simple to run to uh configure it has a lot of different options that um are interlocking and so what we did is we picked the one use case that was relevant for our our customers we focused on that one and um P made decisions to make that use case simpler we tried to reduce dependencies so if you look at the configuration of Loki you can see that you can use all sorts of databases to store the indexes and to store the actual log data um and we've narrowed this down to a single choice and we've also removed the dependency on other databases completely so Loki is the only thing that's running in your cluster um and I already mentioned this Loki configuration is very complex and long so we simplifi the configuration to provide just the options that we think are necessary and group these around uh the customizations that we think are necessary so let's talk about these different points first um the the use case so what we did there is we picked um similar to uh for example what AWS does with their VM instances we picked a few predefined sizes that we based on our customer sizes so we called them extra small small and medium um we started with small and medium and then last year we also added the smaller one the the extra small which used to be our development size but we've made a few changes so that it's also production ready and common to all of them is that they um you can pick the one you need based on the amount of logs that you anticipate to ingest and then also move between those sizes should you need it but for for short short-term spikes in in your usage either on the writing side or on the reading side um there is horizontal scaling possible um for the for the uh components that Loki consists of um and uh the other thing that we did is we said okay we focus on the short-term retention so I know that there are customers that want to um keep logs mainly for auditing reasons for a very very long time or need to do that but this is not the use case that we want to solve with this logging system it's the shortterm I want to debug my workload on the cluster use case um then the thing with the with the um with the dependencies so like already mentioned Loki used to support and still supports a lot of different backends for the indexes so for example candra big table console and similarly the same for data storage and it also supports um because it it consists of different uh components it also supports a lot uh of different solutions for arriving at a distributed hash ring um for the different components to do the same thing consistently and you can use console and atcd for that for example but in the end we decided to get rid of all that and for the datab for the data storage we um picked just the object storage so this mean means using bolt DB shipper or more recently the tsdb shipper uh to store the indexes and also getting rid of the external um distributed hashing by using the uh so-called member list hashing that's integrated into Loki so in the end you just have um the lowy components running and don't have to keep track of any external database anymore um this leads to a lot of simplification while running the workload and also while um debugging any outages and so what you can see here for example is that this enables us to provide dashboards ship them directly with the operator so that um the customers and our support they they can look at this at the same data and um uh debug issues easy more easily instead of having to look at at different um database products to debug stuff um and also we can use our operator to uh extract metrics out of the um of the out of the Loki deployment and I mean in the end once we have the the dashboards the metrics and provided our alerts we can also provide playbooks um that make common failures easy to Deb buug okay and similarly um for the configuration um as already mentioned the low Kei configuration is quite complex if you look at the whole configuration file you get more than a thousand knobs that you can turn that have um different results based on other configuration and it's not of course not of all those thousand uh configuration options are relevant for each use case so again we picked the ones that we think are the relevant ones and we slimm down the configuration to um what we called Loki stack which is a custom resource um made available by the operator in the kubernetes cluster and in that uh Loki stack configuration um you can now do like there's a there's a few configuration options of course that are required like where Loki should put the objects um into the object store um but you can also customize um different performance aspects like limits for the ingestion right limiting for the timeouts and limits for queries and of course you can configure retention like like I said we aim to keep the retention as short as possible maybe with a maximum of 30 days but this limit is not um enforced by the operator it's just a recommendation you can still configure retention as you like and also um by a stream or by tenant um and one advantage of the of the operator uh Paradigm that we're using is that because we only have a very short amount of of um configurations options that we provide we can also upgrade the users to newer configurations options so when when we see um that we have an issue with our current configuration and this is something that's reported by support for example it it's possible to to change the the configuration over all the deployments using a new version of the operator and it also makes migrations easier so for example I mentioned tsdb shipper um this come also comes with a new schema version and this is something that we can't do fully automatic because it's a relevant configuration for the user but we can give hints to the user for example by by emitting an alert that or you're not using the latest schema version with this version of the operator you have access to a new one we think it would be a good choice to update to that and then the user can update the um Loki stack configuration and the operator will do the rest and update the Loki components um and the last thing is we uh running Loki by by default in the fully distributed mode but we are not using all components by default um so um we've gotten rid for example of the uh of the query schedular because in our use case there's not so much a risk of a no Noisy Neighbor for example so this is something that we probably don't need in our use case um or the ruler is only um uh is only activated if the if the configuration requests it and not automatically um and the the third thing is that we of course because we want to integrate with uh open shift as much as possible um and we've customized the Loki tency to match the tency that's al already provided by open shift but um in that way the Loki operator is agnostic to the platform so while there is a mode that's specifically tailored for open shift um there um if you if you use it outside of open shift you can customize this to any um tency model that you need okay um I mentioned as uh as the last thing that it's not just available in open shift and that's something that I mean uh really so um as mentioned here Loki operator. def

