# Grafana &amp; Friends France - 29/02/2024: Talk Denodo - Vincent Fages Gouyou

Accès polyglotte et gouvernance centralisés de données hétérogènes et distribuées : les avantages de la virtualisation de ...

Published on 2024-05-15T10:27:54Z

URL: https://www.youtube.com/watch?v=Iwdiuytj_No

Transcript: soirée et et allons-y c'est Vincent qui prend la parole en premier à tout de suiteci merci jib donc moi je me présente Vincent F je suis responsable du product management chez denodo denodo une société d'origine espagnole qui sommes maintenantcoré aux États-Unis on est presque plus de 1000 employés dans le monde on a plus de 1 clients et en fait on fait la virtualisation de données alors est-ce que vous avez déjà entendu parler de virtualisation de données est-ce que c'est quelque chose de nouveau pour vous ou pas mais je vais vous parler virtualisation de données et je va vous expliquer en quoi c'est important notamment pour pouvoir accéder à toutes vos données euh d'entreprise et et vous expliquer un petit peu notre ADN et pourquoi aujourd'hui la vitation de données c'est primordial dans les évolution et changement qu'on est en train de vivre notamment avec l'intelligence artificielle la complexité des systèmes d'information et la transition vers le Cloud et ce qu'on doit vivre et et en fait la vision des nodo euh si on résume un petit peu on s'aperçoit que en terme de production de de quantité de données on en produit de plus en plus et ça va pas s'arrêter euh pourquoi parce qu'on a de plus en plus de réseaux sociaux on a de plus en plus de systèmees d'information on a besoin de collecter de plus en plus de données et surtout aussi ce qu'on voit c'est que même si les organisations les entreprises essayent de rationaliser tout ça de centraliser les données d'avoir une approche de gouvernance de données centralisées en fait on arrive on narrête pas d'avoir des nouvelles sources on n'arrête pas d'avoir de nouveaux besoins euh des études de de IDC montrent que en général une entreprise de taille moyenne ou une une un business de moins de 500 millions autour de 500 millions d'euros de chiffre d'affaires gère en moyenne autour de 400 sources de données et on parle de plus en plus de démocratisation de l'accès à la donnée de faciliter d'avoir des utilisateurs de prendre des décisions sur la donnée mais de plus en plus les utilisateurs les consommateurs de données sont des gens qui ne sont pas des technophiles qui ne savent pas faire du requetage et pourtant ils demandent de plus en plus à consommer de la donnée et en fait si on regarde un petit peu à quoi ressemble une architecture classique d'une entreprise de taille moyenne on va avir des sources de données le plus gros distributeur de datalec sur la planète vous le connaissez peut-être il s'appelle comment non pas du tout Excel et Microsoft le plus d'information qu'on a c'est c'est dans Excel c'est dans Microsoft on va retrouver dans les bases données relationnelles on va avoir des données qui viennent de de RP des données qui vontre dans pub qui vont être dans des DAT on premise des DAT dans le Cloud on va avoir besoin de traiter des données en massif par processing avec du presto des données en streaming des données dans le Cloud avec des des cloud data warehouse comme snowflake et puis des données qui viennent d'application SAS et tout ça généralement on a besoin de le gérer où on le gère sur des infrastructures en premise dans Desch virtuel dans des conteneurs dans cloud dans plusieurs clouds et je voulais pas vous agresser en disant Google je suis désolé mais en fait Excel c'est le premier vendeur de plein de solutions qui devrait pas être la solution utilisée par Excel et en fait ce qu'on voit c'est que c'est que généralement tout ça c'est en fait le cauchemar des directeurs de SME d'information on a du Legacy on a besoin de maintenir tout ça et en face de ça on a des métiers qui eux se posent des questions et qui ont besoin de faire de la mise en qualité de faire de la qualité de données donc on va chercher à se connecter donc généralement on a toujours un connecteur grafana a plein de connecteurs il y a toujours quelqu'un qui a un connecteur et un connecteur c'est quoi c'est opérer un protocole c'est être capable de lire un foncier un format de données et puis généralement B voilà on remonte la donnée on la transforme on doit la rematérialiser la persister quelque part ailleurs pour que les Data scientists arrivent et commencent à créer les modèles et généralement il parlent 80 % de leur temps si ce n'est 90 % de leur temps à essayer de sourcer la donnée de comprendre les sources de données plutôt qu'à créer des modèles puis une fois qu'ils ont créé des modèles faut les entraîner ces modèles on a des problèmes de fite de données de gouvernance on sait pas qui fait quoi on a des problèmes de souveraineté on a des problèmes de gestion d'information personnelle et il faut alimenter ces plateformes de machine learning et une fois qu'on a des résultats il faut matérialiser ça encore dans des flux de persistance il faut pomper tout ça pour pouvoir enfin présenter des informations sur des tableau de bord pour que au travers d'un grafana un outil de BI le business puisse prendre des bonnes décisions de gouvernance et de leur activité et donc voilà ça c'est une architecture classique qu'on va retrouver dans tous les grands groupes et puis ce qu'on comprend bien c'est que dès que le directeur système d'information décide par exemple de décommissionner terradata parce que ça coûte trop cher et qu'on a dessiné de signer un contrat avec AWS pour tout passer dans du S3 et du du stockage dans deobject storage sur S3 pour offloader une partie de data on peut pas le faire sans implaquer sans impacter 20 ou 30 plus applications métier des centaines d'utilisateurs puis inversement les métiers sont de plus en plus fruit consommateur de données ils ont besoin de nouvelle sources de données donc ils vont se tourner à l'IT pour demander accès à d'autres sources de données des données externes et là l' écoutez il faut que je priorise faut que je change et que je change les priorités sur un projet il va falloir que je mettre une ressource qu'on crée un flux d'intégration et on peut pas se permettre de faire ça pour prototyper pour s'apercevoir ensuite que c'était pas la bonne source de données et puis surtout quand on est dans un processus de merger d'acquisition on va se retrouver dans une situation où euh on peut pas se permettre de mettre le doigt en l'air et se demander est-ce que c'est un bon une bonne opportunité ou pas faut qu'on puisse intéresser donné là donc en fait ce qu'on se dit c'est que si on avait une technologie qui nous permet d'accéder à toutes ces sources de données de créer un modèle sémantique logique des sources sousjacon de gérer de façon centralisée le modèle de sécurité les règles de gouvernance et d'exposer ces données de façon standardisée soit au travers de requetage adoc SQL soit au travers d'API soit au travers d'un datamark dans lequel les datas scientist les business users pourrai venir chercher les données qui sont à leur disposition ce serait idéal et et en fait c'est ce que denodo fait depuis 20 ans en fait denodo c'est un moteur de fédération de transformation d'abstraction de livraison de la donnée on ne matérialise aucune données ce qu'on fait c'est qu'on va introspecter les sources de données on va introspecter les modèles on on parle le dialecte natif des différentes sources donc quand on se connecte à un snowflek on va savoir quel type de fonction peuvent être délégué aux sources de données qu'on va se connecter à un outil tel que du du presto du Spark SQL on va savoir déléguer et pousser du processing s'appuyer sur les capacités de massif par processing du cluster et surtout en fait on a un nouvel nouveau moteur de Data Warehouse sans stockage donc en fait on est une sorte de 4e génération data waros on a un moteur SQL qui implémentons un des plans d'exécution à la volée en temps réel et qui va savoir déléguer le processing à ses sources de données et puis ensuite même si on parle de virtualisation de données ça veut pas dire qu'on ne réplique pas tout c'estàd qu'on peut s'appuyer sur des stratégies de caching on peut s'appuyer sur du massif processing qu'on embarque dans notre plateforme pour livrer ses données au consommateur tout en restant sans contrôle des requêtes tout en pouvant monitorer tout en pouvant gérer la sécurité de façon centralisée et donc en fait la recette de qu'estce que c'est c'est que on reconnaîon évolue dans un écosystème qui est distribué centr cent ça veut pas dire centraliser toutes les don dans un se même système donc c'est reconnaître que sont distribué avoir une approche qui est logique avec un couplage souple être capable de changer assez rapidement le couplage entre euh les sources et les consommateur et d'adopter l'intelligence artificielle à tous les niveaux que ce soit sur l'optimisation des traitements et des requêtes dans les sources de données que ce soit sur la gouvernance euh la gestion des règles de sécurité ou même les recommandations et les modes d'usage et de consommation de la donnée donc c'est ce qu'on fait chez des Noodo et euh en fait notre architecture elle assez simple sur une architecture 3 tiers on va avoir une myiade de connecteurs qui va nous permettre de N connecter à n'importe quelle source de données le cœur de denodo le modèle sémantique va être le moteur d'optimisation d'orchestration des requêtes donc en quelque part on est un peu le chef d'orchestre qui va savoir tirer parti de son virtuose pour obtenir le maximum de performance si on a un instrumentiste qui est un peu moins performant excel par exemple qui a pas de capacité de compute bon on va ramener du compute dans d'autres sources de données pour pouvoir livrer ces données façon standardisée donc soit au travers de requetage SQL avec des connecteurs JDBC DBC des connecteurs natifs dans certains outils de BI bientôt dans grafanap des connecteurs tels que des connecteurs rest des connecteurs SOP on a aussi une interface geoison geospatial et un Data catalogue donc si on regarde la plateforme Deo qu'est-ce que c'est c'est un moteur de virtualisation c'est un outil de modilisation de modèle sémantique c'est un cluster qui va implémenter des règles de sécurité avancé donc on va au-delà du rô base Access controlle qu'on va retrouver dans les Data classique mais on va aussi faire de l'attribut base Access Control du tag base policies et donc en fait on va capable d'implémenter des règles de des politiques de sécurité basées sur de la métadonnées et plus sur des objets seuls on embarque un moteur de massive lip processing puisque'en fait on fait partie de la Fondation presto donc il y a une version d'enodo MPP qui est le MPP basé sur presto dB distribué par et supporté par des nodo et ensuite on prou on mis à disposition des outils donc pour les développ on a un design studio qui est une sorte de SQL devvelopper et dans SQL devvelopper qu'est-ce que vous faites vous allez créer des bases de données mais virtuelles donc on va pas créer de stockage on va pas créer de table on va créer un modèle virtuel notre plateforme forcément c'est une plateforme d'entreprise donc on a un Solution Manager qui va vous permettre de gérer vos différents clusters de serveurs de base de données virtuelle on va pouvoir déployer automatiquement dans la WS dans Azure euh c'estes conteneurs rab donc en terme de déploiement c'est on s à peu près tout faire on a aussi un scheduler puisque ce que je disais c'est que la virtualisation de données doit pas s'opposer à la matérialisation de données de temps en temps on peut avoir un intérêt à précalculer à mettre en place des pipelines des flux de données pour accélérer notamment les requêes on a un outil de monitoring qui est fondamental pour nous puisququeen fait qu' on se positionne à la croisée des chemins entre les sources et les consommateurs on a une très bonne opportunité pour apprendre des modes de consommation pour apprendre des temps de réponses des différentes sources de données pour les business user business analy on va mettre à disposition un catalogue de données qui est finalement un datamar des assets et des Data product qui sont à disposition pour la consommation des interfaces SQL de de requettage pour les Data scientist on préconise aussi l'utilisation du catalogue de données qui va vous permettre de donner un accès très rapide en SQL uniquement et sans que le DAT a besoin de créer des connecteurs des vrappeurs en Python ou autrees et pouvoir consommer directement ces données là on va distribuer un autre BO Zepelin on veut on distribue on peut aussi travailler aussi bien avec du Jupiter ou du Lab et puis on a toute une une couche d'API qui va vous permettre d'exploser les données donc si on regarde un petit peu ce qu'on propose avec des nodo en fait on propose de gérer la création des produits de données puisque tout le monde parle beaucoup de Data product de datab il y a beaucoup de buzzw mais en fait la vision c'est de se dire entre le moment où je modélise quelque chose que je donne un accès à des données dans des nodo tout ce qui est modobilisé dans les LDO est disponible dans un Data catalogue donc les c'est indexé les utilisateurs pouvoir chercher V pouvoir chercher sur la bétadonnées on peut enrichir C dat la béadonnées on va pouvoir collaborer on va pouvoir donner des feedback on va pouvoir demander des modifications la création de vues virtuelles complémentaires pour mettre en à jour ces ces données euh on va implémenter des règles de sécurité donc le développeur va pouvoir mettre en place des politiques de sécurité basé sur la métadonné en disant voilà un objet qui est classifié dans l'outil de gouvernance comme un objet sensible qui est dans une zone géographique en dehors de l'Europe et qui avec des colonnes qui sont de type numéro de sécurité sociale ou email dès que j'ai un objet comme ça qui arrive dans le système je dois appliquer une règle de sécurité particulière pour les utilisateurs qui ne font pas partie de la zone géographique et est par exemple d'offusquer de tokeniser ou de chiffrer cette données et ensuite le DAC World va pouvoir aussi segmenter ces données là pour pouvoir les mettre à disposition les publier et pouvoir les consommer directement au travers d'outil de reporting de Bill an donc en fait quelque part c'est tout le travail de gouvernance que l'on fait sur la mise à disposition et l'accès à données et puis si on développe un petit peu en fait vous avez compris que aujourd'hui on a de plus en plus de données on a plus besoin de plus en plus d'assistant je disais tout à l'heure que les gens qui consoment de la donné sont de moins en moins technophiles et donc en fait comment on va pouvoir faciliter l'accès à ces données et notamment utilisant en utilisant l'intelligence artificielle et en fait ce qu'on ce qui se passe c'est que bah vous l'avez compris on est très très bien passé par notre architecture dans écosystème pour comprendre qui utilise la donnée comment utiliser quels sont les jeux de données qui sont utilisés et puis quels sont les modes de de consommation et donc ça va permettre à des Noodo d'utiliser ces informations là pour pouvoir faire des recommandations notamment dans la gouvernance de données pour pouvoir en terme de productivité aider sur la base d'une définition de modèle de données à mettre à disposition des tags à à créer des descriptions d'un modèle de données j'ai une table qui s'appelle und customer quelque chose a priori vu le reste du modèle de données c'est ma table client et puis je vais avoir des informations sur les adresses sur les colonnes donc ça pourir me permettre d'identifier cette table comme une table potentiellement avec des données d'information personnelles qu'il faudra taguer qui seront utilisés plus tard dans le moteur de politique de sécurité mais aussi ça peut permettre à des d'être plus intelligent donc notamment de d'utiliser euh le langage naturel pour générer des requêtes SQL puisqu'en fait le mode d'accès de consommation de la donnée c'est en consommant de la donnée au travers du SQL mais tout le monde sait pas écrire du SQL même moi j'ai un peu de mal à écrire du SQL de temps en temps donc c'est assez pratique de pouvoir être en mesure de s'intégrer avec des interfaces deintelligence artificielle comme Azure open ai ou comme chat GPT envoyer une requette en langage naturel exposer à la plateforme d'intelligence artificielle notre modèle de données sans exposer not données mais exposer notre modèle de données et donc sur la base ce modè de données comme un copilote obtenir une requête SQL qui va pouvoir être exécutée par la plateforme pour obtenir un résultat et donc il y a aucune donnée qui sort on ne sort que de la métaadonnée et donc voilà ça c'est des c'est des copies d'écran qui vous présentent un petit peu comment on peut faire des requêtes en langage naturel directement dans le catalogue de donné des dono et et en fait ça va permettre à des Donau de donner d'exposer uniquement don ce que je disais un schéma d'avoir une interface assez user friendly sur la consommation l'utilisation de la donnée et de pouvoir transporter ces requêtes dans des outils de bianalytique et puis au-delà de ça aujourd'hui un vrai enjeu c'est l'optimisation financière du compute et du stockage qu'on a notamment lorsqu'on achète du compute et du stockage dans le Cloud et en fait lorsqu'on lance une requête on va demander du compute à une source de données on va avoir du igress alors je sais pas comment on dit le Ress le le le coût d'extrason de la donnée euh lorsqu'on sort du Cloud et donc pareil denodo est capable de voir le trafic avec les différentes sources les temps de réponses des différentes sources la quantité de données qui est rapporté certaines techniques d'optimisation des requêtes que denodo met en œuvre par exemple parce que denodo ne fédère pas les données ramène pas toutes les données en mémoire mais certaines techniques de F de d'optimisation des requettes consistent à déléguer du processing à déléguer du groupe by au sources de données et donc ça on va savoir quels sont les temps de réponse de ces sour là et ça va nous permettre de mettre mettre en œuvre des procédures de logging avancé où on va pouvoir tracer en fonction de la source de l'emplancement de la source de chez quel cloud service provider où est-ce qu'il est dans un dat sensur dans une zone géographique en particulier quel type de source de données on va pouvoir tracer les requêtes de façon très très fine pour pouvoir mettre à disposition des tableaux de bord d'optimisation financière pour que les administrateurs puissent finalement regarder les usages des différentes sources de données en fonction des profils utilisateurs en fonction des clients et des consommateurs et ce genre d'information qu'on présente là sur un autre outil peuvent être tout à fait intégrés avec d'autres outils de monitoring et pouvoir croiser ça avec d'autres données financières de l'entreprise que vous avez à disposition enfin ce que je disais c'est que denodo c'est un petit peu un accès polyglote à toutes les données et au-delà du modèle sémantique qu'on met à disposition on va permettre aussi de se connecter à une myiade de sources de données donc on est notamment un SQL enaableer de n'importe quel type de source de données puisqu'on va se connecter nativement à du sales force on va se connecter nativement par exemple à du presto ou du Athena ou à des données qui sont dans les fichiers parquet ou iceberg dans de l'object storage donc quelque part on démocratise l'accès aux technologies Big Data même pour les utilisateurs métiers et on a une approche unique et une approche relationnelle puisqueen fait on vient du monde de la base de données du monde relationnel et on va pouvoir exposer ces données là à des til de consommation sous forme d'API sous forme de d'interface géospatiale donc par exemple on peut consommer de la donnée geogon des AP géospatiales et puis on peut créer des des vecteurs des form vectorielle enrichie auquel on va pouvoir ajouter des attributs et livrer ces données là à des outils de visualisation et de consommation et donc si on regarde un petit peu donodo c'est un petit peu la seule source de données pour toutes vos données et en fait c'est vraiment le traducteur l'interprète qui va vous donner accès à toutes vos sources de données et donc par exemple dans un exemple de pattern où aujourd'hui par exemple on pourrait avoir une base de données postgré des données qui sont dans l'objject storage avec l'historique de données qui viennent d'un capteur euh d'objets connectés plus des bases géospatiales on va pouvoir utiliser des nodos pour lire mais aussi pour écrire dans ses sources de données au travers des interface à payer et publier ça sur les tableaux de bord dynamiques qui sont monitorés par graph an en temps réel pareil denodo c'est pas que du SQL et que une approche en en Pol mais on peut faire du push puisque denodo est capable d'écouter par exemple des top kcks cfka ou des que JMS et donc sur la base d'un événement qui pourrait venir d'un objet connecté qui est écouté par un topic capcade l Noodo va être en mesure sur la base de l'ID cet événement de lancer une requête en temps réel sur l'objject storage sur le cluster MPP ou sur Athena ou sur un snoflec pour pouvoir obtenir une donnée enrichie avec du contexte enant voilà c'est un capteur avec une erreur qui est remontée par un capteur qui a été installé dans tel ou tel bâtiment et pour pouvoir afficher en temps réel au travers d'un topicque CFC sur lequel on republie de la donnée de l'information qui est publiée sur un outil de de monitoring dashboarding n'est-ce pas donc en fait si on regarde et le takea ce que je voulais dire c'est que des nodo on est un leader global de la data gouvernance et du Data Management on a plus de 1000 clients dans le monde essentiellement des Grands Comptes on a plus de 45 % de notre business qui est en Europe notre ADN est essentiellement européenne puisque en fait notre notre Red est principalement en Espagne plus en Californie à paloto et en Inde on est identifié comme leader par les différents analistes que soit le Gartner le Forester et si vous voulez en savoir plus ben je vous invite à aller sur un test drive donc vous avez des tutorials sur AWS Azure ou ou GCP en fonction des différents tutoriels on a un tutoriel sur les objealit sur machine learning ou sur la B analyique et on a aussi une version gratuite et open source qui s'appelle denodo express et un programme académique qui est à disposition euh si euh vous êtes intéressé pour en savoir plus donc voilà je vous invite on partagera cette présentation et je vous invite à aller voir et à regarder ce qu'on peut faire moi ça fait 27 ans que je suis dans l'édition de logiciel plus de 6 ans et demi que je suis chez denodo et je suis un peu trop enthousiasme et en fait c'est la première fois que j'ai un produit qui fait ce que ça dit sur la boîte et ce qui est assez impressionnant et alors je dis pas qu'il y a pas de bug que c'est pas parfait mais et j'y travaille à l'améliorer mais en tout cas c'est incroyable ce que ça peut vous apporter pour faciliter la vie pour videmment vous concentrer sur la valurité qui est la data observabilité qui est monitoring qui est le dashboarding que vous avez besoin de présenter et en fait le gros du travail se fait souvent en bas de l'iceberg ce qu'on voit c'est l' haut de l'iceberg bah nous on a on a tendance à réduire la base de l'iceberg voilà je vous remercie on a l'opportunité de faire une séance de questions-réponses donc si vous avez des questions n'hésitez pas l'instant est encore là et je resterai là après si vous voulez pas de question si c'est quoi la différence version open source et lasion Open Source ah ben la version pas Open Source elle est payante non non en fait la différence entre la version openource et la version pas open source c'est que sur la version open source vous êtes limité sur le nombre de liges que vous pouvez rapporter vous êtes limité sur certains sur certaines sources de données euh en fait dans la plateforme euh on a un SDK donc vous avez pas accès au SDK euh la la version open source elle est très bien pour apprendre pour commencer euh en terme de binaire et de code c'est exactement le même que la version non Open Source a que la licence qui va vous activer donc vous pouvez commencer avec une version open source euh et passer sur une version payante et vous allez vous avez pas besoin de réinstaller euh ensuite euh si vous commencez avec la version open source c'est aussi un un modèle l'intermédiaire qui est passé sur les Marketplace donc on a sur la Marketplace de Azure AWS ou GCP vous allez retrouvver des no on est même chez Alibaba si vous voulez aller chez Alibaba et et on a une offre de manag services qui arrive euh donc ça peut être aussi un autre modèle euh mais après on a beaucoup de modèles différents mais essentiellement la version open source vous êtes limité sur le nombre de lignes que vous allez pouvoir ramener et et sur certains types de connecteurs vous aurez pas toute la partie intelligence artificielle toute la partie assistance tout y a certaines techniques d'optimisation des requêtes qui seront pas implémenté avec la partie Open Source voilà servicez y a pas une un suivi cas par cas tout est déjà juste à connecter les différentes sources de donn vousvez pas une analyse de de la de CLI pour voir quel alors si si je pense en fait en fait c'est là où on fait appel à nos partenaires quelques PHAs cube OK et c'est vraiment là où en fait nous on est éditeur de logiciel euh on parle d'une plateforme c'est qu'on n pas un outil desktop on est quand même à middleware d'accord et et donc en tant que middleware effectivement on travaille avec nos partenaires nos partenaires on a on a un écosystème partner avec énormément d'expertise chez Fas cube et chez chez d'autres partenires en F des des zones géographiques où vous travaillez mais on est avant tout à midleware ok comment comment gère sur lesn personne Pasir alors ça c'est c'est plutôt une mise en place de règles de protection de données donc on va pouvoir le faire dans des donc ce que j'exliqué tout à l'heure c'est qu'on a un moteur de politique de sécurité qui permettre de de se positionner en fait comme on a un proxy d'accès à la donnée chaque requête en fonction du profil utilisateur vont passer au travers du proxy et on va pouvoir mettre en œuvre ces règles de sécurité donc par exemple HSBC un de nos clients j'étais avec eux hier à Londres il leur use case c'est la gestion de toutes leur données de risque au niveau mondial et donc le principe sur les données de risque c'est qu'il y a des données qui sont en Suisse par exemple pas le droit de sortir de la Suisse donn qui sont en Chine qu'on a pas le droit de sortir et en fait denodo va permettre de gérer justement ces accès va quand même permettre de faire du compute sur des données pour remonter que des données agrégées et donc ça on va le mettre en œuvre ensuite sur la partie observabilité et regarder un petit peu ce qui se passe à l'intérieur c'est là où on va devoir faire appel à les solutions comme grafana à des solutions métiers implémenté par PHAs cube pour mettre en place des des projets de dat observabilité on est un petit peu que le plombier quand même ou alors c'est les présents c'est à disposition dans la prochaine update en terme de preview et d'ici l'été ce sera une fonctionnalité native dans le produit en fonction du mode de licence que vous avez donc ça on a différents t types de licence mais en fait sur la partie intégration de modèle de langage large ce qui est l'intégration même de de base vectoriell pour peupler des bases vectorielles c'est quelque chose qui est très très fort dans notre road map et euh et on va avoir plein de choses qui vont arriver d'ici la fin de l'année autour de l'intelligence artificielle la première étape c'est notamment de fournir cette capacité de roquetage en langage viruel et si vous voulez quand je récupérerai mon ordinateur je pourrais même vous faire une démo tout à l'heure

