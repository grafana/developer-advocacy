# Grafana Tempo Community call 2025-04-10

Join our next Tempo Community call: ...

Published on 2025-04-10T19:31:27Z

URL: https://www.youtube.com/watch?v=W5L6bnRfCII

Transcript: you have started recording. Hello everybody and welcome to the April Tempo community call. Uh we've got an agenda. If you'd like to add anything to it, please pop it on there or um you know ask, put it in chat. Smoke signals, whistles may also work if we can determine them via train. And uh we'll start to uh we'll jump in. Um and we'll have Q&A at the end for anything anybody wants to bring up. All right, we have Tempo 2.7.2 release which looks like we had some fixes for panics. Well, I can and I will talk about that. Uh so 272 um we have one commit in it. Uh we fixed a panic related to uh it was an optimization we tried with regard to quering data from the adjusters. Uh this turned out to be an incorrect use of the ring which was causing panics uh that we did not realize for a bit but we did get it fixed. Um the panic required some heavier kind of querying um and so we were not going to cut a 272 but then this fellow Pav leak submitted a issue and it actually was also causing correctness issues at a much higher rate than we realized. Um and so we decided a 272 is warranted for both of the fixes. So that PR right there that you see linked um fixes a panic that you probably haven't seen and a correctness issue which you may have seen because it's a lot more common than the panic. Um, if you are on 271 or two just 27, I definitely suggest upgrading to this one and hopefully uh, you know, your users may be impacted and they don't even know it. They're doing queries and they're getting 404s on trace IDs. What's the symptom? And they think it's just not in the database, but it might actually be. So, uh, heads up on that and, uh, we recommend upgrading. Awesome. Enhance. Enhance. Okay. Um, so we've kind of had a theme for the past little bit on stability and correctness. So I think we talked about that panic um issue there at the top. Or is that a different issue, Joe? Uh, that panic I think we discussed last time, but I don't think we were talking about 272 yet. I don't think we quite realized it. So, oh my bad. This compactor panic is a different panic. Um, so this was due to us feeding bad data in Park Go and Park Go is panicking while attempting to uh read a buffer. It was due to a change we made recently to stop compacting when Tempo notices a job changes. So they track jobs in the compactor who owns what. Zach's actually worked on some exciting stuff in that area. Uh, but despite that, uh, basically cancelling the context out from under the compaction job made it occasionally panic and this found and fixed it. It's actually been in there for 3 to four years. We just never noticed because we never cancel jobs out from under the compactors. So, that's a nice fix. I doubt you're seeing this, but let us know if you are. Cool. Um, we have something about finish polling. All right, Tenant, before shut down, Zach. Uh, yes, this was to address issues during rollout of the compactors. Um so previously once a compactor received a term signal it would just shut down immediately um or the polar generally. And so what we're doing here is we're going to finish pulling the current tenant before we shut down. And this should alleviate any sort of uh or at least help um the situation where the tenant index is too old during a roll out. So small fix but um should help us going forward as well and will help the future of the the next compaction architecture. Cool. And if anybody has any questions while we're working through here, feel free to uh post them up. Oh, yeah. About the compactor. Um I was going to jump in. And did we mention the new backend stuff that you're working on last time, Zach? I thought we talked about it last time. Yeah. Okay. Yeah. I I think that's just another related thing in here for correctness because the current compactor is too non-deterministic and it can actually duplicate data instead of reducing it and so the new backend stuff will fix all that too. Yeah. Yeah. Good call. Cool. Uh looks like we have a whole slew of TraceQL changes. Yeah, I can cover these. Um so we have the new architecture which is based on the queue and the idea is that it will end up in an uh a different copy of the data in the back end which is strictly RF1. There's only one copy of every span and it's very reliable storage and the way it gets there and everything. Um so what we're doing now is exercising more on the read path to test that for correctness. So that first one is really what kind of like enables us to find all these other things. So, Vulture is the other tool we have which submits data to tempo and then queries it over minutes and hours later and make sure it shows up. Um, now it will do the exact same thing, but it can pass a query parameter. So, if you're have we're we're running the new architecture in some dev cells, we can test both read paths at the same time. So, that's kind of what that is doing there. And um also extending uh you know I think I'm missing a PR in here but we're also extending the vulture tool to do deeper queries and correctness into the metrics side of things. So when it submits a span it'll make sure it shows up in a rate query things like that. Um so yeah so just a couple of links here. We don't need to go through everyone but uh yeah know this is really good. So that's kind of like where we've been spending our time for the past month since the last community call. Hey Marty, if I'm unfamiliar what RF1 means or stands for, how would you explain that? Yeah. So, uh, we use this all the time. So, replication factor. I'm typing it into the community call doc here. Yeah. So, the current architecture writes three copies of all data to three. It writes a copy to three injusters. And so, that's how it achieves uh reliability uh and high availability, right? So if we lose any adjuster, we still have the copy of the data. Um but that goes all the way to object storage. So the new architecture is going to make it so that we have a lot more red. We don't have to pay need that extra replication factor. We can drop it as early as possible. Awesome. All right. That's something from Adrian it looks like for fixing queries um with ands andors. Uh yeah that's that's more for search and then there's a couple ones there for metrics. Um really like the toughest one was that third uh second from the bottom the edge cases. So this is um actually things that have been slightly incorrect for a long time. So Russon really dug into this and it was really good find. Um having three copies of the data ended up kind of masking a lot of things. Uh but now that we have the true RF1, I think it makes some of these things a lot more apparent. So yeah. So that was cool. I would just like to call out how awesome this PR is with graphs and like you know this is like topnotch. Um yeah are there any others here that maybe we want to call out? I don't know what this one is also another agent um person Neil symmetric. Oh, is this where we were not where nil equal equal nil didn't work? Uh, we're working on that, too. Fam actually has a PR up for equals nil. We've had not equal nil for a while and she's adding the opposite, which has been a request for a moment. This actually I guess we maybe put does not equal nil in kind of a less than great way because it all only worked going one direction. Span.food does not equal nil. Like we didn't really integrate into language correctly. like it didn't support nil does not equal span.fu. So this is mostly a attempt to uh just make the language more consistent. I do think you know one of the one of the or the themes here is correctness and I think that's going to be a big theme in the next couple months as well. Next quarter we're going to spin off a couple folks to only focus on correctness issues. For the most part everything's fine but there's just a lot of hard edges and weird corners and we want to iron all this out. um we've added a lot of features to tempo things are settling a bit and I think we have the time to really get into the details and uh attack you know a different level of correctness in the in the database. So these kind of things these PRs are going to start showing up more and more over the next uh quarter or so. Cool. All right. That kind of I think gets us through our primary agenda here. Uh, does anyone want to bring up anything else? Or does anybody have any questions? Could be how things ran, any questions about your setup? Kind of open forum here. If you don't want to speak up, then you're welcome to type in or raise your hand. Uh, Greg, so was you commented about the nil comparison being symmetric. Does that mean we can search for equal to nil or will be able to? Yes. So the PR is up now. Uh Jenny Fame's working on it and yeah, I'd expect it to be in 28 unless something weird happens. So yeah. So my use case was that I want to exclude like except event.exception. Where it's equal to something. But if there was no exception message, I wanted to get spans. I just wanted to exclude spans that had that stupid thing. Right? Uh in fact there was an issue may have been used somebody found an issue about that years ago um and we closed it recently because of this upcoming change. You'll be able to write something like spanfoo is not equal bar and or or span. Is this what you're talking about? You'll be able to write something like that where you can find not only is it not equal to a specific string but it also just simply doesn't exist. Exactly. Great. All right. Other Oh, hey. Yeah. Uh, there is a workound in what one of the original tracking issues for this one. I was wondering if you Here, I can put a link to that one. Um, I mean, it's kind of complicated, but here, let me drop it in the doc or where are we typing here? Yeah. Um, I put it in the doc. I should have put mine in there. Oh, it's okay. Yeah. issue 2188 at the very bottom there is kind of a workaround that you could look for nils. Um not sure if you saw that or if it would be helpful but if it's something you need like today you could maybe try that. Thanks I'll look. Cool. make us all sit here in a little bit of silence for a second to see if anybody has anything else they would like to bring up. You gonna make us sit in silence now? Yes. Wait, is that a a train pun now? I'm not sure. We haven't left the station yet. Oh goodness. There's Janie fam right there. She's going to make equals no work. Have you even looked at the PR? Oh, are you suggesting it's so bad I shouldn't promise anything? I have not looked at the VR or I think I maybe did briefly, but uh I have not dug deep into it. It works. Oh, that's all I need to know. Approved. Let's go. Fam said it works. LGTM. All right, last call for if anybody has anything else they would like to bring up. If not, we will see you all next month and everybody have a fantastic weekend. See you, man. Okay, cool. Bye. Bye, everybody.

