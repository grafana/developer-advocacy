# Introduction to Ingesting logs with Loki | Zero to Hero: Loki | Grafana

Have you just discovered Grafana Loki? In this Zero to Hero episode, we dive deeper into how to ingest your logs into Loki. Buckle ...

Published on 2024-05-24T07:00:35Z

URL: https://www.youtube.com/watch?v=xtEppndO7F8

Transcript: [Music] welcome to this episode of Zero to Hero a beginner's guide to Loki I'm Jay Clifford and today we're diving into the basics of ingesting logs into Loki before we jump in a bit of housekeeping if you haven't seen our what is Loki episode I highly recommend watching it first it will give you a solid grounding on how works and how to deploy a simple demo also check out our other videos for some background theory around Logs with that out of the way let's talk about log ingestion so how does Loki inest logs logs are pushed into Loki via a HTTP API endpoint typically using Snappy compressed protocol buffer messages otherwise known as log Proto but they can also be said as a Json payload these messages are received by Distributors which handle incoming HTTP requests each stream is then validated to ensure correctness and inherence to configured tenant or Global limits and then the data is then sent to ingestor which prepare and write the logs into object storage we'll dive deeper into Loki's architecture in another episode but it's essential to understand this overall flow of right requests next let's talk about how Loki stores these logs if you watched our getting started with Loki episode Nicole mentioned that Loki isn't like your traditional log aggregation tool that essentially would pass an index each field within a log entry instead Loki stores logs as streams of log entries you can imagine this essentially being a highly compressed string so whether your log data is is playing text Json or any other format Loki stores it the same now Loki does have an index but it comes in the form of labels we will chat about labels a little later in the video but labels are important for distinguishing One log stream from another at this point you might be wondering are we leaving you high and dry writing push request of your logs into Loki via the HTTP endpoint the straight answer is no there are several ways to collect and send logs to Loki and we're going to divide them into three categories primary specialized and third party in the primary category we have grafana alloy this is a vendor neutral distribution of the open Telemetry collector alloy offers native pipelines for otel Prometheus pyroscope Loki and many other tools for metrics logs traces and profiles if you're new to Loki then alloy is our recommended method for ingesting logs in into Loki and we'll discuss this more in a moment next is our specialized category which includes the otel collector and promail as of Loki 3.0 you can now push logs directly from the open Telemetry collector via a specialized endpoint that uses structure metadata we'll show you how to configure the otel collector in our next video promel is an agent that reads logs from sources like fil the system journal and kubernetes pods it uses Prometheus like service Discovery to find Targets automatically interestingly promail has been integrated into grafana alloy so you can choose between a general purpose collector or an agent specifically designed for logs our last category is third party this includes clients that are compatible with Loki each of these thir party plugins will receive their own configuration video so I'm just going to list them for you here okay let's get to the meat of this lesson ingesting Logs with alloy from here on out we're going to be doing this via an interactive demo you can follow along with the repository found in the description below or you can use the online sandbox killer coder I will be making use of the online sandbox we're also going to be using the carnivorous Greenhouse demo once again but this time we're going to be working in a branch which has a very empty alloy configuration we're going to be using this to build out the alloy configuration step by step so you can see the changes this has on writing data to Loki luckily due to the nature of how Loki stores logs it offers this flexible Insight the first thing we're going to do is manually spin up our Docker environment once our environment is spun up we can then run the following Docker command to view our running containers which should look a little like this now alloy comes with a UI interface which we can access at Local Host with the port one 2 3 4 5 at the moment we aren't seeing anything too interesting since we have a blank config our first task is to provision alloy to tail our application logs our application logs can be found in the logs directory specifically are we are interested in the app. log file containing our application specific logs it is worth noting that we have pre-mounted this directory as a volume into the alloy dock container so we can scrape these logs let's start by adding a component that's going to locate our log file local file matches a component within alloy that discovers files on your local file system using glob patterns and the double star Library so let's break down how we've constructed this component essentially we call the component and then we name it this name needs to be unique then we have some curly brackets and within the curly brackets we have a set of arguments the first argument is pathore Target which takes a list of targets for the sake of Simplicity we've actually specified our entire path to our app dolog file but you can with star notation be a lot more Dynamic for example what we could have done is we could have said tempst starapp dolog this essentially would have looked for all app. log files within all directories underneath Temp and you could be even more General than this you could do something like Tempstar star. logs to look for all log files within the temp directory we have also an optional argu called sync period this defaults to 10 seconds but we reduce this for the sake of the demo this is important if you're actively adding many log files to your file system and want to sync alloy to these changes now we have a couple of options for reloading our alloy config you could either do it manually by restarting the docker container but there's actually a way more clever way of doing this what we can do is actually reload our config using the HTTP mpoint and alloy essentially called sl- SL reload and this will actively reload our config now let's actually jump ahead and Define where we want our logs to go Loki WR receives log entries from other Loki components and sends them over the network using Loki's log Proto format in this configuration we include the basics as we did before but with one key difference we are also including optional nested blocks the endpoint block describes a single location to send logs to we could actually create multiple endpoint blocks if we want to provide locations to send our logs to different places there are other optional nested blocks that we could have also included as well such as authorization oarf and Q config one once we add this component and reload the alloy config we can see the new component within the alloy UI now let's actually start scraping those logs what we're going to do next is use the locy source file component which is actually inherited from promail it receives a set of log files and scrapes the contents of those log files into an alloy data pipe which is fed to a loky compatible receiver in this case we have two arguments the first is targets which is a list of files to scrape for this argument we're actually providing our file match component as input next is forward two this is where we want to export our logs and we would export these to a chosen Loki receiver component for now this will be directly to the Loki right receiver if we reload our alloy config a final time we can now see all components and how they interlock within the graph panel within our alloy UI let's now create some logs using our carnivorous Greenhouse application we will also toggle on error mode to generate some more interesting logs let's now jump into grafana and check out the current state of logs we are collecting and storing in Loki if you use the log Explorer and filter by our file name a predefined Loki label you can see our application logs so we've mentioned labels a few times but what exactly are they labels are key value Pairs and could be defined as pretty much any anything we like to refer to them as the metadata to describe a log stream if you're familiar with promethus there are a few labels you're used to seeing like job and instance in our case we have two predefined labels by alloy file name and service name labels are important because they improve our retrieval times for specific log streams and also allow us to distinguish one log stream from another now having said this it does not permit us to label everything we will cover label best practices in another video but for now the ru of Thum is to Define meaningful labels which distinguish one set of log streams from another that you should query regularly these labels should also be finite to stop your index from growing out of control so stay clear of Trad or span IDs IP addresses and user IDs as this can impact performance over time remember the secret source of Loki is in paralyzation not indexing as a final step in our tutorial why don't we add a processing step to our alloy config which will pass our log for a specific attribute and turn it into a label we'll start by adding the component called Loki process now I'm not going to lie to you this is where things can get quite confusing so we're going to break it down the Loki process acts as a Loki receiver meaning it can take log entries from another component which exports Loki log entries in this case this will be the loky source file from there we have quite a few internal blocks we can use to build out our processing pipeline now order is incredibly important here the order in which you define is the order in which each stage of the pipeline is executed our first stage is called stage log fnt this essentially allows us to pass each log line that looks like this essentially it's a hybrid between text and structured logging including both key value Pairs and plain text based logs in our case we're going to pull out two key value pairs into variables to be used as labels later in the pipeline level and logger next in our pipeline is stage labels it can read data from the extracted values map and set new labels on incoming log entries in our case we Define the label keys back to their original names and provide the extracted attributes as the values lastly on the list we need to do some Plumbing as currently the process stage of the pipeline is unconnected essentially we need to change the forward two parameter for the the source file to point to the receiver of the Loki process we also need to check to make sure that our process pipe is pointing to the Loki right component which it already is let's save the changes and reload one final time if we take a look within the alloy UI once again we can see the completed Pipeline and then we when we jump back into the grapher Explorer we can now see we have two new labels to query by level and logger we will cover some more complex alloy configurations and best practices for logs in another video but I did want to leave you with one more hack if you're a current promel or grafana agent or even otel collector user then alloy has a neat migration tool which will automatically convert those configs into alloy configs to do this simply install alloy onto your local machine then call this CLI command based upon your chosen config in this case we'll use the promail config run the command and this will output a ready too alloy configuration and that brings us to the end of our intro to ingesting logs into Loki lesson learning alloy to ingest logs into Loki at first might seem daunting but it is an incredibly flexible and Powerful collector which will soon become quite the Swiss army knife in your growing arsenal of observability tools stay tuned for our next Loki inj video All About open Telemetry also keep an eye out for Nicole's next video in the series which is an introduction to querying with Loki until next time my name is Jay Clifford stay curious [Music]

