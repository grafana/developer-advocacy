# Tempo Community Call  May 2025

Grafana Cloud is the easiest way to get started with Grafana dashboards, metrics, logs, and traces. Our forever-free tier includes ...

Published on 2025-05-09T14:23:08Z

URL: https://www.youtube.com/watch?v=FCrf6J-fk8M

Transcript: minute or two. Oh, all right. Jenny's already recorded. So, I'll give us a little intro here. Welcome everybody to the May Tempo community call. I'd like to thank everybody showing up and we got a little agenda here and I'll share my screen here in a second. If there's anything you want to bring up or have any questions about anything, feel free to raise your hand. uh putt in chat um interrupt whatever it takes to uh get some attention and jump in. So with that all being said, let's try to share a screen one labeled tempo community call. Hands and hands and hands because it doesn't work if you don't say it. All right, got a pretty good amount of attendees here. So, I'm going to go through the agenda. If it happens to be something that someone here on the template team worked on, feel free to jump in and start talking about it. Um, so we did a go update. Was this a security one or was this a something else one? Uh, Dr. was security. Um I think it was something related to the OS uh name or the OS module package which we don't use. I don't think I don't think we used it but whatever was just kind of like may as well keep up to date instead of you know not being up to date I suppose. So I don't think it impacted us but yeah one or 2.8 will have us completely up to date on go. Great panic fix. I assume this is one of yours Joe. Yeah. So, we fixed a handful in the past um month, two months. I don't know how long I've been working on this. Uh but uh this was the final one. It was the hardest to find. It involved a fix here as well as one in the Park Go dependency which just got merged yesterday. So, uh I would be a little shocked if people saw it. You kind of you didn't have to have cache turned on, but page caching definitely made it way more likely. Um and we I believe completely patched it up. So, um, if you were seeing panics like this, uh, that should be fixed in 2.8. That's something we've kind of went on a tear about fixing panics. So, if anybody sees any after this, I think that would be, uh, interesting to rise up and notify us about. The worst was fixed in 27 and then there'll be two more in 28. And then we have a trace query fix. author Joe this uh existence problem. This is a weird one. Uh it was related to how we treated the concept of nil in the language. Um and how it was actually treated a little bit differently depending on where you put it in the query. So myself and Marty and I think Adrian got together. Siraj is on that issue. S has some really good input. Um, but we tightened up how you're allowed to use nil uh in the language which helped basically make everything else consistent and we've removed no features. We just removed the ability to do like group by nil or some other bizarre things that were technically possible before or uh but now you can only kind of compare equality or inequality to nil. So we've tightened things up and it was kind of a unexpected issue just weird behavior between those two spand food is not equal bar and then that second one didn't behave the way people would have expected I think so um anyways this should be fixed up I think it's all good it's it's part of the theme I think of 28 and generally the team is to improve uh correctness of the language both for metrics and just search and we some front end caching. We'll figure you Joe. I did that one too. Yeah. Okay. So, there's a bug in trace all metrics in 27 where cache keys will collide if you have more than one um job per block. So, if you have larger blocks, cache keys will collide. And it made the very first query correct. And then if you hit refresh, the second query would be slightly wrong. It'd kind of jump if you hit, you know, run a query, run the exact same one a second time because the cache would be incorrect. that the original query was right. And so this just breaks the collision. Um and so uh cache is correct now for skill metric queries. Check that out. All right. Memory improvements. Ah this is Marty, not Joe. Marty's not here because he's at Graphonic, which is awesome. Uh I can talk about this too. Does anybody else know what happened here? Anybody else is welcome to jump in if they want. Uh, I see that Zach left a knife. I don't know if that So, Zack Zach knows all about it. What's up? Well, you can see from that graph, uh, yeah, those spikes are removed. So, that's looking pretty good. And look at those benchmarks. Uh, this actually pretty incredible to go from what 26 roughly to six. Yeah. Yeah. Scroll down. Scroll down a little bit. I think that last benchmark shows the the goodies. Oh, no. That's actually not as good. The issue was with the pooling. So we were pooling objects too aggressively. And if you had extra very large traces, so like 50 100meg large traces, we would pool those as well and attempt to reduce memory. But this would actually balloon memory significantly um and with not much benefit. And so the fix was to just pool way less aggressively. And so we Marty spent some time kind of getting the balance right to um uh cool enough that it helped because some helps but if you do too much then suddenly you're holding gigabytes in your pool for no reason which is what happened here. Cool. All right. We got a Mac series. Jennif is spam on I don't think she is. I see her little icon. She is. Maybe not. You're muted if you're talking. Siraj probably knows about it. If because he did top if you want to talk Sash. Yeah. I mean max series is basically before this you could write a query that's like group by span ID and get like a bazillion series back and it would blow out your query front end and comp uh quer with this you now can enforce like how many series you want back so it will not blow up your cluster and also you will get meaningful amount of series back. If you hit that, we written a warning also. Not sure if the front end is there yet, but yeah, you will know that you've been limited and fam did all the work on this. Oh, uh, and I believe you also have the next one. Top K, bottom K. Yeah. So uh this is interesting because this now opens the door for what I call the second stage functions. So first stage is your TSQL filter qualifier. Second stage is your metrics. So you can do a trace and then you can do like rate by something and that would give you a series that is pretty much like a brome series and customers was like I want to do more like functions on those metrics that were generated on the fly. So we added this second stage functions and first two functions that are added are top K and bottom K. So let's say you write a query uh that returns 100 series but you're only interested in top 10. You can just do top K 10th and you'll get top 10 series. Uh and the top K and bottom K behavior is similar to bronchial behavior. Uh so let's say you do top K 10 but you see more than 10 series. Uh that's by design because we do top K per step uh per time step. So like there it's possible that you know you you have some series disappear in the middle and new series show up in the window. So overall number of series might be more than the K that you've asked. Yeah that's it. Uh the demo video shows it in action. You want to see who watch that video. I think having a video is super cool for anything like this. So I think it really like extra mile there. All right, what's next? We got something about this Qbased architecture. I'm sure someone here can chat about that. No. Uh, I added this point. I think I I kind of said Qbased architecture and prod because I think it's right on the edge. I'd like Mario to comment real fast. I think he's rolling our first production cluster to our what we call rhythm with basically our Q-based architecture. Don't tell anyone. Oh, my bad. Not happening. Yeah. Yeah. Yeah. So, um this is a uh longunning project in Tempo. We're changing the architecture to solve uh a couple problems that we have right now. And this new architecture includes AQ uh the couples uh read and write path. We've talked about it a couple of times. Uh but essentially we have half of the changes sort of which already gives us uh some benefits uh more correctness for metrics and more stability in in a couple of components and we're starting the rollouts to some production cells. Um we're starting with small ones just um testing the the process. Uh these changes that we're deploying right now have been extensively chang uh tested in in our internal classes and they've been running for over a month already. So we're quite confident on what we're deploying but we'll still being cautious and and rolling out slowly. Um but if it goes well, we'll ramp up um the rollouts of the the new architecture and hopefully we'll um like the benefits will key uh more and more production cells. So yeah, you can expect metrics to be a little bit more correct now than they were before. I I don't I doubt because I didn't I should have asked you beforehand, but uh I know we've done ops and ops was a very very large cluster, right? It's a couple hundred megabytes a second. Um do you have any idea what the TCO changes yet? Or maybe we could save that for next month. That might be a good thing to share with the community. TCO difference between current tempo and the the rhythm style tempo. Yeah, I don't have the knowledge right now. Um, we also need to account the the TCO of the queue and we don't have good numbers about it uh right now because we're sharing it with all the teams as well. Um, but yeah, I have a graph and and numbers and Chinese stuff for uh the next call. All right, we have a question from Andre. Are we already running RF1 internally? Uh well, yes and no. Uh we're running RF1 in some parts of Tempo. Um I mean I can speak about Rhythm for hours. Uh but to be brief uh we've only migrated for now uh the metrics generator and a new component called the block builder uh which is now tasked with building this new RF1 blocks. So what this allows us is to in the back end in in the object storage when querying uh not recent data to query those new R1 blocks instead of the R3 blocks from the Injustice. This is the the state of the project right now and this is what we're running internally for we've been running for um many weeks now and even more in depth and that we're rolling out to some production uh cells or clusters right which I think is basically RF1 there's a second like phase of this project which will reduce TCO more and will better consolidate the recent queries uh to the ingesttor But are we running our fun internally? Yes, in our dev cells as well as what we call ops which is like our internal operational cluster and then one prod cell kind of right in the middle of it I think. Is the is what's happening. Is that right? Should I bring that up or am I cursing it somehow? Am I jinxing it? Is it going to get paged? Yeah, we're on call. I think yeah, we we're currently only missing RF1 in recent reads. uh the rest it's hitting RF1 for uh the back end and even for metric queries it's also querying RF1 data y so yeah we're getting that cool and I put tempo 3.0 kind of at the end of this question I asked the question because I wanted Mario to answer it because I was pretty sure we're somewhere in the middle of and then tempo 3.0 is going to be based on this. We've talked about this a lot over the past months. Um, this new architecture is going to be complete. Once we have it complete and we're running it confidently, we'll cut a 3.0. Maybe this fall, maybe it'll be the last release of the year. Not sure. Uh, no timeline there, but we are running it and we're it is a major priority of the team. Um, I'd say it is the major priority of the team is cleanly swapping over to this. Um however I also put next set of problems because as we are seeing like the light at the end of the tunnel for rhythm um as we're seeing us actually start to roll it out and jumping the major and most difficult hurdles uh I wanted the community to know we're kind of thinking about well what's next and we're going to start seed projects to explore the thanks Maro uh start seed projects for and thought thoughts about what's next for tempo and the next challenge for tempo we believe is the read speed, the cost of reads and improving uh reducing latency on all queries um with an eye on you know we're going to say 10x because everybody says 10x but that doesn't matter. Uh the goal is to identify the major problems and to massively improve our read. And our current major problem and we've spent months repeatedly saying this, writing design docs, finding improvements that don't work out quite like we thought. And the major problem is that everything is everywhere because we have a giant block sorted by trace ID. So any service, any namespace, any cell is spread somewhat equally over this entire block and you have to basically look everywhere to find any kind of data. So that's the next set of problems we're really focused on. How do we relieve this? We have a small group this quarter. I'm going to start some experimental work in this direction. Um Adrian has some really cool ideas. Marty has some cool ideas in this area. So rhythm is number one project now cleaning that up. That's going to be our focus. And then this next step is read performance and scalability on the read path uh for tempo. So that would say I'd say that's the next year plus uh vision. uh we really want to deliver this much uh more performant uh query path as we uh finish up our RF1 uh work uh be uh any other questions besides me promising tempo 3.0 for OBSCON yeah sure you didn't mention the year OBSCON I guarantee tempo 3.0 will be delivered before an OBSCON nice to hear uh I do have a question um is the uh typo 3.0 So Q-based architecture is that replacing any current architecture or is there like is it a replacement or a new mode of running? Um that's a very good question. Uh I think that's something we need to discuss for uh 3.0. There are many questions regarding other modes of operation uh rather than what we run most frequently uh ourselves in Grafana cloud uh that we don't like we don't have answers for those um like monolithic mode is also something we're not sure what we're going to do with simple scalable mode my guess right now would be that it's going to replace the classic tempo or the current way of running tempo uh because maintaining both would be a lot of effort but I think nothing is decided yet. Um I don't know if anyone disagrees with that. No, I think that's I think that echoes my thoughts as well. I think nothing is decided is fair. I am leaning very heavily towards removing the old architecture for the reasons why I said uh maintaining two. We're gonna have two okay options and we're always going to be fighting bugs and improvements and performance on the old path is always going to be way worse than the new path. Um and I think it's just going to be overall a negative for the project is my opinion. Um uh and I think the right choice will be to completely invest in the new path. I think we can make it much better if we're fully focused on it and it's what we're going to be running anyway. And if it's what we're running in cloud, we work every day to make cloud more performant, right? That impacts our bottom line. So of course we do. And that means you all the community, the open source community gets all those improvements. Whereas the old architecture will just languish and it'll always we'll stop doing improvements. We'll stop doing bug fixes. We don't even know about bugs. we will file issues that we can't reproduce because we don't see it anymore, you know. So, my opinion is new architecture only because I think it'll make tempo better. Um, I think there's a little friction there. I know that imposes things on the community like you'll have to find some kind of Kafka implement implementation or just Kafka. Um, but I I think it's the right choice despite some of uh despite those issues. Great. Uh do we kind of open floor as we're coming here? Does anyone have anything they would like to bring up or any question they would like to ask? Nothing. Matt, are we getting beers tomorrow? Always. Nice. Okay, good. That's my question. I Yeah, here. Maybe even next week we'll have beers with Mario. Oh, I don't go. Yes. I was going to say don't make sure you guys don't miss the flight the next day. Okay. I've only done that once. All right. Uh if we don't have anything else, last call for question, comments. All right. Uh, thank you everybody for showing up for the um, May Tempo community call. This will be posted in YouTube here in probably a day or two. And I appreciate everybody coming and we'll see you later. Thank you. Bye bye.

