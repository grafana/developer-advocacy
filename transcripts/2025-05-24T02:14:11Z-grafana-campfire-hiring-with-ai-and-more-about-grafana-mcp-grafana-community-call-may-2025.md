# Grafana Campfire ðŸ”¥- Hiring with AI and more about Grafana MCP (Grafana Community Call - May 2025)

Published on 2025-05-24T02:14:11Z

## Description

In this Campfire community call, we will talk about the new and the future of AI in the field of Observability space and also discuss ...

URL: https://www.youtube.com/watch?v=XJbH7RGxcG4

## Summary

In the May Grafana Campfire community call, hosted by Susman Ahmed, the discussion centered around the impact of AI in various sectors, particularly in hiring practices, and introduced the new Grafana MCP server. Guests included David, Matt, Ben, Siril, and Sarah, who shared their insights on AI's role in recruitment, highlighting the challenges posed by automated applications and the potential for AI tools to enhance productivity. The panel emphasized the importance of genuine communication during interviews and the need for adaptability in hiring processes due to the influx of AI-generated applications. They also showcased the Grafana MCP server, designed to streamline interactions between AI models and various data services, which has gained traction since its release. The conversation concluded with reflections on how to effectively integrate AI in coding interviews while maintaining transparency and ensuring candidates' true capabilities are assessed.

# Grafana Campfire Community Call - May

**Host:** Susman Ahmed  
**Topic:** AI and its Applications in Hiring and Grafana MCP Server

---

**Introduction**

Welcome, everyone, to the Grafana Campfire community call for May. I'm Susman Ahmed, and today we are discussing a very exciting topic: **AI**. We will focus on two main areas: 

1. Where and when to use AI, specifically in hiring.
2. An introduction to the **Grafana MCP server**, which is a new and exciting feature.

---

**Guest Introductions**

Before we dive in, Iâ€™d like to introduce our guests today:

- **David:** I run the Grafana team, working closely with Matt and Sarah.
  
- **Matt:** Iâ€™m Matt Ryer, a senior principal engineer. I enjoy building things, especially AI tools.
  
- **Ben:** Hello, Iâ€™m Ben Sull, another Brit on the AI team. Iâ€™ve been on the machine learning team for about four years and now weâ€™re focusing on AI tooling.

- **Siril:** Hi, I'm Siril. I also work with Matt and Ben on AI.

- **Sarah:** Iâ€™m Sarah, a staff software engineer on the data sources team at Grafana.

---

**Recent Conference: GrafanaCon**

Sarah shared her experience at GrafanaCon this month, where there was a lot of discussion around AI. The conference had a very positive reaction to AI applications. It was a great experience, and I encourage everyone to attend in the future.

---

**Discussion Topics**

### AI in Hiring

The discussion then shifted to the impact of AI on the hiring process. There has been a significant increase in applications, making it challenging for the talent team to manage. The flood of applications is partly due to automation, where candidates are mass applying with AI-assisted resumes.

- **David:** The talent team is overwhelmed with applications. We are using Greenhouse to manage this, but itâ€™s difficult to filter genuine candidates from those using AI to enhance their applications.

- **Matt:** Itâ€™s a challenge to identify genuine candidates versus those who are mass applying without a real fit for the positions.

### The Role of AI in Coding Interviews

Sarah raised an interesting point about how candidates are using AI in coding interviews. Some candidates might use AI tools to generate solutions, which can obscure their understanding of the code.

- **Sarah:** Itâ€™s tricky because we canâ€™t penalize candidates for using AI tools, but we also want to ensure they have a genuine understanding of the code they present.

- **Ben:** The focus should be on solving problems rather than just writing code. Interviews should assess understanding and problem-solving abilities.

### The Grafana MCP Server

Ben introduced the Grafana MCP server, which is a protocol that facilitates the integration of various AI tools with Grafana. 

- **Ben:** The MCP server allows applications to communicate with various services without needing to know the specifics of each service. This standardization simplifies complex interactions between models and resources.

- **Siril:** The MCP server can be a game-changer for how we interact with AI in our workflows.

### Future of Interviews

The conversation concluded with thoughts on how interviews might evolve to accommodate the use of AI. Thereâ€™s a need for flexibility in the hiring process to adapt to these new tools and ensure that candidates are genuinely capable.

---

**Closing Remarks**

As we wrap up, itâ€™s clear that AI presents both opportunities and challenges in hiring and development. The Grafana MCP server is an exciting new tool that enhances our capabilities.

Thank you all for joining today. We look forward to our next community call. Until then, take care and have a great weekend!

**Goodbye!**

## Raw YouTube Transcript

And we are live again. So welcome everyone to the Grafana campfire community call for the month of May. My name is Susman Ahmed and in this call we are actually in uh discussing something very fun topic which is right now popular almost in every space or in every corner of the world or even galaxy is about AI and uh we will focus on the two core parts or two main parts of like where where to use AI when to use AI where it is used and uh for example is it used in hiring and if it is used in hiring then is it using in a good way or a bad way and second we will also touch some ground on Grafana MCP server which is something very new but very exciting. So uh before we start I would like to uh introduce our list of guest and and seasonals today. So uh first I will start with David. So David can you please give your intro? Sure I'm David. I I I run the Gana team. So, I work together with Matt, who's also on this call, and Sarah. Uh, Matt, do you want to go next? I'd love to. Yes. My name is Matt Ryer. I'm a senior principal engineer. I like building things. We're building things with AI and we are building AI tools and stuff. So, I'm very much in the sort of AI uh world in the galaxy, part of the galaxy of the AI. Ben, oh, Ben Sull is also here, aren't you, Ben? Hello. I am here. Yes, another Brit. Yes, another Brit. Two Brits, 30%. Yes, I am also on the AI team. Um, I've been on the machine learning team for about four years at Kafana and now we're kind of renaming a little bit, jump on the bandwagon of AI. So, we're the AI team now as well. Uh, working with Matt and Sirill quite closely on AI related tooling, which is which is super interesting. Yeah, specifically MCP servers which we'll come to later on in the call. Um, who wants to go next? Siril, I can I can go next. Hello, I'm Sil. Um, so I work with Matt and Ben on AI too. And yeah, it's been a it's been a while that I'm at Graphana. Um, Sarah, you want to I think you're the last one. Sure. I'm Sarah. I'm a staff software engineer on the data sources team in Graphana. Um, yeah, that's about it. You make it sound bad. It's great what you do, Sarah. Yeah, thanks. Assuming you're still the one doing it and you haven't outsourced it to an AI. Yeah. Well, no way to know. But, uh, Sarah, we saw each other this month already. What What happened? Uh we went to Graphfonicon which was pretty cool and there was lots of AI uh discussed there during the keynote. Somebody wants to talk about it. Well, it was such a it was such a great conference. I don't know if people at home have actually a chance to make it to a graphana, but you really should. It's so it's so much fun. Like there shouldn't be that much fun, I don't think. But uh but yeah, we have a great time and yeah, there was a lot of um AI stuff and the reaction it gets is surprisingly very positive. I felt like we're in a post kind of AI now with all the hype. We've we've had enough of this. Um but I think people can kind of start to see where the where the good applications of this stuff is. Like they can see like ah this makes sense. So yeah, it did get a great reaction. I mean the conference also was helped by you hosting it. Oh man, I got to say we got some I I reviewed some feedback and uh your name was mentioned a few times as well and positive. Yeah, positive. And then surprise. Yes. No, but also also serial. Yeah. Oh, yeah. Oh, I did I did nothing. I do I do want to know what was what was the best joke you made. Uh Matt Which one walked walked the best? The one that got the biggest laugh I'm not happy with, but I'll tell you what it was if you like. Some basically what happened was because we're doing Q&A all the time. So people start trolling me now like because I'm hosting it and they're just sort of trolling me as jokes. They know that I can see the slideo on the iPad and I'm asking get like the panelists and stuff trying to be all professional and meanwhile they're just trolling me with fake names and all sorts of stuff and silly questions and things. Um, and then one of I read out one of the questions that was also contained a joke and it got a big laugh. It was like a a better joke than I'd written. Do you know what I mean? But then I was annoyed that they got the laugh but they could just do that sat in the audience whereas I have to stand on stage and hold my stomach in for two hours at a time. two two full days actually but and then explaining that did get a laugh but obviously it's self-deprecating isn't it no that's good it's good um so uh I'm so curious or for the for the people who are watching this here like what what did we present uh on the AI side atonic well we did launch the assistant right sirill Uh yeah, so we we launched the assistant during the keynote that was that was a lot of fun. Uh and we we kind of also talked a lot about uh other uh part of our effort uh in AI. We um talked about MCP servers. Um well I think we I don't know if it's S like is it multiple MCP servers Ben? Is it just one or there are many that exist. We only have one. Yeah, I think we probably do have multiple. I think the K6 team have one. Yeah. So, we talked about MCP. Yas did a very good demo on how to use MCP um graphan server with cursor I think it was. Um so that was a lot of fun. Um yeah, there's a scattering of AI features around Grafana where because we we took this approach where we like we're not just going to do it for the sake of it. We are going to wait a bit and we're gonna see where we can really make a difference. The flame graph explainer stands out as one example I think is great. This is because flame graphs that you have to learn how to read flame graphs in a profile. You really have to sort of yeah study a bit and figure out what's going on. It's a perfect use case to have an LLM. What they do is they generate like a mini uh text based version of the flame graph. So it's just text. Give that to an LLM and ask it to summarize it better. or in English uh or any language probably and it does it and it's a great use case for it because it's kind of just translating text and stuff which is very strong at doing um and I think yeah so the like use case there's like cases like that that are just great I mean for me um uh when I was watching the keynotes where you and Sil you mean I mean Matt and Sil were uh uh uh showing this uh integration of AI in the graphana what uh what hit me really or was fascinating was that once the dashboard were created uh it also contains the description. This is something which looks very minor or uh you may say like you can ignore it but this is something everyone misses it because description actually is very helpful to understand what this dashboard means because AI can generate you a lot of text and des details but with that it can also save it in a graphana dashboard. So not only you just viewing your dashboard about some metrics or logs or traces but you have the full detail with full uh definition like what this is about and so on. So that was really an exciting uh feature for me that uh we can do this in Graphana very easily. Yeah, it it doesn't mind doing like the grunt work or like lots of leg work that you you would otherwise have to do and sometimes you skip it. And I find the same in coding like uh when I first started using AI assisted tools. Um I sort of noticed that it was very good at just the sort of boilerplate repetitive stuff. like if I was trying to write a unit test that contained examples and things like this, it was kind of great at doing that those sorts of tasks and and I could be more thorough whereas other times I might have skipped over some things because you know just to save a bit of time or whatever you get to cover more bases. I you know so yeah it's it's nice it does more does more of that leg work it doesn't mind doesn't complain yet. Are you saying you also used AI coding tools to build the AI assistant? Yeah, this is it. It's kind of like already um it's already AI building itself a bit, but we're but we're definitely guiding it. Like I feel like still you have to be the person who knows what what you're doing and you're still taming this thing. It's not like it's just telling you how to do it and maybe that will come. At the moment, you still have to do all the engineering. It's just doing the coding for you. Yeah, it's pretty good. Yeah. Um, uh, Surl, you were trying to evangelize a lot of this in the company, right? Um, yeah, I did not I guess I wasn't the only one. So I don't think it was very difficult though to advocate uh about this in the company but because it was was definitively game changer in my in my opinion for for the reason that Matt just explained like boil break code is is definitely something that can slow you down or writing test um yeah and it really started to be very uh interesting with cursor I guess you know when it's like started to have a great integration um I think there's other example not just cursor to Fair. I think Zed is another example. Maybe maybe Ben, you're using Zed. Are you are you on? Yeah, I'm using Zed at the minute. Yeah. Okay. Yeah. People use that, don't they? Yeah. Yeah. Yeah. So, the the integration did did a big change and so we started advocating in the company to use it because we saw a change in uh in productivity. Um Yeah. Yeah. Yeah. Yeah, there's something about that having the tool in the right place as well so that you aren't going out to JG GPT in a separate place to go and and you have that disconnected thing. The fact that cursor shows you a diff as part of the conversation. It's like this these are the changes I'm suggesting. Uh and that there's quite a big inspiration for the graphana incident um itself because we want that same feedback loop like as it's graphana assistant. Yeah. What did I say? incident. I keep it that keeps happening to me for some reason. Osman's what was the inspiration for incident getting things wrong uh like I just did and then we thought we need a tool to manage this. Um yeah sorry I when I recorded the video we have a demo video and a blog for Grafana Assistant I kept doing that same mistake um and it's too long it's one of those things I have no idea why. Yeah. Yeah. Uh, so but anyway, we took inspiration from those tightly integrated tools for the assistant. And if you look at the demo, you can just you could just Google it or duck.go whatever search engine of choice or chat gpt probably can answer it now. Um, have a look. Yeah, because tightly integrated. It's crazy. And it was very satisfying to get a big round of applause when it created that dashboard out of nowhere like and that's cool. Yeah. And it was it was a very detailed dashboard, right? They got like the units right and all this and then you made it change color and yeah that's a also that the colors were all like nicely done all in sync was really good. I feel like when I introduced the new themes, this was I think my uh that didn't go well that some of the charts are still in the old they're they're in the old color scheme, right? So, you have all the new fancy poppy colors, but then the charts are still the original gradients. Yeah, I think we need to need to change that up. Anyway, um yeah, so uh maybe we should talk about today's main topic, right, which is hiring with AI. And I think maybe that starts not on the interviews itself, but we're seeing um or like there's a lot of pressure right now on our hiring team because they are getting just flooded with applications right now. So this is just this is not just sort of 20% more. This is like 100% more. It's just coming in and it's just crazy with and like also you see at the same time I'm seeing on YouTube and I don't know Instagram how people are like this is how you can like really be super productive on like finding a good job. And I know that on our side, the talent team is just on the receiving end of this automation, right? Where people are just making massive sheets of and then just automating the crap out of this and just firing in all directions. Um and uh and sort of uh really also sprucing up their their resumes, right? Um and so this just this is produces so much work on the talent team now and it's um yeah it's been really a problem like the the and so we use greenhouse to manage all the incoming applications and greenhouse is like I don't know it's nothing really we can do like we can put a we can put a capture in place but that's but that's it. Do they uh do they do they need an assistant? the um what like greenhouse itself I mean yeah so like I telling people yeah go ahead can can AI detect when it's AI generated I mean it's I think so it's that's the problem like it's still real people right and they just get a lot more productivity and a lot more sort of reach and and can just in a short amount amount of time and apply for a lot more positions. They do a lot less sort of like a good check if this actually fits and are just sort of um yeah are just sort of mass applying everywhere. Right. Right. Yeah. Um and that and that's been it's been a real problem. Um I used to do that though when I was younger. Yeah. I've done that thing. Did Did it help? Well, some people this is why like the cover letters is still a question. Do you do you write a cover letter still when you apply? Do you do you tune your CV to the job you're trying to go for? Yeah. Or not. Well, I mean, and and this is where so this is where automation comes in, right? And this this can really help. Like this stuff could read our company values kind of a cover letter totally custom. And the problem is then it's also not genu genuine, right? And then uh and then this is what I talked to um uh our people team today cuz I also really wanted like maybe could we put up more like writing task or something like this. No, but then they just they just make this happen and and we're on the receiving end just make more work for us, right? And and just read more AI generated crap. Um I mean it's probably good crap like if it's really matched with our values. Yeah, but and we can't even do that test now where you ask them to draw a hand because it used to that used to be how you'd do it. You're like, "Can you draw a hand, please?" And then it's like got extra fingers. Yeah. Or missing few. Yeah. No, but they could still be a real person, Busman. They could have just terrible accent. Yeah. Yeah. My bad. My bad. Actually, there are people born there's a piece of piano music that you can only play if you've got uh an extra finger on each hand. Um that's wild. So AI would would easily be able to play that. Yeah. And then and then they're like, "Okay, you're too good at this." Yeah. I I think I think um using like what what we so before AI, we we used to like Google and get some information about the company or or maybe just copy temp the templates of a cover letter. uh but still we have to explain but with AI it feels like what yeah what David says like yeah like it seems like the person is like maybe way overqualified or already know everything what we are doing and this is something can be easy to detect because I feel like uh if someone says like hey um for example hey I know graphana but I do not know everything or what you do I think that's feel more natural like okay it's it's more genuine uh during the interview like ah I know some stuff but not all but if someone says like oh I know all the ST so it's like well I also don't know all the ST so how can you know yeah that's mean you're helping them you know you realize you're helping them I was just going to say it's like oh got to change this prompt I have to temper my excitement some some things you don't know about actually that is a good that is an important thing we've noticed is sometimes you have to tell it what it doesn't know so that it doesn't try because it's just so optimistic it's like, "Yeah, I'll go and I'll help. Yeah, I can do this." It's it's really optimistic. Yeah. No, but it's a it's it's absolutely wild. And so, we're not getting any help from Greenhouse at least yet. And then on the sourcing side, so when our talent team goes out and sort of looks for people, there's also now this sort of uh like Cumbrian explosion of just people who seem to fit our job description, right? And it's it's just fake profiles. And um like this the the best way they can tell right now is that the profile is only I don't know like six months old and there's like 20 years of work history or something like this. But but that's like that's about it, right? Like it still gets shown up. Uh it still gets listed in the search results on LinkedIn. And then what is LinkedIn saying? They say, "Well, if you find those, can you please flag them?" Right? So they're they're like outsourcing this to like the global talent teams of um yeah the on it's a perfect job for AI stinking irony but yeah it yeah it's sort of like yeah producing work for itself. Is there is there a point where they've they've got so good they've managed to fool us that we actually think that's so valuable that you can kind of harness the AI in that in that way and prompt it so well that yeah you you have a job that you've tricked you've tricked us so well that we should hire you for that. So we've so we've had some some recruiter screens where it was uh like let's say 80% clear that that that we reached some kind of call center where this was clearly some sort of job searching farm, right? And and so uh people then had probably a multitude of profiles up and when one of them bit one one of us then they had to answer that call that that recruiting call right and so like it and it it takes people now to detect this kind of stuff right yeah there are also I think stateun uh in different countries like groups funded doing this as well getting obviously in companies like Grafana. Um, yeah, I wouldn't be surprised. Yeah, that's a definitely an attack vector. Yeah. So, um, anyway, that took a dark turn. So, um, so Sarah, let me ask you, do you use AI assisted tools or are you just like purist? Yeah. No, I I use cursor and copilot and chat GPT. Um I think I sort of time. No, but just like when I get when I don't like one answer, you know, you go to the next one, right? And try different things. That's what I used to do with my parents. Yeah. Yeah. Yeah. Can I get $20 now? How about you? You say no, we don't have that currency. And I'm like, all right, I'll ask dad. Dad, $20. But it's like, "No, all I've got is pounds. Forget it." Um, apart from that, no. Yeah. I mean, I think um I think you know, with mixed results, it often doesn't tell me the the hardest parts, right? But a lot of the things that uh you know used to take forever now do not, right? And that's wonderful. So that's great. Yeah. So you wouldn't would you advocate for saying no AI in interviews or since it's a a skill that is useful do we let people would you let somebody use it in an interview? I think um it's a tricky question and I don't think that it really makes sense to just be like no any AI tool is bad. Um, and I also don't think it's really something we have full control over, right? Like I don't think we're at we're going to get to a point at some point where I don't know that we're going to I mean, unless we fly people into a room and lock the door, you know, like I don't even know, you know, how would we even know? Um, I think if a friend wanted to use AI and asked me if they should during an interview, I'd, you know, say like, think about what what are you trying to get accomplished? Because usually the code you're writing in an interview is not that much code, right? You're not generating a ton of unit tests, which AI might be really good at. You're having to solve a problem. And so, and it might only be like five lines of code, but do you know what those five lines of code are? Yeah. And will it be easier having seen it to read it? And will you be able to explain it to your interviewer? And I think some people the answer is yeah, totally. Like I feel super fluent in this language. I'm gonna see an answer and I'm going to be able to tell if it makes sense and addresses the problem and and we're going to have an interesting discussion with me and the interviewer during the process. And for other people, I think they're going to see the code and then they're going to be like, I don't know if this worked or not. I feel nervous reading code while somebody's watching me, you know, and it just trips them up more um than if they had taken a slower time to write it themselves and talk through their process a little bit. Um, but obvious I I mean not maybe not obviously personally I don't really care like for things like you know if you're just using it for adding semicolons or something. I mean like great like I don't want to proofread your code either you know but um yeah it's I think it's really tricky for takehomes which we sometimes do right so that's uh that's really difficult because we used to use the take-home I think like the code that you have generated as proof that you had read and understood what you had gotten right and so now we can't really do that and we instead have to ask you a lot of questions about what you wrote which can feel awkward as an interviewer to be like, "Do you really know what this is?" Like it's hard to ask ask that in a nice way. Yeah. It's It's strange, isn't it? Makes you wonder like um Yeah, like when you said about in person, maybe inerson interviews becomes more normalized now or more common. I mean, Grafana Labs, for anyone that doesn't know, is 100% remote. So, everyone's remote. Um, but we do have the impulse inerson on boarding. So there is because obviously getting together you really still can't beat that to physically be there. Do you think we might have inerson interviews only at some point open to the panel? Yeah. I don't know. Um, I feel like there's there's still such um because because it's also like the recruiting process gets more and more expensive like later on, right? Like then also hiring someone and flying them or just before uh hiring someone and also flying them somewhere just gets really complicated compared to what we do now. So I think even with us trying to figure out if someone is is like really relying also on AI to get through the the process, I think we still have a lot more I think maybe effort that we can we can do there to to to kind of sort this out. But uh but yeah, as Sarah was saying, there's really a big difference now already in the between the live coding challenge where I think you like you could do it without the LLM, but you could also uh do it really well with it and then also just have that experience together and then talk through like oh like this was a good suggestion or this was a bad suggestion. Uh whereas when you get the take-home result, right, like it's really it doesn't like you like you don't know what happened. Yeah. And also uh I have a confession to make. Today was the first time I tried cursor and I tried in our take-home challenge and it took me or so like the first thing I asked it like can you refactor this to TypeScript right and uh it did it obviously in like 4 seconds but then uh so this is like equivalent to what you were saying Sarah also like do you just use it to fix the semicolons it did a lot more it also ref also did the refactoring which is the first step in the challenge right so um And uh and at that point I just also felt quite sort of confident. It's almost like these whimos now, right? Like the first time you're a bit skeptical and then once you've once it's like left the block that you started from, you're like, "No, this is the way now. I'm just I'm just relaxing and letting this thing drive." Yeah. Yeah. That's what you got to be careful for. But the Whimo thing is totally I don't I'm now nervous if there's a human there driving now. I'm like I don't trust the humans. But also um like if we if we see candidates who like take uh use AI or they take uh take away assignments like once they submit that can we identify in a way like hey why specifically use this function or this logic uh why you didn't use the other is there a way like we can filter and detect like why specifically use this function here? So it's not so I mean so that's still part of the actual interview, right? Like even if you do the take-home exercise, you then go through the code together, right? And so uh and this is true for both the live coding challenge and the take-home is that the coding is just the stage, right? Like this is really just what we want you kind of to go through to then have something to talk about and then the way we talk about it is really is really the meat and where you can show your understanding. Um, and I think this is actually where in my experience that I had today with cursor where I was just like happier and happier to accept all these suggestions, the more I felt removed from understanding what this code was doing. Um and uh and so I think you then and to the point where then it ran into problems when I asked it to also expand uh the test coverage then it just got into I don't know some weird some weird loop where it was trying to use one DOM function and the other just went back and forth and like couldn't really progress and but at that point I really wanted to jump in but but the code has moved so far that that I had lost touch and like I know what I was doing anymore. And yeah, I should just go back to management. It's so interesting. I've had that too. Uh and yeah, I think this this is it. You get better at prompting it as well. So you get you it becomes a skill that you do up level as you the more you use it. There are things you can say that get better results. And sometimes I'm I'm very specific and and I'll say, "Don't make any changes. Just explain something to me." And it gets carried away. It'll explain it. And then it's like, "Oh yeah, so look, I'll just change all the and" and then I look and I'm like, "Yeah, fine. That's probably Yeah, that is exactly what I would then do next." But that's not the point. I asked you explicitly not to do that. Um, so but it's they're very eager. But yeah, that's true. You can get if you if you let it run away, then you you you stop being able to like be Yeah. really in control of it and I think that is that is a one of the dangers of it. You have to sort of experience that and then uh avoid it. Yeah, I I kind of I kind of agree with you that it's a it's definitively a skill because in that moment David I would have started to use it uh to understand. So you can ask question about the codebase and like you said Matt but you can um you know you can choose like the agent type and one of them is like just ask and he has he has not the right to use uh modification tools uh and and when you ask question is actually going to be very good at giving you a summary if not like a diagram of how how this thing works. Yeah. So tip for next time David ask it ask it how this thing works. You're very disciplined with this. I would have just said try harder. Oh, I've been Yeah, I do that too. I do that, too. Yeah. But, you know, have you noticed the more polite you are with it, the better the results? Because this apparently is a real phenomenon. Really? Yeah. If you're if you're polite, you're making No, no, no. I'm I'm not. I have a theory that um most intelligent like it's trained on the internet so really it ought to be horrible but I think a lot of the intelligent stuff when it you know is more polite. You mean if you ask it like an English person it goes to its English knowledge base. Yes, it literally is the the the context that you put in is is the waiting of where it then yeah the probabilities then fall differently and it it's somehow superior to the rest of the world. No, I wasn't saying English polite. Did you uh do you do you add mate at every end of your pond? I have done that once and it just ignored it. Yeah, I did do that though, but just I was like I felt like I did feel like for a minute I was just chatting to a person. It was one time. Yeah, that's good. Uh Sarah, uh do you do mostly front-end interviews or or back end or sort of like total mix? Um I think I've done mostly front-end interviews. I think for a long time I was like the only one in the Eastern time zone who could do a JavaScript interview or something. There's like there's a while. So, I did a bunch of them. Um, yeah, I think uh so I and I think our front end is mostly take-home. Yeah. Uh things and so I think that Yeah, there were I think there were a couple of like I didn't quite realize that people were using these tools. I think I I took I tried doing it with cursor in um in January and I hadn't used cursor before. I just like Googled like or I think I went on Reddit and I was like what are people using for these things and after that I now use cursor because I was like wow this was really quick like I think we asked people to spend no more than four hours on some on this take-home and you know I'm a very slow person and managed to get like two out of the four things we suggest people look at within like 2 minutes or something like that. Um, and I was like, "Oh, wow." Okay. Um, you know, it took a lot longer to finish to do all four. But, um, yeah. And then in doing that, I noticed things that I thought were very unusual that I had recognized like patterns that I had seen in previous interviews, which was kind of wild. Um. Uh, oh. Yeah. So, and then and and people who I was like, "Yeah, I mean, you know, that was a fine, you know, imperfect but totally fine solution, you know, like oh no. Um, so that was interesting. And there were things like using emojis, right, of like which I thought was like, "Oh, what a cute human." Like what a nice little touch to add a bunch of emojis to your little widget app that we work on. And it's like that's AI's always always add emojis to things. um lots of like overengineering to show off design systems and things to be like look I know how to use a a factory pattern or whatever and it's like that's great if it makes sense um and you can explain it and then you'd get to the interview and you'd be like oh this was a cool choice um you know and they'd be like what choice and you're like oh well you called this variable this thing you know yeah you called this function and the poop emoji. It's a really interesting interesting choice. Yeah. Do you think it go ahead Sarah? No, I was about to ask like do you think we should do we should make it more difficult the take-home assignment? So that's that that's what I was thinking. Is it time to say, "Right, you can be AI assisted, but you've got to build a fully functioning app and get it in the app store just raise the bar of what's expected and get a thousand users. I'm going to get a hacker news and get funding." Yeah. Something. No, it's it's interesting. and and but um honestly this was my experience today where this like fact of that you sort of feel that the AI has taken over is definitely a challenge and that and but ultimately it is around someone else is suggesting code changes right and kind of reviewing those I think that might be the way forward I just we we just haven't found a good format around this where it's not actually solving the problem anymore, but it's about like if this thing is suggesting this like why are these all good ideas or are these good ideas, right? And so for me that's that's one possible approach. I mean also go ahead. U yeah I I actually just wanted to say what Sarah said like she found this pattern that oh why everyone is so maybe a good use of AI will be like okay you can use it for learning but maybe you are not using in the right way so what we can also do uh is that we can examine all these uh takeaway assignments and ask AI if uh it's intelligent enough to recognize some matching patterns or same tradition ition of line of codes or functions which like looks like very very fishy or very regular in every other exercises. So that could be a good use of AI. Yeah, there are cases where that they are doing that they're using it to uh and the companies that have essentially that they training AIs to to be able to detect fake stuff or AI generated stuff. Um but it's going to be like a b constant battle that's going to continually rage on. Do in do interviews need to sort of fundamentally change? Do we have to do something different for interviews now? I mean I would say no. uh through I was I was wondering like should we maybe try to judge if someone is knows how how to use LLM during interviews like how to use AI assisted tools like you know if someone says like yeah I I use every day maybe we can try to judge how well you use it like try to ask about which which model should be using in that situation which I don't know what would be the answer because I always randomly use any model yeah but yeah but that's it could the interviews sort of could that be part of it like it is a skill that it's not easy to to it's not just like in some cases it just gives you the answers. Uh but to actually use it properly, you really are part of that mix. So I feel like you could have that on a remote thing where you just discussing it and you see what's happening. Doesn't work for the takeaway. So the so the skill you will be testing is making sure that the interview is not losing touch with the codebase, right? Like David David will be will have failed that test. His first time. What do you think? But but I made it um so I made it look because there is like an extension of the coding test where you show the items and it when I asked it to do this it like imported bootstrap and all this. Um and then I asked it to make it look Swedish and I think it it did a pretty good job like it just sort of gusted it to what? Make it look Swedish. Yeah. So, so it made it look like styled it blue blue and yellow because you know we're good at Swedish roots. So, yeah. Yeah. And normally normally in the interviews people have to make it look Swedish manually, don't they? But you can do it now in two minutes with an AI. Well, well, so some some variations of our front end interview have a live coding part. So, kind of like depending on how far they get. I don't know if you ever do this, Sarah, but we asked them to to do tweaks on the on the solution that they submitted, right? Yeah. I think that sort of has to be mandatory now to some degree, right? because it it's sort of like um I think we can be like a little bit more open-ended about how something looks or where, you know, where we go with it, but I think you kind of have to at least pseudo code something at the very least, right? Um if not, you know, extend it, which I think is pretty doable. But is is it valid for someone to do that in a like in a live coding interview to just use cursor, right, and ask it to do what you've asked them to do? Would you say that's valid? It seems like that is valid really. Like you're would our job as engineers isn't to write code. It's to solve problems. And if the way to do that is by using a different tool, then that's fine. It'd be it'd be fun if the interview is like, can you can you just repeat typing exactly what you're saying? I think the tough thing, sorry, I was going to say I think that the tough part for me is not that people are using AI. It's for people who uh tend to be a little bit more reserved and who don't talk through their thinking as much. And I think previously we could just sort of see them code, right? And so, okay, maybe this person doesn't talk a lot, but they they do answer questions and they're clear. Um, and we would say, okay, like, you know, they they're a little shy, maybe they're a little nervous. Um, and that's, you know, that's okay. We don't need everyone to be on a live call, right? Um, you know, we we're happy to just have people who are great coders and who can very quietly and comm, you know, communicate their thoughts um with documentation with just talking to people they're comfortable with. And now I think if you're somebody who feels very comfortable sort of reading aloud what an AI generated and saying hm what does that mean? Oh I see what they're saying you know like you can talk through your thoughts very well then I have no problem if you use a AI because I can understand what you're thinking and together we can collaborate and the problem I have is for people who are more reserved. I really don't want to penalize them because they're not somebody who speaks out loud as they think. Right? that's that people process information differently. But if they don't, I really have no idea if they're just like running it just like asking another side panel somewhere on their screen, you know, what should I say to this interviewer right now, waiting for it to process. Yeah, I think it's really worth like underlining like why this is important, right? like we're like a we're remote first company or remote only and sort of collaborating and explaining sort of our approach to the solution that we're building is very important uh because we can't just go over to someone else's desk and sort of look at that how they're doing it. Yeah. I mean honestly this is why a lot of people get hired through open source is because you have that experience of working with people. Um and that's why that is a very you it's a big shortcut frankly. So that's my always my advice to people when they say how should I get into stuff like what should I start doing? It's always like get stuck in with open source things, build it, join teams basically remote teams is what they are. And Grafana is kind of that um model really, but as a company um yeah, I guess that's like the the ultimate question like should we just hire from the community as opposed to it's definitely a big it's definitely a big time saver, but um these tools are changing also all the time. like Sarah you mentioned you hadn't even used it and then afterwards you noticed some people maybe were using it and given similar patterns that you're recognizing uh that's that'll keep happening too like these different technologies are I mean every day almost every week at least there's new something new space is changing so fast so whatever we do with interviewing we're going to have to be a bit flexible I think with that it's not easy is it no it's Not. Yeah. But um to to fully answer Ben's question, I think if someone wants to use this tool, like we absolutely should allow it. Um but it should be also visible to us, right? And then you can you can vibe code together. Yeah, transparency is the thing, right? Like if someone's trying to mislead you by saying they've written some code manually when they haven't or when they don't understand it, that's that's no good. Yeah, that's a red flag immediately. Yeah. Yeah. And I think it's a bigger problem with with written things and interactive things. It occasionally is a problem in an interview, but you it's so easy to just produce code nowadays. Like you can do it with no skills whatsoever. You just need to tell it to make some code and it will do it. It's not a good measure anymore. Right. That's what I mean. The measures changed. I think Yeah. This is why like I read a thing about uh essentially engineers are now thinking more producty because there's less time sort of uh on the actual coding they actually have more capacity now to think about right what you know what what does this product need to do what problems do I need to solve and a bit more capacity and stuff so that's quite encouraging because I always think that's a healthy thing for everyone to do anyway I mean after my career escapade this today I thought I thought we're all doomed as like not like as programmers, not that we're being replaced, but more that we're sort of get inundated with just a lot of generated code that we have to troubleshoot because I feel like that'll be our future. Yeah. But AI reviewers are also a thing now. I use Copilot in GitHub. It's built in to do reviews, review aer PR, and it comes back with some really useful stuff. It really does. And we aren't even customizing it yet for our own purposes. Cursor has rules. Uh Sirill, you've played around with these and I've played a bit, but we don't yet have like a set of rules for the Reaper that just describe like these are the pattern. These are the patterns we follow. This is what we prefer. Like don't build abstractions if you don't need to keep it flat. You know, these sorts of like engineering decisions. Uh as we start to build that stuff in, I think at the moment it's kind of inferred by looking at other code that's similar. Um, but I think yeah, it's just all good stuff. So, it's certainly like wouldn't want an interview process to discourage its use. Yeah, I'm kind of with Ben on like the code really isn't the point. The point is the problems you've been able to solve with code. Um, but you you can't just can't just be chaos. Just let anyone do anything. Yeah. Uh Ben, we also have you here to talk about the Graphana MCP server, right? Yeah, we do. Um this has really taken off recently. Yeah. Yeah. What is MCP? That's a good question. A good starting question. Um so MCP is a protocol that was released by Anthropic last autumn or winter, so 2024. uh and it's basically I think it's kind of a solution to the M byN problem where you have like M models over here and N clients uh LLM based applications right and they all want to use tools and they all want to like provide resources to each other so you may have uh your Postgress server which has a bunch of tables and you have your Grafana instance which has lots of dashboards and you want to be a your application wants to be able to you want to provide any of these AI applications like cursor or zed or clawed with access to all of these things, but you don't want to make m* n different combinations of things. So, MCP is a way of standardizing the way that these types of applications talk to your various uh sort of services like Postgres, various different things, GitHub. Um, so what it let you do when you're an MCP server author is write tools which can then be consumed by LLM powered applications. Um, and the applications don't need to know which servers are installed or what the tools do. They don't need to know anything about them. All they do is offer them as a list of tools to the model and then the model can just use whatever it thinks is best. And that allows the user to like configure lots of different MCP servers depending on their use case. So if you're using cursor to work on a backend application, you may plug in your GitHub MCP server, your Postgress MCP server, your Grafana MCP server and ask it to add observability to my database queries. And your agent will go away and look for your database schemas from Postgres. It will look for your metric names and like all your different configuration from Graphana and then it will add those things in and maybe create a GitHub issue or create a GitHub pull request for you using those three MCP servers and all without cursor having to know anything to this. And you could also do it with clawed code or with anything else that uses an MTP server. Sorry, that was quite a long so this uh No, I think this is great. Uh, and this MCP server is, I believe, also like free to use. It is available in Grafana OSS. That's right. Yeah. Yeah. Yeah. So, it's a standalone open-source uh project that we've released. Um, we released it after our hackathon last last December um and open sourced earlier this year and it seems to be getting quite a lot of usage. We've had some community contributions. People are adding various different tools to it. So, at the minute, it can do things like query Prometheus, query Loki, load your dashboards, update dashboards for you. Um, it can figure out which data sources to use, which Prometheus metrics. Um, and it can do things like listing alert rules. There's there's loads of functionality in there. Um, and it kind of lets you have like an observability agent in in cursor or in in Claude. So you can ask them to debug this issue for you and it will it will try and go and run a bunch of queries. I have a I have a question for you Ben. Can you when you when you use the MCP server from Graphana, can you select the tools that you want to use only if there's like a long list of tools and I wonder like how does he know which tool he should be using too? Yeah, you so you can filter the list of tools down by category at least. So we categorize our tools in in our MCP server like by searching or by querying or using Prometheus ones. Um so when you start the MCP server up you can say I only want these categories because I don't use Loki or I don't use dashboards or you wouldn't use dashboards. Uh but then it's up to the model to like it knows what tools it's got and it's up to the model to choose the right ones really. Um there's this yeah I guess there's this interesting question of like what happens if you have 30 MCP servers and 10 tools each. You then got like 300 tools for the model to figure out. Um and I think models are getting better at that. They a couple of years ago they were useless and they would never use tools and they they would struggle if they had more than one or two available. Um nowadays you can get by with a lot more but it's this balancing act of not overwhelming the model with too much choice. Yeah, because each tool comes with a description as well, doesn't it? So, it's like a prompt about when best to use that tool. Um, and so you are kind of then influencing it. And this is this is something that we're very careful with with the assistant project is uh if you just if we just put loads of capabilities in there, it might just get confused. It might just have too many things it can uh it can do. model context protocol by the way MCP for anyone that hasn't searched it yet on DuckGo or whichever search engine is your favorite or llmmed it let's be honest can you can you change the description or you have to take the description from the server like it's provided by the server yeah I mean I guess as as the application if you were say you were developing so hypothetically uh an LLM powered application You can do what you like, right? Because you the way you would use MCP servers is you would query for all of the tools from those MCP servers and it provides you with like a list and then you can you can just change those strings if you want. Um you're probably meant to treat it like a a static or dynamic list but like you don't know what any of that is. It's kind of opaque to you. Um yeah, I guess I guess that's another use case for MCP, right? Like people can build on it. It's not just for using with like cloud desktop and other other tool but you can also build on it. Yeah. Yeah. Exactly. And there's other things that it can do as well not just providing tools. It can also provide prompts. So if you have some useful tasks that you know how you know what the sort of prompts that work well for solving specific problems. You can provide pre-anned prompts that will that will be used that the user can select. Um, you can even do things from within an MCP server, you could make a call back to the client to ask the LLM how to solve a problem for a tool. So if you're if you're say your tool is like query Prometheus and someone just says ask me I don't know solve this problem for me. Um, the query from Prometheus tool could then go back to the LLM client and say, "Please ask the LLM how I should solve this problem by querying Prometheus." And it might spit back a query for the to then use. Like, it goes both ways. It's kind of cool. It is cool. I was I was surprised to find out that they're like binaries that run a lot like on your machine, aren't they? Yeah. So there's a few different ways of running MCP servers. Um the sort of beginning one that everyone is using right now is like you say is is you run them locally on your machine and they communicate. So the the application cursor or whatever communicates using standard input and standard output um with the MCP server process and they all just run in subprocesses. So it's kind of like running unsecured code on your computer which is a bit scary. Um, but MCP has this like decoupling that they use called transports. And the standard input transport is the default one that everyone's using now. But there's also like HTTP based transports. So you could host uh an MCP server somewhere and anyone can just connect to it using HTTP as long as they have the right kind of orth and all that stuff figured out. Um, so you don't have to run it locally, right? Thankfully. Um, but you probably will if you're using it nowadays. Um, yeah, the HTTP one makes sense as to what I assumed it was. Yeah. In the in the beginning, but it makes sense because it's about integrating, isn't it? It's about integrating into whatever dis file system or because that's the thing. Yeah. It's a big difference as it's integrated. This is what Sirill was saying earlier and we noticed this with cursor and warp and these kinds of tools. Um it's the tight integration and how well that's done that makes a big impact on productivity, not just having the LLM there, you know. So yeah, I think Yeah, 100% agree. Yeah. Yeah. It's good. I think I think Sorl is coding right now. I think he's he's accepting I see him click and curses. I am sorry. You can hear my click. Um, you can do a Graphfana campfire live and be coding at the same time. Yeah, I was accepting everything. So, well, I I have to inform you guys like we are already a bit ahead of time like 1 minute ahead because there's other community calls scheduled after this one. So uh um we have to say bye for now but uh I think or is there anything someone wants to share or important topic before we close or something we missed to inform the audience? I think we're good. Um yeah the I'm excited about I'm excited about next month where the Gfana department has its offsite. We haven't had this in a long time. And also bringing everyone together, which is going going to be pretty good. Yeah, we'll see which of us are just AI and which ones are real. By the way, do we think we've hired anyone? Have we been tricked yet? Do we know? Do we know? Has AI tricked us yet? In the hiring process? Uh we so people have definitely arrived in later rounds where it was a bit weird and that definitely got caught out and and so a lot of it was just being very slow to answer where it was clear they were running that question through something else, right? And then when they gave the answer, it felt like they were reading from a screen. Yeah. Yeah. So you got to be better than that. Got to be better than that. Yeah, but I'm just wondering if anyone is better than that and they're in. Yeah. I mean, they might hold five jobs at the same time like Sir does. Yeah. Yeah. I'm sorry, man. I don't want to make you late for other these other community calls. No, all good. No, yeah, we we have like this schedule defined in the in the community calendar. So I don't want to folks to just still wait here or or those wait for us. So yeah but I think uh we learned a lot. I think we had a very interesting talk about like the usage of AI both the good side and the dark side and uh there's much to improve even even on both side I would say to to to keep the battle going on but um yeah uh and the second is like yeah definitely check the graphana MCP server. It's something new but looks very exciting. I'm actually trying to learn it. Uh it's uh it's something in my in my not in my OKR but in my wish list. So yeah. Uh yeah, that's all for this uh community call for today. Uh we will join all of you next month. Uh yeah. So till then take care. Bye-bye and have a nice weekend. Thank you. Bye. See you next month. Goodbye. Bye-bye. Bye.

