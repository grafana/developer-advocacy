# Grafana Alloy Community call 2024-04-17

Highlights - What is Alloy? 0:00 - Clustering Q&A 6:48 Helpful links: Join our next Alloy community call ...

Published on 2024-04-22T18:26:09Z

URL: https://www.youtube.com/watch?v=iBYyv-X4y74

Transcript: hello everyone and welcome to the aloy community call in April this is our first technical alloy uh call since we have just launched it and uh changed the essentially the name from agent to alloy so I just want to give maybe a little five minute Spiel here at the beginning about what alloy is um positioning alloy has an open Telemetry col ctor that includes all the Prometheus Loki Tempo Prometheus kind of functionality that existed before in the agent at its heart it is essentially flow mode um with uh the static mode stripped out and moved to New repo now part of me wants to get into a little bit of reasons why we did this and some of it is we wanted to have a kind of strong step forward um and make sure we kind of have a clean separation from static mode static mode is the yaml based very Prometheus inspired mode for the agent and the alloy uses alloy files that were formerly known as River and gets rid of all the static and is fully committed to um running alloy files and open Telemetry and kind of building programmable pipelines now part of the reason we had this 1.0 and the rebranding is to have what we're calling stability promise so beforehand in the uh like you know prev1 days we would have a breaking changes and we would try to limit those but they happened now we are promising that we will not have any breaking changes to anything that is labeled is stable and that is not a security issue or maybe if a setting or something was specifically a bug or added an airror and wasn't used we may remove it so there's a few car out exceptions but for the most part we're going to follow semantic conventions there speaking of stability levels you know how one of the actual changes between agent flow mode and alloy is you have to opt in for stability levels so this could be where you beforehand you could use experimental or beta components and everything was fine now you have to add a command line flag to opt in and the reason we do that is because those components do not fall under air stability promise we can change those beta has a less chance of changing experimental have a high chance of changing you know because we're playing with them and trying to figure out the best way to do them so let's and then everything else is pretty much the same um it is a different image it is a different executable there slightly different naming there um we did change some of the pathing on Windows um it's down under grafana Labs folder the names of the executables are uh follow a slightly different Paradigm um but functionally it's the same as agent flow mode speaking of agent flow mode and agent static the those we will support for 18 months with maintenance this is probably security fix and Bug fixes there's not going to be much in the way of new enhancements that should come as a surprise to no one especially with static mode we haven't really made many changes to that in the past six months to a year and now we're kind of extending that to leg the old flow mode with most of our development going to be an alloy the conversion from the two ideally should be changing your executable or image maybe adding that stability flag and you can pass in the same configuration and with I think we've removed one component the Fe spere component so if you're not using that your configuration should follow through no problem um the only other thing is removed um modules classic and are now using the modules V2 which allows you to create custom components instead of more of a input output pipeline um it's more function based I don't know that might be the best way to describe it but allows you to create custom components and we have some documentation on that and it will probably be a point for a future uh Community call here U looking at my list that's that's probably the most things we have a uh we have FAQ out we have several blog posts we have several videos that um I did for kind of giving an intro to aloy but if you're familiar with flow you're 90% there and then we have the we had a blur B the graphicon keynote and then myself and pascalis gave a overview on alloy and that's currently being processed and ideally will be uploaded to YouTube in a week maybe two um the cliff notes of that are is basically we cover the syntax um how we built the ecosystem and what we consider some of the magic sauce over top the open Telemetry collector which be our alloy runtime and our alloy specific components like remote Vault clustering modules kind of those Advanced features which has the weeks go on WE and Community calls go on we'll dig into those a little bit deeper in each individual one all right that's really all I had um like I said I expect to be pretty short um I'll open the floor for any questions if you have any questions you can feel free to type them in the chat or bring them up and I will give a minute or two for anyone to add those we're waiting uh we're going to keep the same release Cadence so every six weeks we were releasing version we will keep that roughly in line with open Telemetry they have a slightly different release Cadence so we may be a release behind on occasion but um we're trying to limit those gaps where it's months um between updates so that's something we're actively working towards hey man uh this is Harish one question on the clustering SI um so what do you guys recommend like uh how big a cluster size can be because uh we have in a production cluster where we'll be getting 500k preus metrics per second this is more my question is more towards like the Prometheus collection based on clustering B I'm not worried about the hotel thing I'm just focused on the prus metric collection so what do you recommend over there like can you do you recommend like running multiple clusters so like one cluster with a lot of p in it so for so we run um a lot of clusters internally across many different regions so in each one of those we have a we generally have four um we separate Telemetry types into their own kind of name spaces um but if we're speaking particularly of Prometheus metri which are the only ones that really support clustering really well um that we have the most use case for um with clustering um we have an HPA um to scale it up the I would say we run anywhere from 12 I don't know to 30 40 instances of alloy in a cluster and we have tested it I think at least to 150 maybe 200 um the overheads relatively small um on on keeping the cluster um kind of in sync the only the only thing is is whenever you scale up or scale down or add nodes move nodes um it's easier to do that one at a time um if you do a whole lot at once maybe some there's there can be some U more movement of the metrics than we would like um and occasionally you can see some some stellus markers move if they move from a metric move from one to the other or Target move from one to the other but we generally roll out one one at a time and that seems to handle it pretty well um did that answer your question uh so what are you saying is like the gradual increase or decrease wouldn't add any latencies rather it's the sudden Spike of p with joints would add some disturbances is that right yeah I I would much much more goes for gradual increase and decrease than large increases and decreases um go ahead yeah and what size do you recommend like the max do or should we test it out and see like how it behaves yeah I mean that's always tested out as what like I said we we run I don't think I can give exact numbers but we can we we run relatively large scale with 30 nodes um now those nodes are relatively well provisioned um you know um lots of CPU uh gigabytes memory um you know in our largest ones like many gigs of memory um so it's it is better to run fewer large nodes than it is many small nodes I would say Okay um you're you probably have less churn that way um but it it is kind of dependent up on your environment now I know we have in the pipeline to release some documentation on WE T-shirt size air clusters so we have like I think it's four or five small medium large extra large so four um rough categories we do based upon the number of metrics in the cluster so we're kind of we're looking at how we can make that into a Blog poster article um that kind of will give more guidance that's like hey if you're roughly handling this number of things you can this is what we recommend or if you're handling this number of things this is what we recommend now the only thing with that is that is only for metrics um if you're trying to do like an omni cluster where you're handling like logs or profiles profiles especially can have a high CPU cost and that changes the equation but we'll we will release something um that's some guidance on rough cluster sizes per um kind of your environment the expected metric load and the expected number of targets gotcha see if I can pull up maybe share a little a ballpark figure of uh how many metrics did you say you were running or looking to run yeah so our largest cluster has 700 nodes and probably you're looking at anywhere between 300 to 400 K metrics oh K uh K metrics okay yeah you can handle that no problem uh I would three or four or five nodes see how that handles I think you depending on how much you provision them I think you'd be fine there um I don't think that would be a problem at all gotta okay um let me see [Music] here I was trying to get actual yeah D if I can get a ballpark number here mhm yeah just to compare with the Prometheus operator right right now even one TB of memory is not enough for handling that that many metrics yeah yeah so we're we're handling like hundreds of millions of metrics in in our clusters um and a single cluster um with the clustering enabled now granted you need more memory and CPU for that but from a scale perspective it's no problem um handling you know hundreds of million gotcha but I honestly with a few hundred thousand we the um guidance is generally four to eight kilobytes per per sample that you need so you can probably extrapolate um um how many nodes you need based on that based on how big you make the nodes um and that's clustering doesn't really change that math any um and then CPU it kind of depends on what redx and relabeling you're doing if you're doing very little you can get away with a lot with you know very few CPUs if you find your your you're got extremely complex relabel rules then you can up that CPU the other thing you can do is on the relab um components there is a cach there you can increase that cash um and that'll reduce the CPU if you've got a lot of um a lot of U metrics so I would if you're running a few 100,000 I I would honestly make that cash depending like five times whatever your single largest scrape is um but that's a number you can you can play with um to kind of tradeoff between CPU and memory gotta yeah it was always the memory CPU yeah CPU is CPU is generally only a problem if you're running profiles in their experience are really really lot like thousands of Rex roles um memory is almost always the thing that's in in in tight demand um and we do there's some changes that we need to import from Prometheus that will get some savings to memory um they're they're like single digit percentage changes um but you know gaining extra memory here and there is always good yes of course cool uh do you have anything any other questions yeah I think the other questions you answered that on our slack channel so I'm good thank you yeah awesome all right then uh we will cut it here I appreciate everybody showing up and this will probably go out on YouTube and a few days and we'll see everybody next month thank you all for coming thank you man thank you D bye

