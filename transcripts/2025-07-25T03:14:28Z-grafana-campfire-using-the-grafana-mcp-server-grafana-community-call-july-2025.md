# Grafana Campfire ðŸ”¥- Using the Grafana MCP Server (Grafana Community Call - July 2025)

Published on 2025-07-25T03:14:28Z

## Description

In this month of the Campfire Community call, we will exploring the Grafana MCP (Model Context Protocol) server - an ...

URL: https://www.youtube.com/watch?v=6b94tox6Wk0

## Summary

In the July Grafana Campfire community call, Usman, Matt Ryer, David Kalmid, Lucas, and Jana discussed the Model Context Protocol (MCP), a significant development in integrating AI tools with Grafana. The conversation highlighted how MCP standardizes communication between AI applications and various services, allowing for seamless tool interactions. The presenters shared insights into the evolution of AI models, the benefits of MCP in enhancing observability workflows, and real-world applications demonstrated through live demos. They showcased how MCP can be used to automate tasks like dashboard creation and incident management while addressing potential challenges and security considerations. The session concluded with a Q&A segment, emphasizing community engagement and contributions to the evolving MCP framework.

# Grafana Campfire Community Call - July

**Welcome everyone to the Grafana Campfire community call session for July!** Today, we have something very exciting to discuss, which is a hot topic in the tech industry known as the **Model Context Protocol (MCP)**. We'll be talking about how to use Grafana with MCP and integrate it with AI tools so you can utilize AI assistance in your Grafana.

## Introductions

Before we dive into the topic, let me introduce the rest of the panel. My name is **Usman**, and I've been working at Grafana for about three years with the Developer Relations team. Joining me today is the one and only **Matt Ryer**. 

**Matt:** "Hello! Yes, I am here. As far as I know, there is only one of me. Iâ€™m a senior principal engineer working on the assistant project, which is closely related to this topic involving LLMs. Very exciting stuff!"

Also with us is **David Kalmid**.

**David:** "I usually just say I run the Grafana team. I do use AI sometimes, though I think my son uses it more than I do. For work, definitely more than ever."

Next is **Lucas**.

**Lucas:** "Hello! Iâ€™m based in Brazil and have been working on the AIML team for three and a half years. I vibe code every day now, so I use AI daily."

And finally, **Jana**.

**Jana:** "I joined Grafana about two years ago and am part of the MLAI team. I also vibe code! Iâ€™m working on a smaller part of the assistant project that investigates issues in your stack."

## Discussion on MCP

Now, let's jump into the topic of MCP. It seems to be gaining traction among big companies, as it solves many integration issues, making it easier and cheaper for companies to connect different services. 

**Jana:** "Do you think MCP is here to stay? It wasnâ€™t around a year ago, and five years ago, none of this existed. But it seems to be the way to go now."

**Matt:** "I hope it stays around for a bit because we need some foundations to build on. Even MCP itself has changed significantly, and it feels like it's still in flux."

**Lucas:** "MCP is kind of like USB-C for AI models. It started to connect AI but is being used for many different things now."

### The Importance of MCP

MCP is a standard protocol that allows AI applications to communicate with various services. It simplifies the process of connecting AI tools with different APIs, enabling more seamless integration. 

**Matt:** "MCP gives a standardized interface that allows any service to connect with any AI model. This is extremely powerful."

The industry response to MCP has been incredible, with major companies quickly building MCP servers. **Grafana** was also early to adopt this technology, and we now have over **1200 stars** on GitHub for our MCP work.

## Features of Grafana MCP

The Grafana MCP has exposed a big set of tools covering core functionalities for incident response, data querying, and more. Users can:

- Ask who is on call for a specific team.
- Create major incidents based on metrics.
- Query logs for error patterns.
- List and manage dashboards and alert rules.

Getting started with Grafana MCP is easy. You can install it using a pre-built Docker image or download the binary from the releases page. All you need is a service account with enough permissions.

## Live Demos

**Lucas:** "Now, Iâ€™ll show you a live demo using cursor and Grafana MCP. I have a local Grafana instance running, and Iâ€™ll demonstrate how to create a dashboard based on metrics."

*Lucas demonstrates creating a dashboard in Grafana using a simple command.*

**Jana:** "Iâ€™ll show you how it works on Cloud Desktop. Iâ€™ll ask if there are any active incidents and investigate further."

*Jana proceeds with her demo, showcasing the integration of MCP with Grafana.*

## Community Engagement

The community has been very engaged, providing feedback and contributing to the project. We encourage everyone to try out Grafana MCP, report issues, and contribute to its development.

## Questions and Closing Remarks

We received several questions from the community, including inquiries about OAuth support for the MCP server. While this is a feature request that the team is aware of, there is no timeline for its implementation yet.

As we wrap up, thank you all for joining us today! We hope you learned a lot about MCP and its integration with AI and LLMs. We look forward to seeing you next time!

**Take care, everyone!**

## Raw YouTube Transcript

Okay, and we are live. So, welcome everyone to the Grafana Campfire community call session for the month of July. And um today we have something very exciting which is very hot topic these days in the tech industry which is known as the MCP. So model context protocol. Um what we going to talk about today is that how to use the graphana NC MCP uh and integrate it with AI tools and so that you can use the AI assistance in your graphana and today we got two amazing expert with us. Um and before that uh I will introduce the rest of the panel. So me myself my name is Usman and I work here in graphana for about three years and I'm with the devril team and joining me today is the one and only Matt Ryer. Matt. Hello. Yes, I am. There is one of me. Only one of me. Um, as as far as I know. Um, yeah, I haven't checked everyone, but presumably. Do we do we want more mads? I don't think it's a be I don't think it'd be a good idea. Um, I don't think we need one in lots of cases. And yeah, here I am. Yes. So, I'm a senior senior Well, I don't do titles, senior principal engineer. uh but thing is I am working on the assistant project uh which is a kind of very related to this topic uh LLMs and the like very exciting stuff and I'm also here with David Kalmid. David do you use AI? Um I thought you were going to ask me if I use my title. I usually don't. I just say I run the Graphana team. Um I do use AI sometimes. Um I think my son uses it more than I do. Uh but uh not to not to code yet. Um yeah, but uh for work definitely a lot more than um ever really. Yeah, I guess that's on everyone. Um who else do we have here? Um yeah, who goes next? Lucas, maybe? Yeah, I can go next. Hello. Uh I'm Lucas. um based in Brazil uh and I work in the AIM ML team for three and a half years now. Um and yeah, like we all we I I vibe code every day now. So I use AI every day. Jana, what about you? Cool. Yeah. So most importantly, Banna, why did you just giggle Lucas? because everyone is vibe coding these days. No, is there another way to do it. Um anyway, so I'm joined Grafana about two and something years ago now. Um I'm part of the MLAI team. Why did you say AI? It's MLI. AIML. MLAI. It's MLI. Uh though recently it's just AI. It's not ML anymore. Uh so yeah, we I'm part of the AI team now. uh working on uh an assistant as well like a a form of assistance. It's not the generic one that Martin and Lucas are working at the moment. Uh it's a a smaller part of it which is doing uh invest it's investigating like what's wrong inside your your stack. So it's hopefully it's coming soon. Um so yeah and I do vibe code. Yesterday I created like a holiday point just for this meeting. So was great. Yeah. Yeah. I think we I this is Yeah, sure. Well, yeah, I think you still get the credit though. I don't know how it works. Yeah, true. True. I committed it. So, it's good. Yeah. Although now with um we have the we have the cursor bot also installed and we're playing with that where you can just in Slack you can just mention cursor and ask it to do something and it'll go and load the thread. It loads that for context. So you can be having a conversation naturally and then you can just ping cursor and say oh do a PR that uh fixes this thing or something like that and genuinely like it's good and it's you can see it is getting better even it's sort of first attempts are getting better so that is kind of exciting you know we don't use it for complex things but just for little tweaks it's much easier than going through all the workflow with GitHub and stuff and you just get a PR at the end of it it's pretty cool it tends to get the contribute the contributor starts there. We have to be careful about it. Yeah, it's true. Um I think we did have a week where cursor was the most cursor had in in the insights the most commits. Yeah. And I think cursor actually recently outperformed many of the other tools uh uh in terms of usage. So it's quite popular these days now. Yeah, it's they're doing a good job for sure. Yeah. But this this technology it's kind of strange. You say a IML ML AI is nondeterministic but so are the outputs of these models and if you do live demos we did a live demo when we announced the assistant project and um you sort of don't know what it's going to do. It it most of the time does things that are sensible. It usually gets you there but it is kind of creative. Uh why is that Lucas? Even if you set the temperature to zero, why is it still nondetermined? Yeah. Right. Uh it's probabilistic model, right? So I mean if you set the temperature to zero, uh you'll get like the when the pro probability is almost 100%. But uh that also reduces creativity. So for writing code for example, you don't want to be you want to to let it write as much as it can and interact with your code in a more fluid way. So uh you always have these to balance, right? Like do I want to let it be creative or or make sure that every time there will be the same result. So we end up uh sacrificing end up sacrificing the the the right result every time to just have more uh interesting results. Um and turning the AC in your house doesn't help. I try that and it's always the same result. Doesn't matter the temperature. So yeah, that's that's the thing that's strange is you want that creativity like you say. We want it to be slightly open-minded, especially when it's investigating some new issue. And most hard problems in observability in particular is like they're new problems that haven't happened before because usually if they've happened, you've got alerts covering it now or you've taken steps to kind of prevent it. Um, so you do Yeah, you're right. You need that balance which is just really interesting and and imagine then writing tests for that sort of thing which we are doing and it is kind of an interesting world. I was I was actually about to say that the the balance in the temperature is quite important as you said but at the same time if you need to evaluate your systems then this temperature uh setting can actually cause you some issues. So you needed to be creative in some cases but you needed to not be creative at all in some others because when you're evaluating you don't want this to be creative at all. Uh so yeah it's it's a hard problem. It's in I'm going to start I'm going to start using this on people when I think they're not creative enough is the temperature. We just dial it up. It's a good idea. So, uh MCP, this is a standard protocol. We're going to learn a lot about this um yeah soon. But like um is is this something you think MCP is here to stay, Janna? Like do you feel like this is it? They've nailed it. Uh if you ask me now, you're asking me now. So yeah, I'm going to say that it's here to stay. Uh but it wasn't here a year ago and nothing of that was in here like 5 years ago. So I don't really know what's going to come next to actually erase it from the map. Uh but for now, it seems to be the way to go. It's extremely hot. Every big company is is actually creating their own MCP server. It it solves a lot of issues. it it makes integrating with different like services extremely cheap for companies at the moment. So it is a way to go. I don't know for me it's here to stay but everything is moving too fast. Yeah. I don't know if Lucas has a different uh different opinion or not but I feel I feel like I feel like this just makes its way through sort of mainstream now MCP. Like I sort of hope it's there to stay at least for a bit. Uh cuz I don't know how people are supposed to keep up with this otherwise. Yeah. Yeah. Right. We're going to need some foundations to build on if we're doing this. I I think even even recently though MCP's changed in some significant ways. So I feel like it is still uh in flux, but it feels like yeah, at least we're going to end up know what version it's on yet, but we need a we need a V1 really so we can stick to it. Um but this is for anyone not familiar. Oh sorry Y. Yeah I read I read recently somewhere that HTTP was initially created to solve a very specific problem and now it controls the universe. Right. So I think that MCP is sort of somehow very the equivalent of that uh for AI models and yeah so hopefully it's here to stay because we're investing a lot of time and effort into that. Yeah. And if you look at Hacker News, I have this feeling that every week in Hacker News, there's like a post saying that MCP is the it's a terrible idea. It's a terrible protocol. And then next week, you have someone saying that, "Oh, I built this awesome thing with MCP and now everything is easier." Uh I even saw this analogy that we always say that MCP is like USCBC for AI but someone said recently and I thought it was really good that it's more like a cigarette lighter like you remember the car cigarella lighters that we used to have that now they are used for anything else but cigarettes. So we created this for connecting AI but now people are using for many many things. So I think the protocol is already there and people will figure out how to use it still. Yeah, I I think I agree here. So it is definitely going to still obviously uh my take is that it it it there is still improvement needed in terms of security point of view. So there are some barriers or hurdles uh and actually work is going on. So I was recently in this uh conference where uh it was more about like talking about the MCP and um learn a lot that the security measures are taken very seriously when you when it comes to integration. So um hopefully those barriers or hurdles uh get better or get more easily integrated in in many in many various uh ways. Yeah, you want you kind of want things to be developed out in the open. And I think that means using things when they aren't yet fully baked. That's sort of the nature of it. So, you do get there'll always be a load of people that jump on the gaps and really highlight it. And honestly, that's probably what helps drive the priorities as well. But for anyone not familiar, MCPs, I mean essentially like if you've got an agent like you use cursor or warp or one of those or even uh desktop kind of GPT, you can wire up an MCP server and give your agent more capabilities and it's a set of tools and there's some prompts and it kind of changes the agent in some way and also allows it to access things through an API which is the MCP kind of protocol. Um, so very useful if you want to actually wire up say uh, you know, the GitHub, GitHub's a very common one. There's a load of tools in that, but imagine being imagine being able to ask chat GP questions and then have it create PRs or create issues for you, that kind of thing. It lets you do things like that. Uh, so it is exciting when you think of like being able to plug these things together. Should we should we dig in? I know you've got a presentation for us. Should we get in and find out more about MCP? What it really is? Yeah, should I? Okay. Yeah. Augmented. Perfect. Okay. So, well, um, before we just a quick recap, right? Like we were talking about AI and NLMs and I think this is always a good reminder that things are moving fast like we were saying. So uh this is like a um a timeline that shows the evolution of uh LLMs uh from the early stats statistical models uh so like engrams and more uh until uh the groundbreaking models like GPT3 or TPT4 and and here in 2022 we have the release of CHAT TPT which is probably um the moment that LLMs really got popular and uh especially the chatbot interface that we are all familiar with uh nowadays. Um but the main topic MCP is from uh the end of last year uh November last year that entropic released and released out and uh in the open right like like Matt mentioned and probably that's why it got uh really famous because other companies were already doing uh some kind of tool calling. Um so still before talking directly about MCP I think it's good to remind how we used to do things. So um to understand what is the problem that MCP is trying to solve. Um so we used to use LLMs like this right uh the initial approach was uh we provide uh context and some instructions and it was essentially a single call to the LLM and uh uh doing a single action and having a textbased response a chat response. So you ask a question uh you provide some context it has the context of the internet and you have uh an output um and we did that a lot in graphana uh so for example here uh we we were generating uh explanations for uh profiles flame graphs which personally I find really hard to to understand what's going on and the LLMs were were helping like to generate a more human uh understandable explanation uh making it working with profiles uh less of a a barrier. Um and we also yeah sorry yeah this is this was this feature is just phenomenal like um you know literally if you don't know about flame graphs you this is how you could learn this is actually a better way to learn I think than whatever we had before and there's also I think something about the way that this the way that they did this uh which was essentially turn the flame graph into a textual representation uh of of that data and then give that to the LLM and ask it to summarize it, you know, sort of uh so it's got this intermediary text format, hasn't it? Yeah. And being able to interact with the tax the test and and seeing that in the flame graph, it's it's pretty powerful, right? Like it's still just a text interface, but you're interacting with the thing you want to learn more about. And uh I think few apps or u few I haven't seen that much like usually it's just a textbased interface and very chat like interface that's really cool. Yeah, we're trying to focus on that in assistant as well because we we just don't want it to be a wall of text. So any opportunity we can to visualize something and of course it's graphana labs so we've got all the great graphana visualizations we can use but yeah same thing we just don't want that wall of text and and I just wanted to sorry if u I just wanted to say one thing really cool so like this is a prime example like uh it is not just generating the text but it's also uh giving you the root cause and uh I have worked in the tech support industry and when you have like escalations and when you resolve it you always more focus on the root cause rather than like reading the 1200 messages. So you want to find the root cause and this is a good way actually to learn it. So what was the root cause and what are the recommendations? So uh a perfect use case for for your for your support or for your customers who need uh the details. Yes. And Osman, you're going to love some of our new stuff that's coming soon. Just say that. Yeah. Yeah, I think we in observability we always have to have in mind that um in a bunch of moments people will interact with this in a very stressful moment. So having something that explains and make it more digestible if it's 3:00 a.m. and you just got an alert that's really powerful, right? And we we use the same logic the same the same pattern for other uh features in graphana and that's the result of many uh experiments and hackathons like um once that GPT3 was out we start experimenting with this. So here I I brought three examples uh we have uh instant uh auto summary. So like your resolvian instant you you can just uh summarize the conversation the timeline of the incident. Uh the other one that I really like is explaining her patterns. So like you you get this pattern in your logs and sometimes you have no idea what that log line should mean. And of course you can copy that paste in Google and see a result but you're already there and you already have the context. So that's that saves some some time. uh the same with like generating descriptions for changing dashboard and so on. So this is uh LLM's before uh we're calling here like LLM 1.0 zero, right? Uh but what we are talking here today is uh the tool calling and the LLM uh in the agent era. Uh so we want the LM to do more than just provide a chat response. Uh the goal is the LLM to make uh tool calls uh interact with the environment uh use private context that is not the the internet context and execute like custom actions um to reason and reason about these actions and the environment and and and help to solve problems and and that's where the a protocol like MCPh it's really important uh Right, Jana, I think you you have uh more details about that. Yeah, exactly. Um just admin stuff. Do you want me to take over the screen share? Yeah. Okay, good. Uh so I can do you see my screen? Not yet. Yes. Perfect. Okay. So yeah, you mentioned the agentic era. I I really like it. We're entering this and um in this era like the LLMs need to do real work, right? It's not about uh generate text. It's about uh do things like query primitives, maybe create incidents in graphana or check on call schedules. Um we needed them um we needed the we need the LLM to integrate finally with different services and APIs. So let's say that you're building an AI assistant. You want this AI assistant to connect to GitHub, to connect to Slack, to Graphana. Your internal tooling, right? It's integration means custom code, authentication handling. You need to do error management. You need to maintain the code that you just made. So this is exactly the problem that MCP solves. Um Lucas already mentioned that entropic at least like modern context protocol in uh in November 2024. Uh and this is exactly that. It's a standard. is a unified way for AI applications to talk to any different service. Um the metaphor that was mentioned before and this is like what we're showing here in this slide. It's like the USBC for LLMs. And at first I thought, yeah, okay, that's that's cool. It connects things. It makes sense. It moves data around. But then I realized that it's actually even deeper than that. Um if you think about the USBC port, it's not revolutionary because it's a port. It's revolutionary because it's it's a universal possibility space. Uh you can plug in anything to that. You can plug in power, you can charge your phone, you can plug in data, video audio, you can actually transfer information. The same port is able to handle uh everything because the protocol underneath is smart enough to understand what you need. So MCP does a very very similar thing. Uh it gives like the same functionality to AI. It doesn't just connect BLM to services. It creates like a standardized interface that says implement your promp implement your protocol once that's fine do all the work once and then any service can do and any AI any model can use your service and to be fairly honest like it's not only AI services like it it gives you the opportunity it gives the opportunity to any service to connect to any service to MCP so it's not only about LMS uh the really beautiful part for me is that every MCP server uh that is built for cloud desktop that is built for uh for cursor. It works for everything. It works for cloud, it works for cursor, it works for local LLMs um with whatever comes next like whatever comes in the future. So whatever talks MCP if this makes sense. I think it's it's extremely powerful. So you know so this yeah this is this is amazing. So people using MCP for nonAI situations but does does it does that make sense? Does it just use the tools directly? Are they using the prompts? Do they have to generate prompts? Is it is it how how are they doing that? I haven't really done this personally, but if you have MCP that connects to every different service, then why only use AI? Like does it mean that you need to build an AI? You need to build an agent to use MCP in order to reach the APIs. No, maybe you want to do something far simpler than that, right? Uh so yeah, like I I don't really see um how this can be done. I mean this obviously can be done now but I do believe that MCP is going to stay there not only to help AI uh applications but just to help any any developer do whatever they want with the different MCP servers that there that are out there. That's a that is a bold prediction. Let's get that one. We'll we'll track that. That sounds like it sounds good. Let's see. Let's Yeah. And I just want to I just want to make clear one thing as well. You talked about agentic just for anybody that doesn't know. Um basically this has made four loops sexy again. Now you know this is like not just request reply. This is where you your agents can take you know take the response of a tool call and then look at that output and decide to continue or not. And so it literally in a loop they can continue and investigate. We we we we get the assistant to do investigations using that kind of technique. But when it when it's answering questions it might want to go and do a few steps. And so it's this multistep nature of it is this the agentic that makes them agents. It it seems trivial but actually it is what I think it was a big leap in the use of this technology having four loops. Yeah. No I I think that this opens like a massive a massive space. It's a lot of people are experimenting like we are experimenting with this at the moment right? Uh so yeah I think that this opens a massive space uh for more experiments and more uh uh developments on on that. Um and I think anyway and I think the the reasoning layer right like um like we tried before uh aentic or before LLMs we tried to make these root cause analysis or s uh automatizations and then you always miss the reasoning layer which the LLMs are are pretty good at like you get a result from log lines and you there's an infinite amount of possibility ities in these log lines like what it could be an AR it could be from the server it could be from third party dependency and having an LLM in the middle of this action the next action uh and reasoning about it and selecting the next action I mean it's not just the if or else like and that's really powerful it's exactly that like selecting the next step I think it's extremely important uh it saves a lot of time from an engineer that is on call and something is on fire like having something else doing this for you in even in parallel or even just beforehand because maybe this agentic flow started when an alert fired and you didn't have time to look at it immediately. So you spend like 10 minutes trying to find your laptop and charge everything and and set up and set up your uh your desktop. um then you already probably have an answer there because you have an agent at the background saying oh you know what yeah I I have this alert I can reason about okay what to search next and then what to search next and maybe I'm going to find you there because I think that this is uh finding the next step is is nice and important um it's a nice use case I do have a question um uh so in this MCP uh at the end it says like protocol so does it follow a specific protocol or what it means by protocol. So it it is a protocol. Um it it's a specific interface between like an LLM and your APIs. So um you are wrapping your um you're wrapping your APIs with uh with some tools and those tools need to have a description a prompt uh just to explain to the LLM you know what like this is the way to use like this API this is the data you need to provide. Uh so it kind of the protocol gives some instructions to the LLM on how to to use like the the the tool itself and eventually there is the information from the API. Um if someone wants to add more here. No, but well, yes. And this um I thought I was shocked that this was uh it ran a binary like the one the MCP ones that I've used run a local binary on your machine and then it's a it's actually standard in standard out lines of JSON I think was the actual interface. There is HTTP as well. Um I can see why that happened but I feel like you'd I feel like it would just be HTTP, wouldn't it? And then these local MTP servers just make an HTTP server. Surprised it went like that. Any insight? Yeah. Well, I think the the transport layer is still kind of up in the air, right? like they started with uh a a standard uh input and output and then you had the S SSE and that's still in discussion but I think the why MCP became the protocol is just because they started in the open like uh OpenAI was doing uh to calling uh also and now it's integrating more with MCPs just like when you create a a a place so people can discuss the protocol And this even the the transport layer, right? Uh it's it's easier to get adoption because then you can come up with a common uh resolution, a common uh way to do things, right? Yeah, makes sense. Cool. Um just a side note, I just realized that I have a single monitor for this call and I don't see your faces while I'm doing the presentation. So it sucks. So, I don't really know if anyone is about to jump down or Yeah. Anyway, we're just going to be watching We're just going to be watching the presentation. Like, you mean for like in case it's not going well. Exactly. Yeah. You see, you say I don't I I I don't want to see your faces. Yeah. None taken. Anyway, it's a feature. You see, that's why you only have one monitor, isn't it? I do have two monitors, but I'm running the call in in one for some reason. The other one is big one, so I didn't want to present like that. It's It's a bit weird. Fair enough. I know. Cool. Okay. Should they move to the next slide? Is this okay with everyone? Do you want to expand a bit on that? No, I'm excited to see what's next. Yeah, I see. So, yeah, it's about like the industry, how the industry responded actually in this uh in this MCP um protocol. So, honestly, like the response has been incredible. you have within weeks of of the MCP announcement within weeks of the launch like major companies were building MCP servers uh Slack, GitHub, uh Postgress for different database queries like Superbase, Figma, they saw the opportunity and they kind of went for it. Uh exposing their products to AI developers with with minimum fiction, which is it's kind of magic, right? Um and I think that Graphana was very very early as well. We jumped on this very quickly. uh it came up I think as a hackathon project uh I was off at the time so it came up as a hackathon project in early December last year and very quickly emerged as an open source uh repo of Grafana and the response to that also has been great we have at the moment over 1200 stars on Git uh we have we have contributions from the community we have people opening issues uh sending PR over this is actually people are engaging with it they're playing with it um and they're very active so if you see at Yeah. Of course. And I was I was going to say we should also shout out to Ben Sulli as well. Yeah, definitely. Definitely. Definitely. Yeah. Uh he's he's great. I mean, if it wasn't for Ben, I don't think that we would have 1200 stars at the moment. Um anyway, he's he's on top of everything. He's for me, he's a he's a magic engineer. I have no idea how he's doing it. Yeah. You mentioned you mentioned the hackathon like and I was in that team. Yeah. Ben, I and and Jack and the the the coolest thing about that hackathon is that like uh Andreek released MCP a week before I think a week before the hackathon. So uh while we were doing the hackathon things were changing and so from Monday until Friday uh MCP was totally different already the protocol we we interact with other MCP servers like GitHub and um Slack and they were not uh as ready. So we filled like bug reports and PR. So uh working with literally like a a bleeding hedge not literally I I hope um that is what's cool about the hackathons though that we have here I think because it is there are no rules. It's just no rules. Then they can just do anything and and and yeah, like you do see people doing things that they really shouldn't be doing and I'm so glad they do. Yeah. I think that Lucas you mentioned to me that when you started the hackathon on Monday there was no Slack MCP announcement but then suddenly I don't know what Wednesday two days afterwards like there was a Slack MCP something like that. So you started using it something like that I suddenly came came up. Yeah, at the beginning of hackathon we were integrating uh um with Slack using the API like medieval times and by the end of hackathon we we just ask the the agent to okay every time you do something go back in the Slack thread and and mention that you're doing something. So uh we didn't have to like write up the uh Slack API anymore. We just asked the agent to do it. Yeah. It's so good, isn't it? It was really good. Yeah, it's so good. Yeah. We built a thing where we've got multiple agents that are that are helping on a job. And to cancel it, what we do is we just tell them all, "Oh, the user would like you to cancel." And so it's actually up to them whether they cancel or not based on that. What they do though is they'll say, "Okay, let me just summarize what I've figured out so far." And it's a kind of you get this graceful shutdown by doing that trick. So that I thought this is a really strange it's a strange world. You don't have to do the graceful shutdown yourself. Yeah. Um cool. Okay. So we're talking about like other stuff. Let's talk about what Gapan MCP uh is now. So what it can do. Um so in the last few months I think that it's yeah January onwards I think February onwards maybe. Uh so we've exposed a big set of tools uh that cover like the core functionality that you'd want when probably working with reflectically or you want an AI system to handle. So for incident response, let's say imagine asking the question who's on call for this team or create a major incident because I saw a massive API latency spike uh on this metric or and you have the MCP server handling that uh with the on call schedules, the team lookups, the incident management. Um we mentioned SIFT before uh we are exposing like some SIFT tools uh through the MCP server. So you can ask it do you see any error pattern logs uh actually any error patterns uh in my logs for this service? Uh can you go and find them? Uh can you like give them to me? Uh do we have any slow requests in the in this flow in this app? So the SIF integration in the background we're going to call the like CMC will call the APIs and uh SIF will automatically detect anomalies in logs and traces and whatever tool we're going to add. Uh so for data source and queries you can list different data sources that you have in your instance you can query permits you can query loy you can fetch metric with the data um you can ask questions like what's the 95th percentile latency of the service over the last hour and it's going to give you an answer which is actually quite cool um you can search dashboards you can update dashboards you can even create some uh we'll see in a bit we'll have some demos on that um you can extract panel queries and data source information you can list alert rules uh and statuses. You can view and manage contact points. Uh you can list I'm just going through the list now. You can list and users in in your organization. Like the good thing about this MCP server is not that we're not just trying to expose like API calls, but we're trying to provide some tools that understand observability workflows as well. And hoping hopefully this will become better as we go. Um is it flawless? Definitely not. Does it work every time? The answer is no. It doesn't work every time. Um I mean you have an LM in front of it. So but yeah we really hope that the community will engage will explore it more. Uh we're waiting for the feedback open more issues open PRs like we're more than happy see your messages and uh and the contributions from the the messages and contributions from the community. So yeah like the fact that it reached like 12k uh sorry 1200 stars in what like five months it's it was great. it it seems that people are using it which is cool. Um the good news is that getting started is really easy. So you have multiple ways to install Graphan MCP. Um you can use the pre-built docker image from Docker Hub. There is no official elements there. Uh you can download the binary from from the releases p releases page uh and place it in your path. If you already have Go installed and you're a Go developer, then you can go just build and install it from source. Um there are very clear instructions on the on the readme of the graphan MCP repo. So hopefully everyone can actually follow the flows. Um if if no if not obviously like send us a message. Uh we're more than happy to help you debug and the only thing that you need is in terms of obviously because you need authenticated instance is a service account in your funnel with enough permissions so you can use the tools you want. Um so you just need to generate a token paste it in your client's config JSON. If you see here there is a screenshot of how like the Graphana MCP um config looks like. Obviously the command changes based on the installation flow that you ch that you uh that you've chosen and uh you just need to paste like the URL of your instance and obviously the uh the API key that you just created and then that's it really. you restart cursor and you see the tools listed as on the on the screen on the on the left. So yeah, your tool will be available ready to use. Um there is a a question here like how many dashboards do I have in my instance and the answer is 634 dashboards. I don't really know for which instance is that but I'm pretty sure that it's one of the demo instance that we have in um infan. So anyway that's a lot of dashboards. So yeah, now that we've seen sort of what Graphan MCP can offer, I think that Lucas, you can take over again and start with the demos. Um, Lucas can show like the a demo on on capture. So I think that's quite cool. Nice. I love a live demo. And Lucas, how do you feel about live demoing with LLMs? Feel good. Sorry, I'm I was muted. Uh I never like live demos actually, not even before with traditional software. So with LLMs, it's a bit more I don't know. But um we are all friends here. So exactly. Could you zoom in a little bit? Speaking of being friends, yes, be friendly to my eyes, please. Thank you. But man, have you ever have you ever seen it like a demo not work with an LM where it was just like nah not today man? We luckily no. The only time I had an experience like that where I started to demo it and they said we're we're not going to use it. We don't like AI. Don't even demo it. There was a human that was being obtuse in that case. Yeah. That in terms of temperature that's very cold. Oh yeah. He does the same every time. Yeah. I was like, "Are you sure?" Like, "Yeah." "Are you sure?" "Yeah." "Are you sure?" "Yeah, forever. For ages." Yeah. Um so, let's see if this will work. Um so, I have a cursor here and I have my uh MCP Graphana uh server installed here. Uh and I have a local uh graphana running. So don't even try this API key. It's my local one. Um and so graphana instance is here. I will show in a minute. And I have these flask python flask app and it's providing me some some HTTP metrics. And what I will try to do in this demo is make cursor or the agent here uh generate a dashboard based on the matrix. So I already created the app I already I'm already generating some Prometheus matrix. Uh it's going to my Prometheus data source connected to my graphana and now I want to create a dashboard and I don't want to do that manually. So I have uh copy paste here so I don't have to type very slowly but what I'm asking here is uh create a dashboard in graphana for request rates of this app. I'm not even uh going too much into details and call the dashboard um MCP demo dashboard. So, let's see. Fingers crossed. And I really like this thought for 4 seconds. I never thought how much seconds I take for thinking, but I don't I don't take it. I don't take enough. I think I I just take more seconds for thinking. And now it already read my uh Python file and my Prometheus configurations. and now listed the data sources um uh from graphana and I can see the so it asked for all my permuse data sources to graphana and that that's really cool right like uh it's already calling the tool to figure out data sources without me saying that this is a necessary step and now what is doing here in this undefined uh is actually generating the the dashboard and that takes a while and while it's doing that I will do you need to click that button Lucas no it's Oh no, I I think it's done now. Yeah, I don't know why it's showing undefined. I think it's because it's generating the JSON. It doesn't render the title of the tool while it's uh generating. It's a big JSON, right? The dashboard is a big JSON. Uh so it's saying that it created the dashboard. Um it select some things without me asking for it. So you imagine that the LLMs will use the context from the internet to generate these dashboards and I mean it's general practices. So even if you don't have you don't know much what you should put in your dashboard that's a good way to quick quick start it. Uh and let's see if it works right. Uh so I'm here in my local graphana. Let me refresh. Uh here it is. We even created some some tags. And now let's see if the panel panels are working because sometimes they do, sometimes they don't. And we need to tweak sometimes. But here it is. We have total request rate and active users and request rate over time and request rate by endpoint. And this is all for my demo here where I'm generating fake uh load fake traffic. But it it read the matrix and create it and here it is. And one thing that is really useful even especially if you have multiple MCP servers is like directly asking uh what can you do uh in this case what can you do with microphone instance. Um, so usually if you get a new MCP, uh, that's a good way to figure out, uh, all the possibilities. Uh, so it's reading my files. Um, let's see what we'll come up with. So it's basically what Janna just showed, right? like I can do dashboard management, alert and monitoring, data querying and um and I saw someone in the comments mentioning that uh sometimes it works, sometimes it don't and that's it, right? Uh I'm I'm I'm strong believer that if you are setting up an app or an agent that uh more more people you use the evaluation part and making sure that your prompt uh is leading the agent in the correct way is very important. probably the evaluation is uh the core of your your uh LLM app your LLM system because then you can have a bit more of confidence that this will work most of the times otherwise uh sometimes it will sometimes it won't so um for for the dashboard here uh I tried a few times and it it tried matrix that didn't exist or it tried colors even that it didn't exist and broke the panel. So, um, apparently colors. It chose colors that don't exist. Well, uh, color fle was it like that? Like fl No, it was more like continuous red or like tones. Yeah, tones of red. I I asked for like creating media dashboards with tones of reds which is it's your fault. Yeah, exactly. Don't confuse the the model. Yeah. Well, I I wanted something more urgent. I wanted to look more urgent and Yeah. Um so this is is cursor using uh um graphana, right? And it's really useful because you already have your code here and you already know the metrics and you can even like uh uh go back and forth and like instrument and update your dashboard and play around with your graphana instance. That's that's really uh powerful but you can also use in other places right uh I I think Yana you have it set up with cloud uh desktop. I don't have cloud desktop because I use Linux but I think you can show that to us. Yeah. Yeah. Yeah. I didn't know you were cool. Didn't know you were cool, Lucas. Oh, yeah. I I I I I just have to mention that I use Linux. Yeah. No, I'm glad you did because now I know how cool you are. Um, so just we we have a comment that someone answered my question earlier. Um, Max said when I asked about why it was local processes, Max said, um, MCP initial idea was to connect local processes and therefore that would be a simpler thing. HTTP brings Yeah. layer of unsecurity. So, thanks for that, Max. And yeah, we do read your comments. See, we do have a very quick demo for cloud desktop just to show IRM. So, if we can spend one more minute there and uh we can go to the I like this mirror honestly. I like um so yeah like this is me. I've set up like a cloud desktop with an actual instance. So, this is not my local instance anymore. This is an actual like cloud instance and I'm going to go and ask like are there any active incidents at the moment. So it's going to do its thing. I don't have everything all the tools running uh as uh as Lucas as Lucas has like I need to click approve. I want to have this uh this control. Um but yeah it didn't happen here like this is this case anyway. So it says you have some instance. Can you zoom can you zoom in on that a little bit? Yeah. Yeah. Thank you. So it actually went in it found the it found the incidents and it says cool you have like some critical severity incidents you have some major severity incidents you can go and say okay cool I want the payment service error spike okay what's that this is a major incident started by me so I'm going to go and investigate so give me more details about this let's see what's going to say cool so it goes and it does it Okay, now you ask me. Thank you. Uh, so it's going to go it has the ID from the previous from the previous call. So it's going to go and get like the incident by ID. So now it's going to give me all the details that it could find in the incident API. So the instant was created today uh by me. That's good. Uh the cluster, the labels that it has, uh the name space, the service, the team, um if you have any incident commander. So you can see Benal, he's everywhere. I mean, he's everywhere. uh he's in commander um and only one person is assigned. I know that Ben is not on call for this uh for this demo team. So what I can do is you have the team here. So I can go and ask okay who is actually on call in the payments team at the moment because Ben is everywhere but it doesn't mean that he has to solve everything. So, it's going to go find like the tools. We have like some some tools around like the on call and the schedule. So, it's going to go and list the on call schedules. Uh it's going to find the one that is for the payments team hopefully. Um okay, it it quered all the all the schedules and I think that it found the one. Nice. Okay. So, why does it list in the schedule? Okay. It does this sometimes like it it listed like the schedule twice and now it listed the on call teams but now we can get the current on call users. So sorry about this allow I need to have control. Uh but it's that's okay. Do you do you accept cookies? Do you ever accept cookies or no? You just I do sometimes depends I just wonder depends on the website. So anyway um so yeah like it it went in it did like a few calls. I think that one call that to call that he did was not necessary but anyway I'm going to go go afterwards and figure out why. That's a lie. I know. Um anyway, so Rob Willen is actually on call which is quite funny because he's my manager. So he needs to go and check what's happening with this incident. Please add the note to the incident that Rob is on call. So you know it's at least there is a note about it. Hopefully, it's going to do it. Uh, how did it try this? Nice. Okay, let's see. Did it not? Perfect. Okay, I It didn't mention him though. It didn't spam him anyway. So, yeah, it did. It downloaded a note as well. So yeah, I I think that this is this is actually a quite nice uh it's a quite nice flow as well. So yeah, that's it for the demos for today. Yeah, you're showing this Jana in the cloud desktop, but I always think how useful those kind of flows are when you're not in front of computer, right? uh in the hackathon that we mentioned uh the video that we did I I was at the beach solving an incident and although that was pretending um you were the you were not pretending no yeah the beach was pretty um uh it's really useful right like you're you're on a run you're on call for example and you're not in front of computer and you can uh ask some questions make sure that uh the initial investigation is done before you're even uh unfortunately there's no MCP coffee machine because ideally you ask it for start doing your coffee while you're waking up, right? Yeah. Although my my I do have a coffee machine that does support that and you sort of need to make sure the cups are there otherwise Yeah. And you you're never 100% sure that early in the morning. Did I remember to put the cups there or not? So, I'll just go and check. So, I go downstairs. I check. Yeah, the cups are there. Good. Okay. Go back to bed. Then I do it. Pointless. Yeah. Well, now you need a robot that always puts the the cup there. But then, yeah, the camera in there. You can you can have MCP working with a camera and type like, "Oh, there are cups there." And then you can say, "Yeah, cups are there." See? So this is what it's like working at Grafana Labs. It's just these sorts. It's just these ideas all the time. They're always sparking off each other. It's amazing. Okay, I think uh we can also slowly move to questions. Uh we got this question from one of our community user. So the question is like is there a plan around to use uh to add the O2 support for the MCP server as uh as defined in this change logs? And I think the second part of the question is basically that uh I don't want like the person doesn't want want all of his developers engineers to use the service account to be able to talk to Graphana. So yeah that's that's actually a tricky one and the one that was discussed recently as well. So in order to do this uh it requires that Graphana creates their own like our own like off server and we don't really have one at the moment. So the ideal flow here would be um that Graphana is able to issue and refresh uh tokens. So this requires uh some work like further down the the line inside Graphana. So it's not extremely trivial to do. Uh but yeah, like the relevant team is aware and this is a feature request and yeah, hopefully at some point we're going to have like more concrete answers, but we don't really have the timeline at the moment. Yeah, this is where we get this for free with the assistant because it runs in Typescript in the browser. It can use, you know, it uses the same APIs that the user is using. It uses their O token actually. So, it can only see what the users can see and it can only do what users can do. So, that is quite a nice sort of feature we get for free. Good question. I'm I'm interested in the orth as well. Someone else said just a wave just so I suppose we all just wave back, do we? Okay. And hello from San Diego as well. Ravi, hello Ravi. Hello Ravi. Hello. Yeah. And um we do have one question also. This is related to MCP but more for the question on the low key. So is there a like a automated way to query the lo key or loql in more national language using the graphana mcp I haven't tried it actually you can definitely there is there are a lot of lucky tools at the moment like in graphana mcp so you can definitely query loy uh though you have to remember that the obviously quitting loi is uh is not like the most easy thing at least for me uh So the the LLM will do the best it can and also like the the MCP server also like on the loy tool it provides like some prompts so it helps the LLM to do a proper query but it doesn't mean that the query will be successful internally though and for the graphan assistant and then Matt maybe you want to jump in on that we do have like a big set of instructions on how Loki can be quered um and this is actually quite handy and helpful so the assistant doesn't really make a lot of mistakes Uh but for MCP the prompt that we have is quite limited at the moment but maybe we can we can make it better definitely improve it. Yeah. Yeah. With promql we find models are very good actually. Um the LLMs have good intuition about graphana and the ecosystem naturally because it's all open source. It's one of the kind of cool sort of byproducts of that. The LLMs are already trained on all that. So they have a good intuition. So, and PromQL in particular, I think because of, you know, the amount of time that it's been around and um the amount of content there is out there on it, it does a very good job. Less so on logql, but it yeah, we have to do bit more work to help it along there. Yeah. And I see there is a comment from a user u u which says like there is a low key MCP server is available. So I think uh that's something um can be used to to enhance it and make it more natural. Yeah. Yeah. I bet that uh actually anyone can build you know if you've got an API you can build an MCP server on top of that. Um one of the things one of the challenges we're going to face is you know the more tools you give the LLM the more opportunity is for it to be confused in one agent. And I think that is still unless this gets solved in the LLMs at some point and I'm sure they are working on this. We did recently get concurrent tool calls um at least described that way. Um so I think this innovation will come but um but yeah so you can you can kind of like um build I think these different capabilities but if if it's just a load of disperate tools the GitHub MCP server's just got loads of tools of all the things I wonder if we ought to be thinking a little bit higher level um some some lowle really works and it can mix them in good ways but I'm interested also in higher level tools which um which do a bit more work for you rather than just expose the interface. Yeah, I think we all started like this rig like just uh translating from this is a API route and now this is MCB2 and now we are realizing that this is uh if we are saying that this is natural language APIs are not actually like translated to natural language. you need to have tools that do a bit more of steps or have a more u concise uh description and we are all learning that uh like for example uh the lo low key the the loql uh queries right if you have just the translation of uh low key uh API routes then that doesn't help much LLM agent so if you have a tool that validates your query or uh adds some instructions while you're creating that that that's a better MCP tool than um than just translating the APIs. But again, we are still learning how those things works. Okay, I think this that's definitely the most uh hot questions for the MCP. Is there a way to use the Graphana MCP on Kubernetes maybe? Yeah. Well, remote MCP is uh brings a lot of new challenges, right? So it's a container like you can use a docker version of MCP but then you have to set up uh especially I think token and uh and how this will interact with your graphana instance and in a secure way it's it's those are things that we are still trying to figure out even internally and I think u majority of the ecosystem still is. So I see I know that Cloudfire is also providing uh MCP remote and trying to to fix that but it's still hard. So I don't know I don't know if anyone else had more experience with Kubernetes in MCP. the not really but just to add on on the hosted like on the remote uh thing that you mentioned like we again recently we talked in the team about like having hosted uh hosted MC if hosted graphana MCP uh but again we do have the same issue with the off uh so I think that this would be great if you can just have mcpgana or whatever uhnet and use it um but it requires as the authentication flow that I mentioned earlier. Um hopefully it's it's coming. Yeah, again no time bands but this would solve definitely some of those issues. Cool. Uh I must inform you guys we actually out of time already. We spend one minute more ahead than the required time. So yeah um I think that's all for today. Um, any last uh comments before we close or or you want to share some useful docs or links uh before we close this call, Lucas or Ya? Yeah, somebody asked if you could put that repo somewhere, the GitHub repo. Yeah, I think that we can. It's actually code created by Yeah, we can definitely like u you know beautify it a bit and and make it public. Uh so yeah, I don't think that this this can happen. I think that we can Cool. Then um yeah uh thank you everyone for joining again. Um actually uh I learned a lot today about the MCP AI LLM. Um and I hope you guys as well. And uh for now uh that's all we have. So see you next time. Till then take care. Bye-bye. Bye-bye. You just have to hold the smile

