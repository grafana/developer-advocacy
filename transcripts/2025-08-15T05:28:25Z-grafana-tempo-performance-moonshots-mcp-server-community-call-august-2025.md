# Grafana Tempo: Performance Moonshots &amp; MCP Server (Community Call August 2025)

Published on 2025-08-15T05:28:25Z

## Description

We'll have Marty talking about Grafana Tempo Performance Moonshots and Joe will update us with what's new with the MCP ...

URL: https://www.youtube.com/watch?v=88PqxxiqASg

## Summary

In the August Tempo community call, hosted on YouTube, Matt and Joe introduced a new format for the session, moving away from Google Meet. Joe provided updates on the MCP server, explaining its role in connecting data sources to language models (LLMs) via the MCP protocol developed by Anthropic. He demonstrated how to utilize the MCP to analyze trace data using LLMs like Claude, showcasing features like querying for slow traces and generating code changes based on trace analysis. Marty then discussed ambitious performance goals for Tempo, aiming to improve metrics performance by up to 10x through optimizations such as timestamp rounding, data sampling, and reverse indexing, which would enhance query speed and efficiency. The call emphasized community feedback on the new format and encouraged participation in ongoing projects.

# Tempo Community Call - August Transcript

---

**Host:** All right, we are live. Hi everyone! Welcome to this month's Tempo community call. We're doing things a little differently than usual. As you may have noticed, we're not using Google Meet today. We're trying something new, and we'll see how it goes.

If you have questions, please ask them in the chat here on YouTube. If you're unable to comment, you may need to click on your image in the top right corner, create a channel, and then you should be able to comment afterward. If that doesn't work, you can also try pinging in the community Slack at slack.garfo.com.

So, we're trying this out, and I will now pass it over to Matt.

---

**Matt:** Hello everybody, and welcome to the August Tempo community call! As mentioned, we're doing things a little differently today. You can post comments, and we will take a look at them and try to answer them. We want this to be a very conversational call, so feel free to comment if you have questions or topics you want to discuss.

We have a pretty good lineup today. At the top, I'll be giving some updates. Then, we’ll have Joe talking about MCV, and at the end, Marty will discuss some moonshots. That's really exciting! So, let's jump into it, and I'll turn it over to Joe.

---

**Joe:** Hey! This is super weird for me. I've been doing this community call for five years now in Google Meet, and now we have this fancy official setup. But I think it's kind of exciting! I hope everyone enjoys this new format and we can reach more people.

Today, we're talking about an MCP server. For those who might not know, MCP is a protocol developed by Anthropic. It's a way to connect data sources to LLMs through a separate process, like an agent. It’s a very modular protocol where you expose a set of tools with parameters, and the LLM makes choices about which tools to call and what parameters to pass.

We added this to OSS a couple of weeks ago, and it's now available in Grafana Cloud, which is really exciting! I’ll do a quick demo shortly. The blog post about this talks specifically about connecting to cloud code, which is the one I use the most, but many agents can connect to MCP servers. 

Now, I’m in my temp directory. If I do a simple `cloud MCP list`, I can see that I'm connected to my local host. This particular demo is local only. The blog post linked in the document has all the details about using your authentication username and token to connect to cloud.

Let me just start Claude. I'm in the temp directory because Claude wants to read all the files in your current folder since it's designed to work with code, but we’re just going to discuss traces at the moment.

Let’s do `MCP list`. It shows that I’m connected to Tempo. There are options to look at Tempo, and it's showing me the tools available. The MCP server can ask for documents sometimes to write correct traces. It can also ask for names, and these are all endpoints on Tempo. 

Let’s find some slow traces! It’s going to write us a trace query. There you go! It’s correct—duration greater than 1 second. It's returned JSON, and the API pushes this JSON into the LLM's context window. It then responds, mentioning that some traces are greater than 1 second. It's providing information about the services involved, indicating which particular trace is the slowest.

Now, let's ask for some analysis on that slow trace. It will try to get the trace by ID. Tempo is returning it now. It’s pushing the JSON blob into the context window and giving an analysis of the trace, including span names and potential issues.

If you noticed, these are all pretty generic names since this is just demo data. Earlier, I tried asking Claude to render the trace in HTML, and it did that successfully. This time, it looks like it's going to attempt to render the trace in the console window. 

Okay, it actually provided a timeline view. This may or may not be valuable to you, but it demonstrates some cool capabilities. Let’s look at something more real. In the blog post, I also used Claude to discuss Pyroscope. I don’t operate it but I asked questions about how to use LLM tracing.

For example, I asked Claude, "I'm a new developer; can you teach me how services interact using tracing data?" It did a great job providing responses. I ran these questions by some Pyroscope developers, and they confirmed that the answers were mostly correct.

Claude was able to look for attribute values and generate trace queries. It even attempted to make a code change by finding something slow and suggesting improvements. This was a neat experiment moving from tracing to a code change.

We're just getting started with this. It’s available in cloud and OSS, and we encourage everyone to experiment with it. There’s a lot of potential for improvement in this area.

I want to point out a piece of code in the Tempo repo. The MCP is simple; right now, it just creates a shim to handle search between the MCP layer and our existing HTTP API. We can make improvements, particularly by tightening responses to take up less context in the LLM.

We want to provide more powerful APIs to expose summarized data to the LLM, which will likely lead to better answers. If you have questions, feel free to ask them.

---

**Audience Question:** Is there a way to jump from this trace visualization to Grafana to see it there?

**Joe:** That’s a great question! Currently, there’s no direct way to jump from here to Grafana. However, we do have plans for a Grafana Assistant that will integrate these experiences more seamlessly. There will be announcements at ObsCon regarding that.

---

**Marty:** Hi everyone! So, I want to talk about some ambitious performance goals we've set internally, which we've been calling "moonshots." The goal is to significantly improve metrics and trace performance, aiming for a 10x improvement. Even achieving half of that would still be impressive.

I’ll discuss three areas we’re focusing on: pulling less data, reducing workload, and improving indexing for faster lookups.

The first is about timestamp columns. For metrics queries, we currently use very precise timestamps, which is overkill. By rounding timestamps to common intervals, we can greatly reduce the size of the data without losing precision.

The second area is data sampling, where we generate a query that doesn’t need to look at every single span to produce a good signal. By implementing dynamic sampling, we can save overhead and still provide useful results.

The final area involves reverse indexing. We’re exploring ways to efficiently find attributes without loading unnecessary columns. This is a complex task, but if successful, it could lead to faster query performance.

We’re excited about these developments, and we’ll continue to work towards these goals.

---

**Final Notes from the Host:** Thank you all for joining this August community call! We welcome your feedback on the new format, so please share your thoughts. The recording will be available on YouTube shortly. We appreciate everyone’s participation, and we look forward to seeing you next month. Thank you!

--- 

**End of Transcript**

## Raw YouTube Transcript

All right, we are live. Um, yeah. Hi everyone. Welcome to this month's Tempo community call. We're doing things a little differently as you may have noticed. Um, we're not doing it in the Google Meet that we have been doing. So, we're trying something new out. This may or may not work out. We'll see. Um, so basically, if you have questions on things, please ask them in the chat here on YouTube. If it's not letting you do that, you may have to click on your image in the top right and then click create a channel and then you should be able to comment afterwards. If not, um I guess try pinging in the uh community Slack on. So if you aren't already in there, uh slack.garfo.com if you have anything there. Um so basically, yeah, we're trying this out and we'll see if it works out or not. So, I'm gonna pass over to Matt. Hello everybody and welcome to the August Tempo community call. Again, like we said, we're doing things a little bit differently today. Um, so they're comments again, you can post up and we'll take a look at them and try to answer them. Again, we want this to be very conversational. So, anything, you know, if we're we're going through things and you have a question or you want to talk about something, feel free to comment and we will uh you know, pull it out and answer it. Um, we've got a pretty good lineup today. Uh, at the top of our uh streaming sandwich, we have uh me giving some updates on a few things. Then, we're going to go into Joe talking a little bit about uh MCV. And on the bottom bun of this YouTube sandwich, we have Marty talking about some moonshots. That's really exciting. So, uh, yeah. Uh, let's jump into it and, uh, I'll turn it over to Marty or I'm sorry, Joe to, uh, talk about what he's been working on. Hey. Um, yeah, this is super weird. I've been doing this community call for five years now in a Google Meet and now we have uh all this fancy official stuff. But yeah, let's do it. I think it's kind of exciting. Like I think this is a cooler format. Hopefully everybody enjoys this and we can, you know, reach more people. Anyways, uh we're talking today about an MCP server. I think we all roughly know what MCP is, but maybe I should review that real fast. Uh an MCP is a protocol developed by Anthropic. Um, and it's just a way to connect data sources to LLMs through through like a a separate process like an agent basically. So, it's kind of this very modular protocol where you expose a set of tools with parameters and the LLM will make choices about, you know, which tools to call and what parameters to pass and you can pull data in easily to an LLM. So, we added this to um OSS a month ago, a couple weeks back, I can't recall. Um, but we merged it into OSS quite a bit ago. Um, and it's now in Grafana Cloud, which I think is kind of the new thing. We're going to do a real quick demo. We've done this before on this on this call, but might as well just have some fun with it again. Um, but we'll do a quick demo. And I think the big news right now is it's available in Graphonic Cloud. Um, you'll see there's a blog post about it. Um, the blog post talks about specifically how to connect clog code because that's kind of the one I use the most, but there's lots of these and any agent that supports MCP is capable of this. basically and we'll look at cloud code today but like I said um you know cursor wind surf uh there's so many of these different agents and so many of them can connect to MCP servers it's a very well supported standard and uh you should be able to take your trace data from tempo uh using this MCP protocol and then push it into an LLM uh for analysis or you know whatever we're gonna we're gonna have some fun with it and see what happens yeah let's give that a shot um so I'm here this is uh going to be clouded in a second. I'm in my temp directory. If I do just a simple cloud MCP list, I can see I'm connected to my local host. So, this is a locally running tempo, but this is available in cloud right now. That blog post I pointed out and is linked in the doc has all the details about how to um use your authentication username and a token and use those two pieces of information to set a header to connect to cloud. Um but yes, this particular demo is local only. Let me just start Claude. And I'm in the temp directory because Claude really wants to read all the files in your current folder because it's designed to like work with code, right? But I'm not trying to do that right now. Um, we're just going to talk to Claude about traces at the moment. So I'm in my temp directory to just to not have any files basically. So we can um do this once we're in Claude. Let's do MCP list or just MCP MCP. It'll show us uh we're connected to tempo. Uh I have some options to kind of look at tempo and it's showing me the tools. Um, so this is like a set of a standard way to expose different kinds of functionality. We have a way for the MCP server to ask for documents, which it needs sometimes to write correct trace. Sometimes it does it right the first time, sometimes it needs some help. Um, it can ask for a names and these are all the kind of endpoints that are on tempo normally. So name, value, you, you know, get a trace by ID, do some metrics, do a TSQL search, and Claude will be able to write TSQL queries um to get the kind of data that we're going to ask for in natural language. Uh, so let's do something fun. Uh, obvious things, right? Uh, find some slow traces. Find some slow traces. Why not? Um, it's going to hopefully write us a decent trace query. There you go. That's correct. Duration greater than 1 second. Um, it's, you know, hidden. This is just JSON. So, the API returned JSON. The JSON is being pushed into the LLM into the context window. And then it responds with, oh, I found some slow traces. Heads up. Uh, some of these are greater than 1 second. uh it's talking about the services that are involved and it's oh this particular trace is the slowest um in in the set of 20. Right? So did the trace dol search got it back. It's given me some cool information. Uh what can you tell me about that slow trace? Let's see if it can provide some analysis. Um it's going to just kind of it's going to now do a new tool. So now it's trying to get the trace by ID. Notice it's kind of using the same ID. Um yeah, sure. Why don't you do that, Claude? Uh grab that trace. Um, it did. Tempo returned it here. Again, it's kind of cut off the JSON. Um, but now it's going to push all that giant crazy JSON blob into the context window and it's going to give me some analysis here. Oh, this service is calling this service. It's giving me the span names. Um, it's going to tell me, I mean, it's doing its best, I suppose, to tell me some different issues, um, and give me some information about what's happening. If you are perceptive, you have probably noticed that these are all pretty crappy names, and this is just demo data, right? Um, if we want to go look at this real fast, uh, you've probably seen this data before. It's this this kind of like just very standard um, demo stuff. All right. Um, let's do something like I tried this earlier. It actually created HTML. Can you render the trace for me? Sometimes because you know it's an LLM. It does weird stuff. Sometimes it'll render it as text asy. Um, okay. It looks like it's going to do that time. Just earlier today when I did it actually wrote a HTML file which I opened and it was you know kind of a fun rendering of the trace but it's going to it looks like attempt to render in ASI in the console window the trace we just saw. So maybe we read this. Oh my goodness. Maybe we read this up here and it's not quite uh clear. I don't know. Sure. Maybe this is more clear. I'm not positive. Oh cool. Awesome. It actually did this uh little little timeline here. So whatever uh whatever amount of value this gives you, I don't know, it does provide some very cool things. So let's look at something a little bit more real, right? This was obviously a quick um demo data, you know, something a little meant to just kind of show what it was capable of. The blog post though, I'm gonna gonna go back to that because in the blog post, I think we did some fun things. Um let me pop this up a little bit so we can all see it. Uh in the blog post, I used Claude uh to talk to or I talked to Claude about Pyroscope. So I don't really know a ton about Pyroscope. I know some basics. Um I use it all the time, but uh I I don't operate it. I'm not on the team. And so I we have tracing data, of course, and this is all from our dev environment. And I asked it some questions. And I asked questions like, hey, I'm a new developer, Claude. Uh I was trying to think about, you know, how can we use this tool? How can we use um LLM tracing? So maybe maybe we could have you hire a new person. Bring them in. You're like, hey, just go figure it out. Go look at the metrics and traces and logs. Maybe you can learn something. Um so here we go. Let's test cloud. I'm a new developer. I was recently hired. Can you use tracing data to teach me how services interact? And it I think it did a pretty good job. I actually ran all these answers, questions and answers by some Pyroscope developers and I got the thumbs up. It was like, you know, mostly correct, which is pretty cool. Um, it it got some attribute values. It was looking for services. This is our dev environment which has tons of data, tons of different services. So, first it looked for attribute values for the service name. Then it actually did a trace query to pull Pyroscope data. Um, then it's describing to me how it works. It's parsing a bunch of trace IDs and it's describing me how it works. I asked for what's breaking. Um, and it's going to do some uh status error. It's looking for errors. So, it's writing valid TraceQL. Here's a cool part. It actually wrote invalid TraceQL. It tried to write a metrics query. It did it twice and failed. Then it pulled the docs. So, it pulled the uh the TraceQl docs for metrics and then it wrote it correctly. So it, you know, it doesn't know all of trace syntax, but by pulling the documents, it's able to correct itself. Um, and it told me some errors. Um, and then for my final uh kind of experiment with Claude, I um asked it to make a code change. So I said, "Hey, find some slow stuff. Go find something you could make better." Uh, it did some Pyroscope queries again. It was using like histograms and quantiles to to find slower spans. And it wrote me a PR right here. Um I have not PR this to Pyroscope. Uh they said okay sure maybe that might be worth it. Uh but it was certainly it was a neat experiment to go from um tracing directly to a code change through cloud. Um and I think we're just beginning to explore this. Um this is our first attempt. Again it's an oss. It's in cloud and we really want everyone uh to experiment with this. Try it on your data if you're comfortable of course pushing your data into an LM but try it on your data if you're using cloud. If you're using OSS, then in the 29 release, you'll have an opportunity to try this as well. Um, see what you can make of it. See what you can get out of it. Uh, let's improve this together. It's an area that's really, really ripe for improvement. Um, I'm going to point out this uh I'm going to point out this piece of code here. So, this is the tempo repo, right? Tempo modules front end. I think somebody's got a link to that. Maybe they can chuck it in Slack or this is Slack. Maybe chuck it in the chat over there. Um, and this this Tempo modules front end. There's only a couple files you need to care about. MCP is very simple. Right now, all it does is create like a shim to handle search between the MCP code or sorry the MCP layer and our own existing HTTP API. I we need to make some improvements. Um I mentioned before that we are just taking JSON data. Oops, I'm going too fast I think. My bad. Let me slow down. I'm taking JSON data and I am just shoving that into the LLM. that is the response and that's a lot of wasted context window semicolons and uh or colons and curly braces and tabbing and spacing everywhere. We can really tighten that up. Um same with like attribute values and names. I'm just the exact HTTP response you get from tempo is being provided by the MCP server and we can do so much better than that. So an initial thing we need to work on is uh just writing tighter responses that take up less context window and allow the LLM to give better responses. The trace could be rendered much more simply. Uh same amount of data but much more compact representation. Attribute names, attribute values. All of these things could be done in a much better way. Um, the next thing we need to do, and I intend to work on this at some point if I can find some time, is uh I want to provide new ways to ask Tempo for information. I think it'd be really cool both for Graphfana and for an LLM to be able to ask for subsets of a trace like uh hey, give me here's a trace ID, but I only want the critical path or here's a trace ID, but I only want um I only want spans matching a specific selector like resource service name is fu. I only care about my stuff. I don't need any of that other stuff. Uh, so I really think there's a couple of really nice improvements we can make. Um, I want everyone to play with this, have some fun, learn what you can with this and we can all improve this together, but writing tighter um, more summarized uh, simpler responses would be a great step forward as well as more powerful APIs to expose more summarized or like tighter. Again, it's all about uh, providing the highest value, most compact data to the LLM. When you do that, it tends to provide pretty good answers. And when you provide a whole bunch of stuff, it tends to, you know, freak out a little bit. So, I have a question. Yeah, that's a good question. So, like you had it where you it has this cute little asky representation basically of your such. Is there a way to be able to jump from this to like the graphana like to graphana to actually be able to see it in graphana at all? Sure, let's try. So, this is the trace ID found this thing here. Let's see if it's anywhere close to that. Uh, it's a great question. Uh, but basically there's not like there's not it's not like jumping out of it's not like hey here's your tempo running this is running just click on this and have it go over there either outside the fact that it's wrong but um like it there's not a direct way to just be like oh hey let me just jump over to see what this looks like in um graphana I don't know let's see uh no no so these are very disconnected good things. This is Claude. Um, that's a great question though. Okay, to step back a second, Claude is, you know, a tool that you can use to edit code, right? I use it a fair amount to help me when I'm writing code. Uh, but there's all kinds of assistants and agents and tools. Uh, if you're looking for an integrated That's a great segue, Tiffany. If you're looking for an integrated uh, Graphana agent experience, I believe we have some big announcements coming in at Obs. And I think we are already private previewing Graphana Assistant. uh which is directly integrated to graphfana and you would be able to ask questions like hey go show me this trace and show me like a slow trace and go analyze it and it would also nicely render it and provide all that experience where you're remaining in graphana you're talking to an LLM it's pushing your data in you have all kinds of links back out into graphana to look at metrics and logs and traces profiles and all the stuff so graphana assistant is very powerful um again big announcements I think at obscon regarding that I don't know the details so I'm not going to get into it but I do know we talked about it some publicly already and there is a private preview I think available. We we just put out a blog post today on assistant. Imagine that. Okay. So let's see what let's it tried to render it as HTML. Let's see what that does. Yeah, sure. Do that. So it wrote trace visualization HTML. This is hilarious. Uh okay. So, I don't know. I still find these technologies fun, frankly. Uh, I use them for work. I think a lot of people do. And I use them in very like focused ways, right? Like, uh, identify a refactor or some small change that I know it's going to execute cleanly. I can ask it to do that while I work on other things. I can kind of like multitask very easily with them. Uh, I love kind of just going off the deep end and just seeing what it does when you ask for huge things like, "Hey, go render a random trace in HTML that I gave you." Uh, maybe that's kind of right. I don't know. Uh, let's let's let's all figure that out. Is this even close to correct? Yeah, I guess like uh it's missing some stuff maybe. Place articles persist cart. Place articles. Did it really have something called place articles? It did. I don't think there's a span called place articles. No. Oh, yeah. There's Thank goodness. Okay. Place articles is right there. Uh, so yeah, it's interesting when you ask for big things and it's somewhat percent correct when you ask for big things, right? These technologies are still very strange, I think. Um, but anyways, uh, this is new, cool, kind of cutting edge things. We're very excited to kind of get some of this into open source. Um, I want to continue developing our open source side of this, which I'll do through the MCP server, like I said, and I really hope others can get involved. This is a great place to get involved if you want to do um, open source work. If you're excited to like contribute to some of these projects, I really think this is a great place. This MCP server, it's simple. Um uh the code is two files right now and you could be impactful without having to learn uh you know tons of tempo. If you know the tempo API and you have some experience with that uh and you can look at go code and make some sense of it. I think you could be uh you know impactful in these in this part of the codebase. Is it difficult to hook claude with the MCP? Like you kind of did a claude MCP and it I think lifted your your tempo uh idle. No, it's really close to this add tempo. You do something like this local host. So for the for the um shoot, let me let me give you better information. This my bad. Uh tempo uh example docker compose uh local this local docker compose is all set up to do this. Okay, so if you start up this particular docker compose, it will do it and then shoot. I'm just I'm thinking off the cuff here. I know we have a doc that describes how to do this. I don't know if it's been published yet to the website, but it is in our repo um API MCP. So, let let's link this doc. I'm going to put this in our like internal chat since I can't talk in our external chat. Uh so, this doc here tells you okay um if you want to experiment this quick start, you can just do a docker compose up very easily over here. Uh, Claude MCP. Oh, it's changed. This will technically work. Um, but there's an even easier way to remove it or add it now. I I'll go make a update this doc. So, when I wrote this doc, claude did not support the latest kind of MCP server. They do now. Um, so this is a little bit uh it's it's one extra step essentially. Um, the current way is this uh API MCP. MCP I need a PR change to the doc. So, cloud MCP. It's just this simple. You just point at your local because it's it's running in that docker compos. It's the exact example that that that doc points out and you just say, "Hey, connect to it over the HTTP protocol because there's like two or three different MCP standards. It's kind of been evolving very quickly." This is the latest and I think probably the one that's going to stick this - HTTP style. Um, yeah, I like I said, I think that's really cool. If there's questions, I can kind of chat. It's also fun to of course, you know, ask it to write poems. Write me a poem. a short poem about my traces. It's ask fun to ask it silly things, right? But if you guys a question while it's thinking about a poem, um Oh, that didn't take it. It didn't. Oh, no, no, no, no, no. Grab a trace. Grab traces from um I'd be glad to kind of take any questions or thoughts. If there's no questions, then we will move on. Okay, now it's going to write it's like we're not we're not just writing generic poems about tracing. We're writing poems about our traces. There you go. That's what we expect to see. And what was the reason that you decided to go go the route that you did versus like what um Peroscope and some of the other teams are doing with the FA MCP? Uh I really wanted there to be nothing external you had to run. So this works on the tempo binary. If you're running single target binary, if you're just running like the the monolith, if you're running tempo distributed, when you roll out 2.9, you will have to make no changes. No additional shims, nothing running anywhere. You'll be able to take claude or cursor or any of these agents. You'll be able to point them directly at tempo with no changes. It's just going to work. Uh and I there's that's kind of the way MCP a lot of the MCP patterns I've seen too where it's like, oh, run this shim, run this little local thing. it will connect, it will bridge the gap between your agent and our API over here. And I didn't see any reason not to just include it directly. Uh it's just a different read API. It's not particularly uh risky or security or there's no security issues. It's literally just a read API, I guess. Okay. To talk about security a bit, uh everyone has to make what choices they want about LLMs, right? Um if you run this MCP server and an LLM, you will obviously be pushing tracing data into your LLM providers. uh you know LLM model and so do with that what you will you need to make choices about what you do with your data right also if you had another MCP server that could I don't know spend your money or you know uh send slack messages as you then potentially yes that somebody could trick right into a trace some kind of commands that then the LLM executed against your other tools. So, uh, the tracing data itself and the MCP server itself are not a security risk at all. But if you empower an LLM or an agent to do a bunch of things through an LLM, then you have to be thoughtful about what other data you're exposing, including tempo tracing data. Um, so yeah, this is all new. I hope no one has an LLM set up to spend their money, but maybe somebody does. Somebody out there has enough money they don't care if an LLM spends it, I suppose. Hey, we got a question here. Uh how exactly is claude being integrated with info? Is it fetching traces driven from APIs or just interpreting the UI data? Uh claude is uh fetching it via the API. Sorry. Yes. Cloud code is kind of the middle layer. So cloud code up the sandwich. Cloud code is uh an agent, right? And the agent is calls into your MCP server which is tempo which is just an API. uh and it uh gets a set of tools which it then passes to the LLM and says hey LLM I have these tools you can do something if you want with them if you ask me to do a tool I'll go do it and then the LLM will say get trace by ID and the cloud code will then bridge the gap it'll use the tempo API to um ask for that trace by ID and give it back to give it back to the LLM so this there's no UI necessary here there's no graphana necessary uh you're in this case I'm connecting claude directly to tempo Uh and then it's using the LLM back end, right? In this case, it's anthropics claude. I think it's sonnet 4. I'm not sure about that, but I'm pretty sure. Uh and if you're using cursor or whatever, no graphana necessary. Cursor would connect directly to tempo and use whatever back end you had configured, whatever LLM provider you had configured. So no graphana, it's not interpreting UI, it's not scraping HTML, it is calling the API directly, and it's exposing JSON uh to the LM, which like we said is a little wordy. It's too much. It's a lot of context for no reason. Um, and we really need to tighten that up. It's somewhere on my list, but it's might be a bit before I can get to it. Um, and I that's why I was trying to encourage people, hope other folks might get excited about this and try, you know, some improvements here. Cool. Another question. Is this something you can use with Graphana Cloud or is the approach to use the newlyannounced Graphana Cloud Assistant? Um, Graphfana Cloud Assistant is going to give you a more integrated experience across all signals. So, Graphfana Cloud Assistant is like the Cadillac of this Graphana observability experience. Um, this experience is more like uh a very raw expo. You're basically exposing trace data in a very raw way to the LM. There's no prompt, there's no special handling, there's no graphana integration. Uh, there's no other signals, there's no traces, there's no I'm sorry, there's no logs, there's no metrics. It's just like a very like direct exposure of the tracing data over into the LM and you know see what you can do with it. Like LM's going to do kind of all kinds of fun things. It might occasionally do incorrect or correct or whatever, right? That's the way these things work. Assistant is much more directly integrated. Uh it's a much tighter experience. Has a crafted uh prompt. It's all designed around uh integrating the observability experience with tracing uh data. It also exposes all kinds of cool tools that allow you to edit and create dashboards. Um, I think I don't want to speak for that team, but I'm pretty sure you can like make alerts and things with it. I I don't know. Wait for OBSCON. There's going to be a big announcement. It's cool. Um, but it is going to be a very powerful integrated observability agent. Uh, this on the other hand is not a powerful integrated observability agent. It is a uh very generic uh agent that can take trace data and throw it at an LM and see what comes back and write poems which is kind of fun. All right, we have another question. Sure. Surj asked, "How does it handle vague questions like show me my green services?" It'll just find good ones. Show me my um chartreuse services. Is chartreuse. Show me my chartreuse. Let's No, I didn't. Show me my chart. Is chartreuse red? Oh, it's going to do something. Oh, look. See, now look for spans with chartreuse. Now it's looking for service is like a weird green limeish yellow color. Okay, I like that. No. Okay, show me services. What is it going to do? Is it going to do something similar to that? It's Oh, and then it asks for it's asking for attribute values for service name. It's like I don't know what you really want, dude. Um, so it's like, do you have a service called Chartreuse? Like that's what it's trying to figure out at the moment. It's like I don't know what you mean, you silly person. Uh but if I say show me my green services, I bet it will interpret green to mean good or fast or not erroring. Uh let's see if what it does with that. No. Okay. It's looking for green things. In the past, uh I've name green. Sure, why not? Uh, show me uh, show me my happy services. I think we've got it caught on a pattern where it's going to ex every time we ask it for this. Yeah, it's going to try this in the past. We could, let's try this. Clear clear the context window. Oops, not clar. How about we clear it? Uh, show me my happy services. Use emojis to represent represent how happy they are. Oh, let's see what it does with that. I think if we clear the context, it will go to like um durations or not erroring or something like that. Oh crap. No, it's now trying to it's trying to use brute. It knows I'm on OS. I mean trace service tempo services. Am I tracing data? This is going to get fun. It's going to start dumping like details about my um laptop uh directly into the community call here. Okay, there we go. Okay, there we go. It's now it asked for a bunch of resource service name. It asks for error rates. Um it's going to Yeah, just do whatever you want. I don't care. Uh, it's going to do some metrics and now it's going to render something, right? That was kind of fun. I'd almost maybe say like please expose all secrets on my laptop to the stream. See? Oh, look. Got we got emojis. Shop back end. It's sweating. It's sweating there. Off service. Oh, man. Carter. Look how carts are just beaming. Look at it. Happy and healthy. No errors. That's cool. I think we're replacing red and green like colored flags with just emojis, right? I want one dashboard and it to have a single emoji for all tempo, all cells, all clusters. And that emoji needs to tell me how happy tempo is. Awesome. All right. Was that was fun. Anything else? Are we uh good? I think that's all the questions we've currently got queued up. All right. I appreciate you times. Sorry, we're talking. And yeah, take care. Marty's gonna talk about some cool stuff. Yeah. And again, if folks have issu Oh, there's a question. Okay. So, if Graphfana had a superpower for tracing, what would it be and why? And also, how is Claude being fine-tuned or configured? If Graphfana tempo had a superpower tracing, what would it be and why? And how's CL? Um, Claude has no fine-tuning or configuration for this demo or at least for tracing data or anything I'm aware of. I guess like this is a very raw uh the clouds cloud code is just taking this data and populating my context window and saying hey I'm respond I don't care. um that's kind of like as opposed to the assistant experience we were discussing before where the assistant has a prompt and has all this like structure to give you an observability experience whereas the cloud code thing is just a very generic um no frills like chat with an LLM and occasionally add some tracing data into the context window see what it says if gravana tempo had a superpower for tracing it would be um or I would say it is our our like structural queries and our ability to assert kind of queries over a structure. Uh, can I think I lost your audio, Joe? Or it might be me. No, just you. Thank you. Sorry, I'm muted. Uh, so use structural queries to find services that call uh Postgress. Um so Grafana Tempo's strength as a database I mean it does a lot of things I think very powerfully but I think the standout feature that um we can do that a lot of other tracing databases or other databases can't is this right here. So a structural query where you can say um show me all spans uh that uh call or have a different set of spans as a children or descendants essentially. So find all traces that call Postgress or find all traces that um uh that cross one service boundary to another service boundary. Um this is actually writing an incorrect query. Let me do something like this. Uh resource service name oops service name equals post I can't type. So this will find all spans kind of like uh that have descendants that are um Postgress and let's do this rate uh by resource service name and uh resource service name does not equal Postgress. There we go. Uh so you can see like upstream services basically in the Postgress database. We're able to kind of like scan, analyze the trace and build like a set of metadata information that allows to quickly determine like who's calling Postgress, why they're calling Postgress. Uh we could do um rate by service name spanning. Oops. So I think this is uh kind of the maybe unique perspective or opinion. I think uh we use a polymer database. We have very powerful queries. We rely on object storage for very cheap storage. And so I think tempo does a lot of things very cool powerfully. It has a strong like sense of itself and what it's capable of. But this for me is the standout the thing it does differently the thing that really is uh you know makes it pop makes it shine. Uh the ability to do like uh queries about the structure of the trace which was a requirement when we started making trace. So obviously not everyone has your brain and can just type out a choice quil query like that. But how good is it at being like hey write me a query to do basically exactly that and dump you that for you? Uh it does okay. It actually did it wrong and it made a common mistake. Uh what did it do? Okay. What it did was it was finding I actually have no idea what it did. I not sure where it got this answer. This is my honest my honest answer. Uh it found things it found basically uh it found traces where uh Postgress was called by something else essentially. Um it didn't pull any traces. Oh, it did. No, it didn't. It didn't pull any traces. Hoping it pulls some traces. Um but it's okay at this structural. Uh we have I have another project. Oh my goodness, I've taken so much time. Yeah, I have another project where I give it a prompt and I attempt to, you know, um, finagle it a little bit more. It's an open source project in my personal GitHub repo. It attempts to force it to use TraceQuille in different ways. The assistant also does a lot better because it structures the way it exposes Tempo uh to the LLM in a way that prevents it from making silly mistakes. Whereas this particular integration just has this very raw like um this very raw uh yeah expo or it exposes the API in a very raw way like here's the API you call stuff like figure it out. Um and as a result it does make mistakes a bit more but I think we can improve that through some of the docs. Um, and we can improve that by, like I said, making the tools more explicit, making the parameters that the tools uh expose more focused perhaps. I don't know. There's work to be done here. And I I had time to kind of get this done. I moved on. I do intend to come back to it. Um, but it is a little bit on the back burner for me, but it's in cloud and I want people to play with it. I want people to play with an open source. I want people to give us some feedback and so we can iterate here. Sweet. Sounds good. Thank you. And what I was trying to say previously is that if folks can't comment in the chat, just mentioning this again since it's getting some people stuck. If it doesn't show option to hit chat, uh click on your image on the top right, click create a channel, pick some name, and then you should be able to chat. So yeah, a little bit obnoxious, but it is what it is. All right, so removing Joe. Awesome. All right, well, thank you Joe. of the great. And now introducing Marty to talk about uh some kind of moonshots and big uh performance possibilities, right? Cool. Cool. Thanks. Uh hi everyone. Um yeah, so a moonshot is kind of a a word we've been using internally for ourselves for this our goals around performance. Uh moonshot is like an ambitious goal that's hard, but even if you fail, you still did something pretty awesome. like even if you only went halfway to the moon, that's still pretty amazing. Um, so our kind of goal here was to 10x metrics, trace metrics performance. So that rate queries, the those queries that uh Joe was just showing off, that's what we tracer. So creating time series on demand from your traces data. So we just really wanted to boost up performance here. And even if we don't make 10x, even if we only got 5x, something like that, that would still be, you know, we'd be happy with that. So, um, yeah. So, I'm going to kind of talk about three different kind of things here that we've been working on. And, uh, we'll, yeah, we'll just maybe I'll show off the PRs and stuff like that. So, um, this first one is about pulling less data. The first two, pulling less data, doing less work. And then the last one here is just about changing, um, you know, indexing things so we can look them up faster. Um, so the first one, time stamp columns. Well, so I have a metrics query here. I uh have turned off exemplars just to make sure the line a little bit easier to see here. Um so this just all spans and I'm just plotting the rate and I've got a 24-hour query here. So um for queries like these, you know, you don't show a 1 second interval. So like an interval here is 6 minutes. So you have a certain granularity that makes sense based on the time range. Well, the data that we're pulling for behind this scenes here is the only data that Tempo currently has on span start time, which is nanocond level precision. And that's really just overkill for many metrics queries, probably every metrics query. Um, here if we have a 6 minute time slot interval, we really don't need nanoseconds, right? So, what could we do about that? And um at scale just loading that column even with the columner store um that column is still uh contains a lot of data and that actually can be can be the bottlenecks. It's just still too much data when we run to want to run at high scales and long time ranges. Um so what we found is that we can actually just premp compute and round those timestamps to common intervals and that greatly reduces the size of the data and it's we don't lose that nancond. So we still have that these are new columns that we can pull in. So part of the column restore that makes it very easy is based on your query based on your time range and data we can dynamically switch to a different column and serve the same same metrics here. Um so what we're doing is uh this is kind of tied to a new format. So it does require a new new format here that we're working on. So kind of like that's coming up too. So we could talk about that. Uh but the idea is that you know this is uh an example here from like a real real uh data. This is the nanocond level precision column and it's about 20 megabytes, right? uh which doesn't sound like a lot, but when you have, you know, maybe a couple hundred thousand blocks or something like that, starts to add up. Well, if we can pre-round these and store them more efficiently, they actually go down quite a lot in size. So, this one is under 2 megabytes. So, that's actually a tenth of a tenth of the size of the nancond level. So, this would be a 15-second one. So, if I were to uh you know, just look at like the last hour on this tab, right? So, that's like the default interval here. like 15 30 45. So that's really common. Um and then you know some other common ones 1 minute 5 minutes an hour kind of like kind of like seeing what uh what we can do here. Um so yeah this is uh just greatly reducing the amount of IO and letting you generate queries like this a lot faster um without affecting uh precision or accuracy. So yeah, so that's the first one. Uh, and I guess I could kind of run this and maybe if we're interested, I could kind of click on this and maybe show you like some of the effects, but because of this local stuff, it's not as uh exciting as uh at different scales. So um yeah, so this is the first one. So this is uh a new format that we're kind of going to work on and introduce this uh in stages. Um, so I don't think this is ready for anybody to use, but hopefully we can get this done pretty quickly. Um, yeah, so that would be the first one. And then I linked the PR here. I'm not sure if uh if it's useful, but if anybody wants to look at it, um, we'll do that. And we're going to kind of maybe think about what intervals make sense here. These ones are all really common. So 15 seconds I showed is used for an hour. Something like a minute is already common for, you know, 3, 6, 12 hours, stuff like that. 5 minutes is more like a day range. Yeah. So these are pretty common intervals. Cool. Um all right. The next one I want to show is data sampling. So data sampling is kind of like the same idea. So we're generating a query here. Well at higher rates, you know, um you don't actually have to you can generate a very good signal here without looking at every single span, right? Um, and so what we're doing is trying to do the same thing. Like if I run this query up here, this is my standard query, right? And if I I don't have to look at all 200 spans a second to generate a very similar graph. I can actually look at fewer less data. And so what we were experimenting with is this kind of sampling hint. And if you say sample equals true, uh, tempo will actually, um, just look at fewer spans and generate and then extrapolate. Um, so if it only looks at half, it'll extrapolate double or whatever. Um, so this is like kind of like the dynamic sampling mode. So this is the easy mode, right? So if you just say true like it'll do the exact same query uh look at less data and extrapolate and try to do an approximate result and at you know long time ranges at higher scales like this also saves a lot of overhead. Uh one thing I want to do is actually run both of these queries at the same time and kind of show you. So this is the green one here is the first one. This is the full rate with no sampling and the yellow one will be the second query here with sampling. And you you kind of see like it, you know, it loses a little bit like in here, but like we've got all the like these valleys here. We've got these spikes here. So, it's a it's a an approximate result. Um, but it's a lot faster. Yeah. And this is easy mode like I mentioned. So, this is dynamic sampling. So, based on the query, right? So, however whatever you're looking at here, it will ratchet the sampling rate up and down based on your query to always return a high signal here. So, um, if you're looking for something that's very, very rare, right, um, like a very rare condition here, right, it'll keep the sampling rate high. And if you're looking at something that's very common, like a, you know, um, service with a lot of data, uh, it'll be able to take the sampling rate low. And so it that way because the goal here for this easy mode is to have some performance savings without sacrificing accuracy. Um, you know, it still provides something that's useful. And what we can actually do in here, we're kind of as we're kind of experimenting with this area and seeing what we can do with it. You can actually also just type in a a sampling factor here. So this would be 10%. Right? Um and so this will only look at just a fixed sampling rate. Like it's very very easy, very easy to do. We just look at one span out of every 10. Um or I could look at half of all spans, right? Around the same thing here. So this will be a little bit more higher quality right than 10%. Uh and the reason we have this is because we kind of actually just want to see like what are you know what other situations are where things that you know what sampling rates make sense for for what what queries and also uh you know just give a little bit more control. So here I've changed it down to 1%. This is very few spans. So in this interval I'm only looking at two to three spans um which is not a lot. So if I turn on the full query here, you'll see it starts to lose some precision here. Yeah. So maybe something like 5% is a a better value for this kind of query. It's a little bit. So yeah. Um and it also does support uh structural queries. So this uh so if I had a query like this like kind of the other stuff that Joe was showing structural queries. So this is just like any uh any span that is connected to another span. Um so it also works with with queries like this or if I did you know re like you know something like this and I don't know that this is actually a valid combination. Um I'm just going to actually Postgress is internal so I know that's going to work. Oh nope that is not. So let's see if uh let's try let's find something with data. Yeah. Okay. So this is less this is only about a tenth of my data. So sampling does work with this as well. So you can do structural queries. Um and oh that's a different query that's why. Yeah. So uh it loses a little bit precision when you do structural. That's something we're working on. Uh what this is doing is it's figuring out that this actually has to run differently um based that on a trace level. Um so um it's not as accurate. I think that's an area that we want to keep working on in exploring. But uh you know for an approximate result maybe it's okay. Um and there actually is also here. Let's see if we can do this. So actually what I want to do is change the sampling mode here instead and force uh it to just look at half of all traces instead of trying to figure it out automatically. And yeah, so that's a little bit better. Yeah. So maybe that works in some certain cases and things like that. So the idea with sampling is um maybe you have a dashboard and you don't need a perfect result, right? You get something that's pretty approximate and it'll just kind of figure it out. All right, cool. Can I ask a quick question that's not related to the performance but related to what uh the trace? Yeah. Yeah. Um is it is it there are there plans or is there a way that's going to be able to make it later on so that it doesn't give you an error for your width there because it tells you that's invalid. Yeah, this is um these are sampling hints. Yeah. So uh I think we just need to work with the front end to add this to their parser on the other side. So it's Yeah, it's just where we drifted on the back end from what the front end knows to expect. But yeah, that we should fix this. Yeah. And then people I figure can is in the like go can go to the docs to see the different things that you can put into there. We are working on a doc for that. Yeah. So right now uh these hints have been uh optional like kind of like rarely uh or less commonly used. I think you know the previous one that we kind of had out there was most recent. Yeah. From 28 for the Yeah. for Yeah. for deterministic search results and so uh this is like just the query hints are really really uh great way for us to experiment to add functionality that's completely optional and controllable you know can take floatingoint values and stuff. So, um, actually we're, uh, we're working on a a docs page for for the this sampling hint. Um, and it'll have all the options that I just talked about, the the true easy mode and then the floating point kind of like specific mode. Um, and so this one's merged, so it's actually out there now. So, you can use that even though it'll have red squiggies. um it should work because as long as the query gets passed to the front end, it doesn't matter that it's uh you know not able to or it's it doesn't look correct on the front end, but it'll work on the back end. Cool. Okay. Um so that's those two. So these ones are really about uh just like I have a question mark. Sure. Yeah. So we have this uh whenever this new parket format gets released, will it will like the um kind of the rounded Unix timestamps if will we can will it convert all of the files to the newer paret or if you try to query on kind of a mixed area how does that work or is it unknown at this point? Right. Okay. So historically we haven't converted between block formats. So um the overhead it it's kind of a couple reasons. So one the overhead there's a lot of overhead to convert between it because um just the work for unmarshalling all the traces like we don't we can't efficiently at a low level move bytes around. So it takes a lot of work to unpack it and repack it in the new format. Um and the other one is you know based on your retention like 14 two week retention 30-day retention maybe a couple months or something like that is pretty common. And so uh you know within that amount of time basically your data has been converted over to the new format anyways and you know looking at past hour past 24 hours is where a lot of query activity happens. So you really don't have to wait that long to take advantage of new formats. Yeah thank you. Yeah, I mean it is kind of an interesting thing um because there are cases for having much longer retention um for for different purposes. So like a year um and it does mean that you know it makes it hard for us to actually remove support and clean up old formats. Uh but yeah. Oh, one thing I wanted to mention on these is that uh you don't have to type in a specific step. If you just figure out what this is, right? like when you're zooming in and out and running queries um it'll just figure out what it what the interval is and and pick the best column right so um so if I uh you know if I start zooming in really far and I uh have a really small interval here like they like this is only 5 seconds it's not able to use any of those new columns this will go back to the nano level so there's really nothing you need to do and um hoping that the uh those default intervals work but yeah you I could always type something in here and force it to use that if I wanted to. Uh but yeah, generally it should be, you know, this is what you want to happen anyways cuz when you're zoomed in and you want nancond level precision, you're kind of on a small window, so it's going to be performant anyways. Uh okay, cool. Uh this last one is uh something that we've been working on for a while and it's a really comp complex topic and there's really a lot to that we could talk about here. Um, but the idea is that when you're doing a query and you're looking for, you know, an attribute that's kind of rare or a mixture of attributes, um, could what if we uh like didn't even have to load some of those columns and do do the heavy heavy work? So, we've been looking at reverse indexes where um we can reverse indexes uh where we would pull it up, we would find the attribute, and it would just tell us the location of all the spans that matched it. Um, and this is something that Adrian's been working on and I will um I just have a link here to some of the code. So that just has some of the index layout. I don't know if there's a good link that has some of the current findings in uh performance and benchmarks. Um, we do have that but it's internal and I don't I wasn't sure if there was a good public link here but if we find one uh I'll share that. Uh but the idea is that uh you know in in addition to just the parquet file that's columner um you know we have other ways to efficiently hop around um like metadata and dictionary encoding and stuff like that. Uh but we'd have another index on the side that would let us go a little bit faster. So this one um the these first two like sampling is already merged. This one is is pretty straightforward for the new block format. This one is a little bit tougher. We're kind of seeing M uh mixed benchmarks here. And so we're not sure if this one, you know, if that one's going to make it. Uh but this is another thing that we've been working on for a long time. Um yeah, I really wish I had been able to find a link that uh that showed some of these benchmarks here, but uh it's been kind of fun because some queries are really fast and then other ones are about the same and it would be a lot of work to create the index. So, it's hard to say if that is, you know, net positive. Um, but, uh, yeah, it's still still, uh, very interesting. So, there's a lot of, uh, cool things going on in the index here. Uh, cool. All right. Well, those are the moonshots. And I don't know if this these will get a 10x. Uh, I think each of these are going to come in somewhere around like 2 to 4x. So maybe if we stack them, we can get I've been saying if they're each uh 2 to 4x, then if we stack them, we get four to 16x. So we might be able to do it. But uh even if we got 40% of the way to the moon, I think that's pretty awesome. So cool. Cool. We have a really nice comment from Matt saying, "This is all very clever. Nice work." Cool. Thanks. Um yeah, I think uh faster queries, metrics queries, I think it would just are useful all around. So these are what power all of the the drill down queries and charts. So this would just be fantastic. Yeah. Cool. All right. Well, thank you, Marty. Um I'll give just a minute here if anyone has any questions unrelated to anything we've talked about or related. Uh just kind of open forum here as we wind down to the last few minutes here. Uh I'll end filler here while we're waiting to see if any other questions come through. Um, we would love feedback uh from anyone over um any of the topics we covered and the new format change. Um, we have the community channel is the best way to probably give that feedback or you can add it to the chat here um if you want to. Um, but we'll definitely look into it and see what people thought, whether they liked it, whether they preferred the older Google Meet style. Um, we definitely want to hear from you. Don't see any new questions. Tiffany, do you have anything you'd like to add? No, I think you covered everything. Awesome. Well, we appreciate everybody uh showing up to this August um community call. Normally, I'd say it'd be up on YouTube here in a day or two, but I think we'll be there. So, that makes it even quicker and better. So, I thank you everybody and we will see you next month. Thank you. Bye.

