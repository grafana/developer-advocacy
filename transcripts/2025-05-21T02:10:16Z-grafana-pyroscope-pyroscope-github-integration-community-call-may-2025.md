# Grafana Pyroscope: Pyroscope GitHub Integration (Community Call May 2025)

Are you a Go programmer? Come join Christian Simon from the Grafana Pyroscope team to learn about how you can use the ...

Published on 2025-05-21T02:10:16Z

URL: https://www.youtube.com/watch?v=_vQx4Bqk10o

Transcript: All right, we're live. Um, so hi everyone. Welcome to the Pyroscope community call. Um, this is my first time on it doing it this way. And of course, I have my going off in the background um for YouTube. Yay. Um, so yeah, I have Nicole, who is also a developer advocate. Did you just see that camera fall in my from my shelf? I don't know what happened. There's no one there. Hi, I'm Nicoven. Um, this is a great start. I I don't know what I'm doing here. I'm from Loki. I'm here to ask all the questions about Pyroscope because I have only used Pyroscope cursory. So, I'm excited to learn more. And today we also have a new guest. Well, probably not new. new to me mostly Christian. Yeah, I'm definitely new to to stream deck. I guess I think I might um before weard meet Streamyard. Oh, sorry. Um but yeah, no uh to speak about I'm happy to speak about uh what we have in uh kind of in our plans for Pyroscope and obviously answer any questions I guess also on uh on the stream. Um but yeah uh awesome. I will put it so that we can see your screen. Perfect. So like um just going quickly like um through the agenda. So um like yeah I'm uh basically like want to talk about the immediate road map. So we mostly planning quarters um and and so we kind of started just a new quarter and we also happened to have u most of the team kind of on site last week and and we were talking a few um kind of higher level plans but also kind of details. So yeah um I also see agile is on the uh stream. I'm in London as well. So definitely like find me at the next profiling meetup if it exists. Um uh yeah so our immediate road map um let's look at that. Uh so in in kind of uh like a lot of work has gone into um metrics from profiles features. So it's kind of a ruler to write out metrics from uh uh profiles um directly into Prometheus and you can then um I guess put some alerts on top of it. just use basically is it as any other metric. Um and uh we basically like to uh yeah um extend that feature. Right now you can kind of export a whole of a particular um flame graph. So like you would in this kind of quick example here like you would just export 23 minutes and we want to make it possible to to basically go on a sub function and and kind of export that. So that is maybe interesting for some runtime insights or um yeah like basically like if you want to understand maybe how a library is used in in uh your services you can kind of use that feature as well. So that's in development. We have some some backend work that kind of is promising already. Uh now I guess it's about productionizing it and and also making available in the UI. Um yeah. So another side uh another thing we want to do um it's also kind of in flight right now. We want to be able to to kind of um symbolize profiles that are not symbolized. So um again symbolization I'm going back to this example flame graph. In this case, it means that yeah, we have strings, we have function names, we have package names, and if they're not symbolized, we would see hex address lines. And it's kind of common that um for some binaries um to reduce their size, you would strip that information off. And so we might not be able to collect it at the time when we collect the profile. And with serverside um symbolization, we basically like can then bring that information. Um so that's uh um there's basically like different sources where Linux distributions might push that information and and we can kind of pull it from from there. Um so we also have something working. We basically have like uh think it's not quite there yet to to be used. So like we want to um kind of complete basically like how um that debug format which is mostly dwarf can be converted in a better format for for our use case and currently we also don't support line numbers and um understand how functions are inlined. Um that's something we basically like want to fix this quarter. Um another big thing is uh for us uh currently like label names are somewhat limited in in characters that you can use. So uh most missed um is is the dot to kind of align with uh open telemetry sem. So you might have seen that we use service underscore name um uh and and basically like we want to be able basically that you can use any emoji kind of label name as well as the the the kind of dot that is probably mostly missed. I think that will align us better with uh hotel and also make sure when you jump between signals like for for us it's most painful with uh traces which is obviously very the starting point of hotel was traces and then we need to kind of do some form of conversion and just assume that service name means what service name means. Um yeah this is something we we'd like to fix. Um, we uh have to maybe do some minor API changes, but uh I guess most of the work will have to go into graphana. And can I ask a question? UI of course. Do you want me to hold it? No. What happens now? Like now what is what is happening now that it hasn't been relaxed yet? What if somebody does have a label name that doesn't maybe if they are using hotel conventions? Yeah. So like if you actually use the hotel endpoint we kind of convert everything to underscore that that doesn't match the scheme and which obviously can be useless like I don't know uh can't be useless so we will lose some information I don't know if uh you have a minus and a plus it might both become underscore and might even conflict with each other so like this is already kind of a problem in other areas so graphana alloy currently you would just basically get error this is not a valid the label name. So I guess the first step for us is to support that on the Pyroscope and and then we kind of need to communicate that it's supported and then address all the API consumers like alloy um I guess uh the the only thing where it's a bit more um uh yeah like a bit more easier to to change is is just this kind of converting behavior we have for the hotel uh compatible endpoint. But yeah, um I hope that answered your question. Yeah, cool. Thanks. Um yeah, and I guess the the the kind of label selector that we kind of uh adapted from Prometheus and ProQL that basically will change in a very similar way as Prometheus. So you you would need to quote now label names if you want to use one of those kind of extra character set. Um yeah, I I think some work has already been done like the next step is kind of about communicating that it's available. We back and our back ends will add feature flags and then hopefully like yeah like one release from now or two releases from now it should be ready kind of to use and then we can work on our consumers like alloy and the profiling SDKs. Um so so Christian if someone is already using hotel now they're courarssing everything through the hotel OTLP endpoint and right now those labels are being converted into the dots are being converted to underscores then does that mean that potentially they might need to reconfigure their their dashboards? Yes like that is definitely part of the problem and when when we that's why we want to kind of change it early. So our hotel endpoint I I don't have numbers but I think it is rarely used and so like there's definitely kind of basically that step that either we kind of copy each level to have it both with the old and the new name or we yeah would have that kind of breaking change that you mentioned. So basically like our hope is that we do a breaking change but that it kind of is not as impacting as it maybe is for other databases with with a much higher hotel endpoint usage. Um but I think that's maybe also why the time is now. The hotel format for profiles is still changing. So I think I I I don't want to discourage usage of it, but there there's basically like um still a couple more planned changes that will change quite a lot. And like I guess we at least currently can only support one model for hotel and that is kind of the latest. So I I do think I would maybe hold off for another uh 3 six months until like those changes have uh have been basically like um finalized and there's a bit more stability there. But obviously like I definitely also think use it and and report the problems you have because that's the only way we make sure the the format will be right in the end. Um okay. So um I'd like to to speak a bit longer about um the source code integration. You might hear maybe about it for the first time. So we called it before the GitHub integration, but we definitely do realize there are other um vendors out there for storing your Git repositories and um it might not always be Git I guess in in the future. So we we definitely want to make it a lot more generic. Right now the only supported um uh kind of git host is GitHub but but we definitely like are interested if you uh want us to kind of support or drop everything we do and support GitLab as well like definitely let us know like uh we we get asked a few questions but but it's always good to to have some some extra numbers and some extra voices heard. Um so basically the idea is currently like in a profile like we often kind of represent things as a flame graph. So um like this is profiles drill down if you're not familiar like you would kind of first select you would start here and and then kind of select this the service you you actually want to um look into its profiles and then you most likely jump to the flame graph. And in the flame graph, each kind of box represents one function, but you can't really go further in. Let's say if you have a function that that kind of um has multiple places where it kind of consumes CPU, you might not necessarily know which one it is. Um and in in this example, um I'm just going to pick this kind of unnamed main function here. So, we can see that there's uh 16.8 8 uh minutes of CPU time spent. And now if I click on function details, what actually happens? It kind of breaks down this CPU usage within that um anonymous function here um and basically like annotates the source code where this is actually kind of um been been contributed from within uh this particular function. in order for that to work um the the basically like profiles need to have um like we we need to know basically what to look for. So that's why we have the the um git repo as a um as a label on on there and we also um we also need to understand maybe which which version you're running. So that's why you have a good reference. Um I guess in this case it is head. So we will always use the latest for this for this demo here. Um and and that basically like will uh give us enough information to kind of fetch and associate um the the line information that we have from the profiles with the source code. Um you you will also find um there for for example like libraries outside of your codebase that we can also um uh fetch the source code for. So that is using uh the um so the feature currently works only with go and in this case it's because it's heavily tied to the language um you want to profile in because in in this case we we have to go into the go mode figure out where we might get that source code from and then obviously get that source code and and display it to you and there's obviously different versions at play and so it's definitely kind of a bit of work to to get a new language working and that's also something we'd like yeah to to hear from you what what should be the next language we we look into. Um, one of the the the kind of problems we we might face is that not all languages have even the line information. So without line and file information, we obviously don't really know what to display. Um, the go um prof um endpoints they do support full line numbers. Um, and the same way also our SDKs um make make use of that information and send it on. Um currently eBPF is not supported but it is possible. It's also in our plans to do that. Um uh uh it is kind of a similar work item what I me was mentioned before with the inline and line information um from the symbolization. So basically like once once we we can do that we can also do that as part of the EVPF profiling. Um so you know what other is it just go that is supported so far? Yeah, so far it's it's only go basically like uh we won't be able to kind of fetch any dependencies from from from other languages and also the collection side is not necessarily giving us file names we can find in in in source code repos. Um is there a current plan for the next one or is that more based on what people say that they are interested? Uh so obviously like it it is good for for people watching if they have really strong opinions. I do think um that there's a good chance that we can look at um maybe Java. So like we we do have the line information from the async profiler we use there. Um there's probably a wellestablished um way of fetching source code in in in Java. So I I do think that there might be a good opportunity there but like I I can only say that it might not happen in the immediate road map. Our focus is around like fixing a couple of usability things. For example, you can see here how the repository here comes from the label names for Pyroscope, but there's no way to edit or change it. So, um we we basically like would require you to go into your alloy config, update the label name to be representing whatever you want. But in fact, this is a really hard thing to do and and like you won't be able to change it for profiles in the past. And let's say you have this profile and now you think you benefit from the source code integration. We want to be able basically like that you can config in the UI as well and and make use of the information um that is there. Um so is that actually saying um that on line 116 that prof.do line is taking up 16.9 minutes? So yeah. So like there there's also two columns. So you can see that here self and total. So in in self in a profile means what I'm consuming and I is kind of where you are in the stack the function we're currently at. So basically like in this case p point do is not uh um uh consuming anything. So you um it you have it basically only in the total which is then kind of further down the line and that's like per adding up all the different threads too, right? Um yeah so that adds up um so basically like I think what was confusing is because when there's no consumption we actually group or we have a feature to group and collapse and that's why it was not visible here. Um so just for for everyone's benefit so we we can also see the 16.8. Don't exactly ask me why it's point8 versus 0.9. Um uh in in uh in the flame graph here and and you can basically see because the the width of the flame graph is kind of the same as like in the next um uh row. Um that basically like means there's no consumption or no measurable consumption within because basically everything that the puffd do consumes is within the next one and that one doesn't consume anything. It's basically all pushed down to the to the latest um kind of um in in the stack trace. So here is where the CPU consumption happens. Um so this uh also shows a bit of a problem that we still have. Um like you will find some some instances where maybe a comment even consume CPU and that's where we don't guess the version right of the go. So in this case this is a go source code that we look into and like we can see how nano time consumes CPU here. So I I definitely think this is we are in the long row in the wrong line here. Um like this is basically what I wanted to talk about those kind of teething issues we still have. So basically the plan for the source code integration is that we make a G in this quarter and that there's a lot of those user visibility things like changing the repository but also like making sure the alignment works. So um a go mod file for example in your source code base will not tell us which go version you compiled it with. We need to kind of figure that out at collection time and then propagate it through and then um we we can also like because currently like the whole of the standard library has that problem that you will find the the wrong kind of attribution. So like you can see here's no source code line but we do use CPU here which is a bit suspicious and I guess in uh we basically just add some offset here to to the correct information because the source code we use is not the source code that actually is running. Um and yeah that's that's one of the problems to fix. Um, and then if if you're on the right side looking at the code, can you easily from the code to where it is in the flame graph? So this is also something that don't really have an issue for. So basically like I find myself doing this all the time. So I clicked order car here. Mhm. So I can kind of figure it out here that order car is the function I clicked. But obviously we need to highlight this better. And I also think a natural way is basically going down find nearest vehicle is obviously the the native way of of kind of going further down. Like right now I can can't click here or I I can't easily just click here. So I need to kind of click and know where I am in the flame graph and and open function details again. So this is kind of a bit of the I don't know I would say early um UI there. So, so we basically want want to make it a lot clearer where you are in the flame graph. The idea is to also use the sandwich view. So, I I I clicked on the sandwich view here, which shows you a better way of all um stack traces that call this particular function. So, everything that calls find nearest vehicle we can see here. And and that would be a great way to navigate because obviously you know this is where I'm at right now and you kind of know where where to click next. So um basically right now this is nothing working really like natively or or so I definitely think we we do have some uh UX homework to do that is still quite cool though. Yeah, like I definitely think it sorry sorry what happens if you click on the optimize code. So on the optimize code so that is something where we actually like um uh you need to be a graphana cloud users um and and then we would actually take the source code and the information from the flame graph and would send this kind of off to an LLM. um you kind of have to opt in first. So like you might not immediately see it. Um if it's not configured, it will be gray and you can't click it. I guess in this case um uh it basically like tries to figure out from the code that like we fetch why something might be slow or or what kind of next steps there are. Um, so I don't know like the suggestion here is to make something a bit more asynchronous. So I don't know if that is the best idea from the LLM of um so this is used to represent like an Uber style app where you order your car and I guess in this case you kind of order your car asynchronously and then you say your car order is processed but you never know what the outcome is like might not be the best UX but uh in in this case I guess it would improve the the kind of latency for the user but obviously like in the wrong way. Um, I think it works better with a a bit more of a real example. I think this is just too much of a demo. Um, uh, like it it basically can kind of find common hotspots um, in in the code. I don't know if I can and I think you you showed that if people want to try it out because this is not a graphana cloud only feature, right? Just the optimize one is the optimize is a graphana cloud only feature but the source code uh integration itself is part of oss and uh you can actually find it when you um I don't know how to maybe best give you a link but uh so we have play.grafana graphana uh or as a kind of open um it's maybe not the right time to connect to YouTube so I can post a comment but um in here if you can see this screen you can also post it in yeah so I I I have the link there I think yeah so uh that's kind of the full link to to the pretty much the same example just with the LLMs disabled. Um, whoa. What is that? I don't think that's right. That's about graphana cloud profiles and CPU process and random number. Yeah. So, I don't know where that came up. Is it just to play.grafana.org? We can I can do that. Uh, yeah. Here. So, I think it does. It's just this one. And then um this is if for people who don't know this is like a a free public playground for anyone to use not just for profiles actually but for all all the graphana stack and you don't need to sign in or anything to use it. Yeah. So like it's it's a really good way of getting quick feedback for for me as well. I I often refer uh that kind of instance to friends. Can I maybe quickly show you where you need to navigate to? So this is kind of the the URL and then we are profiles. So on on the menu drill down profiles. Um so the example I used was Pyroscope right check go. So you usually start with all services and then if you click into the flangraph you kind of get exactly kind of um the view you need to kind of also opt in with a GitHub connection. Um worth saying this is really only for your browser session. So no one else will have access to your GitHub um when once you connect it and uh if you're uh if you even hard reload it will basically require you to log in again. So okay you were saying that this is available for open source as well as cloud. This is available for for anyone. I'm still working on some OSS documentation right now. We basically don't really explain how to set it up in your OS Pyroscope. I have a PR for that. Um I'm also trying to do this renaming of of uh from GitHub uh to to source code integration. Uh yeah, definitely like watch the space. We we will have some some better instructions. Um the kind of requirement for GitHub is you need like a GitHub app that basically like has access either to your source code or just to nothing. um because otherwise GitHub rate limits will make the experience quite quite quickly um being a rate limited one to to fetch all that source code that that uh you you would need. Um and then basically like if this would have like this is just open source code you will find it kind of when you click here it's just directly on the internet but um even if we would have um if you wanted to show private code here that also works with the GitHub integration you basically like need to allow it to to read source code but then um as long as you connect you can read the source code. So I was seeing like how to find this if you're looking at Graphana Cloud. So you're saying right now there isn't exactly an easy way to look up like how to go through this if you're doing it on the open source side or do can you mostly like on the open source the experience is exactly the same how to open it but the operator of Pyroscope needs to configure a few things and they're not exactly documented right now. Okay. Um and and so uh there's basically like if uh if you're on uh the GitHub for uh um for for Pyroscope, I have basically this PR here. Um it is draft so it needs a bit more work but but basically like yeah we want to make this easy. It was basically just an oversight that we didn't really have a good um documentation page for this yet. So is it is it already something that's like enabled by default or do you you have to specifically enable it? Is it like a feature flag? So in uh in OSS you you basically need to configure something on the back end. Um yeah when you use kind of graphana cloud then it is enabled by default and I guess you you need to currently set the labels that I mentioned the service um repository and service get reference in order to find the source code. Um but from then on it it will just show up um uh like I guess within in the function details here. Yeah. And I was looking at the docs and it was mentioning that for at least on cloud that for now you need to contact Graphana support to during private preview to get access to and then enable it. Yeah, I don't know. It should just work. Um I so there's like it can be switched off here in the settings. Uh sorry I'm not sharing the right tab. Um so I do think it should be enabled by default. I might have been an outdated information. Uh but let let's check that afterwards. Um there's basically also you can disable it here um as a within the settings of profile drill down if you if you're not comfortable with the feature I guess but it is kind of default enabled. So in at the Graphana keynote um a lot of things were announced for Graphana 12 and one of them was git sync is that is are there plans to integrate that with this GitHub integration? Yeah, so like we definitely don't want to reinvent the wheel there and I think that's I guess the name change came with the idea that we basically want to support anything that Graphfana supports I guess for for the git sync. Um, so we we really kind of need mostly I don't know like give me this the file tree for this commit hash and then give me this file or that file. Um, and so this is kind of very close to to uh like what GitSync kind of needs from from its integrations. So I definitely think there's a lot of synergy like so we realize the synergy but right now it is kind of our really own code I guess for GitHub. Um but I do think like we would like to be have uh basically make use of of like all the the kind of supported backends for git um that exist for for git sync but uh this is not a status quo and then like how right now it has like the optimize um code and so can you also do like with how with the flame flame graphs in general how you can get it to use use uh I guess not for the I'm not sure about I guess for cloud version where you can get it to describe the flame graph. Can you still get it to describe things? Oh yeah, explain frame graph. Okay. Yeah. So like basically like the uh um let me maybe just go back to kind of plain flame graph. So I guess the first step is basically like here the explain flame graph. This really does not have information about the source code. So it will not be able to kind of make code suggestions. And so I guess once I guess you you read through this and um like here the LLM says basically the main reason for this bottleneck is written with this function and then by connecting to to GitHub obviously you can get that function and now I guess the suggestions can be a bit um more specific to the code because obviously it has the source code it it roughly knows like um like I guess what what what what what it does currently and obviously like the the whole example is a bit flawed in the sense that we do time sleep in order to make this slow and make this appear on the CPU and so I guess that's the explanation why it's slow. So like definitely the LLM is not wrong there. But once we kind of stop removing this, the demo doesn't work so well anymore because um yeah, just uh returning vehicles without looking for them is it's just not enough uh of CPU work. Um but yeah, if you scroll back up in the code, does it highlight the time sleep? So is it just like replace is this the code before it with the time sleep or is this removing it? Um, I I do think this is like Dakota as it is right now. And I basically like Do you know I guess it's LM. So there's only so much you can do with it, but do you know if there's any plans to add stuff like to show in this thing, hey, what line number it's at or making it more visible because you have to go find the time. For instance, like if there's any plans to make it a little bit simple to find where it is in that. Yeah, like like right now I guess the the the kind of view here is really basic I guess. So I think we definitely could could improve that. Like like for example right now uh if you used to the PR review um kind of thing in GitHub you could obviously like I just want to see line 53 now and I obviously can't I need to go to GitHub and then I can look it up. So so I think right now this is just too too kind of simple. I guess we basically only try to kind of show the find nearest function I guess. And then so for like this one it shows that there's like multiple it shows just in that time frame basically of the lines that are if you go up if you to yeah so it shows like basically does it show everything that's just taking like more than x amount of time or like how is it deciding specifically which things it's showing that there is time for like right now you have two lines there that are showing that it's yeah like I guess these are the only ones that we have uh profile collected um I Guess this is maybe also so obviously the function goes a bit further down but this is another fun like another function call with an anonymous function in in the argument so I think that's why like in theory I would expect this to show the whole funk and a couple of lines before and after and then where is the okay so this one is using the timesleep but you can't see the times sleep that's happening up because it's call Okay. So like I guess we so we are in this tech wrapper and because it doesn't understand that this is still part of the function it doesn't show it. Yeah. Yeah. It's it's a very specific so you can see how how this is a function call within the argument of another function call. Yeah. And this time sleep happens in there. Um so I think that's that's kind of why where that sort of which because we basically like focus only on that function and then the the tag wrapper I guess would would come uh Yeah. So like I guess the So sorry I'm just trying to find the one because I'm curious where it is where it has that one and then it actually has like the sleep that's showing that the sleep is taking that time. So I guess now we are in the track rep. So like you can see how this is represented the stack graph. flag. And here you can this sorry. So like because I guess this this is another stack in um when when when the the kind of argument gets called here that's kind of why we see it within the tag wrapper which calls the the function argument and I'm still like not 100% sure. And then when you click the view it on GitHub, does it just go to GitHub and for that code in general or does it actually highlight because I can't see I don't know if you're just sharing a tab or Yeah. So like when I share that tab it basically that's where it gets you. Okay. So it does jump to the line. So I guess it jumps to a line and then I guess because so I guess if this gets kind of executed here it kind of executes the tag wrapper and then the tag wrapper itself calls the function kind of supplied here in the argument. That's kind of like why why this is kind of following like these these kind of steps in the so I don't know if you can see so this is then the the implementation of the code find nearest vehicle yeah sorry so I guess when I now go here so we can see the find nearest vehicle and we kind of spend the majority of time in the tag wrapper that's also I guess what the flame graph tells And then the tag wrapper itself calls pd do and passes on another callback function. And so so that's kind of how you now jump through all of those levels. So in proof do you can see how then the actual function that we pass gets called with f context. All of this uses kind of the same amount of CPU. So nothing is kind of wasted I guess in those um layers. And then I guess we are in in this function that gets created here. And then we we are kind of finally in the the auto function. And that is where the kind of sleep happens by the for loop I guess that checks if if I've sleeped long enough I guess. And so when we now I guess want to kind of Oh, like or here you can also kind of see the split between like the like I guess uh some of this is kind of spend in time sense. So 16% or like not 3.84 minutes is spent in time sense and 12.9 minutes is spent in the check driver availability here. And what's the samples when you were hovering over the flameraph? Yeah, like that. Yeah, right now it like it's basically like the second equivalent like right now this is the wrong number I think so needs to be ignored. I'm sorry. Um and so like you can kind of basically like this kind of the first bit of the the the sleeps and then we we call check driver. be trying to find a driver for the car that we're ordering. And then when you kind of go further down, you will also find that there's basically just another sleep um hidden when when finding the driver. Do like hope I didn't lose you somewhere between the different level. If people are playing with this and they are like it'd be really cool if X, what's the best way for people to like share things that they think would be neat to add to this? Um yeah and also I definitely creating GitHub issue on graphana pariscope is usually the best way I think if you're unsure how to formulate the problem that he experiences which happens to me all the time definitely feel free to yeah like like erase it in in in your own words in in a slack channel first like we we are pretty active in in uh Grafana Slack Periscope um channel so that that's usually I would say the the kind of best way of of getting getting a um uh yeah for for that problem and like we can help direct I don't know whether issue should be created um what exactly um goes wrong and I definitely think like one of the problem of those kind of demo applications is you don't see a real problem in the sense we created a problem and now we find the problem we created and in most cases in the real world we create the problem but we are not aware of the problem just I don't know we're writing some code five years ago and then now becomes a problem. So I definitely think the best way to kind of approach this would be enabling it on an application you run and maybe get some users using it because I think that's where you will get the extra insights. Here we basically just get reminded how we implemented the demo and um obviously like the the easiest way would be removing the sleeps and we we we kind of reduced quite a lot of CPU um consumption through that. Um, and I guess that's what the LLM suggests here too. I guess I think you also wanted to run us through what else is going to come out in version 2.0. Yeah. So like um maybe just briefly. So um I guess the the kind of key improvements that we want to do um and and uh like I guess we saw them with me clicking through the um through the source code integration. So we want to be better at navigating and seeing where in the code we are. Um, we want to be able to configure the the repository URL and the the Git reference within the UI. Um and basically right now there's some various bugs when you kind of enter the the um the source code view from from the diff view for example like we want to fix them like there's a few problems preventing that but basically we want to make the kind of whole experience a bit more polished and um I guess uh like just by me clicking through it we we can see why why uh I think this this will be a good uh improvement to to the whole thing um separately Um we've uh as I said we um were also kind of talking about what we want to achieve for Periscope 2.0. So we we are currently at 1.13.4 I think. Um and um something that my colleague Anton has worked on extensively the last um probably not 12 months I guess um is uh a new storage layer. So we currently kind of um uh start rolling this out within Grafana cloud and we basically like need to do a few steps to make this accessible for OSS. So to have a a nicer migration path than than we currently have and basically the idea between that storage and I think this probably will be at least two to three um of those 1hour sessions to kind of explain the whole architecture but on a high level view we we basically found a couple of limitations in the way the the current architecture v1 works. So it is heavily aligned to how low key and and cortex mir works. Um but but the biggest downside is we we kind of keep data uh at replicas. So like we kind of every every kind of profile we send we kept three times and then when it comes to querying we also need to reduce those three back down to one because we obviously don't want to show three times the profiles you you sent to us and that kind of caused a lot of kind of friction a lot of bottlenecks as well in in in the way um like I guess we we kind of had to duplicate dduplicate this and storage v2 will solve a lot of those things Um we we kind of are um quite optimistic how how how far it's running. So currently it runs all the profiles for graphana labs um for for a couple of months. We we kind of saw a quite quite a bit of cost savings as well but also the performance a lot more reproducible and like it it should allow us to to kind of be a lot more dynamic with scaling the read path. uh which then will allow longer profile queries, more profile queries, faster profile queries. Um that is the kind of thing that I hope we deliver to OSS users as well. Um uh yeah, so basically like like the the the TLDDR how we achieve that is by moving a lot of the data out of um kind of the the persistent volume um storage from V1. So like injustice have like I don't know the last hour of profiles and we query from them and we kind of move that a lot quicker onto the object store and then also we compact it a lot sooner. So for example we will basically push every profile within half a second to object store and then we will compact it within 10 seconds and then when we query it it will always come from object store. It will not be on anyone's memory or disk ideally. So that's kind of the general idea behind it. Um so this is the this is the Kafka-esque architectural change. Yeah. So we yeah we we can definitely call it like KFK gas. Um it is uh like maybe we describe it like write a headlog in object store maybe. Um so we we we're not using Kafka anywhere in the system right now and and we we're not compatible with Kafka. So even if you wanted to kind of uh use it that that's nothing we've we've done so far as part of the work. So it still will be you will have to give us an object store or your local disk for uh of one replica setup and and we will basically like treat it all like object store rather than some part of it we treat it as files on block disk where we can kind of easily seek into files and jump back and forth and back and forth all the time. Um everything will basically be with kind of optics store in mind and like I guess less random access than I I I guess uh we done before especially for the most recent profiles and will version two also already have storage tiering or will that be a future one? Um maybe you need to explain a bit what you mean exactly with storage tiering. Well, um I was thinking like I think it had been mentioned al so for context for everyone watching the three of us were at a database's offsite um an internal one last week and I think I remember that there was some talk about um storage tiering in the sense of the the most recent um data is kept in a fast storage and then and then you know um older data is kept in a lower tier basically that's less fast, you know, but but it's cheaper to have stored on there and I wasn't sure if if that's something that will be implemented already. So like I guess right now it is not implemented but I guess most of the implementation will be a bit as part of the life cycle rules of your object store rather than I guess in inside our code. Obviously we maybe need to manage those. Um but yeah uh in theory it's possible like to to kind of like we we have slightly different prefixes based on like I guess where in the system you are. So basically in the very first 10 seconds we will have something like basically a block per the scale of the system you run rather than a block per tenant. So that's kind of important for us because we basically like if we have to create more objects per tenant very early on we create quite a lot of objects if we create one uh every half second and and that that will be a major cost driver and so in the first compaction I guess that gets thrown around and this might be a good like might be a good reason to think about the taring in terms of compaction levels maybe um but yeah like right now we've we've not even experimented ourselves with it so I don't think we like we'll have a feature by by the 2.0 release. Um well, I guess in a way it is it is already tiered, right? Because there's still the ingestion layer. So I I believe that profiles will still be stored on the local SSD on injusters and then and then the there's like a warm tier where it's flushed to object storage. Are you so we basically go completely uh away with the local storage. So it's just there's metadata information that is on local storage, but that's kind of everything. And uh right now we kind of rely that that metadata storage doesn't go away, but until 2.0, we want to be able to also have that kind of longerterm storage in object store. So right now we we basically have like that meta store. Yeah, basically it runs on local discs to kind of um like load its previous snapshot into memory when it starts up again. Let's say that you shut it down over the weekend and now you want to be back where you are. You need to read the snapshot and then you have it in memory again. And we basically want to be able to to do this from object store instead. But that's kind of the only place where we use local discs. Maybe my colleagues shout in the chat if that's not true, but pretty confident. Um and uh yeah so so basically like profile data we get the profile request and we basically will only say okay 200 accepted once it hit the object store and then like so that would mean you would never query it from anywhere else I guess than object store you will query from memory the fact that there is a profile that you should read from object store um so that's kind of what we have in memory is the metadata Um but the the kind of actual profiling data will always be as soon as we accepted it uh uh in in kind of upload request it will be an object store. Um so like I guess we also want to do kind of a few cleanups. So currently when you when you kind of do docker run Pyroscope you you see the old UI from uh 0.37 uh version. It is still working. I guess the API compatibility like we don't plan to change this. So we we maybe want to kind of remove it and and and kind of replace it something with more that looks like profiles drill down. Um or we make a very big note that you should move to profiles drill down. So we are not 100%. So that was the the thing that was a bit contentious in the last week with the team. Um so I think we kind of people started moving half votes from one option to the other and and so we we clearly saw that then we need to experiment a bit more what we really think is best ourselves and then um we we definitely think we can't maintain the old UI anymore but we would love to have a UI when you when you've not yet fully in set up graphana but just to kind of see profiles I think that is where the UI is currently pretty good at um So somewhere between bundling graphana on demand when when you kind of want it or basically keeping the old UI and and making very clear that it will really go away in V v3 and uh you should migrate to Grafana um itself. Um so I guess um like there's a few things we want to clean up like we have flags that are slightly not aligned with each other um especially in that new V2 um kind of configuration. Um we also want to release dashboards, alerts, run books. This is something we we heard quite a lot from from the community and we we would really love to to kind of share what we the work we put in. Right now there's only like a few things where it's very graphana app specific and everything else just relies on our metrics and we would like to kind of be able to share this with with OSS users. Um yeah so like uh the disc microservices mode that's basically what I mentioned to be able to kind of read the snapshot from up to store instead of the disks right now. Um yeah timeline like autumn 20 25. So uh like like we we didn't put an exact date toward it towards it. Um so just for people who are maybe in different hemispheres we're talking about September. Uh yeah sorry that that maybe was Yeah. So we we basically like talking about opscon in graphfana lab speak. So that that probably gives us end of September beginning of October. Not exactly sure up at the date right now but yeah that's that's what we're talking about. Okay, cool. Sounds like a lot of changes. Yeah, know like there there's definitely a lot of uh changes happening. We also remember we are not the the biggest team. So like obviously profiles is pretty new. That's like uh so I guess around 10 people I think work on this. Um definitely let us know where we we we definitely need to pick and choose where we focus on. We can't do it all I guess. And uh I I do think we have a good plan um like where where to focus on I guess next quarter for the 2.0. But definitely let us know if you don't think so. And and if if uh you have some some kind of big problems basically like where where you can't adopt uh Pyroscope because of those definely keen to to help you on this. I had a question about the um the function level profiles. I I know the last time I looked into Pyroscope, you had just there was an a tempo integration where execution level profiles had had been baked into the traces so you could see the profiles on that level. Is there a plan to do the same with the with the function level profiles for tempo? So I guess uh like the function level is more like a thing we would do on on top of the profiling data. So I guess we we want to kind of export everyone who calls that function and if it happens that it's a a function that is also associated with a trace that information will be available like but but I guess like it's nothing I guess you would necessarily see from when you come from traces. So currently the correlation basically works. You would look at a trace and if that trace has been using the CPU like enough to be seen by the profiler. So or basically the profiler will only see things that run on the CPU. If um in our example you saw we did not use time sleep. we used like uh self-written time sleep where we kind of check the time all the time uh and if the time has already elapsed by n seconds for example and that obviously causes a lot of CPU usage because every time you need to get the time and then check it um while maybe something like time sleep doesn't because the CPU just knows I don't have to check anything for 5 seconds and then come back to you and so we only see things that happen on the CPU in CPU profiles and So in order for us to see a trace, the trace needs to have have been active right at that time when we look at the CPU and we can't look at the CPU all the time because that would be too expensive. So we look at the CPU around 100 times a second. And so if a trace is for example 1 millisecond long, we only look at the CPU every 10 milliseconds. So it's very unlikely we find it exact in that moment. So I guess the chance is maybe 10%. I don't know if if the math is correct in practice but um like like so basically like that's always what you need to keep in mind I guess with the the traces to profiles correlation and then if that happens to be um collected and someone created a rule to export this function name to metrics then this will be included in there. be um basically like this kind of just comes directly from the profiling information. And then I I don't know if you've talked about it before, but I think there's also an integration with K6, right? Ksix and Pyroscope, which is pretty cool. Yeah. Like I I might not be able to kind of quickly demo something there, but uh Oh, no. about it's about kind of um um pushing the information through. So like having an idea this was part of this test run in K6 and this is the profile for it. And uh we we saw the the tag wrapper when we looked into the the function call um for the the car vehicle. And a bit like that we can basically attach any label to a code path you might run or to the whole program running and then as soon as it's seen running on the CPU it will be kind of associated with it. And that's kind of similar for for K6 where we basically like this is test ID 10 and this is the test name or like information like that can be passed through basically to be shown in the kind of profiles uh labels. So um yeah I don't know I I really couldn't find anything now quickly going um through it but uh yeah so like you it will just show up as a label um name value pair. Yeah, that can be super useful because um you know you it's kind of difficult sometimes you have to test in production right and and when you do you want to know what what type of load was associated with with that particular pattern of CPU consumption for example you might see a spike but then you see that we were trying to spike CPU because of a load test or something. So it can be nice to have that met metadata attached as well just to be able to it's really powerful attribution. So um when you when you actually enable um the the the kind of traces and profiles at the same time we we actually also do that on a span name level. So like that will be done by default. So, and often span names kind of are somewhat associated with the slashget or get slash uh um orders or whatever kind of um HTTP endpoints you you expose and and that's definitely helpful like always worth remembering not everything is kind of necessarily tagged with it. So, so I guess in go a garbage collector language, you will still have to do garbage collection and that will not be like associated necessarily with with those tags if if you do more in one call. But obviously the directly kind of connected um CPU usage is is covered by that and I I find that a quite powerful way of of looking at it. Cool. Was there anything in particular, Christian, that you you're interested in seeing like that you were excited to see in Pyroscope? So, um I can't really promise when that will be, but I I definitely think the the the I I just tomorrow. So, like yeah, like I I I think as soon as possible. Now something I'm excited about is like the the kind of integrations between the databases and going from traces to profiles is good but I I also mentioned the limitation with like it needs to be enough CPU sometimes like obviously our demos maybe are a bit more designed to use enough CPU to for it to to make that work in practice it might only hit like 1% out of your requests I guess right now like you can't say hey tempo give me traces that also have been profiled and So you kind of end up having a lot of traces to click through to then find one where you actually have a profile on. And I would love to have that information a bit more closer to it and then we only show the button to show me profiles when there's actually a profile because right now the experience is basically maybe only maybe 10% maybe 1% of those button clicks will work. And obviously this is kind of frustrating um for the user and at the same time I also want to do it the other way around. I guess obviously we have information about profiles and we know what traces have been prominent in CPU usage. They might not necessarily be um showing like high latency or any particularly other flags that you would look at it from the tempo side. And so it would be interesting to surface those and then open from the um profiling site in in in Tempo. Yeah, that'd be pretty cool because like right now, say for instance, you can go in Tempo and just look up everything that has errors, but being able to just look up everything that has profiles would be pretty cool. Yeah. So that that's definitely like the the kind of longerterm vision I think that that we have. Um I think for a start I think it will be something more like the button will no longer show up. um which is a a good improvement already. But then obviously like like having this within the search I think would be pretty powerful and would would definitely like like help that process of finding something where you you maybe want to uh kind of reduce CPU usage. Awesome. This was super cool and yeah, thanks for doing this and thanks for everyone who showed up. If you have questions um or want to talk or anything like that, you should come join us in the Gana community Slack. So, slackerfana.com. And yeah, so hope everyone has a good rest of your morning, evening, or whatever time it is. All right, see you all. Thanks everyone. See you. Thanks everyone for joining. Bye.

