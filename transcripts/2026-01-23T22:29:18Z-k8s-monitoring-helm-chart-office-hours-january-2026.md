# k8s-monitoring-helm Chart Office Hours (January 2026)

Published on 2026-01-23T22:29:18Z

## Description

In the January edition of the Kubernetes Monitoring Helm chart office hours, we discuss the version 3.7 release, the upcoming 3.7 ...

URL: https://www.youtube.com/watch?v=VyqreBQuVtM

## Summary

In the January 2026 edition of the Kubernetes Monitoring Helm Chart Office Hour, hosted by Pete Wall, primary engineer of the Helm chart, the session focused on the recent release of version 3.7 and upcoming features in versions 3.8, 39, and 40. Key updates included the integration of PostgreSQL alongside the existing MySQL, improvements in metrics collection to avoid duplicates, and enhanced usability for configuring pod logs. Future features discussed included gathering Kubernetes manifests for better observability, frontend enhancements, and plans for a unified Helm chart in the anticipated version 4.0. The session ended with a Q&A where participants, including Carl and others, engaged with Pete on topics like AI integration, fleet management, and security measures for sensitive data in manifest gathering. Overall, the meeting provided insights into ongoing developments and future plans for the Kubernetes monitoring capabilities.

## Chapters

Here are the key moments from the livestream, along with their timestamps:

00:00:00 Introductions and agenda overview  
00:01:30 Discussion of Kubernetes monitoring Helmchart version 3.7 release  
00:05:15 New Postgres integration and database observability features  
00:09:00 Usability improvements for cluster metrics and service monitors  
00:13:00 Overview of upcoming features for version 3.8  
00:16:30 Introduction of Pod logs CRD for log scraping  
00:20:00 Upcoming features for version 4.0 and merging Helmcharts  
00:24:00 Deprecation notice for versions 1.x and 2.x  
00:28:30 Q&A session begins with audience questions  
00:35:00 Discussion on avoiding sending secrets to Grafana Cloud  

Feel free to ask if you need more details or additional timestamps!

# Kubernetes Monitoring Helm Chart Office Hour - January 2026

**Good morning, good afternoon, everybody.** Welcome to the January 2026 edition of the Kubernetes Monitoring Helm Chart Office Hour. I'm Pete Wall, the primary engineer of the Kubernetes Monitoring Helm Chart. 

### Agenda

Today, we have the following topics to discuss:

1. The recent release of version 3.7.
2. Upcoming features in version 3.8.
3. Features planned for version 39 and beyond, including version 40.
4. A deprecation notice—don't worry, it won't affect you immediately, but I wanted to bring it to your attention.
5. As always, we'll end with a Q&A session.

### Kubernetes Monitoring Helm Chart Version 3.7

Version 3.7 was released about a week or a week and a half ago. Here are the major changes:

- **Postgres Integration**: We added integration for Postgres, complementing the existing MySQL integration that has been part of the Helm chart since version 2.x. This new integration includes database observability, allowing you to use the Postgres integration with Grafana Cloud's database observability product.
  
- **Sidecar and Service Metrics Integration**: A new integration for sidecar and service metrics has been added, making it easier to gather metrics from your sidecars and services running in the cluster. This feature has been in high demand, and I'm pleased to have included it in the Helm chart.

- **Usability Improvement for Cluster Metrics**: If you have both the cluster metrics and Prometheus operator objects features enabled, it’s possible to hit duplicate metrics when deploying on a cluster that already has the Cube Prometheus stack installed. To address this, we implemented a warning system. If we detect existing service monitors for kube-state metrics or node exporter, a yellow triangle warning will be displayed, prompting you to check for potential duplicates.

- **Pod Logs Features**: There are enhancements in the pod logs features, including more options for the secret filter, allowing greater customization of how it works.

- **Bug Fixes and Dependency Updates**: As always, we’ve included numerous bug fixes and dependency updates.

### Upcoming Version 3.8

Version 3.8 might drop as early as next week, featuring a significant addition:

- **Pod Logs CRD**: The Grafana agent and Grafana Alloy can now deploy a pod logs Custom Resource Definition (CRD). This CRD encapsulates pod discovery rules and log scraping rules, which allows for a third method of gathering logs. This functionality mirrors that of the Kubernetes API but utilizes the CRD for targeting pods instead of directly writing configuration into the Helm chart.

- **Environment Variables for Alloy Instances**: We will automate the setting of standard environment variables such as cluster name and node name if they are not provided. This will result in a cleaner values file configuration for deploying Alloy instances.

There may be additional features coming in version 3.8, but these are the primary highlights.

### Future Releases: Version 39 and Beyond

Looking ahead, we are targeting features for versions 39 and 40:

- **Gathering Kubernetes Manifests**: We're working on a feature that gathers Kubernetes manifests. This process will run inside the cluster, collecting YAML manifests for various Kubernetes objects and delivering them as log files to Grafana Loki. This will allow users to view and compare historical manifest versions through the Grafana Kubernetes monitoring plugin.

- **Frontend Observability**: This feature is still in development, but we aim to enable users to deploy based on how the cluster rolls.

- **Cloud Provider Label Enrichment**: This feature will receive more attention soon, as we want to ensure it integrates well with existing setups.

- **Prometheus Rule Object Support**: We are considering introducing support for Prometheus rule objects to enhance monitoring capabilities.

### Deprecation Notice

After the release of version 3.8, versions 1.x and 2.x will be deprecated. While I don't have a firm date, this is expected to occur in the first half of this year. We encourage everyone to upgrade to version 3 or the forthcoming version 4.

- **Migration Utility**: For users on version 1, we have a migration utility that can help transition your values file to be compatible with version 2 or 3. Users on version 2 will find minimal changes necessary for migration to version 3.

### Q&A Session

Now, I'd like to open the floor to questions. If you have anything you'd like to ask, please feel free to do so.

---

**Thank you for joining today's office hour!** We appreciate your participation and look forward to seeing you next month. If you have further questions or need assistance, please reach out via the Grafana public Slack or check our GitHub repository for issues and PRs. Have a great month!

## Raw YouTube Transcript

All right, good morning, good afternoon everybody. Welcome to the January 2026 edition of the Kubernetes monitoring helmchart office hour. I'm Pete Wall, primary engineer of the Kubernetes monitoring helmchart. Uh, and here is our agenda today. So, we're going to talk about the 37 release which came out just a little bit ago. Uh, talk about what's coming in 38. Uh, which I already have one of the big features ready to merge. Uh, and so that should hopefully be coming up soon. Uh, talk about the upcoming features for 39 and beyond and for 40 even. Um, and then talk about a deprecation notice. So don't get too worried. It's not going to affect you immediately, but uh, but I wanted to put that out there. And then finally, always as always ending with questions and answers. So Kate's monitoring homechart version 3.7. Uh, this was released uh, last week or a week a half ago or so. Um but fairly recently uh the major changes that we released into 237 is that we added the integration for Postgres. Uh so we've had the MySQL integration in the Helmchart for a long time since the 2 something release. Um and so we've added a parallel one for Postgres but this one also comes with the database observability. So if you are using graphana cloud and the database observability product within graphana cloud then you can also use the postgres integration to get that. Uh another integration we added anto sidecar and service metrics. Uh so if you add this integration you should be able to quickly easily uh be able to get metrics from yourto sidecars and from the stood service that's on the cluster. Um that's been a requested feature for a while now. Uh, and so I'm glad to uh to have gotten that one into the Helm chart. Um, excuse me. Kind of a a a just nice usability improvement for the cluster metrics feature. Um, if it's it's the cluster metrics and then the Prometheus operator objects feature if you have both of those features enabled. So you want to get your cluster metrics from things like coupube state metrics, node exporter, that sort of thing. And you're also looking for service monitors, pod monitors. Uh what I've noticed is there's a a chance that you might hit duplicate metrics. Uh especially if you're installing, you're trying things out, you're installing onto a cluster that has something like the Cube Prometheus stack Helmchart already installed that uses service monitors primarily for its uh metrics targets. And so what would happen is that you deploy the case monitoring helmch chart and we start scraping our own coupube state metrics but then we seed the service monitor for the existing KSM uh metric source and we scrape that one too. And so this there's just kind of a nice usability feature that says if we enable both of those features, we look at the service monitors that exist on the cluster. And if we see something that has the name gubate metrics or node exporter in there, then we just we throw a little warning, a little yellow triangle that says, you know, just by the way, we saw this. You might want to take a look at this and make sure that we're not going to hit that. Um, typically the easiest way to avoid that is that you can adjust your Prometheus operator objects feature to exclude those service monitors or ultimately you pick one or the other. Either you're going to use k um coup kubernetes monitoring homechart or you're going to use cubia stack you know pick one or the other. Don't have duplicate services on uh the pod logs feature. So the podlogs features uh there's two of them two of these features. So this applies to the the traditional podlocks features as well as the pod logs via Kates API feature. Um we've added more configurations to the secret filter. So if you're using the secret filter, there's a lot more options for you to go in and tune how that works or load I forget what they call it, but the load the secret configuration file or something like that. And then as always, lots of bug fixes and more dependency updates. Uh all right, 3.8. 3.8 date might even be dropping early next week. So, this will be coming pretty quick. Uh the biggest change that we're adding, the biggest feature for this one is that uh Graphfana agent from before and Grafana alloy now can deploy a pod logs CRD. The pod logs CRD essentially wraps up uh pod discovery rules as well as uh log scraping rules. And so back in the graphana agent operator time frame to you would use podlog CRDs to target pods that you wanted to gather logs from. So this is kind of a third way of getting logs. Um it it kind of mirrors functionally it does the kates API thing. So it mirrors the functionality of the Kubernetes uh pod logs via the Kubernetes API. uh but using the CRD is how you would target it rather than writing configuration right into um into the Helm chart. This was a feature that we had in version one and it's finally time for me to bring that into version three. So this is going to be coming into the version 3.8 release. Uh so if you are using the podlog CRD uh if you're on version one or if you're you know way back using the graphana agent operator and using podlog CRD with that um this hopefully will be something that you'll be looking forward to. Uh another nice to have another usability improvement. Um if you're using the Kates monitoring homechart and you've enabled the alloy instances with fleet management. So the remote config section um there's a lot of required environment variables and a lot of them are so standard it's things like cluster name and node name and you know namespace name and all those sorts of things. Um I've found by just looking at tons and tons of values files from people out in the field that man that gets really verbose. And so what we're going to do in 38 is if those environment variables aren't set, we'll just set them automatically. This is one of the the benefits of being able to deploy the alloy instances using the alloy operator. We can modify those sorts of deployment things when you deploy the Helm chart. So if you already have them uh those environment variables set, that's fine. We're not going to modify them. We're going to respect what you've put onto the onto those alloy instances. uh but we can iterate through what's going to be deployed and if they're not there we can set them automatically. So the end result of this will be hopefully a much cleaner looking values file. You just say here's my alloy instance. I want remote config. Here's the URL username and password and that's all you need to do. So that'll be that'll be nice. Uh probably some other things coming in 38, but these are the top two kind of feature type things that I'm looking for. I'm happy with this being a little bit more of a leaner release because then I can focus more on some of the other stuff. Um, all right. So, upcoming features. So, these are this is kind of targeted for maybe a 39 or a three something or maybe even the upcoming 4.0 release. Um, some of these will be familiar if you've been to the uh the office hours before. The the biggest change is the one that's at the top. Uh, this actually started as an internal project uh but uh but it got a lot of a lot of attention and a lot of people really liked it. And so we're going to be rolling this out into uh into the Helm chart and that's gathering Kubernetes manifests. So what this entails is a process lives inside of the cluster and based on configuration that you give it it will uh gather the actual YAML manifests for pods for deployments for you know any kind of object that you've got in your Kubernetes cluster. uh it will save those and then we actually deliver them as log file up to graphanaoki um or to whatever your log storage is uh and then you can use log parsing rules to find those manifests. This will be paired with a feature with the graphana kubernetes monitoring plugin in graphana cloud uh that as you're navigating through those you'll actually have a button that will let you view the manifests. Um, and even cooler of a feature will be can you see the diffs from previous manifests to current manifests. So, we're really excited about this one. Um, I am actively working on the code that's going to be doing the manifest gathering. I did kind of a hacky way of just proof of concept, but I'm polishing it up right now. And so hopefully, you know, in a near-term upcoming release, that'll be available. So, that'll be exciting. >> Uh, front end observability, we're still working on that. Carl, you had a question about that. >> Yeah, I had a had a quick question on this. So, >> yeah, >> could this be used for finding deployments that don't have like a odd disruption budget or deployments that have single replicas? >> That's a great question. Um, you know, we are actually just starting to figure out like what all could we do with this sort of thing. And I think that's a really great idea, a really great use case. you know, if you're you could set up a rule or a policy that's something that says, you know, look for deployments with single replicas or um one of the things, you know, I I mentioned the change management or the change detection. One of the other things that we're looking at is, you know, can we find things that don't have uh resource limits or something like that. So um definitely something that I'm interested in pursuing you know if we have this sort of information you know on your telemetry data storage at back end if that's graphana cloud you know what can we do to actually take action on that so great suggestion >> yeah because we're we're um we're doing some work around uh resiliency and availability and things like that so we're trying to find ways to look through our Kubernetes environments to find things that don't have pod disruption budgets or >> their single pod deployment sort of scenario where it's like oh well that pod goes down well that service just went down. >> Yeah. >> Yeah. >> But when you have hundreds of clusters and hundreds and hundreds or thousands of services it's a little daunting. >> Oh sure >> uh task. All right. Great. Thanks. >> I'll I'll tell you a little bit behind the scenes secret with how we're we're making the code work. Um we are you know everybody looks at Kubernetes manifests in YAML. We're actually going to be capturing and storing this in JSON. Uh and one of the benefits of that is uh since we're storing this by default into Loki part of the logql syntax allows you to do a JSON parser and then iterate looking for things based on JSON. Um, so I imagine with maybe even a single liner logql, you could say find all deployments that have replicas equal one or something like that. Um, nice and then either make your own alerting rule against that or just a dashboard or something. So it should be possible. Um, frontend observability, this has been on the list for a while. Uh, the ability to deploy with how the cluster rolleds. The the code is in there. I just need to make a good example for that. So, um, shoot, that sounds like a nice Friday afternoon project. We'll see. Um, cloud provider label enrichment. Again, that's been on the list for a while. Um, this one I think is going to get way more attention soon. So, I think this will actually rise up to the top. Uh, Prometheus rule object support. Uh, I think I talked about this in last month's one where, you know, I actually was I talked with somebody and it was convinced that this is something that we should bring in. So, we're going to do that. and then agents.mmd or claude or some sort of you know rules that could be used to enhance how uh you know AI coding agents work with the helmchart give it some intelligence on how to do things like uh you know from a user standpoint I want to you know limit the number of metrics coming out of this service what settings do I set or from the developer standpoint you know hey I would love to add a feature to this or modify this feature you know give me some uh some insight on how to do Um, so I'm I'm still really keen on doing some of that stuff. Uh, all right. I want to talk a little bit about what we have planned for the 40, the next major release. Um, so this office hour talks primarily obviously about the Kubernetes monitoring helmch chart. Um, beyond that, I spend a lot of time within Grafana Labs. uh from looking at all aspects of what do we ask our users to deploy onto their Kubernetes clusters to get started to get uh the telemetry data um it's no surprise that I work really closely with the alloy squad because of that um in public preview right now there's a tool called instrumentation hub that just got added to graphana cloud instrumentation hub is kind of a new onboarding tool or a new instrumentation tool uh where you can kind of just click and pick and choose choose what sort of services and applications or clusters that you want to monitor and how do you tune it from within graphana cloud and it's really really cool and powerful um for reasons that I don't really have time to get into uh the instrumentation hub uses a separate helmchart the graphana cloud onboarding helmch chart they are so similar but there are technical limitations from both perspectives that meant that they had to be separate for the time being so it's been my goal for a long time to like we should be merging these. We don't want to have confusion for people where they have to pick and choose what do they install onto their cluster. So in the upcoming 4.0 release, we are working on ways to merge both of the Helmcharts together and it will be a Kubernetes monitoring Helmchart 4.0. Uh so for everyone who's using the Kates monitoring Helmchart today, it will be a Helm upgrade situation and not a major uninstall and reinstall. Um the goal is to make the 4.0 release serve both uh experiences as best as possible. So it feels like a really first class experience. If you're using Kates monitoring Helmchart, it should behave exactly the same way. If you're using the Graphana Cloud onboarding Helmchart, you're using instrumentation hub, it should feel like it's built for that just as well. Um, one of the the most visible changes if you're used to using the Kates monitoring Helmchart is what I have on the second bullet point here is that we're going to get rid of the hard-coded collectors. So, uh, today there's five of them and you pick and choose and we kind of put the configuration onto these different alloy instances based on what it's meant to do. Um, and I think it's it's worked well. It's kind of hardcoded in our best practices. Um, but it also adds a lot of verbosity into the values file and it it limits the configuration and the um the configurability that you have with that. Um, and so kind of like what we did with destinations from the V1 to the V2, uh, we're going to do the same thing for collectors. So, collectors, even though I have, you know, alloy metrics hardcoded over here, this will just be an array and you'll name the collector and then from there, you'll be able to define all of the, uh, the same fields that you had for that. Um, the upside to that, even for Gates monitoring, Helmchart users, uh, you get to define exactly the alloy footprint that you want. It doesn't have to be any bigger than that. Um, if you want to put everything onto a single alloy instance, you can do that. We're not going to limit that to you. We'll still give you the same feedback based on, you know, if you're deploying things with configuration that require certain fields like uh volume mounts for pod logs or something like that, we'll still tell you about that. Um, but uh but it's it's going to be way more up to you to be able to figure out how you want those alloy instances deployed. Um, giving you a lot more flexibility. Uh and then I said better integration with fleet management instrumentation hub. I think even Kate's monitoring users um will see this too. The goal will be if you want you'll be able to deploy the Kate's monitoring helm chart but still see that configuration experience with fleet management or something like that. Um this is a little bit more up in the air so that I can't really promise too much about that. But I think that we really want all of these things to feel like really well first class citizen and experiences. So that's a bit of what's coming in 4.0. All right, now you're all worried about this one. Deprecation notice. So after 3.8, V3 will have the same feature parody with both version one and version two. So 3.8, the big feature that's coming in is that pod log CRD uh option. Um so I it's been time for a while, but versions 1.x and 2.x X is going to have to be deprecated and we're looking at sometime the first half of this year. So I don't have a firm date about it yet. Um but I am declaring that we're going to deprecate those versions. Uh and so I'd love to have everyone move on to version three or onto the forthcoming version four. Version four, you know, like likewise, I don't have a firm date for that yet, but I'm really hoping to have the first drop that soon. Uh maybe even this next quarter. So, if you're on version one, we have the migration utility. Uh, you drop in your current values file and we build the values file that will that should work with version two and version three. Um, there could be there often is some manual changes that you might have to make if you're using extra config or things like that, but for the most part, if you're on version one, use the migration utility. It gives you a very fast jump start to a version three compat compatible values file. Um, if you're on version two, a migration to version three likely has little to no changes. Um, the values files were mostly unchanged between there. We the biggest change, the reason why we called it version three is the change to use the alloy operator. Uh, and so if you're using things like um, uh, private image registries, you may need to set the image registry path for alloy operator. That would be an example of a change that you'd want to make. Uh but for the most part, all of the features, the destinations, those things are going to be the same. Um if you'd like to wait for version four, uh so that you don't need to make another change. Version three to four will require changes to your values file based on you know what I said in the previous slide. Uh there would the migration will be migration utility will be updated to make that version 4 a valid target too. So we will be updating the migration utility uh to work with version 4 as well. um the the long string at the bottom there, the URL, there's the Graphana Cloud document on doing all of the different migrations. Um you can also find that if you search around in the Helmchart Git repository. So, um I hope this doesn't freak anybody out. Uh we've supported version one and version two for a long time and I've been saying that, you know, I'm I don't want to be in the game of maintaining four major releases at the same time. Um, and so this has actually helped me gain a lot more velocity to, uh, to better support and add features to the to the top versions. So, uh, if you have questions about that, good news. Now is the Q&A time. So, let me go back to that. So, if anybody on the call has questions right now, I'd be happy to answer them. I'm gonna take a sip of water. Actually, I'm going to call out someone. Meet, you joined in the call. Um, you had a PR for a moment about adding like clawed intelligence and stuff like that. Um, but I think that PR dropped off. How I just want to hear about what's been your experience with with using AI agents? Um, did you have success with getting something that was uh semi-intelligent to work with the uh kids monoton? >> Uh, yeah, I'm exploring this this new feature or n paradigma like moving more that an agent modes and even going further with hooks and working working with enable the um the rough loops but just explaining playing around. I thought maybe kubernetes and charts could be a good start for this. That's why I created but um it's very unmature the state I said that's why I create as a draft say okay I need to let me take this back work a bit a little bit next week or so few weeks then I will create again so that's why I close the edit from um yeah it wasn't anyway as a draft so that's why I took the back but I see some good feature that will help us especially for um new folks who want to contribute the chart or who want to use the chart. So exactly what you pointed, how can I add new label, how can I >> right >> replace some certain things. I guess for those things would be really good >> if you could um give this capability to the hand chart. >> Um Beverly Buchanan from our documentations team inside Grafana Labs actually has been experimenting with uh an AI uh like training an AI based on using the Helm chart. you know, answering the questions like, how do I add another metric to the metric filter or something like that? Um, and so I think we're actually pretty close with being able to have something from that perspective, which which is excellent. Um, I expect if that gains maturity that probably gets rolled into the, you know, ask API on the graphana.com docs. Um, which I think would makes a natural fit for that. Uh, I'd love to be able to link to that from the the Helm repo itself, too. >> Um, >> that would be great. Yeah. Yeah. No, thanks for exploring that. I appreciate it. I uh I remember I was talking with someone back at KCON uh and talking about I want to make an AI that knows how to do this Helm chart stuff. You're like helmchart stuff that's simple. You know, it should be easy for an AI to to do that. Like you don't know my Helmchart. It's a little bit of a different thing than most of them. So, um but it should be possible. It just needs to I feel like you just need to drop in the right context, the right clues for it to know what to go for. >> I mean chart is awesome. So with the functionality I mean some things are uh complex but the nature of because it's handling a lot of work. So it's you cannot make is as simple as possible in this case for my understanding. >> So that's why it is how it is a bit but it's still easy to use. What uh I would have like question but if you uh can a little bit elaborate the combination with the fleet management because this is one of the most hot topic always for us when we work with that. >> Can you a little bit give us some hints? >> Yeah. Yeah. So, um, what I'd love to see is the the Kubernetes monitoring Helmchart does a really excellent job with building fullfeatured, you know, production ready alloy configuration for your Kubernetes cluster for all sorts of monitoring targets. Um, what I'd love to see is that giving people the choice of um, to be able to use fleet management also to be able to monitor or to to see those configurations and also make minor changes to those configurations too. where it gets a little bit tricky and this is where we kind of need to just still explore and think about what's the best way to handle this sort of thing is when you deploy the Helm chart the the the source of truth for how your alloy configuration gets built is your values file. Um, if you make a change to you want to change your alloy config, you change the values file. You redeploy the Helm chart. We build a new alloy configuration based on those values. Um, when you add fleet management into the mix, now you have to have you kind of have these conflicting uh sources of truth. Do you make a change in the values file or do you make a change up in graphana fleet management? Uh because if you change fleet management and it pushes it down and alloy starts using that um now the values file is out of date, right? Um one of the things I'm trying to avoid is having a persistent synchronization between values and fleet management. I think that that gets unexpected. It gets a little bit too surprising, too much magic if that is, you know, if that's the term I'll use. Um, and so I think that how it has to work is if you've configured your alloy instances to use fleet management, you need to kind of declare when I when I build this config, where's the source of truth going to be? Um, and so if you're do a Helm install and your your source of truth is declared for this is a config map, it's going to be coming from the values file and it's going to be a local config only. You have to accept that you're not going to see that config in fleet management. if it's going to be fleet management. So say, you know, maybe there's a way and I I can't promise that we figured this out yet, but maybe there's a way to say, I'm gonna Helm install and then fleet management is going to know, hey, when you installed this cluster, you wanted cluster metrics and you wanted the application observability pipeline. I'm going to create those pipelines in fleet management right away. Um, so you can just do one helm install and and get productive right away. You don't have to go and and then finish building the pipelines. But from that point on, uh, the pipelines live in fleet management. And so then you have to think about, okay, what do I tell the user when you helm upgrade if you changed your values file and say, uh, no, no, no, no, you got to go to fleet management to change this now, right? Um, so it's not that it's an impossible thing. I just I really want to find the the right user experience to make it work the way people expect it to um without surprises. >> Yeah. Would it be that direction? So like say if you say you user be able to manage some certain either based on signals or the component for example I want to manage cluster metrics with with kubernetes hamchart but let's say podlocks or um traces application observability with fleet management >> is this >> that is something I'm definitely thinking about is at the feature level making those configuration or making those determinations like is this a local config is this remote config. >> Um I think that definitely is the right way to do it um for that purpose where you can kind of mix and match. >> Okay, thank you. Thank you very much. >> No, excellent question. I mean as soon as remote management, fleet management, remote config, as soon as that was a thing that we were able to do, uh it's been the question, right? I want to see my Kates monitoring Helmchart configs in fleet management so I can see them there so that you know my app developers can make changes like it makes a ton of sense right I want to monitor the service that I've deployed I don't want to ask ask my operations team to redeploy a Helm chart to pick up you know new information um and so you know I'd love for that experience to to show up but again I I don't want to sacrifice usability or introduce things that feel surprising or like, "Oh, I didn't expect it to do that." So, >> yeah, it's not easy to task. Definitely. >> That's why they gave it to me. >> All right. Any other questions? >> Uh, speaking of of easy tasks, um, on the Kubernetes manifest stuff, >> yeah, >> how how can we avoid sending passwords or secrets to Graphana Cloud, >> right? Um, so one of the things that we so in the the code the way that I'm I'm anticipating how we're going to build it, um, we're going to we're obviously we're not going to let you set like secrets as a manifest target or maybe we will, but you know, we'll tell you, you sure? Um, so we won't capture secret objects by default. The other thing that I'm pretty sure we're going to do is just automatically filter out any environment variables. Um maybe we capture the environment variable definition, but if you define the value in line, we'll redact that. Um >> okay, >> probably we would end up redacting a lot of things that aren't secrets, but I'd rather air on the side of caution than than accidentally capture credentials. Um we're going to filter out by default like the status object because for the most part though, you don't really care about that. You don't care about, you know, the uptime dates or things like that. you care about the actual declared manifest. Um those typically should cover most of the credential, you know, potential places for credential injection. Um but yeah, it's it's something I've definitely thought about and uh want to make sure we are very careful about. >> Sounds good. Thank you. >> All right. Great. Um well if you think of other questions uh if you watch this recording and you have other questions uh please find us on the graphana public slack graphana.slack.com we hang out in the Kubernetes channel there. Um this uh all gets built out in open source. So you can go to our git repository graphana monitoring. Uh please file issues, please create PRs. We uh we'd love to see it. Um but for the most part, thank you so much for joining. I appreciate everyone who joined and asked questions uh and learn more about this project that we're building. Um have a great month. We'll see you next month.

