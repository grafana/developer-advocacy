# Scaling Observability for Dexory&#39;s Global Fleet of Autonomous Robots with Grafana Cloud:

Join Dexory's VP of Software, Matt MacLeod, as he explains how Dexory, a Grafana Labs customer, uses Grafana Cloud to ...

Published on 2024-11-14T20:08:39Z

URL: https://www.youtube.com/watch?v=XoqeF9bl_OY

Transcript: all right hello everyone um so yeah my name is Matt I'm from dexu um I'm the VP of software there so very pleased to be invited here today just to tell you a little bit about how we use graan cloud how we scaled um our observability for a fleet of autonomous robots we have out there so very few of you are going to be familiar with us so far um you know we're not a big household name yet so I'll give you a bit of a brief introduction uh we're startup based out of London um we've got a a product that offers autonomous uh inventory control for warehouses uh this is a product we've been working on for about 10 years now and we are proud to count some of the world's biggest um Logistics providers and manufacturers as part of um you know our customer base so uh also very pleased to just a little bit of self-promotion here we've just closed our series B in the past few weeks H so very excited to be scaling heavily in the next few months if anyone is excited about metrics which hopefully some of you are then there is a QR code there please have a look at the website please come and talk to me later we got a lot of rols open so what is it that deori does um super high level we come and install this big autonomous robot in your warehouse this robot has a collapsible Tower it expands up to about 14 M and it is the world's tallest autonomous robot uh it's loaded with cameras uh 3D scanners and uh lights and barcode scanners and the idea is that um you know there are a whole bunch of data capturing devices on this uh as well as Edge Computing for processing the data that we collect so we come to this uh come to your Warehouse we install this robot every night uh it drives around your Warehouse by itself and takes an inventory of all the items that are stored in your Warehouse uh and on the racking system it scans about 10,000 items every hour for barcodes takes pictures of them and then measures the dimensions using the 3D scanners and then this data gets uploaded to a customer facing platform um and then it gets compared with your internal warehouse management system data and the idea is that you can detect anything that's been out of place in the warehouse anything that's kind of got lost or stuff which is otherwise out of compliance with what you expect to be there so inure what we do is provide observability for warehouses and you can see the theme here um at Dex we tell our customers that observability is the key to uh scaling their operations because observability is the thing that uh lets businesses react quickly identify bottlenecks and then streamline their operations so if you have the right insights at the right time that's going to be critical to your operational success but it's not just our customers that use that that's something that we need internally as well so as we scale this Fleet of robots the problem is that we have the same challenges as our customers have there's going to be um you know information that we need to gain about our Fleet we need to understand if it's functioning optimally and we need to rapidly identify any problems in there so that we can resolve them quickly and the idea is that we need this whole visibility across our whole globally distributed environment just like our customers need it so the key lesson we've got so far is that if you don't know what's happening then you don't know where or how or what it is is that you can improve so observability is the first step in improving processes or outcomes regardless of whether that's happening to our customers or whether it's for us internally so at Dex what we do is we talk about the visibility Gap there's this idea that warehouses are quite complex environments stuff comes in from one end some blackbox stuff happens in the middle and then at the other side some stuff leaves uh much of what happens in the middle is a bit of a black box for operators and traditionally there's not very much insight into what's happening in there and so our solution is the thing that bridges that for warehouses but we tend to think about our own internal operations in a similar way so without observability that Fleet of robots is a bit of a black box we don't know what's going on if there is anything uh that is not optimal or how each robot is performing or if anything is impacting their performance so that's where for us graan cloud is the answer to that like our platform is the answer for um our customers so I'm here to talk a little bit about what our journey has been so far um and how we've Incorporated graan Cloud into that and I think it's a bit of an interesting Journey because the requirements of our platform are a little bit different from um other ones or what they might be like a kind of more traditional SAS platform and so our journey has been a little bit different and our priorities have been different as well so I've got three questions to answer here first off what is it that observability actually means for us like what uh what are we trying to capture why do we think it's important and then also how we structured our approach to it so a little bit about how we did that technically and how we did it um you know in terms of how the engineering um organization is structured and then I'll talk a little bit as well about uh so what the future looks like for us there and some of the lessons we've learned out of it and I will be rushing through this a little bit because I Am Naturally a bit of a fast talker but first I will share this is one of my favorite quotes from the animating kids to Adventure Time if you haven't seen it um sucking at something is the first step to being sort of good at something and for us it's very important to not that I'm here to talk about how we skilled everything we're not doing uh perfect job of that so far we're very much at the start of that journey and we're about 18 months into having the product out there so we're going to scale fast and we're going to constantly be learning new lessons and observability I think is one of the key things that is going to help us learn those lessons as we go forward um it's not just about monitoring things and understanding how stuff that we have is performing but it's about educating us about what things we don't know what stuff we need to learn about how that platform is operating and then using that to improve our processes so as long as we get started we have a good Pathway to Improvement so I'll start with what it is that we actually need to observe most of you were probably working on some more traditional sort of SAS platforms maybe some on Prem installs some data center stuff like that and I know that some people will be working on iot devices as well and I think those ones are interesting because those are the devices where it's not just about observing things that are under your control but it's about observing the environment around you as well and how that has an effect on how your systems work so I'll start at the bottom level for us we do a lot of observability at the hardware level we have this big Hardware platform loads of sophisticated Electronics so important for us is stuff like power consumption are we using more power than we expect less power than we expect does that indicate a problem with efficiency or some kind of like failed system that we might have we're observing a lot of information about our actuators as well so we got a lot of Motors in there doing stuff uh we need to understand torque voltage current consumption uh status other metrics that they might generate those can indicate things like physical impairment to the robot or the devices uh we need to know a lot about our sensor systems as well so are they working correctly if we're losing Precision from sensor systems we might be navigating suboptimally we might not be gathering the data that we expect to and a lot of this you know all of this information this is stuff that can point us towards failures that might be happening in the system or something that has kind of moved out of spec that we need to investigate and a lot of this for us I think we're again it's a little bit different from maybe some of the other systems you've been count of before we're tracking quite a lot of very high frequency data so we're maybe sending metrics in at one Herz or 10 Herz or even 100 Herz in some cases where we need to track um you know things like power consumption over very short periods of time so what's key there is just making sure that we have scalability for those metrics that we we kind of gather uh on top of this Hardware each robot is essentially just a big distributed system internally as well um it's an application server uh it drives around by itself and it's got 96 cores so you know it's a big um complex environment to measure uh we need to make sure that all the applications running on it are reliable that they're using the correct expected amount of resources we can scale a robot once it's deployed it's deployed you know we can add more containers to it we can increase a number of PODS that we're running so once it's out there we need to make sure that we are operating within the resource budget that we expect so we got a bunch of teams there maybe a 100 containers I think running on a robot we need to understand what the impact of things like software releases are are we within scope of SSD are we writing too much to an SSD it's a usable resource and uh you know are we using too much network band to send this data back so much really similar to the sort of things you might observe with a more traditional application uh one of the more unique challenges for us is this environmental thing I talked about so we are you know really subject to and what we need to be honest about is that the environment we operating is really really horrible um we need to measure temperature humidity how does that affect the system that we're operating with we need to measure things like dust in particular it's in the atmosphere if they um if they sensors are in dust they are less effective at gning um we need to look at one of the things we do look a lot at as well is um our IMU data so we've got a robot with an inertial measurement unit and those can detect things like shocks and vibration so if the floor in the warehouse starts to degrade as well that's one of the other metrics that we feed into the system and can tell us in advance if something is deteriorating in the environment and when I see these are horrible I really do mean it these are some examples of what a robot looks like after I think two weeks of not driving around they get absolutely kaked in dust and um you know this sort of like massive potholes on the floor that if a robot drives over them it will get stuck and somebody's going to have to make an expensive uh visit to fix that so you know getting this sort of information in advance can really cut down on the cost that we then have uh for providing that service to customers uh connectivity is another big challenge for us as well so we are dependent on customer networks for uh our backhole connectivity we don't provide our own network so we spend a lot of time observing what customers networks are doing they're all terrible like universally so robots need to adapt to lots of variable Network conditions um handoff between different access points you know massive uh Warehouse racks full of Steel plates a horrible radio environment so just for us being able to observe what's happening as we drive around um are we falling back to our cellular data are we using too much of it so basically understanding you know what is the impact of that connectivity and then finally big thing for us is the service level so it's not just how is the robot performing out there but it's also are customers getting the dataa that they expect from us so we need to monitor you know when the robots out there driving it's in whatever 25 km a day is it capturing all the data that we expect are specific missions that is trying to execute filling has that changed over time is there a physical change to the warehouse that's caused it so we're trying to understand you know what is the pattern that's happening here and then how can we make sure that we you know deal with any challenges that are happening as soon as we can and key here is also the effect of our own software release process so when you're working in robotics you know you are deploying uh these uh new software releases to fleets and we do it roughly every two weeks we're quite big on that continuous deployment sort of thing um but uh deployments to that Fleet can have all sorts of effects on the quality of scans the results of um you know just navigating around a dynamic environment like that so key for us is having a deep understanding of release metrics and how those have affected how the robot works so you got a kind of idea of the sort of things that we're trying to capture um understanding what's happening in the area um you know like stuff that we might not otherwise be able to observe this all lets us intervene quickly before the issues escalate and then start costing us more money like start requiring us to make service visits to customers and that translates directly into uh an impact on customer satisfaction they like it if they don't have any downtime and it's a lower operational cost for us as well so I'll tell you a little bit about what the system looks like today now uh just organizationally and technically and how we ended up getting into this state so we started out um 2019 I think actually to building our metric deck we had a very early sort of prototype version of this platform um my battery is running low uh on on the laptop here um we initially started building out this platform around 2019 yeah using the tix deck so this was primarily there is a side channel for us and the idea was um we just needed to know a way we needed a way to know if robots were um uh going offline or had their power lost unexpectedly you know that sort of thing was mostly just a way for us to be getting notifications about what was happening and so we built this like very easy to kind of scale up with a separate um selfhosted platform there and it did work quite well for us initially um it was easy to get a simple install running and then uh we uh you know were able to just kind of get that that second channel for us but in particular once we started building out our newer and bigger platform we were kind of realizing that um you know self hosing in particular was going to be a very big challenge for us um we got a very small team and the level of reliability that we needed to kind of get you know of different machines of metrics into there was was not going to be something that we could scale to so we started exploring new platforms at that point and then we ended up essentially settling on graan cloud as being the best inclass hosting platform out there there are other ones out there you probably know some of them um and for us what we wanted was something that was much more um open and would cut down on the amount of effort that we were having to put into a sort of running systems ourselves so you know the main use case here was still about alerting and notifications we B to know when stuff had gone down um but as we started pushing more data into that platform we were realizing that it offered quite a lot of potential for better visualizations and like richer analysis of the data that we were getting as well so it wasn't just alerting and downtime reports but it was also understanding you know more more data about what was happening uh technically it's a little bit complex the robot but um you know you can see kind of how the the core little bits work here uh the robot does have multiple Compu nodes in it each one of these runs uh a telegraph worker each one of those uh sends it down to a central node that's in the base of the robot so we maybe got 10 machines all running Telegraph uh Telegraph in the base acts as for us basically like a roto sort of service it makes sure that everything is getting collected from all the different machines uh that are running inside the robot uh individual application containers submit their metrics directly to it it tells um Docker output for for log information and that all gets rooted up to graffan Cloud either to Loki for a log data or um you know using the inflex DB line protocol I think for sending our metrics data and there's a big buffer on the robot because we can quite often lose connectivity for pretty extended periods of time so generally one of the things to call out here um is that Telegraph has been a massive help for us I think the the markets moved on a little bit but for us it still captures about 50% of our metrics overall uh in terms of the data that we we need to kind of grab from systems within the the robot um generally you know as well we are very supportive in that is open in the community one of our engineering principles is that we don't use single Source lockin platforms that's why we're not on data dog despite how many times they call me since we got the series B and one of the core it's one of the core principles for us we need the scope to control how our platform works and so having these sort of like open things that we can plug into the stack as we want is really beneficial so telegraph was great for us we were already using it we were very easily able to take it and just drop into the the sort of lgtm Stack that made much more sense for us with our graffan Cloud use and you know that same sort of applies to open Telemetry we haven't started making that migration yet but it is encouraging that we can then just start to plug those systems together as we need it talking a little bit about how this has worked organizationally um as we've skilled we realized the only way to keep up with what we need to do is to make the tools as accessible as possible to as many people within the organization as we can so we started building these systems out initially we had this idea there was going to be a platform team and they were going to be the people who get kept all the metrics and they were going to all whole stack and provide all the analysis and results that everyone wanted um that quickly exploded because we grew fast and we only had about 30 engineers in total so we didn't have the resources to have a dedicated platform team um we do have a small team that call techops internally it's kind of a platform team they provide Operational Support so they can do things like bring up um uh instanes they can help us uh debug uh uh issues with the system but the core thing for them is they got a little bit of specialist knowledge that they can help train other people within the team and within uh different teams so we've adopted this kind of Federated observability model and I've heard a few different people talk about this um over time and the idea is that we don't have one internal team that's The Gatekeepers we don't have one internal platform or metrics team but what we have is sort of like a central source of Truth and some basic tools knowhow and infrastructure that our internal teams can then use to take ownership of their own observability story so key thing is that means all of our data is still in a single location it's all in graan Cloud it's not like every single team is going going and building their own observability system but we've got all that data in one place and um even though this is like kind of evolved a little bit organically so far there has been some work to sort of train these individuals up that we're calling observability Champions or metrics Champions within teams as the people who can um kind of spread that knowledge throughout the individual teams and help support that scaling so I've told you a little bit what we're trying to measure how that kind of works for us um and how we've approached the challenge but I'll also mention just some of the Practical anecdotes of where it's worked out quite well for us so the sort of weird real world situations where we've used this platform to solve some kind of problem that we've got uh interesting issue that we picked up on a site we were deploying to in Europe so uh we uh noticed that Chargers were just failing at a surprisingly High rate um we had robots in there charging and the team would realize that they were just taking too long or maybe they weren't charging up um in time to go and do their their scans that night and this was mostly showing up anecdotally but what we realized is that we were tracking in the input grid voltage of all of these charges and at particular times a day the voltage in this Warehouse would just DRP and we uh you know after doing a little bit of uh investigation into that we could see that this was happening you know regular repeatable time every day our assumption is everyone was either turning the kettle on or charging forklifts we don't know which one and um that caused the voltage in the warehouse to drop and if we were chargeing at the same time that pushes us out of the uh the input voltage design of the charger causing it to fail now what was super cool about this is that I didn't do any of it none of the metrics team did any of it but the site operations team handled this themselves uh so they had this nice visualization of all the different input voltages uh for charges across uhit the fleet that they were looking at um so long as they they had this data and they were able to get right into doing practical debugging on it so they didn't have any Gatekeepers in there there was nobody telling them uh you know how to do that and then over time they've been able to take this sort of thing and then build it into a more comprehensive strategy this is now part of their daily and weekly reporting process part of the thing that they use when they first do surveys on sites as well so good toing is super key here um what we found is that once someone had set up the basic install of these systems then teams are really enthusiastic about grabbing it and then building up dashboards and allows that are going to support them in their goals another interesting case here where we had a what we call the one in a million sea issue so sea are just a bug and one of the third party libraries that we were using that caused it to crash uh when scanning bar codes about one in every 1 million images that we scan that adds up to one C error roughly if you're on a robot scanning hundreds of images a second so it's actually quite a high failure rate even though it only sounds relatively low and what made this one tricky to catch is um this is a sfall in a library that causes through various recovery processes if you've ever worked in robotics or iot you'll understand that nothing makes sense it caused a USB bus reset somewhere else which then caused another unrelated application to fill and this is a normal thing to happen for us occasionally applications fa uh noisy environments uh so it wasn't something that we weren't expecting but what we saw is that that frequency just increased very slightly and it was very hard to track this down because again nothing had changed in a particular application but the failure rates had started to increase after a release so we had this um you know we we're having all the metrics and login information in a single platform was super useful here so I think the Ops Team spun up this Mission specific investigative dashboard uh we've heard a bit about some of the other tooling that supports this now um but at the time you know this is the sort of approach we were taking um one of the things to bear in mind here this is a big complex thing affecting maybe 10 different machines in a cluster with um maybe sort of like 20 containers involved in it so trying to understand you know what was the event that was causing a failure how was that cascading through the system and you know being able to sort plot that data for multiple robots and dig down into it is something that was really enabled by having all of that data in one place for us uh and it's not just the engineering team either so I think metrics are a great resource for some of the the strategy side of things some of the business operations that we want to do as well um you I mentioned our series B round earlier if you've ever been through the investment process you know that investors cannot possibly get enough data so often we can start then creating some business relevant metrics for them as well from the data that we already have um and you know because we collect that data already we kind of reflecting on the fact that observing this stuff is not just about how the technology is doing and how our success rates are but how the whole business is doing overall uh we've got this very nice dashboard that we use internally for the strategy and that shows us I think the number of hours every day that every robot in the past year has been scanning and we can see you get to see some nice effects in this um you can see you know sort of the scaling strategy the burnning process for robots you can see the robots that are working uh seven days a week and the ones that have the weekend off and the ones that are scanning a lot and the ones that are not scanning a lot uh and I think there's an Easter holiday in there and you can see areas you know where we've kind of Taken pauses to work on our technology a little bit before rolling out so good visualization tools are key to making use of that data Across The Wider business so yeah hopefully you have a bit of an idea of the kind of things we're doing with metrics and observability we're still quite early on but um I'll finish up just briefly with some of the priorities that we think we will have next as well and some thoughts on what we've we've kind of learned so far so everyone's mentioned AI everyone talks to us about AI a lot AI robots it goes hand in hand um we don't have very much AI yet but as we look to the future we should probably have more of it so for us um predictive maintenance is a big thing that we want to look at more what that means for us well traditionally robot maintenance is a case of something fails and then you send an expensive service engineer to go and fix it in reality you can actually predict quite a lot of those failures ahead of time if you have the knowledge about what you're looking for so you're going to see things like Motors start to work harder you're going to see things like power moving out of spec and if you can detect that in advance you can get a service tech out there in advance and then fix it and then you've got happier customers who don't have downtime um that is also quite a hard thing to do though um it's really hard to look for these signals in all of that data so this is where we've kind of got these a little bit of work with AI and ml approaches that we can um that we want to use to sort of understand more what we what signals we can extract from the data that can tell us when these things are going to work going to happen in advance without us having to do all of that work of setting up those predictive maintenance processes ourselves so you know things like outlow detection and um you know other techniques that could could reduce the overhead to doing that uh other thing for us as well is really empowering more teams with this self-service model of observability so I told you that it had evolved organically and that really means that everyone just did it and then it sort of worked out all right so we're planning to formalize it a little bit more like we think it's worked quite well um but uh now we want to have this bigger sort of like support role where we can say you know what this is the right way to run observability in a scale up like hours let's make sure that we have the proper resources available for everyone so the idea is you know we can document the infrastructure very well make it very accessible and then continue to invest in building these observability Champions within each team so the goal just being to make sure that everyone has the skills and tools that they need to be successful and contribute to that overall mission so yeah I'll wrap up briefly um three KT takeaways from the journey scaling so far um observability is absolutely key to scaling operations so whether it's our customers with their internal teams um or or our with our customers or our internal teams having the right data at the right time is the thing that is kind of critical for Success so it lets us make the right choices about where we invest our time and effort we don't have a huge amount of time or resources available to us all the time uh second this whole thing about empowering teams uh with this self-service model is absolutely crucial to keeping on power rapid growth I'm delighted when I come across dat boards that individual teams have developed to support their use cases that I didn't know anything about and it's really you know in my view made a signific difference to how quickly we've been able to scale that and uh third you will always have more data than you thought and it's still never going to be enough for everyone and they're all going always going to want more and they're always going to ask you for more so you need to design your systems with that ability for scaling uh in place you data volumes are going to increase more than you think um and you know having the tools in place to handle that scaling is going to be important so for us you know I'm coming back to why we are using graan Cloud that's one of the key reasons is it's really been one of those things that let has scale very quickly and why we're very happy to work with graan Cloud so thank you very much I think we got a couple of minutes left

