# How to turn logs into metrics with Grafana Loki (Loki Community Call July 2025)

Published on 2025-07-09T03:17:16Z

## Description

Cyril Tovena shows us how to turn logs into metrics with Grafana Loki using metric queries in LogQL. What do you do when all ...

URL: https://www.youtube.com/watch?v=tKcnQ0Q2E-k

## Summary

In the July Loki community call, host Nicole Vanderhovven and developer advocate Jay Clifford, along with technical documentation writer Julie Stickler and expert Serial, discussed metric queries in Loki. The conversation explored the distinctions between metric queries and log queries, emphasizing the importance of understanding the theory behind them. Key topics included the difference between instant and range queries, the functionality of the newly formed Loki Helm Maintainers group, and the mechanics of turning logs into metrics. The team also addressed community questions about recording rules, performance considerations, and the potential of the Grafana Assistant for streamlining metric query creation. They provided insights into how to effectively use Loki for metrics, including tips for optimizing queries and understanding the implications of different query configurations. The session concluded with a demonstration of the Grafana Assistant, showcasing its capabilities in generating queries through natural language.

# Loki Community Call - July

**Welcome everyone to the latest Loki community call!** My name is Nicole Vanderhovven, and today we’re diving into metric queries. I’m joined by my partner in crime, Jay Clifford, who is also a developer advocate for Loki. 

**Introducing Our Team:**
- **Nicole Vanderhovven** - Host
- **Jay Clifford** - Developer Advocate
- **Julie Stickler** - Technical Docs Writer
- **Serial** - Expert on Metric Queries 

**Agenda:**
Today, we’ll be discussing metric queries, and it’s a great opportunity to ask questions. If you feel the documentation doesn’t cover something, now is the time to bring it up!

### Announcements
Jay: Last time, Ed discussed the push for Loki 4.0, focusing on architectural changes. If you haven't seen that yet, I recommend checking it out. We’re also forming a new group called the **Loki Helm Maintainers** to focus on updating and maintaining Helm issues for the next three months. 

A special shoutout to our volunteers: Hans, Christian, Yan, Otto, and Muhammad. They have been working hard to manage old PRs and ensure community contributions are merged. In just two weeks, they’ve closed 46 PRs and merged 9!

### Introduction to Metric Queries
Nicole: I’m excited to talk about metric queries because they're often overlooked. Serial, could you clarify what a metric query is and how it differs from a log query?

**Serial:** A metric query allows you to analyze log volume over time or extract specific data from logs. There are two types of queries in Loki: **instant queries** and **range queries**. 

- **Instant Query:** Returns a single value at a specific moment.
- **Range Query:** Returns multiple values over a specified time range.

Let’s explore some practical examples to illustrate these concepts.

### Practical Demonstration
Nicole: Let’s demonstrate how to use metric queries effectively.

Serial: When working with metric queries, it’s crucial to understand the difference between an instant and a range query. The **instant query** gives you a snapshot, while the **range query** provides a more comprehensive view over time.

For example, if we’re counting logs, we can break down the data by various labels, such as the service name, to see log volume over time.

### Querying Metrics
Serial: To demonstrate, let’s query some logs. We can illustrate how to count logs per service and visualize that over time. 

*Using the example of a service graph:*

1. **Count of logs** over the past hour.
2. **Break down by service** to see which services have the highest log volume.

If we switch to an instant query, we’ll only get the latest value, but using a range query will give us a more detailed view.

### Handling Questions
Nicole: We have some questions coming in. One from Quantum Q80: “Loki recording rules don’t output anything if the range is one minute. Can you elaborate on that?”

Serial: If the evaluation interval is too short, you may miss data points. It’s often best to increase the range to capture more data. 

### Using Grafana
Nicole: For those who are new to Grafana, our **Drill Down** feature allows you to explore logs without writing complex queries, making it easier to get started.

### Introducing Grafana Assistant
Jay: As a new feature, we’re working on the **Grafana Assistant**, which will help users generate queries using natural language. This tool will help make metric queries more accessible.

### Conclusion
Nicole: Thank you all for joining us today. We covered a lot about metric queries, and I hope you found it informative. If you have more questions, feel free to leave them in the comments, and we’ll make sure to address them in our documentation.

Thank you, everyone, for participating! We look forward to seeing you in the next community call. 

**Goodbye!**

## Raw YouTube Transcript

Oops. Hello and welcome everyone to the latest Loki community call. This is the one for July. Today we're going to be talking all about metric queries and my name is Nicole Vanderhovven and I'd like to introduce you to this guy over there. I've got Have I got the directions right this time? Am I still pointing the It's okay. Let's say yes. Right. My name is Jay Clifford. Uh, and I'm Nicole's partner in crime as a developer advocate for Loki as well. And I know nothing about metrics queries, so I will be fully reading from the script this time. Great. Today we've got down there Julie Stickler. Julie, welcome. She's she's our awesome technical docs writer and she's been working on metric queries for the docs. So, I'm sure she's going to be taking notes on on everything that people ask. Ask your questions. This is the time if you were like, "Hey, why don't the docs cover this?" This is the time to ask your questions because we also have our expert over there, Serial. Serial's been on a few times before, I think last time, um, he was on to talk about log queries, and one of the things he said after that was, "Hey, we didn't even get to metric queries." And that sounded to me like he was asking for more work. Yeah. Did you not write like eight blogs like like about four of them on metrics queries? Like I feel like up how many you did on logql. Good question. Actually I think I I wrote a couple of them not 10 but a couple of them. Uh and I still think they quite are relevant to the topic writing queries. Uh so metric queries. So yeah good call out. You should you should definitely check those out. Yes, I will link them in the description below. I think I already have at least one linked down there. But first, before we get into metric queries, Jay, you had some yet an announcement for us. Yes. So, um, last time Ed came on and talked about the push for Loki 4.0. Um, plug for the last one. If you want to know about all the architectural changes and the assumptions we're making for 4.0, definitely check out that video call. Um, and then catch Ed out on anything he said in that and make sure you hold him to account. Uh, we love to be held for account on these community calls. Um, unless he's put like a provision, especially when it's not us. Yeah. Um, we obviously want to make sure that we keep up support for Loki as it currently stands. And one of the big things that you've been asking for is making sure that we keep our helm up to date and solve helm issues as we go. Um, so we've been working with our Graphana Champions and we have formed a new group called the Loki Helm Maintainers. Um, we're trying it out for a little while. Um, we're looking at sort of a scope of three months at the moment. Um, and I will put on stage right now. Um, so here is our three extremely worthy volunteers um, who have been fantastic so far. So we have Hans, Christian, Yan, Otto, um, and Muhammad. Um, you might see them in the community answering questions on Loki already. Um, they have been doing a stupendous amount of work trimming down our old PRs for Loki, um, the Loki helm. Um, and also contributing also making sure lots of these have been merged where they make sense as well. Um, we're focusing primarily on merging PRs that have been contributed by the community at the moment. We're not really looking at issues as it currently stands. We just have so many to get through. Um, but I wanted to give them a special announcement here because I think we've only had the group running for two two weeks. Is that it, Julie? Wow. Yeah, it's two weeks and we've been picking off all the lowhanging fruit. So, there's a bunch of PRs that have been closed because they're already solved or they're duplicates or they're so stale that it doesn't even make sense. So, I might have the numbers wrong here based upon when I copied them in. Um, so Julie, you can correct me here. Um, but yes, we've 46 PRs have been closed. Um and up I think we're up to nine maybe merged now. I need to check. I think we're up to nine this morning. Amazing. And then yes, we still have quite a lot to go, but honestly um we'll keep working against it. Chipping away. We're going to be looking at some test automations to make it easier to validate um the helm as well. Um so yeah, so watch this space. Um so yeah, passing back to the actual topic at hand, Nicole, all about metrics queries. All right. Awesome. Uh I'm excited to talk about metric queries because we don't talk about them enough and it seemed maybe we should talk a little bit about the theory of it. Um serial could you talk us through what actually even is a metric query and how is it different from a log query? Yeah. Um so a metric query from logs by the way. Um so it's always from from logs themselves allows you to ether have a sense of like the log volume uh over time for a given query or also just like trying to extract data from the log uh itself and break down the data uh into you know a specific uh testing column for instance. Um and that's actually what the drill down does automatically for you. When you go around in the in the multiple tab there is on the drill down you can see different type of breakdown and they are all built using metric from um Loki. Um maybe like I I guess you know probably the first reason will be like if you need a ether breakdown or a sense of the volume over time but I think it could be interesting to talk about um how those queries uh work in general because they are very much different than uh quering logs but they also are very similar than quering uh primitives ranch queries. Um so there's there's definitely two type of queries you can do when you're writing a metric in local. It could be an instant query or a ranch query. The the the difference between the two is very simple is uh the instant query let's say it's the base it queries over uh you know a specific amount of time based on a given point start time uh or actually end time uh and it looks backward depending on the amount of data you want to look at and it just returns one point for that data set. Uh the range queries is the same except that there's a start and an end and each point is an instant query. Uh and it looks back based uh on a range interval which we'll look after and and then there is the space between each of them is what we call the step. Uh so it's the same step as in chromatus. Um so I just wanted to talk about this. It's probably still very abstract but we we we'll dig into this as we are um doing some quiz. So, so I guess in this is me from my basic term here is like you would use an instant query say in just like in a like a value visualization block where you or a gauge where you just have the the most inst the last value that you need from that calculation versus like a range query where you would use say like a time graph um or bar charts in order with a like time on the axis to work out the iteration. Yeah, the the the instant query can return multiple points but they are all from the same time. So multiple point if there is like if you for instance we could try try this now. Um so I was looking at um the service graph and I here. So interestingly I discovered that today as you were talking uh Jay that if you removed everything you have this the magic step here is happening actually uh which is uh a variable that allows you to decide what will be the step for that query that metric query. I suspect it's going to show up again when we're going to build a metric query, but as as you enter a log query, it just disappear. Um, anyway, let me show you. So, if we do a count of a time, um, so graphite tries to ask me. So, let me let me do it here. So, I'm not going to talk about auto for now. Just going to show you. Um, if we do, uh, if we do this one. All right. So this one shows there's only one uh pods basically running and man one apparently uh but we can break it down by for instance um cluster just to show you um that there is already labels um right so there's already labels for that log selector and one of them is cluster and I can show uh the point over time of how many log line there is so this is an hour but we can we can you know go back to six power on that on that instance. Um, if I switch it to an instant, um, now I only have one value. Um, if I were to ask something else, let's say something that has multiple pods, which will be more interesting. So, let's try to do that. Um, I don't know if this one has one. No. So, I can try to find maybe uh cluster, right? This one must have multiple one. No, there might be another um Loki data source in there that might have some more interesting. Ah, there we go. Yeah, I have another after. If if I'm not finding I'll switch to it by the way. Uh but yeah, just showing you this is an instance. So right now, so the stat actually is almost not useful except that it's still there because we'll we'll talk about this one here, the auto. But as you can see, this is an instant query and it returns a table. So you have one time stamp but multiple uh values break down by service name right. So uh in the last 6 hour this is the amount of logs each of them each of those u service name have uh and the biggest one is for instance the alloy is it the alloy oh sorry the k6ph uh service name so that's the amount of log line there is um I could you know combine with another with another uh search could combine with like errors for instance and so it gives it gives me an idea of the volume at at a nstant for each of those service name. Any any questions so far? All right, let's let's keep going then. Um so let's talk about the auto what what is the like what's the how does query works. So as you can see um the middle here is a log uh pipeline. I think we call it like a log expression pipeline. So it targets uh you know a set of logs uh it can filter them can filter by uh label but also by all the properties of the the the log line and we talked about this in in the previous call. Uh when you wrap this with a range vector and you always have to use a range vector operation when you wrap this with a range vector and we'll we'll discuss about all the possible range vector operation that exist. Um you also need to provide uh the interval. So the range of the range vector. So the range of the range vector means like for how many uh time back I should be computing that count. So because right now it's an instant query of six hour and I use the auto which basically pin the uh range to the same as the step. Again step and range are not the same but in that case because I use the auto it will automatically use the same step as the range. Then I end up with the last six hours of data. So in terms of like and this is where I get confused in terms of like the the the the step or the aggregation amount is this basically so let's say we were doing a range query um this is kind of like the bins that we're putting into so like if we like if we say if we put a five minute um bin rather than an auto there we would aggregate in a range query we'd run that aggregation against every five minutes within the range or is that the different from the auto step there. Yeah, if you so in in the case of a ranch query, if you do 1 minute and the step is like uh 15 minutes, then you're basically sampling. You're only looking at one minute for each uh range of uh 15 minutes. Yeah. Okay. So that's why I'm talking about it. It's nice to be aware of it. Most of the time you just use auto so that you have the step and the range that is exactly the same. Yeah. Yeah. Yeah. No, because it makes sense cuz that's that's where I want to make sure that it's where we know where it's useful that you start manually defining those where it makes sense rather than just using the in-built um auto mode of graphana because I think that's like a I I think I just rely on auto and then start changing like the actual parameters in graphana versus like actually hard coding those and I wondered where it'd be interesting to actually like hardcode um that parameter. I'm assuming it's probably when you don't want if you this if you change the range in graphana but you want that the specific step to stay the same you would just hardcode it so that visualization was always um static. Yeah, it can be useful for um uh visualization but also for like alerts sometime you want to set it to the same as the duration of the alert or things like that. Yeah, let's play with it. But for instance, like instead of using the the uh auto, which normally should set it to six hour, if I set it to one minute, we should see uh so I think the influx one go down obviously because it's just one minute and it's even worse than that. Like most of them are gone now in the last minute those the rest were not actually logging anything. Uh so I'm looking at an instant query. The time is now, but I'm looking only one minute back. So that's what instance query are. Um if I'm doing a range I can show you the difference between between the two because it kind of changed the uh uh sampling. So on one minute it looks like this now right now. So u looks like 14 u sample for the influx one. If I if I go back to auto which I don't think it's actually going to be six hour this time because it break down the six hour window into multiple steps. Um but we can play with it. the result is not exactly the same as you can see uh there. So this time I'm actually not missing a single log line. Uh and it sounds like now I'm actually going over the less than 10 minutes. So I was doing more. So I think the amount of the the current range which I still don't know what it is right now. Um I don't know if there's any way I think the coin inspector can show us that. Uh yeah the step was 20 seconds. So because before I was doing one minute I was counting more than there that that exist in each step I was counting like you know part of a step even before I was including multiple step data into one point. So I kind of think of and correct me if I'm wrong Sariel I'm just also trying to paraphrase and and see if I understand correctly. Um, I kind of think of instant vectors as a snapshot. Like it's a single moment in time, but it's everything that was happening for that whatever you're measuring at that time. And then the range is more like a historical thing. It's it's over it's everything over that interval. But I also think that more than that, um, it seems like the the output of a range vector is actually raw logs. Is that correct? They're raw log lines. So technically, if it were just range vectors, it's not a metric query. They're used as input for for aggregation functions that turn it into instant vectors. Right. Right. Yes. And which is the count of logs. Yeah. So what we're seeing here the the graph that's actually a graphical representation of the output of the range vector query, right? So it's not it because the actual output is the log lines, raw log lines. Yeah. So range vectors are are for log queries and instant vectors are are the outputs of metric queries. Yeah. Okay. Great. By the way, I can show you also something that is interesting. So um found this this blog post here which is in Japanese apparently. Uh and it show it explain a bit what is this range vector window. So every time stamp as you can see uh is like five minutes more and if your range window um so that the resolution is five minute but if your range function is 15 minutes you can see that you're actually aggregating logs that are for multiple uh steps and the auto just set the two together as the same. The resolution step and the range vector window are the same. Hopefully that helps understand a bit the difference between the two. Can we maybe step back a little bit here? There have been a few questions on recording rules that I'd like to get to. Um, but maybe we can also talk about like why would we want to turn logs into metrics instead of just just capturing metrics themselves? Like Prometheus I know does a pre-agregation of metrics. Why would we not do that rather than than turning getting logs through Loki and then turning them into metrics? Yeah. So there's there's definitively ways to uh um generate metrics from logs without ingesting the logs or from from the collection automatically. And I think you should do that if you if you are 100% sure you're going to use that metric. Um the reason why will still be interesting is that there's sometimes um cases where you didn't know in advance you will want to do that query which is you know trying to figure it out the unknown unknown. So when you're doing an incident, you find something interesting in the logs and you want to see the trend of what you found over time uh you know you know it could be an error could be a panic you cannot really foresee in advance that this keyword is going to show up during the incident. So that's when you want to maybe use the the metric code to see over time um you know when how does that trend if it's like thousand of log line per second or if it's 15,000 um with using just log it's difficult to see but using a metric query you can see larger time range and the trend over time. So that's one example. Um there's other example where you may have for instance an engineic server and you don't have access to change the collection to start collecting metric from it or it's just not possible to do that like it's not instrumented at all. Uh and you want to and you want to use the logs themsel for uh monitoring uh the the engineic server. So like the latency or the request rate per status code or region something like that. Yeah. Okay. So um and this is a Prometheus question so I don't know if you you can answer but um I know that Prometheus does pre-agregation of the metrics but does that happen at ingest? Uh does Potus uh aggregate? I don't think it does necessarily aggregate uh metric. Why uh why do you think it does aggregate metric together? Uh I maybe I'm mistaken but I was pretty sure that it did do some pre-agregation but anyway the question that I was interested in was one from rock darco here and I will get to the recording rules questions by the way people in chat but I just this one was more topical. Um, can Loki generate metrics at ing inest time rather than by doing periodical querying? And if so, what is better to use between Loki and the hotel collector to get that done? I believe the answer is no. Right. Loki keeps logs. It doesn't generate or store metrics. The metrics are calculated at query time. Is that correct, Serial? Yeah, that's correct. We are we are thinking about collecting some some uh I think metrics in the future like using the new format in V4. Uh but right now yeah right now metrics are only computed at query time. Uh but if you do want to do that I think alloys allow you to create like style metrics from the collector directly. So it kind of is at inest time and I will suspect that the hotel collector does that too or has the option of uh computing metrics from logs. Uh but if you yeah the answer is like if you need to do that the the using a collector or uh or um you know whatever you use to collect the data is is the good answer like Loki doesn't doesn't do that. I just Yeah. So I just wanted to check that a little bit though. So we we I mean there is some form of aggregation that we do in a way. I don't it obviously happens in the background because we use it for drill down logs right. It's like so with the pattern in gestures we actually do some metric aggregation like that's setting within the Loki config but I know it's only for specific parts of like you know sort of volume or like look when we actually look at patterns we then give specific points but I like that might be me misreading how we use the Loki configuration um but we we enable it by default now it's like pattern ingesters enabled by true and metrics aggregation um and then we kind of like write those to to Loki or sorry we don't write them to Loki but the the target for the Mmetrics aggregation is Loki yeah if I can find some but yeah you're right we we do have this uh stored now but they are only for the pattern yeah only for the pattern and so the pattern is only like where so basically at ingest when we see a specific we discussed this in the last log query um but basically when you see a specific pattern we we aggregate those so you see the specific count of how many times that pattern showed up in your your log lines. Yeah, that's correct. And we do store them as as aggregation. Yeah, that's the only case. Uh and it they are not available via explore and loql. So you can only have access to that either via the API uh or using the drill down pattern page. Gotcha. Gotcha. Gotcha. So it looks like Prometheus does do it doesn't do rollups but it does pre-agregate by metric name and labels and it does it before ingest. It does it in the exporter itself. Um so it's very different from Loki because Loki does it only at query except for except for the the pattern. Um can we talk a little bit about the recording rules as well and how does that figure into metric queries? Um yeah I mean pretty sure that um alerts um you know in graphana I can only be done using metrics. So it's definitely a good use case to turn uh logs at query time into metrics because then you can uh start alerting on error rate for instance or or bad worlds happening. Um so yeah I think it's definitively a good good use case. uh and I think the graphana manager app allows you to you know set up either a promot or a low key data source and then you have to write uh a metric query for the logs. We have a question here from quantum Q80. Low key recording rules don't output anything if the range is 1 minute. Could you talk more about Loki recording rules and about sending them to Prometheus as metrics? Um yeah, I guess you know I don't necessarily exactly for that for that reason why it's not uh working but I will I will just try to increase the range to see if it's if it it is related to the range. Uh but if you are if you have an interval that is different uh than the range itself. I don't know if we can see it here in the alert. Let me see. I'm wondering if this because I think this came up when I was doing like the Helm meta monitoring. Um so I definitely think it is something to do with like the um the amount of points the step being returned. We ran into this together too, right? I think it might not be the time, it might be the number of data points because I believe they're batched. Yeah. Um, so I don't know if I can change this one, but this this one is is definitely an example where you're looking at just 1 minute uh over 20 minutes. Um, so you might be missing points for sure. I I understand that's kind of confusing, but you want to have the the range here to be the same as your duration of the of the alert. Otherwise, you can you can be missing um some some data. Um, don't know if I have that's not this one. Let me see. Maybe you should create a new one, which I don't know if I can do that here. I don't think I have the right to do that. I need signing. Uh well, I I would suggest for for quantum Q80, um one thing you could do is increase the range. So try like 5 minutes or something, but also decrease decrease the evaluation interval. So maybe the issue is that it needs to run more frequently. the the evaluation needs to run more frequently. Maybe it's not finding anything within that minute. Um, and then maybe an offset would be good to give logs time to arrive. Maybe the the logs that you're you're expecting have just not arrived yet. Um, and if if you need some examples, I definitely recommend looking into um the Loki mixins because we actually have a bunch of recording rules there um that we use to support the Loki mixins which run on um one minute intervals. Um so you can definitely check those um and check them against what you're trying to run um and see where things might be going wrong. Yeah, the golden rule is to make sure that you cover with your range of aggregation the same amount of data um as as the evaluation period. So if it's one minute you want to cover one minute of data or you might be missing uh data. Sleman Kutlu has another recording rule related question. We have a recording rule for one of our apps and when there's no log records for the interval, we don't get metric data point with a value of zero. So they are getting a metric data point with a value of zero. How can we get zero if there's no data? Um I think you can do that with the vector operation. Um uh actually we can try it. Uh let me see. Can we try the same? So I think this one's going to help us. Right. Right. Let's see. So I think all and then vector zero. Yeah. So that that does add properly the missing point. So something like that should probably uh help you. So adding the all vector zero to fill the missing uh point. Um obviously I should be summing this one to not to see all of them um as a single one. But yeah, now we have the zero point. Oh, that's really cool. I did not know you could do that. Uh, and then if you don't, I don't have the zero point. Dang. Wow. That that could be a way of uh solving that. Uh, making sure you always have a value at least zero value. Well, poor Sleman just went uh had to go on call. So, godspeed. Um, but hopefully he catches that in the recording and he can add that to his alerting rules because that is really cool. Yeah, it's not it's not on by default because zero is not necessarily always the value that you want. uh in case you know zero could means actual zero and not value missing. So this is why it's not really the default. Oh that's also important that it's a it's a vector right it it will add zero to every data point that doesn't well if it doesn't exist right other than just like having a z a flat constant zero value. Yeah I guess if you do end you're going to get uh probably just zero actually no. So probably add zero if both are zero or something like this. I don't know. I guess yeah. Um okay, another another comment from Andre Zivani. I would love a way to generate metrics from logs that have a delay to be received like the AWS ALB. If I use offset, then the metric time does not correspond to the actual event. H Oh, that's a good one. I don't know if we have an answer for that. Um honestly we we will need to ask the alerting team if there is a way to uh include uh in the alert the the the original original time that maybe could be done with like on the visualization side like as a transformation uh could be. Yeah. So there's I guess that's also a question that I have. When would you use a metric query within Loki? So using a query to generate a metric in Loki or and when would you use a transformation in graphana? Uh I I honestly always use like Loki as much as possible because that's what I know uh more than transformation uh themselves. uh but I I think I think transformation are basically the the the case of like um you know transforming the result and and the query language capabilities is not there. So it allows you to overcome this and has been done in a way that you can apply to the data frame format. So not just low key. Um so yeah think it really it depends. I don't I don't think there's a golden rule. So it's more like if the language doesn't allow you to do exactly what you want, then using a transformation is probably the the right way of doing it. Yeah. And I think if you do a transformation that will only apply to the dashboard and you can't then run an alert on that, right? So that would still be a good case for doing it through Loki. Yes, you can use the graphs. Yeah. On top of that. Okay. I think uh I think this is like a interesting thing. I don't know if we get to this later, Nicole, about sort of performance considerations and the workloads expenses, but um I think the this is a good one from I apologize. Nicole, would you Jerry Roll? Jerry um so he he struggles with the cost of metrics from ephemeral workloads like Cunners. Um what would it make sense to save on metric CPU as log lines and then query them? is this should he be doing these as recording rules and you know pushing these to Prometheus and saving the expense that way rather than running constant logql metrics queries over the top. Um yeah I I guess that's not like the problem that uh that we're talking about is more like I have too many uh runners and then running a a large uh queries and it's not loy but promotous queries I guess uh is is causing problems. Um I think I think the right way to solve that is probably remove some labels that are not really required. So aggregating those uh runners to together. So find a way to aggregate uh one of the label that are causing many runners to appear to be different. Could could you could we could we move that into structured metadata or would that compromise the the metric query? Um I mean I think you yeah I don't know the like it depends the on the limit like how much can you uh can you query depends on the on the query itself. Um for the structure metadata I guess they they are helping for avoiding to do a parsing but that's pretty much it. So it's not going to solve the high cardality problem really. Okay. Okay. I mean, maybe this isn't exactly the answer that Yuri was looking for, but since he specifically mentioned CPU, I would look into Pyroscope because it's optimized just for that. Yeah. And also for sure. So, I I actually have a question. Um, how do metric queries actually like what what's going on under the hood? Uh, so Loki isn't storing them. Are they getting sent somewhere else? Where is where are the those numbers stored? Um actually only for cache purpose we are uh writing some of the the data in our infrastructure but otherwise nothing is really stored. So obviously if you're asking like a six hour of data and then you ask asking 12 hour the first six hour is going to get cached and not quite again. Uh but other than that we don't necessarily um yeah store that uh data to be reused after. So it's just caching. Okay. Can maybe we can show them a a demo of what you can do with metric queries. Yeah. Um so I think so we haven't we haven't uh dig too much on on like um more complex queries than like the count of the time but obviously the documentation has um all this information. One thing that we haven't touched on yet and I'll show you a dashboard uh to showcase this is that right now it's all about counting the amount of volume that you have but also like you know by per second in term of size of the log line but you can also extract directly data like if it's an engineic server you can extract the status code and then rate per status code or if there's a latency you can write a query that builds a quentile time or an average latency See um and we have a dashboard on the uh play.graphana.org that showcase uh this. So this this dashboard here is from a engineix server. So the engineix server output in JSON but you should be able to use the the same technique whether it's in JSON or or plain enginex uh format. Um and this this wall dashboard here is only built on top of uh log log queries. Well, metrics on logs. Um, so we can maybe uh look at a couple of them. So the total request um doesn't necessarily need to um so we don't necessarily need here to basically uh pass the data. So it's a very simple count of time. Um actually we have a count per sum by O but sounds to be aggregated entirely here. Um right so that was a simple one. Now if we look at so this is just showing the logs so it's kind of nice if you want to see the the recent request. So this is a log panel uh so it's not a log query but it's nice to show you that you can also reformat the log line to show exactly what you want to show on the panel. So this line normally is like much much more awful uh to look like. Let's see. Yeah. So it's like terrible to look at this uh in a dashboard but the line format allows you to uh make it a bit nicer for consuming on your dashboard. And then those here are the most interesting one I guess because they are uh metrics from this uh engine server without having metrics but just uh using the logs themselves. So we can look at what what is happening here. So this is a count of a time uh and you can see that uh it's summing by status but status is not a label that exists actually. don't suggest you to have that label. Uh but it's it's coming from the fact that we have this JSON parser here which extract from the log line uh all the JSON values and one of them is a status. So I guess sort of like under the hood this is like the order of execution. It's like we're performing the standard logql query underneath. We're extracting that information and then we've wrapped in the metrics query. So we have access to all of those extracted attributes afterwards. So I guess what you showed last time the ability to um pass your log body into different like you could reformat it, you could reshape um and extract values. So that could be an interesting way of um you could reshape your log structure completely and then um then run a metrics query over that log shape. It would be an expensive query, but it gives you the option of um having some more interesting metrics on very ugly log lines. Yeah, that's correct. uh it will be more expensive than just using the parser definitively and and one of the most expensive operation is definitively rewriting the log line. So that's why usually you rewrite the log line just for log queries because log queries just return a limited amount of result and range queries on metric basically go over each log line in the in the range. So it's it is going to be expensive for sure. You can in the same uh kind of idea you can run multiple passer by the way if you're not if you're not specially sure you can do JSON and lock format uh together but again it's not super efficient to do that I guess that kind of kind of brings us into one of our community questions we released beforehand um and I'm just going to copy it in Nicole um the Here we go. And then hopefully I can show it. is posting. Um, so he said, "Hey Nicole, um, it would be great to have some advice on running metrics queries on tens of terabytes of logs. Um, I'm aware that the current Loki architecture is designed for such a use case. That's where Loki 4.0 is coming. Um, but is there any performance um, areas in performance that we could look at or tips to improve? Um, and I think this is kind of highlighting what you're saying in terms of like we look through every log line and we iterate through those in order to generate the the metrics. So is there anything that we can do? Is it is it based like having better label selectors? Is it you know how do we sort of or is it you know basically just increasing capacity because we do do these types of queries at scale for some of our larger customers. So how are we currently trying to tackle this with the current architecture of Loki? Yeah. So the so most of those like the the the the metric that we've seen like the simple one that we've seen so far are actually the same one that I used to show you you know the histogram on the explore uh and those work for like terabyte of data for one of the you know for some of our largest customer for sure show and and the reason why is first they are like a bit simpler usually so they are like just count over time of the data they don't do processing um and they also are streamed so there's a streaming um algorithm running in explore that you know queries a bit of data and then slowly show the data. Um so there's like some some some tools that we have in graphana to make it feel like um you know it is executing a lot and and you don't have to uh to wait for a very long period without having result but ultimately it's all about making sure you write a efficient query. So I will say most of the time make sure you just filter out as much data as you want to graph. So if it's an error over time, make sure to pinpoint well that error with a equality operator in your uh in your pipeline to to filter down the data and then run the the metric on that. Um and even though if you have like terabyte of data and if you're well deployed, you should be able to handle that. Okay, the same person actually had another question that also relates to um the performance impact. So, Avananishes Vagela, which who is by the way one of our graphana champions asks um it would be good to know what kind of performance impact does structured metadata and different parsers have on the metrics queries? Yeah. Uh I'm trying to think about this one because I don't know on top of my head. Um I'm trying to think about how this the the structure metadata are executing. Um I think you can so the structure metadata is nice because you can avoid the parser. Um but it's not always it's not always uh uh faster uh because some structure metadata operation like equality for instance are not as fast as the uh pipe equal operator. Uh so there's some case if it's like pure passing uh basically you want to do a breakdown of uh log line per stages code uh and and the the the log line that you have are HTTP codes uh contain codes I think it makes sense to use structure metadata there so that you can break it down. uh if you wanted to just filter out uh you know just the 500 it will be better to use equality operator uh in your in your without using the actual social metadata it will run faster so there's total subtle like improvement but it's not it's not unfortunately solving like uh all the use case and this is why the team is still working on like a v4 where there will be a a truly columnar storage format so so that's interesting so I kind of understand then. So it's kind of like if we just do the pipe equal operator, we're just basically searching for that that key key value in the in the the bytes that we have of that log line versus checking the structured metadata store of saying okay does that log line have that specific pass. But it does benefit us if the if we start adding log MFTt or JSON to the pipeline because we have to then unpack the log line and bring out those attributes and then you're going to do the metrics query over the top. So the you that's where we see the benefit of structured metadata come into play. Yeah, definitively. Gotcha. Sven also has a question. Are there any Grafana apps that can help me with querying data from Loki without writing metric queries manually? And one of our champions, Andre, has already answered. Have you checked logs drill down in Graphana? Isn't it great? I um I can't I cannot believe this Ven is watching this. Uh but yeah, I can I can show you a drill down uh very very quickly. Um so the the drill down app is a bit different than explore where you have Yeah, thank you Andre where you have like a queryless experience. So you don't necessarily need to be an expert in LoQL or PMQL to be able to get started. Uh and you never have like an empty page. Um so the the the the beginning of the page is showing you all the services that are available and some sample of the log line and you can start drilling down um from from there. So I selected a service but I can swap can change the different label um I can search log line uh but I can also see breakdown. So those are actually metrics from LoQL that are generated automatically by the app without you needing to do that and it shows you the breakdown per different level. Um and this is a JSON um so this was a JSON uh and so we have a field here where it's automatically using the JSON parser to show you some of those uh breakdown per the JSON value. Uh and that can be uh interesting if you want if you're looking for uh spikes on specific um properties and the pattern one is the that's so all of this is like without any uh um lo but you can still do a bit like almost the same uh feature. So if you're looking for error for instance, you can have the error added here. And if you want to only select for instance uh let's say uh this one here. So we can include that and auto automatically. So it's all clicks and if you want to go further you can always um I think it's let me see it's in this one explore can always go and then see the query that you built. Um nice. Yeah. So that was not for the log but this is the log one. So you can go to explore and see the query that we built together with this app. Very very cool. It's nice way to get started learn about the language without you know without having to learn from the start. Yeah I really love it. Like if I can if I get it in logs drilled down why would I why would I write actual queries when it's already there? Um, but I also one of the reasons why I wanted to show that Graphfana play dashboard is because I still go to that like when I when I was learning I mean I'm still learning but um because it has so many different ways to turn logs into metrics. Anyone without an account can go there hit edit and then you can just see exactly what the query is and then you just change it to have your the variables that you have. Um and and and it's a lot easier so that you're not starting from nothing. You're just like copying and modifying slightly. Yeah, definitely. And one I also Oh, sorry. Go on. I was about to say one advice I will give to anyone trying to write a a query metric query from from the start is to always query for the log first. Um so you always want to see the structure like how the logs like logs are before being able to you know use the right parser or you know aggregate on the right column. Unfortunately you need to build from the log expression first and then once you're happy you start doing metric code. So in in honor of Andrea who's just left us um and I have a lot of questions about this because I have no idea how to use and wrap. So I guess we kind of had it in our our script for this one as well. So could you take us a little bit what uh around what unwrap is used for? Um you know how do we sort of like unwrap a metrics query and and then actually yeah if we can drill into Andrea's um request here around his use case on unwrap that would be awesome. Yeah. Um I'm trying to think about so I think the engineext dashboard has some unwrap uh but I think I can maybe use something a bit more uh interesting. Let's see. All right. So I have this thing here. So I guess while you're doing that quickfire question is unwrap only used for aggregation queries or like metrics queries. This is me. I have no idea because I've not used unwrap. Yes. So the way that Okay. Okay. So, what I know about it, again, chime in Serill if I'm saying something wrong, but the way that I understand it is like, you know how we were saying that um the the output of of a metrics query is usually a vector. So, it's like it it's not a single number. It can be multiple numbers, but it also is paired with like it has key value pairs. It has a time stamp. So, it's not just one number. It's like it's multiple things. When you unwrap something that means that the result is a single scalar value. Yeah. A a single number based on the entire time period. Yeah. So the unwrap is trying to get a number from the log line and use it inside the Yeah. as the as the base for the sample for the the the aggregation that's going to happen. Ah okay. So is is this where they're sort of like setting about sort of like the dynamic unit here? Um it's like they're basically trying to like extract a value in this case what was the their value is the size field which is 100 megabytes. Um and they're trying to use that as a scaler or a number in the actual calculation. Yeah, that's right. So the so you know so far the only queries that I wrote was like count over time which is just counting the amount of log line matching. uh when you start using wrap you can do other operation like average over time quantile over time you can do much more uh you know different operation that are on scala um and I can show you those so those one here are those log line here they contain like numbers so we have like I know it's very it's very small but let me see try to remove this um so some of them have like duration for instance or let me see you have so this is a duration you have a the the amount of return line and we can use those to aggregate over not just on the line like the the fact that the line exists or not. That's the that's the idea of the uh of the unwrap. Um so we could try for instance like those those are like queries that are made against the cluster. We could try to uh unwrap for instance uh the total entry for each of them and do uh so we could do a max over time on the total entry to see which one has the max total entry or like how the max total entry graph over time. Um so I'll show you I'll show you how how how usually I go about this. So I usually you know target the log line that I want this one as like metric.go but it could be something else depending on what you're looking for. Again, this is like best practice. Try to always filter out, you know, things that you don't want in your metric queries. And then here we want to do a log format. So we can we can grab those all those um you know, log format uh values. So it's like passing JSON but this time this is for log format. So when I do that now uh I can see that this p here which says passed. So this come from the uh the actual log line and those are values I can use in aggregation or unwrap if they are an integer value or with with Andre's question in mind can we do like the I think there was a total bytes something there we can do the bytes one yeah I just need to find a bite so so far we have only duration but yeah there's there's a total bite all right so we have a throughput here so we could do the average or the quantile let's do a quantile so what's the p9 throughput for queries here and so let's do that. This is already blowing my mind. Did not know we could do this. So this is cool. So you need to unwrap and then because this is uh the total bite you need to say bytes I think. So wrap unwrap cannot have function. So this one is a bite to convert a bite into bytes like a bite as in uh this format here. Uh so I'm going to use the throughput here. Uh, and I need to do quarantine over time. So, let's just remove all of this and then add the auto uh, and we're going to give uh a by empty for now. So, it's over not necessarily over a pod or an infrastructure label, just over all of them. Um, and that should already that should already do it. So it's going to take this uh throughput value which is in uh megabyte or sometimes it's not in megabyte I guess it could be in bytes transform it into a float value and use that vector to create to create a quantile over time. So for the quantile I actually need a value for the so I'm going to use this P99 right so let's run that query right so um that works which is a bit what's a bit annoying is that um you know we can see that uh the uh units are not like there's no way to set units I think in explore uh so in a dashboard I could use that in the dashboard uh and show you you know the maximum uh throughput so it's like 12 billion I suspect It's nine. So it's like 12 12 terabytes. And this is saying that this is the 99th percentile um throughput, right? It's only 1%. So it's only 1% uh of the yeah queries that are considered. Yeah. That go above that number. Um but Andre was asking specifically for the bytes. uh is there a way to convert the unit of the field so that it dynamically displays like megabytes or gigabytes or something or bytes? So for the MRAP that's how you do it. Um if you if you need to uh display D I mean this one wasn't um let me see but is there like a dynamic unit that we can have like a a measure a unit of measure this was the unit by the way you can see if I don't if I don't do that it's going to fail so it's transforming the it's failing because it doesn't have Andre has left but I I think what he's looking for is like for it to output put 100 megabytes as an MB if it if it needs to. Yeah, that that is that like what I just show here was just a problem with uh explore but ah okay because it's because we're not seeing it because it's not on a dashboard. Yeah, it's not a dashboard but once I Okay, I see once I run that query um there's units somewhere. You can convert from there. Yeah. Where's units? If there's a little search bar at the top if you just want to try that for it to uh at the top of the that's the little search icon. Yeah. And that should give you the the just to see where you got time series and if you do the little search I think that will give you and you can just start typing units in. Yeah. I couldn't search it. I don't know. Oh, sorry. The um next to time the time series down. Oh, no. Sorry. Yeah. Yeah. Yeah. There you go. Yay. Uh yeah, this one is megabyte. So I don't know. I think it's bytes. Uh this is really cool though. I did not I this has given me ample amount of possibilities in extracting numeric values from look. Yeah. So this is uh this is this is doing the uh quantile over time of the throughput but we could we could have played with like latency. I think there's a duration too uh in there. Yeah, there is a duration. So you do need to add duration I think here or duration second. We'll convert the label value from go duration format. So I think that's the one and this is showing uh duration this time. So I think it's in seconds. Um there we go. Yeah. So that's the and you can see they are actually matching like the the the time it took for the the the throughput are matching the two queries obviously large queries are are executing longer for a longer that's why um cool so it is possible so it is yeah to use the so if you use a graph you need to look at the documentation of loql there's like couple of function here that are supported uh but then after when you're building a dashboard it's up to you to know what's unit in the case of those function they are very special unit definitively. So duration second is in second and uh bytes in is in bytes. So I've got a question. Well go on Julie this all seems terribly terribly complicated. We've recently built logs drill down to make it easier to get into the logs. Are we building anything to make it easier to do metric queries? Um I'm handing you your segue here. Yeah, I mean kind of kind of like the the in we were we were looking at it. So some of those um so if I show you some of those um if I if I take this No, she's giving you like a a perfect in to talk about your baby project. Oh yeah, we can talk about that. It's okay. I understood you. Yeah, let's let's let's talk about the assistant. Um, I wanted to use it to fix this unit thing, you know, but I couldn't I couldn't really save that. I'm hoping the graphana assistant will make me as good as Sirill and Metrics queries. So, I don't have that same look on my face that Rock said I had anyone else that missed my segue there. Sirill is working on a project that we decide to talk about the end of the hour and it's the end of the hour. So, yeah. All right. Yeah, let let me show you uh how you can use the assistant uh which is a cloud feature we have now uh to query your logs. So what's interesting is that everything that I said is kind of built in like you know what you should be doing and the best patches. Um so you can you can ask for instance uh do we have logs that contain metrics go in the low key? So you can use just natural language. Um, and it's going to go and try to find if I have a log line like this. Um, I could have asked. Oh, Entropic is overloaded. So I don't know if we're going to be able to do it. U sounds like they have an outage. Um, oh no, classic demo card. All good demos. Yeah. Uh, let's let's see. Sometimes it's just like temp temporary. So let's see. like entropic API is really really under pressure these days. Well, we actually have another question maybe while we're waiting. Andre said that he would that it would be nice to talk a bit about the ruler evaluation mode for local and remote. So, I don't necessarily know about the difference between the two. That's a good question. So this is what from what I understand this is like the difference between like the ruler sending basically the ruler evaluating itself versus sending it to a querer um or the the inverse of that there was basically um this is where I'm I always get lost between which way local and remote is but you basically it's either the ruler evaluates it itself or the um it asks it sends off asks one of the queryers to do it and then brings back the result. Um I don't so I guess I think that what Andre is asking is with metro queries in particular should we is it better to say to say that to set the ruler valuation mode to local or to remote? Uh I'm looking at what is the difference between the two. So I think um if it's local then the ruler executes the query itself and if it's remote then it delegates a query evaluation to the querer API. Yeah. So I think the answer is remote is better. Is that is that a is that a question or just it's a question but I I think you're I understood it too that the remote mode would be better. Um because if you do it locally then uh each ruler instance does the work independently. There's no shared query cache and it doesn't use the query front end optimizations. Right. Yeah. So the query font has a lot of optimization like splitting queries into you know smaller subset and execute them in parallel and like you said like caching uh there's query shing. So there's a lot of like query acceleration technique in the query front end and if you use the local execution mode from the ruler the ruler access the data directly from uh uh the binaries so it doesn't use the query front end uh and doesn't get access to those uh acceleration so um I think the remote has been added after so that it will be uh more efficient and also easier to scale because you only have to scale query front end and you know something it's it's it's much easier than the the the ruler themselves. Okay, let's go jump back to your your demo which I think is working now magically. Yeah, I don't know. So, I don't know how long it's going to it's going to work. Just show you I just asked about you know do we have uh this logs and it just did the query for me which was obviously a very simple uh query. Uh but it does it does have a sense now because I did that query. Uh the assistant has a kind of like a sense of what that uh log line is and we can ask maybe more natural question. Um for instance like what's the uh latency per um is it tenant I think we have tenant on that or or maybe or what's the latency per or um so the assistant like you know knows knows how to do those uh queries uh and he has the context now of the log line so it's just going to generate those complex and wrap query for me um and hopefully Okay, I'm assuming like we can take this even further and ask like the once it's generated that to put it into a dashboard or you know a certain visualization. Um what was the question? Sorry. Uh Jay, so that I'm assuming we can take this even a step further. Rather than just replying or regurgitating the metrics in the chat to us, we can ask it to implement that metrics query in a dashboard or a visualization. Yeah, take that. Very cool. Yeah. And we can go to explore at any time. So, um if if I like if I like that query, I don't really like it, but if I like that query, I can go in explore and see what it did. So, it basically reformatted uh the query. So, I don't think he understood my my question. So, I'm going to wait a bit, see if it if it does it. Uh yeah, this time it does it nicely, right? Um Oh, that's wicked. So, it just did this one. So, this one is a bit tough to load because I think there's a lot of data. So let me just pin it down to less like half an hour. So I just did this query which is valid by the way. Um and it's going to show the average per organization. All right. So it's not it's not showing up but it did get the result. Um I will do it differently by the way. So the assistant did it like this but I will do it like like this. Um which I think is more efficient. Don't say this, Sel or we're just going to have to put you on a 247 call rewriting logqlmetrics queries. Introducing Grafine assistant. His name is Seriel. Um yeah, it's struggling struggling a bit because I think there's like a lot of data uh in that in that stream. Um yeah, there's a lot of them. Well, there's one one of them is causing trouble myself. uh the the latency because I'm playing with Loki itself. Uh but yeah, so yeah, you can use the assistant, you can see the data, you can build dashboard with it. I think it's a nice uh it's a nice way to uh yeah, to learn also loql and and just use natural language to uh create those complex complex queries. Very cool. I mean, okay, we do have one more kind of niche question. So, I I don't know if we can really answer this, but um we might as well ask. There's one from from a champion, and it's uh compare one of our graphana champions, Wilfried Rosette. So, uh he says, I'm quite interested about rate in logql and whether or not they have the same unexpected behavior as they do in as it does in prompql due to extrapolation. And then he links to some issues with uh some in Prome and the Prometheus repo. I had a look at this. I didn't know about this until he brought it up. Um but from what I understand is there is an issue with with Prometheus and how it extrapolates like if it doesn't have on the edges of of a range interval. if it doesn't have values there like it tries to extrapolate those values which can make it not not exactly accurate because it's guessing it's it's estimating. Um so his question is does does Loki suffer from that as well? Um I don't think it does but it's even more complex with Loki because um rate rate they like there's multiple rate there's like the rate of logs like how many log line there is but there's also the rate of an unwrap uh value. So it becomes it start to be a bit more uh complex. So let me show you for instance um so if we do if we do the rate of that one and we remove the unwrap it's the rate of log line. So if I remove all of this uh and I need a sum. So let's do some I'm going to give like Wilfred the title of chief question ar uh like asker because he is like on point questions. I think I'm going to say like it's not it doesn't have the same problem because it's not the same uh data model but at the same time it's not less confusing. Uh so let me show you why it can be confusing. So this rate here because it doesn't have any mRAP is just the rate of logs over time. Uh so there is like you know if we zoom in here there's 250 logs at that you know per second and that's that's the right rate. If we look at the count it's another so 200 per second normally you know you should multiply by 60 count over time. and there's only five right so rate is a I guess is a bit uh it's a bit different here because it counts the uh number of log line over time um but you can also do rate uh on and wrap value and then you have another one which is like rate counter so let's try let's try the other one um so I'm going to just open another one I like to do this open two uh visualization like this so I can see uh the log line uh when I'm building the query right so now I can see the log line and we could do a rate of uh one of those value here so I can unwrap those value and do a rate on it and that's a different rate uh and that one is going to be the sum of all value divided by the uh time uh so we could do the return line. Let's do this one. So, we do um unwrap return line. So, bring back this summary. All right. As you can see, it is different. So, it's not definitively not the same. So, just do an hour. Might be a bit too much. An hour. So I'm going to hide the other one. So as you can see that it is not the same. Um it's counting the rate of return line. It's not counting the rate of logs that that happen. And then there's another one which is like the rate counter. And this one does the uh extrapolation. Uh, and it's not technically going to be exactly the same result if it's not a counter. Uh, and I don't think I don't think this one is a counter. It just returns line. So, it's probably going to struggle. Uh, and and so this one does the extrapolation. This one probably has the same issue as primitives. So, it shows zero here as you can see because it's not it's not actually a counter. So, it's same thing. Uh, and you have to force yourself into using it if you want to use it. So this is for extrapolation of n wrap value without it is like um rate of the value and then the normal rate without unwrap is the rate of log line. Hopefully that is less confusing than okay. Yeah. So I I think I also think that um the difference is pretty key that in Prometheus uh they they are using it is the counter is being used and in Loki it isn't. We're counting log lines, actual log lines. So there's no need for extrapolation because it's just what were the log lines and then what was the time period and then that's it. So I don't think that Loki suffers from the same issue. No, there's no scrape interval too which is which affects this problem here. Yeah. Okay. Well, glad to have answered that these question. Thank you so much for coming on. I know we're a little bit over, but we did get to see Graphana Assistant, and it does look like it's going to help us vibe code our way through metric queries and logql as well, hopefully very soon. I would like to Oh, sorry. And I would like to thank you all for this metrics query crash course because I knew nothing before this video. Um, and now I feel dangerous. So, this feels good. Oh, no. I don't know if that was the intended unintended consequence. you can go and make a make a dashboard. Um, you know, like Walt did on the playground.com. As good as Walt now, J. Yeah. Oh, yeah. Oh, yeah. Well, if if there's anybody that's watching this back, there was so much that we could have gotten into and that we didn't get to cover. Why don't you leave your question in the comments down below and then I'll try and get them answered. And you know, since we're we we can we have our awesome Docs writer here and we can all also update docs, we'll make sure that the most common questions are are answered in the documentation so they don't get asked again. Yeah. And the assistant use the docs now. So if you have a question, you can use the assistant and automatically ask the doc for you using intent. So you know you can ask it about locally. Awesome. Well, thank you Sel for joining and you too Jay and Julie for for being here and asking the questions. Um and the community too. We got some pretty good questions in amazing questions. All right. Well, thank you everybody for for joining and we'll see you in the next one. See you later. Bye.

