# Observability for GenAI Applications (Grafana â¤ï¸â€ðŸ”¥ OpenTelemetry Community Call)

Published on 2026-01-22T05:12:52Z

## Description

In this episode, weâ€™re diving into observability for Generative AI apps. AI helps us write code and monitor applications in production - but how do we observe the AI itself? And how do we make sense of complex, non-deterministic AI systems?

ðŸŽ™ï¸ Weâ€™re joined by two great guests: Ishan Jain, working on GenAI observability and Luccas Quadros, working on Grafana Assistant.
Together, they bring both platform-level insights and real-world perspectives.

ðŸš€ Weâ€™ll explore the fast-evolving space of observability for generative AI apps, including:

- How non-deterministic responses change the way we think about observability
- Recording prompts and completions in telemetry: when itâ€™s useful, when itâ€™s not, and how to do it safely
- Using automated evaluations to understand whether things actually work
- Monitoring agentic workflows and multi-step reasoning
- What OTel GenAI group works on

ðŸ’¬ Join us live for an open conversation! Bring your questions, opinions, and hot takes - itâ€™s going to be a fun one ðŸ¤–

URL: https://www.youtube.com/watch?v=LfDVygRWQJU

## Summary

In this special community call hosted by Ludmila Mulova from Grafana Labs, the discussion focused on the observability of generative AI applications, distinguishing it from AI-based observability. The call featured guests Ishan and Lucas Cuadros, who shared insights on the complexities of monitoring generative AI, particularly with the rise of large language models (LLMs) and agentic applications. Key points included the significance of latency, the challenges of evaluating LLM outputs, and the evolving nature of observability as it relates to AI systems. The speakers emphasized the importance of tracking costs, using traditional machine learning for evaluations, and the need for robust instrumentation to manage the vast data generated by AI applications. The conversation also touched on upcoming events and resources for those interested in delving deeper into this field, particularly through the OpenTelemetry community.

## Chapters

Here are the key moments from the livestream along with their timestamps:

00:00:00 Introductions and Overview of the Call  
00:03:15 Guests Introductions: Ishan and Lucas  
00:05:00 Announcement: Final Chance to Register for Hotel Unplugged  
00:06:30 Discussion: What is Generative AI?  
00:10:45 The Evolution of AI and Observability Techniques  
00:15:30 Challenges in Observing Generative AI Applications  
00:20:00 Importance of Latency in AI Interactions  
00:25:00 Demo: Observability Metrics for AI Agent Execution  
00:32:00 Discussing Costs Associated with AI Model Evaluations  
00:40:00 Evaluating AI Responses with LLM as Judge  
00:50:00 Closing Thoughts and Resources for Getting Started with Observability  

Feel free to let me know if you need further assistance!

# Grafana Loves Sub and Telemetry Community Call

Hi everyone! We are back with the Grafana Loves Sub and Telemetry community call, and this call is very special. It's about **observability for generative AI applications**. Let's clarify: this is not about observability using AI, but rather the observability of generative AI itself. Weâ€™ll touch on both topics today.

It's the year **2026**, and everything revolves around AI these days. We can't miss this great opportunity to discuss what's happening in the space, as it requires a new approach to observability. We have two amazing guests today, both working on observability for generative AI. I'm super excited to have them here!

My name is **Ludmila Mulova**. I'm a developer advocate at Grafana Labs and a member of the OpenTelemetry Technical Committee. Welcome to the call! 

Before we dive in, I want to say hi to our awesome audience. I see we have someone from **San Diego**! Your questions are very welcome. We have a live chat on YouTube, so please post your questions there. Feel free to share your thoughts, controversial opinions, or anything else youâ€™d like to discuss.

Now, Iâ€™m going to hand it over to one of our fantastic guests, **Ishan**. Would you like to introduce yourself?

---

## Guest Introductions

**Ishan:**  
Hey everyone! Iâ€™m Ishan, currently living in India. I work on a lot of tech related to observability, particularly observability agents. That's why weâ€™re here today!

**Nicole:**  
Hi everyone! Iâ€™ve previously had a call with Ishan during the Grafana office hours where we discussed observability in AI. That was my starting point for using many of the concepts he shared. Today, weâ€™ll see what has changed since then. Is he still using the same tools? Did he lead me astray when he first introduced me to OpenTelemetry? We'll find out!

Also, Iâ€™d like to introduce a new face to our community calls: **Lucas Cuadros**.

**Lucas:**  
Hello! Iâ€™m Lucas. I work on the AI team at Grafana and have been here for four years, so I was involved before the LLM hype. Recently, Iâ€™ve been working directly with LLMs in the assistant and investigations projects. I'm looking forward to discussing how to observe these systems and optimizing them.

---

## Announcements

Before we start, we have a small announcement: This is your final chance to register for **Hotel Unplugged** happening in **Brussels** in just a couple of weeks. Itâ€™s the perfect place to meet with OpenTelemetry leadership and contributors. If youâ€™re in Europe, itâ€™s easy to travel there. The cost is around $30, which is not a big deal. We would love to see you there!

---

## Discussion: What is Generative AI?

Letâ€™s kick things off by discussing **generative AI** since itâ€™s relatively new. Lucas, would you like to start?

**Lucas:**  
Sure! Before the LLM hype, we already had AI, right? Traditional machine learning has been around for quite some time. What weâ€™re seeing now with large language models (LLMs) is an evolution of that technology. Most people recognize it from chat experiences, like tools such as ChatGPT. Recently, weâ€™ve entered the agent era, where these LLMs can perform tasks, observe environments, and orchestrate processes. Thatâ€™s where the complexity of observability comes into play.

**Ishan:**  
To be honest, I share that view. I've been working with AI for about eight years now. Earlier, we trained our models and neural networks, but now it's more about using AI as an API. From an engineering perspective, itâ€™s much simpler to adopt AI in features and services. However, the core concepts havenâ€™t changed significantlyâ€”AI has always been able to detect certain features in images or create content. Itâ€™s just more widely available now due to the API approach.

**Nicole:**  
I like that you mentioned that AI isnâ€™t new. Generative programming has existed for a while; it simply means producing outputs that arenâ€™t predefined. Thereâ€™s an input, and while the output might follow a structure, the exact result is unknown. Adding the AI component makes it both evolutionary and revolutionary. 

Weâ€™ve moved beyond chat being the end goal. Itâ€™s now just one interface among many for accomplishing tasks. 

**Ishan:**  
Absolutely! The interesting stuff happens when we give AI some level of autonomy, which makes observability critical. Back in 2023, I had my reservations about AI. It felt like magicâ€”how could I know what it was doing under the hood? Working on observability helped me understand the mechanisms better. When we have observability, we can see how the system works, recognize its limitations, and identify when it makes errors.

**Lucas:**  
Yes! Generative AI is nondeterministic, which means it can produce unexpected outputs. It also often involves multiple steps, which complicates testing and observability. In traditional ML, you could set up datasets and clear metrics before hitting production. However, with LLMs and agentic applications, ensuring that you're accomplishing tasks before reaching production is much harder.

---

## Observability Challenges

**Nicole:**  
How do we deal with the complexities weâ€™ve mentioned? 

**Ishan:**  
What we do now is create a "golden dataset" of use cases that we want to excel at. We monitor every prompt change and tool addition to ensure that weâ€™re accomplishing our goals. To verify this, we use what we call "LLM as a judge," which is basically asking another LLM to evaluate the output of the task. 

The complexity increases when multiple outputs are involved, and we need to track the entire conversation trajectory. Once we hit production, we aim to capture as much data as possible without compromising privacy. This is crucial for maintaining quality as users interact with our systems.

**Lucas:**  
Absolutely! And we can't rely solely on offline data anymore. We need to observe our agents in real-time. This is why thereâ€™s so much ongoing work in the industry focused on achieving both confidence before deploying and ensuring everything works properly after.

**Ishan:**  
I totally agree! The aspect of evaluation is still unclear and evolving in this space. Thereâ€™s no established process for evaluating how we classify outputs, especially when it comes to AI.

---

## Observability Tools and Techniques

**Nicole:**  
Speaking of evaluation, letâ€™s touch on instrumentation. Choosing the right library is crucial for troubleshooting. I initially started with OpenLit; should I have used OpenTelemetry instead? 

**Lucas:**  
Choosing the right tools is essential. When I began instrumenting AI, I didnâ€™t know what I needed to monitor in the future. Opting for OpenTelemetry offers a wealth of default options you might not have considered. Itâ€™s different for AI-based applications, especially concerning latency. 

When chatting, latency matters significantly. We notice delays more acutely in chat interfaces. Also, the volume of data involved is immense, and we need to ensure our databases can handle it efficiently.

---

## Cost Considerations

**Nicole:**  
Letâ€™s not forget about cost! Weâ€™ve discussed the importance of monitoring latency, but how does cost factor into this?

**Ishan:**  
Many customers I've spoken with emphasize that while they care about latency, it often ranks lower on their priority list. For instance, with streaming responses, if the first token takes too long, itâ€™s considered slow. 

The way we track costs is still aligned with traditional APM (Application Performance Monitoring). We use traces, logs, and metrics, but the data we track has evolved significantly.

**Lucas:**  
Exactly! Itâ€™s also about managing the costs tied to generative processes. We often try to delegate simpler tasks to cheaper models while balancing quality. This trade-off is a constant challenge.

---

## Closing Thoughts

As we wrap up, I want to highlight that while weâ€™ve covered a lot today, the landscape of AI observability is rapidly evolving. For those interested in getting started with Grafana Assistant or instrumentation with OpenTelemetry, I recommend visiting our community Slack for OpenTelemetry and checking out the semantic conventions.

Thank you all for participating in this discussion! It was great to hear from everyone.

**Ludmila:**  
Thanks, everyone! We appreciate your time and insights. See you next time!

## Raw YouTube Transcript

Hi everyone, we are back with graphana loves sub and telemetry community call and this call is very special. It's about uh observability for generative AI applications. Don't mix it with observability using AI, right? But we are going to talk about both a little bit. Anyway, so we are here uh it's year 2026. Everything is around AI these days and we cannot miss this great opportunity to talk about what's going on in the space because it needs some new approach to observability. Um, we have two amazing guests today. Both are working on observability for Genai. Um, I'm super excited to have them here. Um, my name is Ludmila Mulova. I'm a developer advocate at Graphana Labs. I'm open telemetry technical committee member and welcome to the call. Before we get started, I want to say hi to our awesome audience. I see we have somebody from San Diego. Um, hi. Uh your questions are super welcome. We have a live chat in YouTube. Please go ahead post your questions there and if you have any thoughts, controversial, hot takes, anything questions, send them there. Awesome. I am going to hand it over to one of the awesome guests, Ishan. Do you want to introduce yourself? >> Uh hey people. Uh I'm Shan. I'm living in India right now. uh and I am also working on a lot of tech for like observing yeah agents and stuff like that and that's why I guess we are here today also um so yeah I amum I can pop off to I guess Nicole >> sure I've previously had a call with Ishan on the graphana office hours I think we also talked about observability in AI and I Actually, that was like the starting point for me to use a lot of a lot of what he talked about. So, we're going to see now like what changed. Has is he still using the same thing? Did he steer me wrong when he told me to use open lit the first time? All that and more in today's episode of community call. But before that, I'd like to introduce a new face to this community call. I think to the community calls in general maybe. Lucas Cuadros. >> Hello, I'm Lucas. I I work in the AI team in Graphfana for four years now. So before the LLM hype, but more recently working directly with LLMs in the assistant and investigations project. uh hoping to chat about uh how to observe those things and also how to do some evo or try to do some evo. >> Awesome. So before we get started, we have a small announcement. These are the final chance to register for hotel unplugged. It's happening in Brussels uh very soon in just couple of weeks. Uh it's the place if you want to meet with open telemetry um leadership and contributors come register. If you are in Europe it's super easy uh to travel uh it's it's you have to pay for it but it's like 30 or 30 bucks so it's not a big deal. We would love to see you there. And with this uh let's maybe start by talking about what what is generative AI just because it's relatively new and uh what are your thoughts? Lucas, do you want to talk first? >> Yeah, sure. So um like I said uh before the LLM hype, we already had AI, right? our machine learning or what we call nowadays traditional machine learning and what we are seeing right now it's the large language models so the evolution of what we used to have and most people recognize it from the chat experience so like uh tools like chat tpt and etc but more recently we entered the agent era so these large language models uh can perform tasks, observe your environment and do all sort of uh orchestration. Um and that's where the complexity is to observe um uh Asian um do you do you have a different view or >> uh to be honest? Uh no. uh but I've seen like how uh AI has grown like I was working like I have been working with AI almost for like eight years now to be honest and earlier like we used to train the uh like the AI and like the neural network and stuff like that but nowadays like what I see is like it's more like a API usage like AI is more like an API usage than like creating models and stuff like that. Uh so from an engineering side like I've seen a lot of things change it's a lot simple for me to like adopt AI as such. uh in features and services and applications and stuff like that, right? Um and hence we see a lot more adoption but like the base I don't think has changed a lot like even earlier like you could use AI to detect like some few like a few things in in an image or like create an image or stuff like that. It's just more like it's more widely available since it's just an API caller to be honest. >> Yeah. And I I like that you mentioned, Lucas, that um AI is not is not new, right? But also neither is generative programming. Like we've had generative programming because generative just means that you get outputs that weren't predefined. So there's some sort of input and then the output you still kind of know like maybe what structure it's going to be in or format or template, but you don't exactly know what the output is going to be. and adding the AI part and the generative comes up with something more evolutionary than revolutionary. But I also like Lucas's point that LLMs are very like a LLM are different from the this agentic era that we're entering where agents can actually call other tools not just so it's not just about chatting and it's we've moved beyond chat as the goal. We've moved to chat being the interface to do something else. >> Yeah, for me I think that the interesting stuff happens when we give the thing autonomy and this is where the observability becomes very critical. So my personal uh reservations when I'm using AI or maybe back in I don't know 2023 that I have no idea how it works. It's pure magic like how do I know that it it what it does under the hood and like working on the observability made me understand some parts and like when you have it observable you kind of understand oh this is how to call works. This is kind of wonky but but but fun and like you you can now understand the limitations right? Oh, it picked the wrong tool and then the result is uh this bad or the tool returned an error. So, I think the the important parts for for me for generative that it's it generates some random stuff, right? It's nondeterministic and that it's autonomous to some extent and it does more than one step usually and I also feel like it's a step beyond what we had before which is declarative. So imperative programming was like first do A then B then C in that order all the time and you always know that the output is going to be D or whatever and then we had like declarative where it's like well we don't have to define the steps we just define the output we know what the output should be we don't really know the steps that we kind of leave for the computer to figure out then now it's like generative means we have these inputs go. We might have like some frame about what to do with it, but we don't really know the outputs or the steps, which is like how do you test this? How do you observe this? How do you even know it's working? >> Yeah. Right. and and besides the don't non deterministic side so we increase the unknown comparing to the former traditional ML ma me methods uh but also the task got way more uh complex. So uh before you would be able to have a data set and you have your train data set in your test data set you before you reach production you would have an occurrency some metrics and then you can observe production and say um I'm drifting for my test scenarios maybe I need a retrain or something like that but nowadays with the LLM uh agentic uh applications it's it it's hard to before reading hitting production to be sure that you're accomplishing the tasks, but also when you get there, how can you with the volume of data and and and the nondeterministic of every token, how can you be sure that no matter what numbers you got uh in development stage, they will reflect uh in a real world environments where people can uh start all sort of conversations. all sort of tasks. Imagine how many different tasks graph assistant receives every day, right? >> How many do you know? >> I don't have the numbers, but definitely more than we account for in the development stage. >> So, how do you deal with this? Yeah. So what we do right now and I think before uh anything my my my point of view of the the ecosystem nowadays the the environment is that no one actually knows what is uh the best way of doing this. So what we do is we try to get a a golden data set of some use cases that we want to be really good at and we try to uh every prompt change, every tool uh that we add, we try to make sure that in these scenarios we are still accomplishing it. to verify that we are accomplishing it is pretty common to use what we call LLM as a judge which is basically asking another LLM to verify the the output of the um of the of the task right and that get more complex if it's not a single output sometimes you have multiple outputs you have to verify the whole trajectory of the conversation and then when we um get to production. The idea is to uh capture as much data as we can without hurting privacy to make sure that whatever we thought it was the golden data set is the same as what the users are seeing. Uh that's one thing when you have a single agent in a chat interface and that's a whole different uh problem when you have multi- aent talking to each other and spawning new agents and accomplishing tasks in parallel and doing all sort of uh crazy stuff. That's why um you can't rely only on the offline part. what it means in on the before hitting production you have to have something observing observing your agents and how they're behaving in the real environment and that's why uh we are seeing a lot of work in the industry to uh accomplish both. So have the evolves have uh confidence before uh deploying something but also being able to uh uh make sure that it's working properly and every change that you do is not adding regression because every token counts and that's why works like what is mostly working on are so so important right now in the industry right >> uh yeah I totally agree like even the uh aspect about like eval specifically uh like you mentioned like a eval even right now a lot of people just evaluate on the base prompt and the base response that the LLM gave but as you mentioned like the tools that it used the attack context it had for like specific agent to call like why a specific agent passed the like hand handed off the next task to a specific agent like how do we evaluate that that's like very still unclear and very like new in the space program to I don't think there's a proper or um I don't know like a process to evaluate that as well right now. Uh people are trying I've read a lot of things but none of them to be honest like works uh as of now. Um but I I really like like how fast this space is growing and how like unclear like people still are about like eval but they're still trying to like adopt because like how useful the features are at the end of the day. Um but like I agree like as long as we have observity at least we can make sure like the issues are not happening uh um is always useful >> and then there's the other side of it too right which is the instrumentation because like evaluating is definitely important but like even before that to to be able to have enough enough data to reason about what our AI is doing um I I think it's really important to to choose the right library because I don't know when I was getting started with instrumenting AI I was like I don't know what I need to know to to troubleshoot things in the future and picking like openlit was what I started with first and apparently maybe I should have been using open telemetry I don't know we'll find out after this um but picking one of those gives you already like a whole host of default things that you might not have thought of it is a little bit different when it's an AI based app, right? Like even just like with latency, when you're chatting, when you're talking to someone, latency really matters. Like we notice lags and we kind of excuse it if it's like on video, but when we're chatting to someone, it can't be like 30 seconds before a response. It has to be like immediate typing at least some sort of indicator and then a response. has to be like a back and forth. So we notice those things more and then even like the sheer amount of data that's involved you know that we there's a database that has to have that and how was how are those databases structured and then those need to be instrumented too and then the cost side of it everything you send back and forth how many tok it's it's just so much to keep track of. Yeah, I wanted to bring up the point from the chat. Um, so it's not just about the AI, it's also about to calling an MCP. It's even more than that. So what I was surprised like positively surprised. So when the AI boom started there there were a lot of companies that were focused on AI observability alone and they still are there and it makes sense. So some some uh products just rely on AI observability but there is an ecosystem you need to call databases there is incoming HTTP request that you process you need observability for this too and uh I I'm curious how world will keep evolving while AI observability specific solutions well will they merge into general observability vendors will they stay separate will this be there be I don't double uh emitting and using different back ends. But anyway, um to the point about latency, it's not just one latency anymore. It used to be okay, HTTP request duration, RPC call duration. Now it's time to first token, time between tokens, uh and all the other cool stuff that's going on. But it's not even the latency, it's the output itself, right? So we need to store this output at least sometimes uh like the input and output to the model. We need to store it somehow in our telemetry back ends. It is it blows every traditional monitoring person mind that we're storing this verbose data. So how do we feel about it? Maybe I'll start with Ishan. What do you think? How how do we process and store this data? uh to be honest like starting from your aspect about latency like that strikes a very important aspect like which I hear from a lot of customers nowadays like uh especially for like when they're using LMS and like agents or anything they're like we care about latency but that's like the 10th factor we care about to be honest right now uh like time to force token especially when streaming like how the assistant we have like it's a streaming response right so if this first token takes like 30 seconds then uh I would feel like this is like very very slow, right? Um, so yeah, the the way we track hasn't changed. It's still like APM like we still do traces, we still do logs, we still do like the way we used to do APM. Just the things that we track has evolved to be honest. Uh things that are important like output is very important but how do we classify output also is like again like a separate process. So um it's like a connected uh data set at the end for observity like you connect like a log to a metric and a trace uh and how that evaluated like the evaluation score should be connected to the actual trace output itself. Um yeah there's a wide new data set I would say uh like a not exactly like a new data set but like how you observe uh things because it's like a very big connected system and that's how you would need as well uh in your observity system end of day um I don't know if it helps you answer the thing that you are uh asking for but just clarification like what I've been hearing from a lot of users at the end of day like things which are very very important to them which I didn't think like were important that much but like as as you talk to like the actual LM agent interview users they give the experience that they're having and I'm like this is interesting this is new to me as well uh so yeah >> I'm curious how we treat it in in graphana with graphana assistant do we actually store the conversations or how do we even evaluate them? >> Yeah, we do we do store um uh the conversation and the the tool call in the outputs. We we also uh another thing to mention is the feedback uh because this is so brand new that every human feedback counts a lot, right? uh we do some LLM uh automatic analysis but whenever we get a user saying uh this is not good or this is really good this is the kind of use case we we want or I wanted to do something with the system and that's not possible that's um where the production to development feedback loop uh uh gets uh really good so for us we have is a whole uh view and the thing is it's mostly separated from uh the observability. So we do have traces, logs, metrics, but we store the conversations and the trajectories in a separate place and try to link them as much as we can. But um at the end um we end up treating a horse latency and everything from uh traditional observability uh different from all these metrics of the LM. So for example, lat latency is something that we really uh want to tackle not only because people don't want to wait in a chat interface but also in like for example investigations where we want to do root cause analysis really fast with uh 10 different agents. If that is is slower than a human uh debugging uh what is the point right? If you find the root cause before uh 20 agents then what is the point of uh creating 20 agents right? So um we try to to have both but I think one of the main challenges like said is uh connecting uh the the traditional telemetry data that we are used to to this huge volume of data that it's it's hard to just uh store like normal uh blogs for example Okay. >> Maybe we could have a look, Ian, at what you wanted to show us today, just to kind of see what these metrics all look like. >> Uh, yep. Let me share screen. And I have a very simple agent that does none nothing, but I'll uh it'll help me plan my next trip, let's say. Uh, right. And I'm using I'm still going to be using open it um for this um test I guess. Um I'm like I've agent is like a CI agent um and it uses uh open a uh APIs to uh make the alumn itself. So what I do is like I run the uh the actual agentic uh thing is not that important like how like the information that we can extract like with the open information is like the important aspect that I really want to go over but let's say the agent run happens I want to say I travel to say I'm um Boston uh I'm going to go there for like 3 days I'm traveling alone I'm going to be generous and I have three 3,000 USDs to travel for like uh 3 days and I'm going to do Feb um 10th to 13th 2026 right and Boston is like an old town so let's do history for now uh and food right uh and I can stay in a metro in so the agent take run uh run has started it will it's start like three sub agents and I don't know which agent will call which agent when right and it does tool calls as well like it uses the serer API to to do the search uh thing um right so I don't know how the agent run would happen but I as an engineer like if this is my application I want to understand like how each tool how each agent when the llm call was made the request was made right uh so I pre-ran the agent obviously to save time uh and I wanted to show like once you have the data flowing in from the upload instrumentation to any of the observability stacks to be honest since it's alternative. Um what you get is like the the the most overview based information like cost and tokens which are like all right uh as an user I do want to track my cost and tokens uh the request latency and stuff like that. But as you drill down like um if I am the engineer who built the AI agent I want to actually track the sequence of events that happened in the agent tech execution run right um and for that we do need like traces itself um so just for context let me open this can you how do we open this in I mean this should be doable here uh and I'm going to use the assistant as well because this is like a long a span of events that that happened from the single agon that I did right there's a lot of spans in here uh and as an engineer I don't have time to go through each and every agent that might happen I have 100 users I can't go through each 100 users uh agent runs right so I'll use the assistant I'll see all right where do I have the issue because I do see errors a lot of places but where did the error happen uh which tool calls uh had the error and So I do want to get like a very big picture overview of all right which agent called which agent when the LLM call was made which LLM call is like very very slow the duration is very slow and where I can actually optimize that's where I find the assistant very useful uh like explaining these long uh traces right so um like I with the assistant I obviously didn't run this before um right uh a lot of my requests are like wasted on local uh like retries itself which uh as an engineer now I know like where I need to optimize right in the long run of things um and it does also a lot of times does explain me the entire sequence of agent pass but this time since it's live it did not tell me that uh which is expected I guess um but yeah like as an engineer like now I feel a bit more safer since I have the information about like all right how my tokens have been using how much cost I've been using how what my users have been prompting because each LLM call like let's let's open this for example um I would have the information about all right what was the completion like the the response from the LLM what was the input what was the output and how much cost like each request had like uh there should be a parameter for cost where that goes Um, yep. Uh, cost. So, I can associate like cost with each LLM request at the end of the day. And, um, so this is very useful information for me to be honest. And also this is not just me tracking the LLM calls. This also helps me track like all of the vector DB calls that I might have, all the tool calls that I might have. Um, so and I can always map this much more cleanly. I can close a lot of this so that I can drill down into all it which agent had issues where I can optimize things. Um and obviously drill down into more aspects like which LLM I've been using. um how much uh by system by environment by platform uh I can track the latencies I can always find the time to force token and all right how much is anthropic doing and how much is open air doing and with that I at least I have the information if I want a faster replies from the I might end up using uh openi that's it right um this is like the observability aspect of things right but we also So uh like as we discussed we also always want to do um evals right um all right I have the the prompts and the responses from the LLM the the response from the LLM for a specific prompt right but how do we classify the pro the responses actually right we as Lucas mentioned like we use LLM to evaluate the first LLM itself right um in this as well I use the open net SD itself to calculate the evaluation since I expose like a notal metric which is easier for me to validate a lot a lot of the AI tools also have like online evaluation itself. Uh we don't have that feature as of now. Um but still um for some people I think code based evaluation and prompts in the obserity vendor because for like security reasons sometimes some vendors cannot right. Um, so with that like I at least have the information already but what problems does the response generally have? Like why is it being classified as a failed evaluation? Like I have factual in inaccuracies in my responses. So maybe there's a context issues. Um I always have information already why a specific text from an LLM response was tagged as a like a failed evaluation, right? And I can always see like all right, what is my hallucination score? What is my toxicity score? Um and based on this information at least I have a very good head start to build the second iteration of my agent uh LLM feature that I released in the first uh in the first itself. Um yeah that was the entire thing that I wanted to show demo I guess. >> I wanted >> Sorry. >> Sorry I couldn't hear you for a second. >> Yeah I was muted. Sorry. Um is this the integration in the graphana cloud and maybe is it available in some form for the OSS users? >> Yes, this is an integration which is available in cloud as in cloud you can find it here. Um but this since the the visualization uh like you can always do this in uh like open source as well. There's no restrictions as long as you have a uh a tracing back end or metrics back end. You can send data there and use the same JSON that we have for cloud integration. and uh do it in the OSS. I can also drop the link to the the visualization the JSON files uh in the uh yeah um >> yeah please do I will share in the chat >> thank you cool demo and I'm surprised we spent 30 minutes uh talking about AI and not mentioning costs thanks a lot for bringing it up so like uh in a ballpark like okay so we we do some AI stuff it cost money. Then we evaluate and maybe evaluate multiple times uh and maybe evaluate the evaluations and there's there's some number of some precaution here. How how much does it cost? Do people care? uh the cost for like evaluations itself >> and just the story of uh AI plus AI observability plus embedded cost of of of the evaluations like can we have any interesting stories to share what customers think what we think we are the customers as well. Yeah, I think Lucas might have like very interesting stories from their >> Yeah, a cool thing that it's related to that that Ian showed is the different models. So, one thing to tackle cost that we usually do is uh try to extract some of those subtasks and uh handle it to a more cheap and fast model. But then it's the tradeoff of uh quality and reducing cost. So usually we can do a small task like summarization or um um usually summarization or some quick uh uh finding in in large tasks uh the information that we want. we handle that to like a mini model or flash model and then continue the the flow with the bigger models. But since every token counts, it's very I think the main challenge is how do I add a new functionality a new feature uh make sure that everything else is working and don't blow my costs. Right? So whenever we do a change uh what we used to call prompt engineering but now it's more like context engineering right um uh whenever we do a change we have to uh make sure that latency is not uh increased and costs right um so uh right now we track costs uh very closely both in assistant and in investigations and for investigations s since we handle a very large amount of telemetry data. So we scan logs like Ishan showed we scan traces and sometimes it's huge traces we have to uh use those techniques. So sometimes those techniques are uh uh handling to smaller models sometimes is uh removing it from the LLMs totally. So sometimes we end up using traditional uh techniques from traditional ML or just uh summarization and things like that. So we can send last tokens and generate last tokens. Um but that's definitely uh one of the biggest challenges in this space and I would say that in the near future future is it's possible that um a lot of people try to tackle that with self-hosted uh models. Um I mean then there's the trade-offs of hosting large models right and the cost of that but the the better those models gets uh I I I I will see I imagine people will will handle instead of handing to this uh mini models we handle to local models and then the observability nightmare of then observing your own LLM models running in GPUs and things like that. Uh, so there's a lot of trade-offs. Um, and every token counts. I I said that before. Uh, so every little change has to account for all of that. >> Yeah, that brings an important point that we mostly talk about the when we talk about observing AI about the client side. There is the huge portion of the server side. It's the when you train the model, when you finetune the model, and finally when you host the model and I think there are like projects like VLM that integrate with that provide a lot of VLM observability, but then there are GPU metrics from the hardware side and a lot of the cache monitoring stuff that it's important to to take into account if you host this. Um, I wanted to bring up some conversation from the chat. So we've been talking about observability in general but we still um rely on the fundamental telemetry signals right we use metrics for aggregations we store them in mimer we use traces for the individual flows debugging the demo the start of the demo Ian showed it's stored in tempo we might use logs for example in hotel we recommend using recording individual evalation results with like some reasoning and correlation uh as logs um and maybe there will be more coming through logs and events. So like it's not the question of which specific database you use but more like okay this is the whole observability story and things come together and you correlate them >> and I think it's interesting that it's becoming the same way for AI right it's now not just which model are you using like which AI are you using like what AI it's not it's not one thing anymore it's like sub agents and and evaluating ing AI evaluating AI and which ones are those and the models for those and then the database like it's a whole stack there's now a whole we're creating a whole other stack that we also have to observe somehow and Rav Singh here um has a comment and does for example one hard part about how to observe non-ext uh input output in the case of voice agents so yeah like I I think um for example I know that that these tools that you know look listen to meetings like Gemini or or um like I'm actually wearing I'm actually wearing like this plaude a wearable AI that I I use it to to listen to the conversations and what it does is it translates it transcribes it to text and then from text you can use your typical um ways that you would observe the text but the transcription has to happen first because cuz if you had to observe it directly I I I don't know if anyone knows how to do that with with like voice signals or maybe it's not feasible. >> Yeah. Uh I do agree like even for like observability for like uh voice agents like even image based agents is like very complicated because like the the like hotel we cannot store like an image we cannot store like an audio file itself right as an observatory uh like text can be stored in a span attribute let's say but we can't store a file uh right so how but that's an output from an LLM as well uh right how do we store them some people like just um track like the link to the actual file uh which works uh but in a lot in in like a lot of cases when you're having an issue you don't want to go through click on a link go to a file you want the actual file to be there itself in your trace visualization um is what I've seen um some new like AI dev platforms have been doing like adise does that to an extent um right but again That's the complexity because you don't want to store like an image or like an audio for like each trace as we saw like the trace is not very not like small enough it's like very very big and if you start like storing like files for each trace like agent run then storage would be another issue that you you'll have to solve. uh what do we store what do we not store like always a debate uh and has been debate for a very long time but nowadays I think it's a bit more important in observity for agents yeah >> we're sort of getting into oh sorry go on >> it's just last point on this uh another another ad for a rice so they when you do embeddings right you call you you calculate the vector for the the text. They could show you the actual in the 3D model of your vector space and show where the embedding is. So you can kind of visualize inside the cluster uh that the cluster where where your uh question belongs to where the query belongs to. I'm I was always curious how much of it is some something people actually use versus it's an awesome demo feature. I think maybe it's more of a demo, but yeah, I I think it's super cool. Sorry, Nicole. Go ahead. >> I was just going to ask Lucas since we're talking kind of about how to observe and evaluate AI, like what what are the I don't know some approaches to evaluating AI, especially like with the assistant, what do we actually do to evaluate it to make sure that everything's correct and as we expect? Yeah. So we don't have a voice or video and not much image. Uh but we do so in our case is a bit meta right because we we have our final data is telemetry. So it's logs and traces and profiles. Um so one of the challenges of evaluating uh especially uh evaluating in a sandbox scenario where where we can do multiple runs, compare different prompts, compare compare different versions. Um can you hear me fine? I think I'm I'm lagging a bit. >> You're good. >> Am I back? >> Yes. Yeah, sorry. Um, uh, so one of the main challenges is being able to, uh, create a reproducible, uh, sandbox, uh, environment. So uh how can we do that without having to duplicate all the logs uh metrics profiles traces uh and make sure that the LLM will be able to access all the data that initially accessed. So that's one of the main challenges because uh majority of the evolves that we do is rerunning those scenarios and make sure that we're improving and mostly not adding regressions because every token that you add uh because of the context window um and context rot right what we call context rot that means like every new thing that you put in the context uh degrades the model performance. So whenever we come up with a new tool, we have to make sure that there's no regression. So there's two parts here. one making sure that you have access to real data because these agents will have to interact with real data at the end which is a new thing um comparing to the initial LLMs uh where it was just input and output but also how to make sure that uh everything is still working and it's not just data that's changing. Um so uh coming back to the cost uh we spent uh in some of those rounds like 50 bucks just in the evaluation part of it which is uh uh mad if you think uh uh but uh it's the the best way we can find. I'm not saying it's the best long-term, but this is the best available right now, which is rerunning as much as you can. Trying to have a very uh close feedback loop between what is happening in the real world and what is happening in your sandbox scenario and making sure that every new addition is actually making stuff better. I'm curious from the technical perspective uh like so we people talk to graphana assistant we record this conversation as you mentioned separately from the telemetry we probably have a link and then how do do we run evalations do they run later on offline or do we like schedule the evalations to run immediately how does it look like from from the technical stack if if it's not a secrecy No, it's not a secret. We do some automatic evaluation on the trajectory using LLM as a judge, but we have to keep it cheap. So, it's not a a very fancy or robust evaluation. It's just to make sure that the clients the the user uh experience is not being degradated. uh but most of the evaluation runs offline and recently we are working on adding a CI integration. So the idea is every time you change a prompt, every time you change a tool description even uh we run those tasks the scenarios to make sure that uh we're not breaking stuff because uh different from uh traditional ML where the accuracy would uh be very stable. If you change something else, you might be changing something completely different. You might be changing uh tempo tools and that hurt logs for some reason because the context window is the same, right? >> I want to ask one thing about like evaluation which uh some people have asked me as well. um like when you evaluate when you use like a second LLM to evaluate like the first LLM's response like the output the entire trace how do you make sure like since the second LLM is like technically of a smaller LLM right how do you make sure that the second is not hallucinating right uh because there are chances like the second would elucidate and we can't like keep a chain of LLM evaluating each other uh does not work like that >> we defer of using LLM as judge as much as we can. So if we are able to check with rejax with comparing it with real data any other way to compare that data we will use it. But one thing we learn is that whenever you're asking to LLM if you have a boolean criteras so instead of asking like is this good is this bad in a scale usually we started with scale like and zero to 10 is this good and the LMS are not good at that. So we usually say um for example if you're evaluating a traces analysis uh workflow you would ask like is the bottleneck mentioned is uh the trace a link to traces present I mean all the criterias you want on that answer in a boolean true false uh way that way you can control a bit more that the LM is not just inventing stuff But you're not guaranteed for sure. But then if you start to try to evaluate the evaluation and evaluate the evaluation >> it's infinity. Yeah. >> I've seen uh teams maintain like a data set uh as well like for like which is used in evaluation because what they've mentioned to me is like as long as the LLM knows the correct output uh it can always make sure like it it will hallucinate a lot less compared to a scenario where it does not know the the right output. Um and that technically helps like having a data set kind of thing. Uh I I haven't tested in like myself so I don't know how much it works. Uh since I've only tested in like small LLM runs and age inte runs uh so the LLM as a it works right. Uh but on like big cases wanted to see how it uh works in actual scenario in the real world. I was super skeptical about the whole LLM as judge thing because of the reason that you mentioned that like what we're just chaining AIS now and they just talk to each other and I don't know how can I trust the testing AI anymore than the one that's being tested. And then um I and then I actually I actually did it. I actually came up with a testing framework for a simple AI app that I made that's just like a thing that runs a D&D game for me and I'm the player and I had specific like traditional testing prompts. I was asking it specific things and then I was requiring a response, a certain response. And then I found out very quickly that because I was sending it to the AI, the AI realized it was being tested and then started rate limiting me because because I was asking the same questions over and over again. It was automated. And then I'm like, "Oh, I actually now I need AI to test AI because I can't keep up. I can't keep changing these prompts. It's it's like AI is changing and the tests are no longer valid." So, I needed to bring in the LLM as a judge in order for the tests to mean anything. So, then I paired down the explicit tests and I still have them. They're still important, but they were much simpler. And then I use the AI to test AI and it was like, okay, what world are have we stepped into? But I guess we're there with observability, too, right? Like if in even in the graphana stack like every Prometheus instance we start in ops has a meta Prometheus that's in a separate availability zone that's I mean it's the same thing there's like Prometheus on Prometheus on Prometheus to bring the question from the chat if we considered running some simple online evaluations using traditional ML model for sentiment analysis and which would be cheaper uh than using LM as judge maybe Lucas you have some thoughts. >> Yeah. Um that's a great idea not just for the evaluation part but also like I mentioned before right whenever we can defer a task uh to uh traditional ML some other aristic whatever is not LLM we will do it. So um if you're doing evaluation we try like I said we try rejax whenever it's possible comparing with real data whenever it's possible but the thing is uh sometimes uh it's so complex it's so much tax data that the having I mean sentiment analysis is one thing and definitely uh it's worth doing um it's one of the most common criterias like negative and or positive But sometimes uh it's easier to use LLM as a judge at least initially and then you can create a data set uh that then you can train a normal natural language processing or not normal maybe traditional uh natural language processing to uh replace this LLM as a judge. That's a technique that uh a lot of people use. uh it's a way of starting with LLM as a judge but whenever you uh have a data set uh you switch to a cheaper alternative um another thing is um oh I forgot well anyway uh definitely replacing uh as much as you can also judge is a good alternative but it's not always possible I think it makes sense. If we're testing non-deterministic systems, then at some point we have to do non-deterministic testing. We can't possibly know all the acceptable versions, acceptable results that that we want. if we don't know what possible results there could be. >> Yeah, I I remember the other thing I was going to mention um a way to make sure that your LLM as a judge is not drifting too much uh and it's more or less uh related to the truth is whenever you can compare that with human evaluation. So if you get human feedback uh that's great, right? So comparing the human feedback with the LLM as a judge, it's a way to have at least a sense if your LM as a judge is actually representing a human as a judge. Uh it the other alternative is having humans annotating data like we used to do with uh uh traditional ML, right? But um the volume is so big that that's not as feasible anymore. I wanted to also chat a little bit about uh the evaluations for agentic scenarios. we uh slightly uh we started talking about it that okay if we are just evaluating one LM operation one inference operation that's okay there are like pros and cons to a judge and but it's like the method right but when it comes to the agentic or even worse multi-agentic scenarios now there are so many aspects like if you do the search like even uh like semantic search like you need to evalate the search what it returns. You need to evalate the tools uh that model picked to run something and then we need to evalate individual steps and then maybe we need to evalate the total the overall workflow that uh agents did like do we have any framework on how to think about it how to approach it >> from like what I've learned from the customers I have a few uh like they've shared ideas like what they would like to see and they've always been like um right with a single trace we can see all right a single agent to run but we as an engineer we are not doing this every day uh too complex too hard uh we we don't have the time for this so what like they generally ask like we if you had the same information not like in a stack in a like a trace tree view but instead like in a uh a flowchart kind of thing and based on that I can always see like all right which tools are being called and um at least have an error signal or something like that like make it simpler from the visualization side because at the end of the day I don't want to see each and every trace uh run from an agent I just want to see like which times the agent actually errored out right um those are the important aspect we don't want to see the success always we just want to see the failures where it happened and how can we improve them right so highlight just those aspects in like a flowchart form uh the rest of the trace the cost the tokens my operations team might care about that but I as an engineer don't care like it's not on me um right so the that's one aspect like I found like very interesting like flowcharts might be interesting because at before like an APM I didn't consider like flowchart to be like very useful it's just a like an API request calls an API request what do you do with that but since in agents a lot of different operations can happen um it seems to be um growing and I've seen like a lot of these LLM observatory tools like adopt a similar approach as well. Um so yeah uh I guess that that's where the AI observility starts to differ a bit like how do you want to see the data at the end of the way we see it in APM or like a classical application is might be different compared to like in the future or like how we'd see for a specific AI application. Um yeah, the that's one thing just I've like heard like a flowchart kind of thing and it was like just interesting to me to be honest. I just wanted to share that. >> Uh yeah, I wonder if you would like to talk a bit about the hotel side of this as we close this out. Yeah. So we've talked a lot about the general approach to observability. We uh didn't talk about much but essentially the hotel is a mechanism you create and export telemetry right and most of the things we talked about are based on open telemetry and or primed uh to a large extent. Um up telemetry is not just the the API SDK and protocol. We also have the uh semantic conventions and instrumentation libraries and this is what um there is a bunch of interesting things that are going on both in the auto community and in the wider community around the observability. So like I wanted to share a few uh highlights of what we're working on the generic sig and open telemetry. Uh and let let's just walk through the slides. Um so one of the things that uh we started with is inference and uh like LLM calls and we have semantic conventions and instrumentations that pretty much do this maturely. There might be some breaking changes coming up but this this is very solid. Uh we have things that describe embeddings tools and uh those are a little bit less mature I would say. they might not contain everything we want but there is a a lot of effort to um build the missing pieces. Then uh there are a genetic scenarios and we're still evolving them for example like the how do we record the orchestration how many layers of agent runs we would record um the evalar in like early stages but there there is some stuff and it's it's been used by some of the vendors and it's pretty cool that we can correlate individual evals to the actual uh LLM call that they're evaluating. ing we've just added MCP conventions um and like multi aentic scenarios um they are like super early um I wanted to tackle something that we talked um about like what do we how do we store binary content where do we how do we manage the costs of storing the text um and it's not just the the costs or volume it's also the privacy right so whatever I'm talking to a model is is private I don't want it to be available like when I talk to charge I don't want anybody who has access to open AAI internal telemetry to know right I want the separate set of uh privacy controls for the gener generic telemetry data and my data it's like what we are suggesting in in open telemetry is that okay for the baby applications you're early in your life just just record them as span attributes and for example here is we have geni input messages they are structured um they are formal they have formal schema but you can also uh have a hook in your instrumentation that let's say uploads it to some cold storage like S3 or something uh and then some humans would have access to its could have access to it. And it's interesting to learn that in Grafana Assistant, we do something similar, but it's just not part of the the the telemetry pipeline. Um, and the last thing I want to highlight, okay, there are open telemetry instrumentations. They are kind of blessed by the community. They are developed by the community. Uh, some of them are mature, some of them are relatively new, but there are tons of instrumentations out there. They pretty much all use OTL but they don't necessarily follow the same conventions or the same version of conventions and we have trace loop identifier rise with open inference length use open lead and you name it there are so many different instrumentations um they would still work perfectly with hotel back end uh the dashboards you would have for them might be slightly different uh the process of standardization around them takes some time and Yeah, it's the standardization. We try to find consensus and um answer all those interesting questions of like how do we record the Vs and how do we record prompts and completions. Yeah, that's the hotel part of the story. But I think this this area is so evolving so fast that uh we will have a lot more to cover in in the next few months or years. Cool. Yeah. I wonder if um e if you would like to like give us if people want to get started with both assistant and with like instrumentation with open telemetry for example where should what should people do? Where should people go? >> Yeah. Um your phone assistant is available in cloud. Uh like uh Ishan briefly showed in his demo. Uh you can use to analyze traces, logs, anything. Uh it's very useful uh if you're new to observability and u promeuse and traces. It's pretty useful to to learn and and get more productivity. But even if you're an expert, I would say it's pretty uh helpful and and handy. >> What about for the hotel seg? >> Yeah. So, it's the great place to start would be uh the Genai. I'm sorry, Slack for the OpenAI. I'm sorry you caught me off guard call. Uh we have a slack community slack in open telemetry for geni observability and uh we you can take a look at the semantic conventions and kind of figure out uh where they implemented. I'm going to pause the comment lic conventions. Uh and if you are interested to participate in how it evolves uh this is the Slack channel. We would love to hear your thoughts there. Awesome. Thank you everyone. This was a great discussion. I really loved hearing from all of you. Thank you. >> Thanks, folks. >> Thank you. See you next time. See you next time.

