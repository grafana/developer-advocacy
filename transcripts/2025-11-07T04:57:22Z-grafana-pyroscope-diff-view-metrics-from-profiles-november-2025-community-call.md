# Grafana Pyroscope: Diff view &amp; Metrics from Profiles (November 2025 Community Call)

Published on 2025-11-07T04:57:22Z

## Description

Slides: https://docs.google.com/presentation/d/1sRT4o5uC8Czpw1F4QH3suPeoWKIO8pc_QUg4xpVC_7Y/edit?usp=sharing ...

URL: https://www.youtube.com/watch?v=ozf2dtUtohE

## Summary

In the November community call for Pyroscope, hosted by Tiffany, key updates and topics related to profiling were discussed. Alex, an engineer on the Pyroscope team, provided an overview of labels in profiling, explaining their significance and demonstrating how to explore them using Grafana. He also introduced the diff view, which visually compares two profiles to identify performance changes, particularly useful during deployments or incidents. Alberto then presented the new metrics from profiles feature, highlighting its ability to export profiling data as metrics to Prometheus-compatible sources, which facilitates alerting and performance monitoring. The call concluded with a discussion of the recent release of Pyroscope version 1.16, which introduced OTLP format ingestion, new Helm charts, and various improvements. Participants were encouraged to ask questions and engage in future discussions on Slack.

## Chapters

00:00:00 Introductions and overview of the community call agenda  
00:02:30 Introduction to profiling and the concept of labels  
00:05:00 Explanation of different types of labels in profiling  
00:10:00 Demo of exploring labels in Grafana’s profile drill down  
00:15:00 Introduction to the diff view and its purpose  
00:20:00 Demo of diff view comparing two profiles  
00:25:30 Overview of metrics from profiles (recording rules)  
00:30:00 Explanation of how to create a recording rule  
00:35:00 Demo of viewing metrics and dashboard with recording rules  
00:40:00 Release notes for Pyroscope 1.16 and new features

# Pyroscope November Community Call Transcript

**Hi everyone! Welcome to the November community call for Pyroscope.** 

If you haven't been here before and are wanting to try commenting but are having issues, click on the top right corner with your image to create a channel. We would love for you to ask questions throughout the call. 

Today, we have a few topics to cover:

1. Continuing our series on introduction to profiling.
2. Discussion on labels and the diff view by Alex.
3. Metrics from profiles by Alberto.
4. Information about the 1.16 release that came out yesterday.

Please feel free to ask questions or make comments in the chat at any time. I’ll be relaying your questions to our speakers. Let's kick it off to you, Alex!

---

**Alex:** 

Hi everyone! I'm Alex, an engineer on the Pyroscope team. Today, I will be discussing the introduction to profiling, continuing from our previous call where we talked about flame graphs and profiling.

### Agenda

- **Labels**
  - What are labels?
  - Why do we need them?
  - Different kinds of labels in profiling.
  - Short demo using Grafana profiles drill down.

- **Diff View**
  - What is it?
  - Why do we need it?
  - How does it work?
  - Short demo.

Let's start with **labels**. 

Labels are common to all observability signals, such as metrics, logs, and traces. They are key-value pairs that provide metadata about the signal, adding a second dimension to the data on top of the time dimension we have in continuous profiling.

**Examples of Labels:**
- Labels are essential as we profile many applications. The same application may be deployed in different regions and environments, running on various machine types. We use labels to narrow down the analysis of profiling data to specific subsets of applications. 

Labels also enable features like the diff view we will discuss later and traces to profiles.

### Types of Labels

1. **External Labels (Static Labels):**
   - These do not change during the application's lifecycle and are uploaded with the profiles to Pyroscope.
   - Example: Kubernetes labels collected by discovery components.

2. **Sample Labels (Dynamic Labels):**
   - Collected by the profiler itself and embedded in the profile.
   - You must modify the application code to include these labels, similar to custom metrics with your own tags.

In Pyroscope, even though we differentiate between types of labels, once sent, you can query them in the same way.

### Demo: Exploring Labels in Grafana Cloud

To explore labels in Grafana Cloud:

- Navigate to the profile drill down from the left menu.
- You will see a list of all your services.
- You can directly go to exploration type, pre-selecting a service and showing all labels associated with it.
- You can also filter data using labels in the flame graph view.

We can visualize resource usage per label over time and filter our data to focus on applications emitting specific label values.

If you have any questions about labels, feel free to drop them in the chat. Now, let’s move on to the **diff view**.

### What is the Diff View?

The diff view provides a visual representation of the differences between two profiles. 

**Use Cases:**
- Spot changes after deploying a new version of your service.
- Compare profiles before and during an incident, identifying code-level changes.
- Analyze differences across different regions or environments to focus on improvement areas.

In the diff view, we can merge two flame graphs and color them to indicate changes. 

**Demo:** 

I’ll show you how to jump into the flame graph directly. 

- We can select different timelines for comparison.
- We can filter data and visualize CPU time spent during the selected time windows.
- The flame graphs will indicate which functions have increased or decreased in resource usage.

### Recap

We discussed labels, their importance, and types, and provided a demo on exploring labels in profiles drill down. We then moved on to the diff view, its purpose, use cases, and a demo.

Now, I’ll hand it off to Alberto.

---

**Alberto:**

Thank you, Alex! I’m Alberto, another member of the Pyroscope team. Today, I’ll present metrics from profiles, also known as recording rules.

### What Are Metrics from Profiles?

Metrics from profiles allow you to export profiling data as metrics to Mimir or any Prometheus-compatible data source. You can choose which profiles matter most and derive and aggregate them for different consumption.

**Advantages of Metrics from Profiles:**

- **Alerting:** You can now alert on profiling data, notifying services consuming too much.
- **Dashboards:** Create dashboards to visualize consumption over time rather than relying solely on flame graphs.
- **Function Targeting:** Export metrics for specific functions to monitor their performance in real-time.

### How to Create Metrics from Profiles

Metrics are created through recording rules, allowing you to choose which data to export, define metric names, and set labels. Once a rule is created, any future ingested data that matches the rule will be exported.

### Demo: Creating Recording Rules

To create a recording rule:

1. Start at the Prometheus recording rules icon.
2. Choose to add a recording rule and define its context.
3. Create a simple rule to export CPU contributions as metrics.

It takes about a minute to start recording after creating the rule. 

### Exploring Data

You can query the metrics to understand CPU contributions over time. For example, you might see that a specific service spent a certain amount of CPU time in the last minute.

The context from where you create the recording rule will carry over to the rule itself, allowing you to filter by service name or profile type.

### Targeting Functions

You can also target specific functions by creating recording rules directly from flame graph nodes. This allows you to track metrics for functions like garbage collection across all services.

**Questions & Feedback**

We are looking for feedback! Any Grafana Cloud user can request to use this feature. It is available for open-source users in the latest version of Pyroscope.

Now, let’s switch to the last topic for today.

---

**Pyroscope 1.16 Release Notes**

Yesterday, we released Pyroscope 1.16, which includes several updates:

- Support for ingesting profiles in the OTLP format over HTTP.
- A brand new Helm chart for monitoring your Pyroscope installations.
- UTF8 label name client capability, performance improvements, and various bug fixes.

If you have any questions about the release, please ask them in the chat.

---

**Closing Remarks**

If you missed asking questions during this call or are watching the video later, please join our Slack at slack.grafana.com, where you can find the Pyroscope channel. You can also visit our forum at community.grafana.com.

Thank you all for joining us today! If you have ideas for future community calls or want to present something, please reach out to me on Slack. The next community call will be next month. 

Thanks, and see you next time!

## Raw YouTube Transcript

Hi everyone. Welcome to the November community call for Pyroscope. Um, if you haven't been here before and you are wanting to try commenting and it doesn't let you for some reason, uh, if you click on the top right uh, corner with your image, you can go and create a channel. Um, we would love if you could ask questions uh, throughout, if there's anything you have that you want to know or any comments you want to make, that would be super awesome. Uh, so yeah, basically we have a few things that we're going to be covering today. So, we are continuing on the series that we were doing for intro to profiling for folks to just kind like if you're maybe newer to it or if you're just trying to get more familiar. So, we're going to be doing that about labels and the diff view that we have. Uh, and so Alex is going to be talking about that and then we're going to be having Alberto talking about metrics from profiles. And then lastly, we are going to talk about the uh 116 release that came out yesterday and just kind of telling you a little bit about what's happening there. So yeah, again if you have questions, comments, etc., please do in the chat. I will be asking these lovely folks uh whenever you have questions and also things that I might have as well. And so I will kick it off to you, Alex. >> All right. Uh hey everyone, I'm Alex and I'm an engineer on the Pyroscope team. So today I will talk about the intro to profiling. Uh as we said this is a continuation of a previous call uh where we talked about flame graphs in general and profiling. Uh today we're going to talk about labels and the deep view. Um so this is the the brief agenda. So for the for labels, we're going to talk about what are labels, why do we need labels, a bit about uh the different kinds of labels that we have in profiling and then I'll do a short demo uh using graphana profiles drill down uh and then we're going to do the same for the diffu. Again we talk about what is a diff, why do we need it, how does it work uh and then a short demo. So let's start with labels first. So uh labels are common to all observability signals like we have labels or tags in uh metrics and logs and traces. Uh there are key value pairs and they provide some metadata about the the signal. Uh also provide like a second dimension to the data on top of the time dimension that we have in continuous profiling. Uh here are some example labels that you might have seen uh across the different signals and also in profiling. Uh why do we need labels? Uh well like typically we profile a lot of applications and even if the same application will be deployed in different regions and different environments and we'll have different replicas uh and and they will run on different machine types. So we use labels to narrow down the analysis of the profiling data to a subset of the applications that we profiled. but also labels uh enable other features such as the DU that we're going to talk about later but also traces to profiles and and others. So we'll briefly talk about the different types of labels and again this is not specific to profiling this uh is common to to the other signals but we still want to clear clear this up because it's important uh and the different kinds provide different flexibilities. So have external labels which we also call static labels because they don't change for the life cycle of the application. They're not part of the profiles that are being collected uh but they're uploaded together with the profiles to Pyroscope. uh typically with uh the graphana alloy components like in Kubernetes we have the discovery components that can collect Kubernetes labels uh and similar and again like some examples of such labels as I said like these labels don't change they're static uh while the application is running then we also have what we call sample labels but uh also call them dynamic labels these are collected by the profiler itself and they're embedded in the profile. Um to have these labels you have to modify the application code. This is very similar to how if you have metrics uh you can have custom metrics with your own uh tags. Uh to do the same in Pyroscope, you have to use the Periscope SDK and define your own labels. In this example, uh if we use for example Java and Spring MVC, like maybe you have a a middleware that uh creates labels for your uh controllers and controller methods. We have docs in our uh uh documentation about how to uh specifically add custom labels for every supported language. Uh one last thing about this like even though we have this division between different kinds of labels on once they're sent to Pyroscope you can query them in the same way. There is no difference anymore. They're just collected differently. Now we're going to do a short demo uh about how you can explore labels in uh drill down in graphana cloud. So uh you can reach drill down if you don't know like in the left menu here we can reach profile drill down this way the first thing we will see is a list of all our services. Another many ways how we can explore our labels. Uh the the most basic one is to directly go to the exploration type in the top top. Uh and uh this will basically pre-select a service for us and it will show us all the labels that we have for this service. Uh another way is if you already know what service you're interested in, you can directly navigate to labels from from the panel for that service here. Uh and then if you're already looking at a flame graph for a service uh and maybe you want to narrow down uh your data to to a label, you can you can navigate to labels through this button here in the top right which will show you the same view. Um what we see here is a list of all the labels. As I said uh we can have different visualizations of the data. Here we have time series where we can see the overall like resource usage per label value pair over time. We can switch over to totals where we can see the total CPU time spent during the time window that we're looking at here 30 minutes. Can look at histograms uh and and and other visualizations. Um we're going to talk about labels a bit more uh in other contexts as well but uh the labels are present in the other views. So for example in the flame graph view we can filter our data uh using our labels. Uh so we can select the label name and the label value and the data that we see is now specific to uh applications that uh were profiled and were emitting this label value. Um, if you have any questions about labels, uh, feel free to drop them in the chat. Uh, in the meantime, I will continue on to the to the next topic, which is the diff view. So, what is the diff view? Uh, diff view uh gives us a visual representation of the difference between two profiles. Uh, this is important for a few reasons. Uh let's say you're deploying a new version of your service uh if you want to spot like uh that something changed so that something uh like maybe maybe you had a regression or maybe you improve the performance you can use the diff view to very quickly identify what changed before and after the deployment. The similar way if you have an incident uh again you can compare before and during the incident to see what changed down to the code level. And if you're looking for uh maybe areas to improve uh and you uh you maybe you've noticed some differences uh across your different regions or different environments. uh you can use DP to compare and to find like which areas you want to focus on more so that you can you can be smart about where you spend your time improving things. Um talk a little bit about how it works. Uh so we have two flame graphs here. It might be a little bit small to see but these are like relatively simple flame graphs. uh the one on the left we spend a total of four minutes of CPU time and in the one on the right we spend like 16 minutes of CPU time. Now this one is a fairly simple uh case and if we spend a little bit more time like comparing visually left and right we can see that maybe this node where we said we we have the order car method is bigger on the right side than the left side but you can imagine if we have a more uh complex flame graph it's not going to be immediately obvious what what actually happened. So it would be great if we have something like a git diff uh but for flame graphs and and this is what the diff view actually does. So now we see uh a third flame graph which is a combination of the first two flame graphs but it use colors in a different way. So uh whereas in the first two flame graphs colors uh define like packages or like where where the functions are coming from uh in the def flame graph the colors uh show us what changed between the two. Um I'll first talk about the how we built this a little bit so you have a better understanding and maybe that will answer some of the questions you might have. But you'll notice that the total for the diff is actually the combined total of the two. So what we do technically is we merge the two plane graphs together by actually adding them up. Uh this is good because if we have things on the right that don't don't exist in the left, they will show up in the in the resulting def graph and vice versa. So this will allow us to see maybe things that are new and things that were removed. Um and then talked about colors a little bit but uh it's important to note that the the way that we color the flame graph is uh we don't use the absolute values uh that are in the left or the right but we actually use the percentages or the the share for each function. So let's say the order car function here takes like maybe 80 or 90% of the total and here the same function takes 50% of the total. The coloring will be based on the difference between these percentages. So like if the percentage went from 50 to 80 then this is going to be more on the red side and vice versa. If the percentage decreased then we're going to uh color it green. Um, and let's go for an actual demo again. Uh, I'm going to go back to give me a moment. Yeah, I'm going back to graphana and then again uh in the similar way to before we can jump into the flame graph directly here. Um so before we go further I just want to highlight a few like visual elements here that might be important for you. Uh switch over to the checkout service first. So we have a left and a right component. Uh have basically like the timeline for the left and the timeline for the right. These are both for the same service. Uh and right now they're showing the same data. uh we can change the the range independently uh on both sides. If we don't want to click too much, we can also sync the time ranges. So like if we make a change on the left, it will reflect on the right side directly. U as we mentioned earlier, we can add filters. Uh but we're going to talk about an easier way how you can do that a little bit later. And then the last thing that is maybe confusing to some users is that we actually don't see a flame graph right now and for that we we need to select what we want to compare. Um so I will go a little bit further back in time because here we can see that something happened like there is a time when uh we had like a lower CPA usage and then a time when we had a higher one. So on the left side we can select this region of smaller lower CPU usage and on the right we can select different one and we can immediately see what I what I mentioned earlier like we have two major flames that uh are now red like we have one here that comes from this place order method in our checkout service and then there is another one here uh or our garbage collector Um so what we can do from here like first of all like this is a regular flame graph. So we can do anything that we can do with the uh with a regular flame graph. We can change the colors. So like if you're color blind like me like maybe you want to switch to to this view because then uh we don't have a red and green but we actually have red and blue. Uh and we can then focus. So uh like maybe we want to see like what exactly is happening in this function. So we can narrow down like where where is the CPU time spent. In this case this is a deliberately introduced regression in this service where we uh spend time on compiling regular expressions where uh ideally we should move this to the application startup and only do it once. uh but uh in a real world application like uh we will we will see something something different maybe want to briefly talk about this other part here. So we see that we spend more time on garbage collection uh in the baseline this was 12% of the of the overall time and in the comparison this is 20% so we jumped uh by 70%. So while we're here, we can already switch to a memory profile and look at the allocations and this will tell us where uh why the garbage collector is working harder. Uh which again will come down to the same uh method that we saw earlier process order but we can also quantify it. uh we can see that in the baseline 23% of the memory was allocated by this method but in the comparison now it's almost 90% so this part of our application is now predominantly allocating memory u so this was an example where I used uh a comparison across time uh we compared a period of lower usage with a compare with a period to higher usage which is what you would used during incidents and maybe when you deploy a new version. Uh but we there's another way that we can initiate comparisons and that's when we notice that maybe the uh resource usage between different replicas of our application is uneven. So in this case we see that let's go back to CPU. >> Um so for some reason the resolution is being locked as 720. Would you be able to zoom in a little bit so that maybe it's a little easier for people to read text? >> Yeah. Uh >> oh, that went the opposite way. >> Is this helpful? >> I think that's where it was before there. Okay. >> Thank you. >> Yeah, no worries. So uh looking at the CPU profile and we notice that uh for this label for the region label we have a discrepancy in the resource usage. So uh for this region we've used two cores uh and then for the other ones we've used less. So this sometimes happens because of difference in like the infrastructure but often uh also the problem could be in our application. So what we could do is we can initiate a comparison right from this uh view here where we can select the baseline. Uh we can select the comparison and then we can press compare. Uh we go again to the diff view. Now the the labels are pre-selected and what we can do is we can use one of these presets to select the whole range uh for for the diff view. And then we're going to uh do something similar. We're going to get a similar view where we can see what's different between one of our replicas and the other replica. Um lastly, uh I want to briefly talk about the table view. So the top table uh we did talk about the table in the in one of the previous community calls. It's a powerful tool to highlight maybe like where uh where certain bottlenecks are as opposed to the regular flame graph. Uh this table uses the uses percentages u instead of absolute values. And because we use percentages uh the values can be a bit noisy but you can still get some uh insight from them. For example, if we sort by the baseline and we we see which components were dominant in the in in the left side, we see that for example here this is the pyroscope profiler itself and we see that uh the profiler is now having a smaller share in the comparison than the than in the baseline. Now you might think like oh the profiler is now more efficient uh and that's very strange why is this happening but in this case this is not actually the case uh the this is explained by something else uh this means that in the baseline the application was maybe not very busy so the profiler uh took a larger share of the overall CPU usage but in the comparison the application was doing much more CPU work so the the time that we spent profile ing is actually the same but the share is is smaller. But if you do introduce a performance improvement in your application, this is where where you'll be able to see that. Similarly, if we sort by the comparison, we will see things that uh became worse here. Very quickly again we we will see that the place order uh function that we saw in the flame graph as well is one of the ones that uh is where we're spending more CPU time. Okay, I think that's it for the demo. Uh go back to uh slides. Uh I'll do just a quick recap of what I talked about just now. Uh we talked about labels. uh what are labels and when and why do we use labels, the different kinds of labels and then how we explore labels in profiles drill down and then just now we talked about diff what it is when do we use it how does it work under the hood and we saw a quick demo and now I will hand off to Alberto >> yeah thank you so much and then I'm going to switch refer to Alberto's slides and yeah, anyone if you have questions, please please ask them in the chat. Thank you. >> Cool. Thank you, Alex. Um, I'm Alberto. I'm another member of PCOP team and today I'm going to present matrix from profiles, also known as recording rules. And >> I'm going to come in and ask a real quick question on the last topic before you jump in. Um, okay. >> Brian's asking if you can add a diff visualization to a dashboard. >> Uh, it's a good question. I I'm I'm not sure if you can. I would like to double check that. >> Okay. Sorry. I'm going to >> Cool. Thank you, Ryan. Um, I we can we can go through that maybe at the end. open explore and see how it looks like. Anyway, um metrics from profiles. Um this is also known as recording rules. You will see why later. But yes, we're pretty excited to offer this feature. And let's just start with with what is metric from profiles. So metrics from profiles let you export profiling data as metrics to mimir or any prometheus compatible data source. So you may choose which profile matters most to you and you may derive it and aggregate and send it to mimir so you can consume it differently. So we are moving from flame graphs where are pretty pretty convenient for a lot of tasks um are very expressive on how resources are being spent. But you can also start seeing consumptions um as metrics which are also very familiar and sometimes are more convenient because we have this time point of view. So what are metrics from profiles? Which why you should adopt metrics from profiles or what can you what advantages can you take from metrics from profiles? So we started this project because we wanted to offer alerting from profiles and we found that there were a lot of people interesting on alerting profiling data and we didn't invent alerting and we just wanted to use like the custom alerting system on Grafana and we thought that the easiest way would be to just export profiles data to Mimir and just start there. We found that this kind of data is pretty useful and is filling some gaps that we are not currently um supporting on the on the UI on the read path. And here are a list of things we are using uh right now at graphana apps for for metrics from profiles. First of all, we now um unlock alerting. We can alert on consumptions. We can alert services that are consuming too too much. We can find functions that are increasing um also the consumption on on on percentage for example and we can yeah just out of the box of having um mimir in the middle we we we unlock alerting. Same for dashboards. We can now create dashboards that explain consumption over time instead of using flraphs. What else? Um we believe that one of the most important or more useful uh feature of this is that you can target functions. You can uh choose to export metrics on specific functions which means that you may understand how a function performance goes over time. Imagine you are deploying a new service version and you are releasing a new library version that you've been working on. You may see how it affects on real time. You may see how it drops and how it increases. And you can also compare with other services or or other installments between ports, between regions, wherever you want. Another useful case we found in graphana apps was to do coalocation. So we have these big services that are shared by um a lot of teams. We have gateways, we have load balancer, we have distributors that are shared by multiple teams. And it's sometimes hard to allocate cost once the bill arrives. And now we can know for sure uh according to data which resources being used by every team. So we can split the bill more accordingly. And finally there is a cool feature we found cool um also advantage we found that it's it's we can find which service is running a dangerous function once a CVU a CV is uh published. So we found this function that was dangerous and we just created a reporting rule and we could find over time which services were using that function. There's a caveat here. Caveat here. We don't really we can really trust on the probabistic nature of profiling. So maybe there is some service running the function but you don't really know because yeah it's probabilistic. Maybe the profiler cannot hit any instance of the of the function being run. So yeah there is um it may be a bit risky but it can be a a good starting point to find vulnerabilities. How do you how do you do metrics from profiles? So you do it from recording rules. Recording rules is a name we found convenient because a lot of people in the community is familiar with uh Prometheus recording rules and also because it explains how it works. It's a rule that records data. Recording rules let us choose which data to export. So you can have a filter on a service name. You can have a filter on any proof of label. So in in any label that Alex just explained. You can also optionally have filters on function names. And you can also explain and you can also define how to how you want to export. You may choose a metric name and you can also choose some set of labels that you want to export to be included on the metric. Why is recording? Because once the rule is created and after the rule creation anyested and any ingested data that matches the rule will be export not beforeh. So anything after the root creation data is um aggregated and exported to Mimir and yeah we aggregate through these group by labels here which is the the ones you choose to be exported and yeah that that happens at ingestion time. Let's do a a demo. Let's let's see how it works. Cool. To do metics from profiles, um you will need to start here at the Prometheus uh recording rules icon here on the top a little. >> Sorry. >> Could you zoom in a little bit? >> Yeah. >> Thank you. >> Thank you. And you can create the recording role from here. You can choose ad recording rule. If you are now using piscope uh cloud and graphana cloud, you may not find find this. Um yeah, you will need to enable this. Yeah, I'll I'll have some words later about this. But yeah, once this is enabled, you will be able to create a reporting role from here. Um so now we are at the all service view which means that we are looking all the services and this this context will be carried on to the recording role option. So when once we click here we will see that the recording rule we are going to create is targeting all the services. As I said we want to tell how to export. So we want to tell the metric name and the labels you want to export and we want to say to the recording rule how to export. So sorry what and which data to export. So now let's create a very simple recording rule. Let's export every CPU contribution as a metric to mirror something like this. So I will create a metric name called profiles recorded CPU nanconds. I'm going to export as well the service name because I think uh it's important and this recording rule will target every service. This also targets >> the part that you put at the end is what is coming directly like a specific Prometheus metric name. >> You mean this? >> Yeah. Is that based on like is it something that you're deciding or based specifically on what the Prometheus uh metric name is currently called? >> I mean um I decided this but this for the convention. >> The convention is Thank you for asking Tiffany. This is very useful. Um the exported data is a gaus. it's not a counter and gaus are conventionally suffixed with the unit. So in this case as we are working on CPU usage I know this is nanconds and that's what I'm trying to export. Um then uh let's check the filters. The filters are fine with the view we were uh working on which is service name all services profile type CPU uh it's the one that I choose here. I will leave function name empty. I'm not interested in export any function name yet. And I'll choose not to ex uh to target any other filter service um profile type is enough for me. So let's create this. Cool. Once the recording rule is created, any ingested profile that matches the rule will be export. It takes about 1 minute to start recording and sometimes they maybe one or two minutes. And also every exported um profile will have the ingested time stamp which means yeah even if you see a bit of a delay don't worry data is with the correct time stamp theest ingestion time stamp. Um it takes some time to see the data. So um let's wait like one or two minutes and in the meantime let me explain you something else. Um so the context the context that is brought here when creating the recording rule depends on where you create the recording rule on this flow. So for example, if you are looking at the checkout service and you are filtering by region and you create a recording rule from here, you will see that the filters are brought as well as the service name and the profile type of course. Um let's let's go to explore. Let's see if the recording rule has already uh start recording and and we can show some some data. this one. Cool. So, we have already some data. Let's craft a query from this. Let me use a zoom by service name, which means that I can see that the series in fact targets every service on our um PIS code. It will take a bit to be cool but yeah we are seeing that more services are joining as I said the exported metric is a gouch which means that every profile that arrives will change the gouch it's not a counter so it's not commulative that means that reading this data can be a bit tricky sometimes as profile um contributions can drop and increase uh abruptly to deal with gaus h we can use the sum over time function but let us define a time window in which we want to aggregate over time which means now I'm aggregating the house over time so I'm minim min minimizing this drop and um effects that can appear from changing the house drastically and this will help me understand the the ingested data data better. Um, let's see some dots, some points here. So, for example, this one is 1.2 billions billion nanconds on CPU consumption. What what does this mean in this minute? Because I said 1 minute time window there was a spent of 1.2 billions ncond. Let me divide just to easily understand the units. Okay. Now this is seconds. Let me use also this. Maybe if we if we use another step we can see there are more cool. Okay, now we can now we can start understanding this metric which means checkout service at this minute took 2.33 seconds on CPU spent for every minute it was running. So the last minute this was captured it just spent two seconds on CPU which is pretty low but still we see we can see from the metric that the check out service is the highest of all all of them. Could I have uh some dice prepared because as you see recording rules may take some time to like show cool data. So I just created these three examples and we just did the first one. So we have did this CPU usage by service name and and here we can visualize the CPU usage of every service on 10 minute windows which means check out service here spent 19 seconds in the last 10 minutes of um execution. Now let's do this second other example. In this other second example, we are just focusing on the check out service. You will see that this line is the same as this one more or less. So we are we are going to export we are going to target this service name and we are going to split it by region. This is something we can do on with recording rules and let's let's do it. Let me go back to the profiles drill down app and let me choose checkout service. From here I can create sides of a recording role. I just want to create export this service CPU contributions but I want to also include the region um label which is important to me. Um if you remember Alex example h she he showed that there is a region that is contributing a lot more on CPU than the others. And that's also something we want to see from metrics from profiles. Let me name this function ID this recording rule as profiles recorded CPU checkout service nanoseconds which means the CPU contributions of this service in nanconds. I'm going I'm going to add the region label because it's important to me. Let me check the filters are fine. Check out service CPU no function name another filter. It's okay. And let's create. Cool. Well, as as I said, it takes a bit of time. Um, we should be able to see something similar what Alex show, which is that there is a region that is spending more. And then I want to yeah, let's let's not wait. I mean, we could we could see here on explore or metrics uh drill down app um we could see the profiles recording rules um that I just created. This is the the first example. The second example hasn't shown yet. So yeah, it it takes a bit of time. So let's move to the dashboards I created. And as you may see this been running since the morning. This works as you can see how it how every service so how every region is contributing to the checkout service and CPU total. Finally, let's do our favorite example which is targeting functions. And we are going to create a recording rule that show us which service is spending more on garbage collection by percentage which is a pretty interesting thing that we cannot really do right now from the profiles drill down app UI. Cool. So let's move to the checkout service and just to well let's take an um service as example. accounting service. For example, you will see here that there are some um garbage collection. This one, this function name is known to be like the garbage collection thread um doing um draining jobs on background. And this one uh explain us how this garbage collection worker takes 5% of CPU on the service. We cannot really compare how the CPU um on garbage collection goes on other services from this view. We are just on the accounting service. We don't know about other services and we don't really know how the garbage collection evolves over time. This is something that the frame graph cannot explain as this is just an aggregate of all the last 30 minutes. So we can improve this by just exporting as a metric and visualizing on another way. There is a hidden way of creating recording rules which is from the f graph you can choose any note to create a recording rule. So for example if you if I click here on the total I can create a recording rule. It's the same as if I was uh clicking on add add recording rule but you can also target functions by clicking on their note. So for example I can choose to create a recording rule for the garbage collection only. This is um how I would create a recording rule for this service and for um garbage collection but I want to create for every service. So I'm going to copy um this function name and then I'll move to the all services view. I'll create a new recording rule and I'll use this function name as the function name I want to target. Now I can create a CPU function garbage collection nanconds for example um metic name and I can include the service name which will be the the one that will let let me understand which service is spending more on garbage collection. I can create that. It will take some minutes to have insightful data. But if we go to view recording rules, you will see that there are the three rules that I created uh in this demo and the other three that were created uh this morning. Then if I jump to our demo uh dashboard, we can see um real data on this. As soon as we have CPU usage by service name, once we have CPU usage of collection by service name, we can compute the radio. And that's what I'm doing here. Let me show you. Let's go to edit and let's see how I'm just dividing the garbage collection nconds contribution for every service by the CPU contribution of every service. So that's how we know the checkout service is spending around 20% of CPU usage on garbage collection and we can see how it evolves over time. We can also compare to other fun uh to other services for example accounting services accounting service that we were seeing before that one had an example of 5% and now we know that over time over the three last hours it has more like a 7% and we can see how it increases and drops over time. Cool. Um, any questions so far? >> I have a question. So, like right now you have your profiles and you're specifically creating metrics based off of your profiles. If say you are collecting your profiles and you're collecting Prometheus metrics in general, is there any way to be able to like connect those two if they're already existing or do you have to create your own metric from your profiles to do that? Um you always you will so for right now you will need always to get a recording rule and you cannot really link to any other Prometheus metric existing. Um this feature can certainly evolve um in some directions. We have an example where we create different recording rules and we target the same metric name. So we are aggregating results from different metric rules and there is some other hidden features where you can tag um the metric name. So you can tag met series coming from one rule and series coming from another rule and aggregating them together with a new tag called first rule second rule. And that's how we did for example in cost attribution where we had recording rules hitting one service and recording rules hitting another service and they were being exporting exported at the same metic name but we have a new label called um team that can distinguish them. So and Fer can abandon and can pretty evolve but we we we believe this is um uh you know a good stage that can be launched and yeah we are we are pretty happy with the state and we're just see how people start using this and maybe they found new beautiful ways of uh taking advantage of their profiling data that's been sitting there on the Piscope and maybe you cannot really really extract all these insights without use this function this feature. I just want to mention that it's pretty obvious but once you have this as a dashboard you can just create alerts out of it pretty easy. Um yeah you can use any graphana stack that relies on on on Prometheus and it just gets unlocked. You can create Prometheus recording rules from this um metric as well. Yeah anything you can do on metics will be able to be done. Yeah, you can do it on on profiles now. Cool. So, let me go back to the slides. Um, yeah. So, we are looking for feedback. Any uh any Graphana Cloud user can request for for use to use this. You just need to ask to the to the support team. Um, this is available only on private preview, but yeah, you can use it already. any open source um user um can use it. It's available in the last version. It's been available since older versions, but this is a quite experimental uh thing. I we recommend use the last one and yeah, just take into account that you need Piscope V2. Um and if you have anything any problem to set up, just reach us on our graph apps community slack. And that's all I'm from the metric form profiles project. Any questions? Um yeah, just chat and we will answer. >> I have a question. So say you have created a metric spec like you picked some like your CPU nanconds. What would happen if you tried creating another one with the same uh name? Does it just like tell you hey you already have this or like >> no >> no no but um let there is so if if I created this twi so let me share again prescope if I created this twice right >> well it depends how you query it if you query it um like this uh let me copy and go to explore and explain it uh properly so if you go to explore and just query like this you will see that there is a profile role ID here that distinguish rules. So if you are doing some security conditions here like um profiles role id s zoom by so if you are doing a query such this uh you shouldn't be you shouldn't have any problem because every rule will have a different uh rule ID but if you are not using this yeah then your medic will be magnified like you are adding in the same metric two different things and then uh you will have this I mean we're not limiting this but may because maybe people want to add two different sources to the same metric because it's relevant for them >> okay so there's question of is there a way or plans of a way to collect historical metrics for example instead of starting the recording rule at a creating time it uses data filled for the past week >> so right now this is not supported um I think that it's not too crazy to think that this I mean this is possible. It's just maybe you just don't want to do an ingestion time but maybe users you want to export something that goes from a query. I mean we I believe that Pisces scope has a increasing performance a lot and we were not supporting such of such of such type of query. So give me um garbage collection information for the last week. That's not allowed. But yeah, taking into account this feature is only filling a gap that is not supported right now. But maybe in the future we can support such of such type of queries, but not not right now with um recording rules. There is no plan for that. >> Okay. And then yeah, folks keep asking questions. I'm going to switch it over to talking about 116. >> Yeah. So, the last bit for today uh yesterday we released Pyroscope 1.16 uh has a long uh change log which you can access at the link at the end there. But what we want to highlight is that we added support for ingesting profiles in the OTLP format over HTTP. Uh we also added a a brand new Helm chart for monitoring your Pyroscope installations. Uh it's based on the monitoring that we use uh internally at Graphana. Uh we added a UTF8 uh label name client capability. uh a few performance improvements and many bug fixes and security uh patches. You can see more on the link as I said and I think that's it for today. >> So is that the first time now that you can actually use OTLP for Pyroscope then or was there something beforehand like for gRPC or is that not there yet? >> Uh I don't I don't know I remember the details. I think I think this adds another way to ingest OTLP. I think it was already a possibility before. Um, but I would need to double check that. >> Okay. So, basically just making it more easy for like say if somebody's already like I don't know collecting traces and they have all their stuff set up there now that there's less you have to specifically change or do to be able to get percope as well. >> Yep. >> Yeah. So, yeah, if you have any questions, uh, please ask them in the chat. Otherwise, if you missed asking questions now, whether it's right now or if you're watching this video after the fact, um please go over to uh slack.garfano.com. Um I see that's not on there. So, if you go here, wow, that's huge. If you go here, um basically uh you can go and join us in the Slack and there's a channel for Periscope and can talk to us over there. There is also uh the forum which is community.garfono.com. grapho.com. So yeah, uh I guess in the meantime uh thanks so much for both of you for doing this today and uh thanks to everyone else. Also, if you if any of you have ideas about uh things that you would want to have in a future community call or things that you want us to answer in future call, please let us know. Also, if there is something that you are like, oh hey, I would love to try showing this sometime in a community call. um please just reach out to me on the Slack. It's I'm just Tiffany on there. Uh and we can discuss that as well. And then we'll also continue doing more of the parts where there's like the intro things for folks who are newer to profiling as well in addition and then so the next one will be uh next month. So yeah, thanks everyone for joining and I don't see any other questions at the moment. So I will uh just hope to see questions over in the Slack that we have. So thanks everyone.

