# Grafana Loki design basics with Ed Welch (Grafana Office Hours #27)

Ed Welch, Principal Engineer on the Loki team, is talking to us about how to get started with Grafana Loki for beginners. He tells ...

Published on 2024-06-08T04:22:02Z

URL: https://www.youtube.com/watch?v=3uFMJLufgSo

Transcript: hello everyone and welcome to another grafana office hours it's been a while since the last one but we are back and we're going to be talking all about Loki but first I'd like to introduce you to my new colleague over here thank you Nicole I think this is quite fun because I used to be an avid watcher of Graf office hours so now the co-present to host this is this is a bit different so I'm for my first time um but hi everyone my name is Jay Clifford and I'm one of Nicole's colleagues actually I should way mirroring there we go yes no it's it's it's funny because now you get to see how little there is the prep before these now you get to see it warts and all but today I figured that I would have not just Jay in you but also over here Ed Welsh welcome Ed thanks for thanks for inviting me um looking forward to to talking about my my favorite software project yeah well you know not like you're biased at all or usually I qualify in my opinions as being completely biased so I'll just kind of set that stage for the whole show here I guess yeah so tell us what you actually do and why we would invite you here to talk about this new project you probably haven't heard about called lowkey yeah absolutely so uh uh today I am a software engineer um that is predominantly what I've done for my career um done a bit of different engineering along the way and actually uh even at CFA I did some some management for a while um largely because I think I just sort of you know fill the roles and do the things that are needed and kind of I've always adapted to so I'm kind of you know pretty good at a lot of things and not great at anything um so specifically though uh at grafana I started um me do the math in my head probably about five and a half years ago now um so Loki is a as a as an idea was born just about six years ago uh and it was sort of announced to the public in I think November of 2018 and I started with grafana in uh January of 2019 and have been working on the Loki project since that since that entire time so that puts me as I think the first engineer officially on the project and uh it's been quite the journey so um but yeah that's that's kind of my my background in a nutshell um I do tend to always end up towards engineering but kind of more of a tinkerer than I think a we we have better Engineers than me on the project now luckily so how do you mean you said you you were the first engineer officially on the project but it was announced before you joined Graff sah hat how does that work yeah so so um David kmid and Tom Wilkey kind of created the idea um and uh we we can kind of dig into that idea too um but I'll give you a little bit of the story of the folks involved early on so so Tom and David kind of worked with the idea and then there were a couple other engineers at grafana at the time graan Labs was um you know around 30 people I believe um so uh there were a couple other Engineers that were working on like the cortex project that which Loki gets its its a lot of its origins in I mean Loki uses or at least use cortex um as a library of sorts um and so some of those Engineers like um Gotham sorry Gotham I I haven't tried to pronounce your last name in a while so I'm not going to try now um uh and like Jacob lissy and some other folks that uh you know kind of were involved with the project early days it were sort of borrowed Engineers I just hired in and essentially you know from day one worked on Loki and uh I have been associated with those other projects along the way but my focus has always been around Loki so so how much would you say was done at this point was this very much like a proof of concept document and then you've already started committing code or like you know yeah it's funny like there's there's some elements of Loki that exist today that are still code from that but it's it's pretty small at this point like Loki you know sort of shares in the concepts of you know the the idea like I think that's you know when it was announced nobody really had any idea if the project would succeed or become popular right like the announcement was about this sort of concept you know the the time it was a bit funny because we ran a a cluster that we called a production cluster and had like uh like an unlimited free you know free beta period or something like that you know and we were just kind of hoping that nobody took us to seriously the unlimited parts of that um but Loki you know very early on was was really the idea of of maybe this is is a good sort of segue to kind of to talk about that right so the you know the the year is what I said 2018 um and I'll I kind of of frame this a bit because I'm going to talk about kubernetes quite a lot and um Loki is not particular to kubernetes certainly not anymore um but it is a big part of why it was created uh so you know I always want to phrase that a bit because I don't want to steer people away if they're not using kubernetes um there's some cool things about kubernetes there's a lot of things that are I mean they're cool but you know there's always some additional complexity there anyway kubernetes and Prometheus are like becoming a lot more popular and in that space um there wasn't really a great way so you have this very nice solution in Prometheus for getting metrics out of your applications coming with I think one of its most powerful features which was service Discovery making it you know sort of trivial to you know use this Dynamic environment of scheduling descheduling pods and getting you know one of your Telemetry signals very very easily um the the same wasn't really true for logs it was kind of hard to you know there there existed Solutions but um you know nothing that had been really built for this kind of world and so that was one of the criteria around Loki was making it sort of fit into a kubernetes native World very easily so that same kind of service Discovery and getting your logs without having to think about it um aggregating them all in one place and so it borrowed the concept from Prometheus uh of labels so this sort of key value pairs that become um metadata for finding your logs like no we use labels a lot differently than Prometheus because we um we don't really want to store any cardinality in them so we we really just want very very simple labels to define the source of log streams to make it easy to find your logs at query time we can talk a whole lot about labels but I'll skip it for now so uh because it is kind of the key component to how low key was built as a as a database because it's the index and you know index is a complicated thing in a database world and that was another thing with Loki was this idea of keeping it very simple or as sort of simple as we could or can or try to um to simplify a lot of operations so that was kind of the second part was you know we wanted something that was native to inputting logs very easily into a system but also that would run and was built around you know the the sort of things that we knew about building distributed systems and and scalable compute in 27 2018 um so it's built as you know microservices components um and then really the key to what Loki does is use object storage to take a whole class of problems and offload them to systems that are very wellb built for those problems so you don't have to worry about running out of disc space you don't have to worry about scaling your dis infrastructure or monitor like nothing related to discs if you've ever been in a position in your job in your career where you've been paged because of dis rated things like you would appreciate how nice this is discs are really awful um I mean obviously we need them but I like it a lot better when they're abstracted to me through in HTTP API like what object storage does so so Loki was simple yeah go ahead you mention that Loki is a database why would why would you need a separate database like a log specific database because there are already lots of databases yeah good question so um the this one is fun to to me today because you know if You' asked me this a couple years ago I think Loki's identity was it felt a little bit more solidified but I think it's evolving again and you know the reason I referenced that is because the way I would have answered that question before and it's still what's true for today is that um Loki makes tradeoffs around the type of data and how we expect it to be used and no database really gets anything for free so these trade-offs sort of determine like what it's better at and better might mean cheaper or you know more cost effective I guess is nicer way to say that but really cheaper um better might mean faster for this or faster for that and so the trade-offs that Loki was designed around were very simple ingestion so there's no schema requirements we don't have any opinions Loki you know does not care what format your logs are in you know if if it's a consistent format or not because you know the reality of the world of logs is it's sort of a huge nightmare you know like there's sometimes structure usually not like I mean people can maybe agree on um you know a formatting like Json but that does nothing to sort of standardize on a schema within Json of common keys and values and so there are databases out there that are very good around working with schema is um and you know the trade-off you make there is you have to normalize your data to ingest it um so we didn't want to do that um and this sort of differentiated it from some of the common log players at the time um certainly not all of them but um and then also leveraging you know object storage and small indices by Design so that we can leverage parallel Computing and you know horizontal scalability as another trade off for you know how it costs like sort of what the cost trade-offs are on running software and infrastructure like it's it's actually cheaper in in most cases even to you know pretty impressively large scales to do more Brute Force searching um with dynamically sort of scaling infrastructure than it is to build and maintain a really large index that requires machines that have hundreds of gigabytes of RAM um or the complexities around how you maybe sharded index across machines um so the trade-offs Loki was built around where to simplify a lot of things and exchange on the ingestion side and then add you know a basically a query language that lets us do we call a schema at query it's it's sort of the idea of like use a parser to extract the content at query time and then you can write your query to kind of normalize your data um however that means those queries require more more resources right and so you know um those tradeoffs you know they tend to work well for what we built Loki is a target audience which is people that run infrastructure right I'm I'm a developer uh I need to know if my stuff is working I'm you know operating some software I'm on call for it so you know making it quick to find your logs to find your problems um but there's a whole class or several classes of other use cases around logs that exist um two of which I'll kind of call out like business intelligence like you know give me the distinct you know the count of orders that you know went through or like how much you know people log so much data you know I don't know this is what logs were meant for but like people will you know log financial data and transactional data and things in their logs and then want to do analysis on that you know as part of their business analysis um and a lot of those queries ask questions that um especially over really long time ranges that you know Loki has to work harder to answer um because we didn't pre-compute those values into an index right so we go scan all that data and read it so um you know so the the interesting part about like where we are today is um you know we find ourselves more and more interested in use cases which you know Loki wasn't originally built for and now we're trying to decide you know should we extend it right add more levels of pre-computed indexing or store data in different formats like column or storage or things that have some advantages for specific access patterns um but that's really the key with databases right is they're usually built around their use cases and that sort of determines you know how they store data and what they're fast at or what they're slow at what cost them money um so at the time Loki didn't you know there there wasn't really a system quite like it um you know that was built around you know uh like a microservices architecture leveraging commodity compute and horizontal scalability and object storage and scheal ingestion so we built it and it turns out that that scratched a niche for quite a few folks can I can I ask you some so I feel like you've touched on a lot with the I think this is the main thing that Drew me to Loki was like the scheist design and how you store this in object stage so but then I feel like having built this you've done something different it comes with like a ter like a ton of terminology differences um and it'd be good if you could sort of like explain some of that so you know we've got terms like log streams and then we've got chunks and then we have labels and then we have and and like you know how do chunks relate to labels how do they relate to log streams and you know all of that good stuff and I just yeah for me it great describe these sorry I'm I'm only laughing because I've probably created all of these problems over the years is I've like there's a common problem that I've run into I think folks run into it's like when you're new to a field and you don't understand the terminology it's like pretty daunting right and you so you sort of try to learn it and like problem that I have is when I learn it then it's like that's all I'm going to use now is that terminology and I sort of forget about my former self that didn't know those things and didn't have them well Define and so um it is confusing and then if you throw in that there are there's a lot of uh overloading of terms especially if you use a word like block or chunk um even within you know the the databases that grafana builds those terms mean sort of similar but different things um so yeah let's start with a few of those and see if we can I can I can tell as much of a story about him as you'd like with an index because that's something that's common across all databases so this is funny because depending on who the audience is uh again I am uh I will I will give you my definition and I'm sure there are people that have other definitions um you know I'm not sort of by education a computer scientist so you know I've learned stuff through others and through uh anyway um so an index if you think about um I mean I guess I can explain it in Loki terms right like if you have a whole bunch of log lines um and you know they come from a whole bunch of different applications and you want to go look them up right so so the most sort of basic index usually is like you know if they're stored on a file system in files you would navigate that file system like the directory structure like that's an index of a file system like file systems have um you know internally they do really wild stuff now too in terms of optimizing that so like if you have a lot of files in a directory they can um you know sort of make sub IND like anyway what an index does is take a large set of values and reduce it to a smaller set of values based on any of a zillions of a criteria like the you know in Loki's case we take that huge amount of data and simplify it into some key value pairs that let you you know go find it again right so like nals app equals Loki um if you were thinking of like a reverse index um which is pretty common for a search engine um this is like the index in the back of a book right like I go look for a word and it tells me what pages there on and so when that data was put into that system every word is sort of stored in a separate file and then the references to where it was found um and so this is very fast for search right I go back at look at that index but it can also get huge like that index can be as big as the data that you ingest because every word is going to get stored with multiple references to where it's found um now these are very simplistic descriptions because a lot of really smart folks have found really really clever ways to like optimize these things and build levels of levels but you know effectively what you're doing like you know if another common case people come across stuff like this is in like a you know relational database um you know where an index is a way of taking different columns and sort of pulling their values out into a separate place where you know maybe even you do some math operations on them the idea is to make a smaller data set of a bigger data set that you can look at the smaller data set faster to narrow down where you might have to go search in the bigger data set or even completely answer your question from the smaller data set so that's how I Define an index so yeah so I I kind of think of it as like I kind of think of it as you know let's say there's a whole bunch of physical addresses for people and businesses and and a bunch of whatever else is there and you could just look if you were looking for something in particular you could look one by one but an index is kind of like breaking that apart into different categories so you could do it by suburb you could do it by like whether it's a business or an individual uh you could do it by a location you could do it I mean you could do it by house number um you could do it by street so there are some ways to break up that data that are better than others so for example you know there's probably a one like the house number one for many streets so is that really like you could do a search for all houses that that have the number one but that really relevant so an index doesn't necessarily mean like it's a good way to organize but it is one way to break up that data does that sound you touched on a thing that this is my I'll pivot into like you know distributed systems 101 so so what you just described there was what I would call Key spaces so so you know in your your index there right you have a a key space and you describe several of them right like it could be you could have a key space that was based on street names you could have a keyspace that was based on house numbers or zip codes or uh you know guess zip code is a us thing but there's equivalent right like um you know first name last name Z codes too do they call them zip codes though or is that just a um no I don't know we have postal codes yeah right the zip part anyway um so you have these these key spaces and what's really fascinating for a distributed system is you you you succeed or fail by how uniform the access patterns are over the key space because what you end up building and this is all a distributed system is is a way to split work across multiple machines using a keyspace and so for example if you you know maybe a bad key space here would be if you took um I don't know let's say took last name this actually not a great example of a bad key space it might work out right but where they tend to break down is if all of your access pattern goes through a very small number of keys you you want the keys to evenly distribute the work over all of your infrastructure because you end up basically sharding is the word we use constantly so sharding is um you know splitting data up into smaller shards and you do that by taking so if you took um you know postal codes would be a pretty good example because they're basically Built For This purpose right they're designed to divide up the address key space uniformly across the whole physical world so um in theory now where that can go wrong is like if you have a postal code for like one major city and 99% of your population is in that City now that you know City basically becomes one server in your infrastructure and it's doing all of the work all of the time right so you you this is the sort of name of the game so for Loki our key space is the key value pairs you assign to a stream at ingestion so um in practice this so now what is the the difference between an index and a keyspace um I would say the the the an index is like a method of storing a keyspace or using a keyspace um so you you're they're they're it's a little ambiguous in that sense right but a keyspace is going to be part of the definition of an index like an index will have a key space that sounds reasonable to say I'd have to think about if that's universally applied or not now how how about a label cuz cuz Promethea I mean Loki does use indexes but but it uses labels much more but it sounds like it is just another way of organizing or categorizing data um labels are in are index so Loki is not index free um nothing really is I don't know we might say a little bit because it's it's not a reverse index like we don't look at the log content and take every value and put that into a lookup table um which is a type of index um uh instead we we index the metadata which are these labels which um you hear tags a lot too so this is another area where there's a lot of overloaded or that's not overloaded but basically you know multiple terms but um you know it's these key value pairs where maybe we went wrong with Prometheus and Loki and you know is is sort of how fle ible so like there's you can Define any sort of labels that you want this is true for you know Prometheus and Loki and I think maybe what's interesting about something like open Telemetry is they have some opinions they call semantic conventions which are are ways of specifying common labels because if you go between company to company or even you know team to team within an organization you might find it hard to have consistency in labeling because everyone had their own idea maybe they use app maybe they use um you know project maybe they use system as Keys instead of you know so it it's like you you have this huge flexibility here um but the key value so as an example if I have a web server that's running in a Dev environment and maybe my us east region it would have you know it could just have one label which says you know app equals web server but that's going to now mean any web server anywhere will have the same set right so then usually we add you know cluster equals US East one um environment equals Dev or n equals Dev and and that that those three label key value pairs Define that log stream and then that goes into Loki and that starts filling a chunk so it's the the log streams end up as separate files on disk essentially or in object storage um and if you had NV equals prod even though it shares two of those the same that one value change makes that another stream and so that gets stored separately um and you can have we do limit the number of labels you can have several I generally recommend using the fewest labels that you need with Loki because we just want to if you think about it the other way we want them to be useful for querying I want to be able to say go find all of the logs for this application or this environment or you know this cluster and I can use each of those labels individually or combine them to find my logs so you might have asked answered one of my other questions in there by saying Stream So a log stream is what the unique combination of key value pair labels and that makes a stream yeah so so stream is a word that we made up in Loki so it's very analogous to Series in Prometheus right yeah you know why did we call it a stream I don't know it was a log aggregation system not a metrics database we made up another word um now though I will I usually throw in that I think conceptually it helps a bit so good Stream So Good label design in Loki makes long lived streams that live forever right so so you want your label values to be really consistent over time and that means you don't want label values that include data that changes frequently like and Order ID or you know a trace ID or something that has a very very short lifespan that makes a very bad label in Loki um so so stream I think maybe helps a little bit for folks understand you really are dealing with a an infinite stream of data like you're just it's always coming in and you're you know compressing it and writing it out in practice like labels definitely churn you know like we have labels that you know will set the lifespan of streams but um you know conceptually like Loki's really designed around you know aggregating all of this data you know sort of sorting it into these that are hopefully useful and then building up objects that are a bit bigger than you know big enough that we can put them in object storage and so there they a stream a log stream is not a single instance it's not a log line it's like the pattern for a kind of log like for example in the address example an address is going to have a number a street name a street type a suburb a postcode and so on and that thing is a log stream yeah so so in that analogy which is is kind of fun right like your your stream would be what you'd be you know essentially your house right and then and then you would basically what you would be indexing is all of the the letters that come to your house like but not you're just tracking they went to your house right so all of the letters that ever went to your house and it's the thing is that could be any you know name shape or form like now you know maybe you want to to this is fun for this analogy maybe you you put up another mailbox in your house that's specifically for junk mail or whatever right and so like you get the post office to somehow do that and then you add a label that's like you know the log value which is like high or junk I don't know I'm making up labels again here but um but then that's all of the messages ever that go and that's a good example because you know that's going to be data that's consistent over time right so so now if I'm going to go search through people's mail I would say you know I could search by you know town or address or I could do Town address zip you know in narrow right down to a specific house um and yeah within Loki we're just bunching up all of those letters that come in and then putting them in objects and in object storage um and then so time really becomes your useful thing when you want to search that much data right it's like do I want to search for a very specific window for this mail or do I want to search all the time for this mail because that is also in part of in the index so you can say you I want to look at this I want to look through all the mail I want to search everybody's mail uh that lives on this road for this weekl long period um and that's how you would you know one of the ways you could approach that so so just to fully grasp I I think I get this and I like this example of City so say if I was like I had this project in the UK where I was going to keep in Loki all of the people being born so a label for me would be cities and because that would be non-changing so I'd have Edinburgh London abedine and that rarely I'd rarely be adding or changing that label because it would all be a city and then a so a log entry into that stream which would be for a city would be each new person being born and then that would be and then you could have a a country and a continent and some other you know labels that would give you some ability to you know dimension that data for like you know queries that you um you know I want to look you know count all of the people or you know run some queries on you know and Loki can extract the value out of those lines too so if that line contained a bunch of data about you know maybe the weights um you could do you know what is the average birth weight of all of the people by region or something like that right so but that's a yeah you got it so an index a label a metadata and tag are those do you think that that's just interchangeable or are there semantic differences in there um yeah it really depends a lot on your audience I guess or your um they are all sort of related to each other um label and tag are almost synonymous I think you know it depends on systems you've worked with whether they call it tags or they call it labels but I think my opinion there is they're kind of of sort of nearly interchangeable um you know the index I I yeah it it does depend a bit but they are all super related I as an engineer it becomes hard for me to make absolute statements because like someone is gonna someone's gonna argue with that yeah yeah but you know like you do have to abstract it somewhat and and accept that you're going to lose some nuances to be able to understand something to me it sounds like these are all just ways to organize the data they're all key value Pairs and they might differ in terms of the the level like how high level it is or low level but really it's just a way to find the stuff it's a way to categorize data so that you can find it later yep um and that is you know that's the the database game right is is there's kind of three parts of it there's sort of how you ingest the data um and what requirements you put on people at ingestion time and that then determines sort of the the cost of that system both in terms of its complexity to use and the resources that it used to store the data um and then you have to access the data and you have now like the same thing right like if your database was built to store data around specific access patterns it'll definitely be faster and cheaper for those access patterns but that requires that you know those ahead of time right like it requires you know how people will use your system um and want data and that can be tricky right so almost all databases are making these kinds of trade-offs between how much it costs to ingest and store the data versus how much it costs to you know query the data and then you know Additionally you can compute you know multiple indices if you want like you know within Loki we're sort of adding another one now via um Bloom filters which are kind of an interesting talk about probalistic probabilistic data structures but there's this whole interesting world of um of things that you can do but it is a form of an index basically that we will compute after the fact on the data or maybe during injest we're still currently it does it you know after the fact because we're kind of still building it and it lets us rerun it um and then you store that and now that's another thing you can go look at when you query your data but there's a cost to building those there's a cost in terms of time to querying those so you know we're building those because they're are specific access patterns that will really benefit from that which is you know people searching for needle and Haystack type queries like a order ID in a whole bunch of log lines or something like that um but yep that the the there is no there's no free lunge you know but there are like you know the tradeoffs that we make with Loki to try to make it very sort of general purpose work really well right now at least for you know the world of infrastructure logs where applications produce who knows what um makes it very easy to ingest and then you know the access patterns that we you know built Loki around are um finding those logs to look through them in order to sort of find problems or you know do some analysis like you can definitely run metric queries over long periods of times to pull you know to pull numbers out of log lines and view them as graphs and things like that um but it is a essentially it's a row oriented time series database so it works best when you're you know kind of scanning stuff um it's not as good at trying to find a you know small piece of information over a huge data set um and that's why you know we're adding Bloom filters for one of those examples but um it's all a set of trade-offs in terms of what because at the end of the day like the amount of of money and infrastructure that goes into monitoring is massive and so we're always trying to find the set of trade-offs that makes things as sort of cost effective or economical as possible without sacrificing usability and that's that's a hard problem I think that kind of is a good segue onto like the the primary star here which is like the the log entry itself and you talked on this like earlier so we've gone down from our labels and we talk about log entries so you said about Loki being able to essentially store anything so in that instance does it almost just treat every single line like one big long string um you know I guess my followup questions here if I was coming at a new log engineer and I go okay my label is going to be sis app. log and it's all storing in Json and now it's going to swap to a different format should I continue storing it under that label stream should I do different does it really matter all of these types of questions what how what's the tradeoffs of it being one store everything type model um yeah you're G you're going to touch on what is a really sort of hard question for any you know Loki does escape this it just changes the like like the idea of a schema right the idea it exists whether or not Loki cares about it right so you know you generate your Json log lines and they have a format to them right and it's consistent through whatever time period that the application isn't changed or that you know produces that output um so Loki does just store a string um I think it's a you know just an array of bites basically so we don't care um at all and then you know the query language has parsers for you know common things like you know Json and then a a pattern parser which is like a simplified way to do you know wh space patterns like the patchy you know Common log format which is um ironically not common at all because it can be configured differently by everybody but it's a space delimited you know a log line format we use something called log fmt um which is a key value pair sort of format that's kind of my favorite um so you know the the content of log line like this is hard for everybody um we made it easy in the sense that you don't have to make any changes to your system when you make changes to your application or if an application changes it'll still injust but you can definitely break queries that were working if the log content itself breaks and so the different ways to approach handling that like you mentioned changing a label um you can definitely do that um you know if you sort of know the time like these problems become hard very very quickly and I would say like you know we have some answers for that and you know is it it's hard to say like the you still have sort of quer well now if you're normalizing the data on inest like you have to either coordinate like rolling out your application change with your schema change to your database which is hard or you have to coordinate your query changing over time um to match now I guess the advantage of changing the query is that you can retroactively do that quite easily right like you can change the queries as many times as you want whereas the ingestion part if you got that wrong and missed data they're all interesting trade-offs I don't know I mean I prefer obviously by my biased opinion no schema at ingestion and I think a lot of folks do because of just how chaotic the log world is like if you you know said today I'm going to build a new company and write all new software and had strong opinions around how to structure your data like yeah you could probably leverage some column or storage to get advantages there it's just in in practice few if any folks that I've ever seen are even successful doing that today like even if you set out today most folks are focused on building an application that works not how do I make sure I'm having consistent log formats over time you know it's it's just kind of an afterthought a lot of times un Ely but I mean I guess fortunately for us because that's why Loki's good at this so you said you mentioned um that you mentioned that there are kind of two different ways to to splice it slice it up you can look at it by column and you can use it you can look at it by row and and Loki is more of a row oriented database can you talk a little bit more about what that means and how that's different from a columnar one yeah so um there are are there are other cool database designs I think the most common you come across in Time series data so we're we're talking about time series data here which is my favorite data which is you know essentially this Perpetual stream of data that is always indexed with time so I use the word index again there but it is the almost every time series database will have indexes that use time as a as a because you just you know it's the thing that's sort of always is increasing as your data comes in but anyway um colum or versus row so so Loki is a row oriented database and that it receives log lines in rows and stores them you know in rows in files and then puts those files on disk um what a columnar oriented database will do is you you need to define a schema so you have to Define your columns um and then when you store your data you you fill out a column with the specific ific data so you know if your log line had you know 10 key value pairs those 10 Keys become the columns and then you fill the values um and where this really matters is is on how the data is queried and access so if you say with Loki if you wanted to say column A you know I want to get the sum of all of the values from column a um we would have to read the entire row for every one of those log lines which means reading a whole bunch of data wasn't part of the ask of the query um and you know we we essentially scan all those add up all the values return a result where a column are oriented database because the data is stored the column data is stored continuously sounded a close um you know as a continuous block um so you can go grab the block of an entire column and say I want all of the data from column A and it's stored basically physically together so you know Loki stores the rows physically together and we have to read the whole rows you can pull back that column um and so now you can return just a subset of the data that's relevant to your query um and that can you know seriously simplify the amount of moving of data that you have to do in searching of data you have to do um you know it becomes a bit interesting when your query touches all the columns because then it's like is that better than a row Orient database eh might be a wash um but also column or your databases often do things like compute statistics on an individual row so like that's another form of an index where they might index things like the max Min average you know some basic statistics on uh anyway some basic statistics and they uh then you can just go query that header and maybe you if you were just looking for the max value over a Time range you can get it while by querying a couple bytes right like so um they have some real advantages for you know storing well structured data and and if you have access patterns that only look for p of it so is Prometheus a columnar database or is it also row [Music] based can I answer this question it's okay if you don't know I was just wondering more of a colum than a row datab I'm trying to think of what their actual storage format looks like um the reason that I'm asking is that um you can really send a log can be any anything it can be you know a number it can be a string it can be objects or arrays or uh lots of different things so um but I think in the example that you mentioned where you're adding up something in a single column so that's that's numbers uh that's metrics and in that case even though Loki could do it it sounds like you're saying it's not really that's not what it's optimized for and maybe that's why something like Prometheus would be better for that it's not that Loki can't do it but maybe if it's columnar then that kind of makes sense why they would be why Prometheus would be a lot faster no you're you're you're spot on here um I think I would get yell at if I called prome store but it it it's similar in the idea that it essentially does two things it stores metric data for one metric in proximity right so that it gives you the ability to query just the data from one metric just like you would pull a column back um but also metrics do another thing which is really nice like you could you know based on what we talked about before you could consider I could consider metrics as an index of logs so so they're a like and we you know Loki has this thing called recording rules where you can run continuous queries that generate a metric and then write them into um you know Prometheus or mirror so that you have um metric data because again it's it's about using a smaller set of data to answer the question you're looking for so what metrics also do which is really nice is they normalize data over time so so if you imagine you have you know you you're you have a log entry for every order your company processes and you get increasing and increasing volumes that's more and more and more log entries um but if you wanted to just track the the number of orders over time maybe by a few Dimensions like success and fail if you turn those into a metric they get sampled at as as counters at normal intervals and so now as that volume sort of es and flows the amount of metric data you're storing is consistent it doesn't change so it's it's in one sample every 15 seconds or minute or whatever and that it has a value that corresponds to that counter so that makes querying metric databases way faster and easier over much longer periods of time because they're their data is consistent over time it's also much smaller and like we mentioned it's stored in a way that makes it easier to access so it's in my opinion logs is the the only true observability signal so metrics are a really nice optimization on log dat and then you know the next step is you just instrument your apps directly with metrics right so you don't have to do this log line to conversion thing you know um but metrics are a really nice way in fact you know this is how sort of Loki was always meant to be used was you know using metrics tools to narrow down time frames and systems that were affected by whatever you searching for that you can then translate into your query to Loki um in grafana we've kind of made this as easy as we can just you know click a couple dropdowns and you can start searching your logs but you've now used you know essentially the metrics database as a giant index for narrowing down the time frame and some of the systems you're after so that Loki doesn't have to search such a big data set so I've got a really um beginner question because we've used like the index of Time series quite a lot and this was like my biggest mistake with Loki and like I feel like I should know this but so I come from like previously life influx data land which was a metric database um and we had this notion of like the time stamp was God this is our primary key and if you inserted a row into influx that had the exact same Tim stamp and the exact same tag set or label set if we want to call it in lowkey then we would instantly overwrite the fields in that row now is that the same in Loki or is it a pend only store and you would depend under that there be another log for that time stamp in that label yeah so I don't know we made a decision here that might have been the wrong one it certainly I don't know so anyway the way Loki actually works here is it's complicated in a couple directions but the simple one is if you send a two log lines um with the same timestamps down to the nanc precise exact same and they have the exact same content um we just store one copy the other is thrown away it's just it's automatically D duplicated on ingest you send two log lines that have the exact same time Stack Down to the nanc and they have different content uh we store both so you can have numerous log lines at the same timestamp um and this in practice happens a lot more than you'd think because a lot of systems don't have very precise clocks um or they don't generate precise timestamps in their log output so if you only have a second precise log line or time stamp you might get you know hundreds or thousands of lines at the same one second Tim stamp um so we didn't want to like throw those logs away it just becomes hard now because they have the same Tim stamp so you you what order did they come in at right like and when you go to query them it's it's it makes our life at query time um way harder um and there's one other level of of of where you can if if you send older data to Loki it will inest it under some circumstances um or there's just kind of some rules you have to follow but you can send older data and if that had the same Tim stamp as the newer data um in the same content they would all get stored um but at query time any exact duplicates are removed and that's because Loki uses a replication Factor system um to run as high availability so we actually have exact duplicates in storage as part of replicas in some cases and so the query engine always just removes exact exact duplicates see I I mean I kind of agree with the logic because I feel this is kind of like a Leisure thing right where it's like you would want to keep all of your information rather than you know drop or overwrite in potential critical information so I mean that I think that was my learning curve here because I was like I feel like I you should use um labels and streams and anger because I'm going to lose my data otherwise but and once I discovered no it's like a pend only I'm like actually well there I can sparely use my labels a lot more often and then just deal with it at query time when I'm basically passing whether it's like with e m or Jason and then because I've noticed that and probably a good segue a little bit querying if we have time is kind of like how does this sort of piping operation work and it kind of like you feed that string line right into that piping and then it almost creates new labels for you in grafana land which you can utilize in in the UI there yep yeah we again I don't know if for better or worse I struggle to sort of understand sometimes if decisions that we made were good bad or there is no good there's only but yeah we the way the parsers and Loki work is you know if you had a Json log line and you do pipe Json um all of the keys and values become parsed labels um but we don't really disambiguate that in the UI and we kind of go back and forth about whether we should or not because they functionally in terms of how the query works at that point it don't matter the only time that it matters is whether or not you can put them inside of the curly braces and the cery language and then we're questioning whether or not do we even need curly braces and like do we even need a query language like you know a lot of feedback that we get is folks don't like learning query languages I can empathize um I think our query language is interesting you know it was essentially you know heavily inspired by Prometheus because that was how we got our entry into the world right we we wrote on prometheus's coattail oh that's not the there's a Bjorn ravenstein gives a really good lightning talk on the pluralization of Prometheus um so we followed the I wrote on the coattails and so as a result we were looking to gain folks that were familiar with using Prometheus to use our tools so the query language the labels all that and then you know fast forward to today and you know I think a significant portion of of our customers have never used Prometheus or Loki users right and so now that query language is sort of maybe more confusing right because like I have to learn Prometheus and Loki like yeah kind of I guess the benefit is you learn Prometheus too so if you look at it on the upside but but really the the you know the goal here isn't about learning query languages it's about getting your log so we're always trying to figure out how to so we have Works in place now to try to make you know more user interfaces that are kind of point and click and but the Cory language is really great like if you there's sort of nothing you can't do it does get a bit weird because it embeds like the entire go templating language in it too and I don't know like we that kind of got that for free because we write in COD but in retrospect it's kind of awful you know like the queries you end up writing can be sort of hard to see so it's like you know on the one hand we didn't have to solve that problem because we just used one that existed but I don't know I this stuff is kind of hard and you know the growth of the project has been um we're just trying to keep keep ahead of it's been great you know and it's just it's it's been super fun it's just you know in hindsight you know what would we do different and like yeah I don't know maybe the Cory language but but then again probably not because that allowed us to get where we were at the time so it's really maybe about what do we want to do now and how do we so if you all are learning Loki I I am interested in your feedback on what you found easy or hard or like what we could do you know to to change things um you know coming from you're coming from an influx background I've never spent much time with influx but I know they've got a few Corey languages and have finally I think settled on SQL which um we get a lot of asks for and you know maybe this is my fault because I you know I have no interest in writing another squel query ever again opinion but yeah we could do an entire like conversation about this in itself but for me personally I actually feel and I'm probably really going against the curve here is I don't mind learning a language that's dedicated to the the database that's been built so for like lql here makes perfect sense for Loki and the way it operates if you try and skin SQL across the top of it um it's going to be a very interesting you you almost have to go okay how does Loki work underneath to then be able to write the most efficient or you know usable query on top um so it was yeah I I you know people can scream for seful or they like I think sometimes you just need it to solve your problem um rather than like just because you like SQL um so I know there might be a very for that one I think that when it comes to time series data and trying to query and visualize time series data SQL is a terrible language for this I'll I could defend that statement if anybody wants to fight me um however and this is why like promql and log ql like the way the functions work you know it's it's better at this it's simpler and easier to write a query on time series data however you know anything that you're doing that's trying to do something that's more analytical like count distinct or you know counting and grouping or you know sort of looking at maybe joining d right like SQL is a much better language for that right so that's I think why folks like it's it's both because I mean it is kind of how it was built and what it was built for right so it's like that's I think usually the Divide where like I will say yes like if you're trying to do more of those types of queries it's hard to do that in promql and lql um vice versa if you're trying to query time series data and represent it as such it's much easier to do that on promql and log ql okay so maybe we could talk about some some best practices for querying uh data from Loki as well so you already mentioned that when when we're writing data to to Loki in the first place we should try to use fewer labels like the fewer the better pretty much and the labels that we do use should be ideally very longlived it probably you know things don't often last forever but as long as possible so something that's not unique like a like a trace ID you said um and also something that people would actually query for so not like house numbers more like post codes yes you you've got it in fact um maybe we can throw a link I wrote a a I've wrote A Few blog posts over the years on labels but recently a couple months ago I Revisited um and I even spent I mean I don't know how many hours trying to build some really poor looking animations that I think are actually quite helpful it's just embarrassing how long it took me to make that um so yeah you post will be in the description below yeah the on the ingestion side yes ex exactly what you said um so on the query side and then writing good queries um you know the the first thing it would normally tell folks is the um the number one tool like there's no sort of magic here like the label selectors you use directly determine the amount of data you're going to query um I've used analogies like a table of contents in a book you know like we call it an index it is an index um but like the back of the book we talked about as being like a reverse index where it's very specific and it tells you exactly where to look Loki is more like a table of contents where like you know you your streams might Define a chapter this is maybe not the best analogy because I don't know I like the the house analogy now I gota got to figure out how to adapt that one the address analogy but um you know I know what town I want to look in I know what but the more specific you can be off the start the less searching Loki has to do the faster your result is um now you might not be able to narrow it down right so then if you can narrow your time frame down right so if you have to search through all of the addresses in a town um or in a state um if you can narrow the time window down you'll search through less data um however you know maybe maybe not so then the next thing that becomes really important is filtering so filter expressions in Loki are these pipe equals or exclamation equals which means must contain or does not contain um use those and use those right after your label selectors we do optimize queries a little bit when they're out of order but it's just kind of good practice to always use filter expressions and this is the next fastest thing that Loki can evaluate right so when we read a log line we very quickly can check to see if it matches the filter Expressions um and then stop processing it immediately if it doesn't and that means you then save the processing effort of something like a parser if it doesn't match your query at all um and then from there it's like the more complex you make your query probably the slower it takes like you know there are lowkey queries that are run that will process like 30 billion log lines you know and if you think about like how absurd that is you know like the it's really hard to I was trying to to do an analogy to all of the pages in Wikipedia one time right like like if you imagine basically that query probably searches every line of Wikipedia right like 30 billion lines is a massive number um Loki does it fast because that will execute on thousands of computers like we will parallelize queries like that across 2,000 machines all of them processing but at the end of the day it's going to come down to however many CPU instructions it takes to execute your query on every log line and even though they're in parallel you're still going to get this sort of long you know long tail problem whichever subquery takes the longest kind of dominates your query time um so the the simpler you keep query the better um but yeah use labels narrow down your time range um you know continue to narrow your query as well too right like if you have to search very broadly if you find a result immediately narrow your time frame to just that time frame right or add a few labels to to narrow the result to make subsequent queries faster so on on that note then what what takes priority does it uh is it narrow down the time first and then it checks labels or is it does it check like the labels first and then checks the time range kind of yeah it's a good question so um time time range first so so the way Loki actually stores data is the index file that we build gets stored with the log lines in Object Store storage and so when you run a query um we'll go fetch the index file for you do one per day in most cases so if your query spaned seven days we'd have to go fetch the index file for each of those days um and then within that we would then look at the label matchers to figure out which of these chunks we have to go read so so narrowing by time just means there's less index files you have to look at so it's it's a bit of a in practice it's probably sort of moot you know like it I mean you know it it's more like the amount of actual data you have to search is really going to dominate how long like if you're going to search 10 gigabytes or 10 terabytes like you know whether that 10 terabytes was over 24 hours or um seven days is probably not you know as important as if you could turn it into 10 gigabytes over seven days with a label selector or 10 gigabytes over five minutes or it's usually probably more about the volume so in the address example so let's say we're looking for a specific bit of mail or or mail that was sent to Jay um within a certain time is it better to like jump straight to the post code or should we go like let's let's filter out by country first and then you know by state and then go down that way or is it just better to jump to the more specific one um if you have the you know the way that it actually looks up in Loki will just be the most specific label matcher that effectively determines what or I mean technically the combination of them I guess could be more specific I don't well is that true the way I would answer your question is just use the most specific label that you have you know if you already know the postal code um and that's going to make the sort of data you have to search the smallest then yeah go with that you you you wen't going to really gain anything by adding more label selectors in there um in theory you could actually maybe make it worse but I I don't know that I want to introduce that kind of complexity to folks just usually the easiest thing is to just you know use like like it's really common in our queries because we build out dashboards a lot of times with template variables to have drop downs for things like clusters and name spaces and um but they might also have a a you know a very specific you know label selector it's like you only need that one but folks always just kind of fill out all of them or you're you're not going to hurt anything by adding more labels generally but you really only need to pick whatever your most specific is okay so so in that example so first you would suggest we narrow down the time range so we'd say I'm only interested in the mail that Jay has received in this last week and then second you would say like I'm not actually gonna say you're where you live but like you know um only search for this post code which I know that Jay lives in and then after that you said then do the filter expressions like maybe Jay has a brother that also lives on the same street so then maybe let's filter out the you know Jay's brother's name or something and only then would you parse it yes yep you're you're essentially the the pipelines of a query are executed for the most part sequentially so you know whatever you put in the front gets done first so the filter Expressions being up front will just allow you to completely you know bypass in fact Loki can sort of short circuit that extremely quickly like if if we as soon as we read the line out of the file if it doesn't match the filter Expressions that's it it's just ignored forever um it doesn't take place in any other part of the processing so it saves memory and CPU and you know in and and I will say there's like an example of this again we try to optimize a little is it's just hard to write query optimizations that don't cause you problems sometimes too you know but for example if you were say you there was a specific key or a value in a Json pair you were looking for um and that value was you know cat or something like that um now you know in your Json object cat might appear multiple times right and you only want to know if it's like you know uh parent equals cat right there's a bad analogy I'm going with it but there might be like sibling equals cat or you know uh cousin equals cat or something like that but you only want to match parent equals cat so ultimately you're going to do a Json parse and then you're going to do a filter that says you know parent equals cat um but you could up front just do a filter expression that says pipe equals cat and that makes sure that the log line at least has to have the word CAD in it before anything else is done um so that can be a nice optimization to just make sure and like you know wait wait oh my oh my days so that me you don't even have to pass it at all you can literally just do like a a search on that yeah so if that line doesn't have CAD in it at all then we don't ever have to run the Json parser on it or do any other comparison so that's um and like I said Loki will do some of this for you it kind of sees those cases I just tend to be explicit about it honestly a lot of times I think the interesting thing about any database is um there's some amount of data where it doesn't matter what you do right like if you use a relational database with less than a million rows in it there's almost no query that you can write that won't execute in less than a second like it it just doesn't matter right um if you get more than a billion rows um You probably have to have an index to query that table right you can't just row scan it you know you probably might need to query on multiple indices um you might need to use hints or something right like like at some scale it becomes hard to query um so it's similar here with Loki right like most of the time for most folks like just kind of write your query um I would say it's not until you're trying to query terabytes of data in a single query that you really want to care about optimizing to that level you know I mean certainly you know hundreds of gigs too and again I'm talking about the Loki the way we run it which has lots of compute tied to it if you're running your own Loki your mileage may vary on what that amount is where you have resources to make it quick so always the general good practices are there but I don't want folks to walk away feeling like they have have to you know like I run I got a bunch of raspberry pies back there with with Loki on it and like you know I can query over several days worth of data because it's you know it's maybe 10 gigs a day of data I don't quy the whole thing like um so it it is a bit of you know and that's maybe the interesting part about Loki is because it is a horizontally scalable system you can make those numbers quite big right like you can you know we regularly run queries that are over terabytes of data um can process in you know 20 30 seconds maybe 45 seconds it depends but query is less than 100 gigs or you know subc or second long like on my Raspberry Pi that wouldn't be true right it's gonna be a bit grumpy if I tell it's a query a terabyte of data um but yes the so the tips like generally you know filter first is just kind of a good practice to get into no matter what um you know that optimization of filtering before Json parsing and then filtering again after Json parsing you know certainly use that if you're quing huge data sets but you don't necessarily have to all the time that's a that's a new one for me I did not know that was possible just to do on Purely on the log line before you did any passing so you learn something new every day yeah and I'll also throw another one in there um always prefer you know the filter expression like pipe equals and exclamation equals so contains or doesn't contain um to the regular expression equivalent um Regular expressions are just hard like I was joking with um Dimo from the pyroscope team who who's one of the creators of pyroscope about how much of the world's CPU is spent on executing regular Expressions it it has to be like 50% or something like that so um you know I mean they're kind of a necessary evil but like if you don't need one like for example if you wanted to search for um you know it'd be better to write an expression we have this or syntax you can use now where it's like you could put two viations of the same search term than it is to use a a a regular expression because that's faster like regular expressions are generally slow um never Ed the case in sensitive variant we made this very hard to use in Loki Not By Design but it is the way that it is and um maybe it was a good side effect but like you put in Pen's question mark I inside a regular expression matcher and it does the case and sensitive match and it is it's just so slow it's just so much extra CPU work so it's faster to search for two variants of the same case with like pipe Expressions than it is to do a Rex matcher that's case insensitive um so my general advice is you know avoid regular expression should be Last Resort I mean we use them all the time right like I'm not saying don't use them like constantly you know I wrote really one of the worst regular Expressions I've ever had to build the other day like because I needed to solve a problem with it right it's just it's gonna limit maybe how much data that I can search with it or how long it takes if it gets really complicated and then I might be on the other side because you know we're running these clusters and it's like oh that's a lot of CPU wonder what's going on over there right okay those are some some really good tips for querying logs um we are out of time but thank you for for coming on and and giving us some context about lowkey really appreciate you coming on awesome thanks for having me that was super fun we have a bunch more content to to look forward to on Loki Jay and I have already done a Zero to Hero Loki series which we are still adding to I will link to that plus all the documentation for Loki you know the forums for Loki and all of that in the description below I guess thank you everybody for watching thank you both for joining me yeah thank you Nicole it's awesome yeah thank you

