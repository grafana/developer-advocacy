# Tail sampling vs. head sampling in distributed tracing

Published on 2025-12-18T16:53:31Z

## Description

In this video, Grafana Labs' Robin Gustafsson (CEO for K6 + VP, Product) and Sean Porter (Distinguished Engineer) discuss the ...

URL: https://www.youtube.com/watch?v=8JDgAEM5nHs

## Summary

The video discusses the importance of effective sampling in tracing for data storage, featuring insights from industry experts. It highlights the challenge of managing the vast number of traces generated by numerous requests, emphasizing the need for valuable data storage. The conversation contrasts "heads sampling," a random sampling method, with "tail sampling," a more sophisticated approach used by Grafana Cloud. Tail sampling waits until a complete trace is available and evaluates it against specific criteria (like errors or status codes) to determine its value before storage. This method enhances the sampling process by focusing on quality over quantity.

## Chapters

00:00:00 Introduction to the topic of sampling in tracing  
00:01:15 Explanation of the volume of requests and traces  
00:02:30 Discussion on heads sampling and its limitations  
00:03:45 Introduction to tail sampling and its advantages  
00:04:30 Explanation of how Graphana Cloud uses tail sampling  
00:05:15 Criteria for evaluating traces in tail sampling  
00:06:00 Importance of attributes in sampling decisions  
00:06:45 Transition from random sampling to intelligent sampling  
00:07:30 Final thoughts on the value of adaptive traces  
00:08:00 Conclusion and Q&A session

It's about making sure that the thing that ultimately gets stored is actually valuable, right? This can mean sometimes more and sometimes less.

Sampling is inherently part of tracing because every request technically could produce a trace. If you're handling hundreds of thousands or millions of requests a day, that equates to hundreds of millions of traces. 

One approach that most organizations take is referred to as **head sampling**. This is essentially a sophisticated method of random sampling done in the instrumentation. Up front, you decide to capture just a fraction of a percent of all your traces, primarily due to sheer volume. So, you sample randomly and hope for the best. This strategy, however, doesn't sound very appealing. You're relying on sheer volume alone to surface unique and interesting traces, but you're still left sifting through all that data.

What adaptive traces does at Grafana Cloud is referred to as **tail sampling**. In this approach, we wait and collect all the spans or the individual components that make up a trace. We wait until we have what looks like a complete trace, and then we apply policies to evaluate the traces. 

We assess whether the trace contains errors, if it is particularly long, or if it has certain attributes related to a specific team or namespace. There are many different ways to evaluate these attributes, such as checking the status code. Based on these evaluations, we can then make a more informed sampling decision.

This method gives you much more power. It shifts the process from simply sampling randomly and hoping for valuable traces to **intelligently sampling** what actually has value.

## Raw YouTube Transcript

It's about making sure that the thing that ultimately gets stored is actually uh valuable, right? Which can mean sometimes more, sometimes less. >> I mean, sampling is inherently part of of tracing just because every request technically could produce a trace. Um, so if you're doing hundreds of thousands or millions of requests a day, that equates to hundreds of millions of >> traces. Um, so one approach most organizations take is referred to as heads sampling. um which is basically a sophisticated method of random sampling done in the instrumentation. So up front you're like I just want a fraction of a percent of all my traces um just due to straight volume. So you're kind of just like sample randomly and hope is the strategy. Uh [laughter] which is which just doesn't sound very appealing. You're just hoping through sheer volume alone like the unique and interesting ones are going to surface and be stored. But you're left still sifting through all that. So what adaptive traces does at Graphana Cloud is referred to as tail sampling, which is we wait and we collect all the spans or the the individual components that make up a trace. We wait until we have what looks like a complete trace and then we have policies that can evaluate the traces to see like does this contain errors? Is this particularly long? Does this have these attributes for this team or this uh name space or so many different ways to look at does it have this status code? You can look at all these attributes and then you can make a sampling decision. Then um so you have a lot more power. So it turns you kind of shift from sample randomly and hope to like intelligently sampling what actually has value to

