# Cómo Grafana Cloud te ayuda a ver lo que realmente experimentan tus usuarios

Published on 2025-12-12T23:52:12Z

## Description

Descubre cómo viven realmente tus usuarios tus aplicaciones gracias a la monitorización sintética, las pruebas de rendimiento y ...

URL: https://www.youtube.com/watch?v=uYMD3RiLDWw

## Summary

En este video, Yaima, una ingeniera de software de Grafana, presenta cómo mejorar la experiencia del usuario en aplicaciones web utilizando herramientas de monitoreo. Con más de tres años en Grafana y experiencia en el equipo de Cloud Provider, Yaima aborda la importancia de la confiabilidad del sistema más allá de los dashboards y logs, centrándose en cómo los usuarios perciben la velocidad y la interacción con las aplicaciones. Discute métricas clave como el tiempo de carga, interactividad y estabilidad visual, y presenta herramientas de Grafana, como Synthetic Monitoring, Front Observability y Grafana Cloud K6, para evaluar y mejorar el rendimiento. A través de ejemplos prácticos, como la prueba de un sitio de demostración llamado Quick Pizza, muestra cómo detectar problemas y realizar ajustes para optimizar la experiencia del usuario. También menciona futuras actualizaciones en las herramientas de Grafana para una mejor gestión de secretos y métricas de flujos de usuario.

## Chapters

00:00:00 Introductions and background of the speaker  
00:02:30 Introduction to user experience reliability  
00:04:15 Human perception limits and user experience thresholds  
00:06:00 Introduction to web vitals metrics  
00:09:30 Challenges in understanding user experience  
00:12:00 Introduction to the Swiss cheese model for risk evaluation  
00:15:00 Overview of Grafana tools for measuring user experience  
00:18:00 Demonstration of synthetic monitoring with K6  
00:25:00 Real-time monitoring and alerts during the demonstration  
00:34:00 Summary of the tools and features discussed and future developments

# Presentación de Yaima sobre la fiabilidad de Grafana

Hola a todos, soy Yaima y soy Staff Software Engineer en Grafana. Llevo aproximadamente tres años y medio trabajando aquí. Actualmente, formo parte del equipo de Cloud Provider, enfocado en el monitoreo de aplicaciones en la nube de AWS y GCP, aunque ocasionalmente colaboro con el equipo de frontend. 

Hoy vine desde Alicante para hablarles sobre la fiabilidad de afuera hacia adentro. Es decir, cómo Grafana puede ayudarte a mejorar la experiencia del usuario. Veremos cómo medir el rendimiento desde el navegador, las APIs, y cómo analizar todas las métricas y trazas que recolectamos para entender mejor la experiencia del usuario y cómo mejorarla.

## Comenzando con un Problema Común

Empecemos con un problema que muchos de ustedes probablemente han experimentado. Imaginemos que estás en una llamada, revisando los dashboards y todo parece estar bien. Las alertas no indican problemas y los logs no muestran errores. Así que, te relajas y te pones a ver las noticias. De repente, empiezas a recibir mensajes de que algunos usuarios están experimentando lentitud en la aplicación. Para otros, todo va bien. 

Entonces, te preguntas: "¿Qué ha pasado si, según los dashboards, logs y alertas, todo está en orden?" La realidad es que la confiabilidad no solo se refiere a si el sistema está caído o funcionando, sino a cómo se sienten los usuarios al usar nuestra aplicación. Lo bueno es que esto lo podemos medir.

## Límites de Percepción Humana

Los humanos tienen límites en cómo perciben la interactividad. Según Jakob Nielsen, hay tres umbrales de tiempo que determinan nuestras capacidades perceptivas:

1. **Primer umbral**: 100 milisegundos. Es el tiempo en el que el usuario siente que el sistema está reaccionando.
2. **Segundo umbral**: El tiempo justo antes de que sentimos que estamos pidiendo al sistema que haga algo.
3. **Tercer umbral**: 10 segundos. Este es el tiempo máximo que intentamos mantener la atención del usuario. Si supera este tiempo, el usuario puede perder interés y abandonar el sitio.

## Métricas que Importan

Ahora, ¿cómo podemos medir esa experiencia de usuario? Muchos de ustedes han oído hablar de **Web Vitals**, unas métricas definidas por Google. Hoy nos concentraremos en tres métricas principales:

1. **Loading**: Tiempo que toma la estructura principal de la página en cargarse.
2. **Interactividad**: Cuánto tiempo tarda en ser realmente interactiva.
3. **Estabilidad visual**: La estabilidad de una página mientras carga, evitando movimientos inesperados que puedan frustrar al usuario.

Aunque estas métricas son útiles, no ofrecen una visión completa de lo que realmente está sucediendo en el sistema.

## Desafíos del Monitoreo

Para entender todo lo que ocurre en tu sistema, es necesario estar atento a muchos factores, como el uso de CPU, saturación de memoria, el funcionamiento del GC, y el estado de la base de datos. Además, hay que considerar las distintas conexiones a Internet, tipos de navegador, lenguajes de programación, y dispositivos desde los cuales se accede al sitio. Sin embargo, esto no es de interés para el usuario final, ya que solo quieren cumplir con su tarea y luego irse a casa.

Queremos saber qué está sucediendo desde fuera hacia dentro, para poder validar problemas antes de que se conviertan en caos y así evitar perder clientes.

## Modelo del Queso Suizo

Hablemos del **modelo del queso suizo**, utilizado para la evaluación de riesgos. Cada "lascas" de queso representa una verificación. Aunque ninguna verificación es perfecta, al combinarlas obtenemos una validación que nos ayuda a prevenir problemas.

Un ejemplo en la vida real sería la conducción. Existen herramientas como cinturones de seguridad, espejos, límites de velocidad, etc., que nos ayudan a mantener una buena conducción. De la misma manera, necesitamos herramientas que nos permitan validar la experiencia del usuario desde distintas perspectivas.

## Uso del Stack de Grafana

Para medir la experiencia del usuario, presentaré tres productos de Grafana:

1. **Synthetic Monitoring**: Permite detectar regresiones en los flujos de usuario en las aplicaciones web. Realiza chequeos periódicos que alertan si hay problemas.
2. **Front Observability**: Herramienta de Real User Monitoring para obtener una visión más amplia de lo que está sucediendo y validar problemas.
3. **Grafana Cloud K6**: Permite crear scripts de carga para reproducir problemas y asegurarnos de que todo funciona correctamente antes de hacer un lanzamiento.

### Ejemplo Práctico: Quick Pizza

Voy a usar un sitio de demostración llamado Quick Pizza. En este sitio, el flujo de usuario típico es hacer login, pedir una pizza, y dar feedback.

Para monitorizar esto, crearé un script en K6 que se ejecutará en Synthetic Monitoring, registrando los eventos y los tiempos de respuesta.

### Demostración

Voy a utilizar **Casis Studio**, una herramienta de código abierto, para grabar el flujo de usuario en Quick Pizza. Comenzamos grabando varios eventos mientras interactuamos con el sitio. Después de grabar, podemos filtrar solo las llamadas a la API y crear un test en Synthetic Monitoring.

Configuraré el test para que se ejecute en varias ciudades y estableceré alertas para que me notifiquen si el rendimiento no es el esperado. Esto nos permitirá detectar problemas regionales.

## Análisis de Resultados

Al ejecutar el test, observamos métricas clave como el tiempo de respuesta y la tasa de éxito. Si un porcentaje significativo de los chequeos falla, podemos concluir que hay un problema que debe ser resuelto. 

En el caso de Londres, por ejemplo, si las métricas son rojas, sabemos que hay un problema que debemos abordar. A partir de ahí, podemos hacer un test de carga en K6 para verificar el rendimiento bajo diferentes escenarios.

## Conclusiones y Próximos Pasos

Para resumir, hemos visto cómo monitorizar la experiencia del usuario utilizando tres herramientas de Grafana. Identificamos un problema, lo analizamos con Synthetic Monitoring y Front Observability, y finalmente, ejecutamos un script de carga en K6 para validar la solución.

En cuanto a las novedades, estamos trabajando en una aplicación de gestión de secretos para manejar tokens y datos delicados, así como mejoras en Synthetic Monitoring y Front Observability para facilitar el análisis de flujos de usuario.

Muchas gracias por su atención. ¡Espero que esta información les haya sido útil!

## Raw YouTube Transcript

Pues nada, eh, me presento, mi nombre es Yaima, soy staff software engineer en Grafana, llevo unos 3 años y medio. Eh, actualmente estoy trabajando en el equipo de Cloud Provider para el monitoreo de aplicaciones en la nube de IWS, GCP Nature, pero a veces colaboro con el equipo de frontend. Y bueno, vengo desde Alicante hasta aquí hablarles un poco sobre e fiabilidad de afuera hacia adentro, es decir, cómo Grafana te puede ayudar a tener una mejor experiencia de usuario, cómo ir desde lo que está fuera en el browser, el performance, las APIs, a lo que está sucediendo dentro, es decir, de todas las métricas, trazas y los que enviamos, cómo podemos analizar con todos estos datos de la mejor manera posible eh cómo está haciendo la experiencia de usuario y cómo podemos mejorarla. Eh, ups, aquí. Entonces, e empecemos con un problema que seguro muchos de ustedes habrán vivido ya. Imagínense que están en un call, están pendientes de los dashboard, todo está bien aparentemente. Miráis las alertas, ninguna está activa, así que bien. Miramos los logs, eh, no hay ningún error, así que puedo estar tranquilo, me pongo a ver mis noticias, a ver la televisión y de momento siento un pin. Mm, ¿qué pasa? otro pin, otro mensaje, otro mensaje, Dios, está sucediendo. Y me empiezan a llegar mensajes de que para algunos usuarios las la aplicación está yendo lenta, para otros va bien, pero para algunos no. Entonces decimos, "Pero, ¿qué ha sucedido si según los dashboard, los logs y las alertas está todo bien? Eh, ¿cómo es posible?" Pues la confiabilidad se basa justamente en eso, no es no se no se trata solo de que el sistema esté caído, de que el sistema esté funcionando, sino de cómo se sienten los usuarios cuando están usando nuestra aplicación. Lo bueno es que esto lo podemos medir. Entonces, eh los humanos tienen límites de cómo perciben la interactividad. Por tanto, la confiabilidad eh la podemos y por tanto, bueno, la confiabilidad, es decir, vamos a representar este esta situación en un gráfico en una línea de 10 segundos. Según eh Jacob Nilsen, él estudió que la hay tres límites de tiempo principales que están determinados por las capacidades perceptivas humanas. Entonces, debemos de tener en cuenta estas valores a la hora de eh optimizar el rendimiento de nuestra aplicación. Entonces, eh esta límite se basa en tres umbrales. El primer umbral se que es de 100 milisegundos, es más o menos el tiempo en que el usuario siente que el sistema está reaccionando. Luego tenemos un segundo umbral, que es el tiempo en que justo el tiempo antes que sentimos que le estamos diciendo al sistema que haga algo. Y luego viene un tercer umbral que es un poco más largo, que es el que intentamos mantener el foco en la tensión del usuario. Y este tiempo es de 10 segundos. 10 segundos. Parece corto, pero es 8 7 6 5 4 3 2 1. Como podéis, 10 segundos es bastante. Pues en ese tiempo intentamos que el usuario mantener su atención. Si se demora más de este tiempo, ya el usuario empieza como, "Ah, esto está tardando mucho, me voy a poner a hacer otra cosa, pierde el foco de lo que está haciendo y en algunos casos incluso puede abandonar el sitio y no queremos eso." Entonces, ¿cómo podemos medir esa experiencia de usuario? No sé si alguno de ustedes ha oído hablar de estas métricas de las webs. No sé cuántos de usted ha oído hablar de web vitals. Okay, pues existen varias, son métricas que ha definido Google. Eh, y hoy nos vamos a concentrar en tres métricas principales. La primera, eh, se basa en el loading, que es más o menos el tiempo en que demora la estructura principal de la página en cargarse. La segunda se basa en la interactividad, es decir, cuánto tiempo tarda mi página en ser realmente eh que puedo interactuar con ella, que puedo hacer clic. Y la tercera se basa en la estabilidad visual. No sé si os ha pasado que abriste un sitio y está ahí con el loading, con el spinner y vas a pinchar en un botón y se va y viene una publicidad, pues hasta que el sitio no es finalmente estable, no podemos, no sé, dar clic en en un botón. Pues esa es la tercera mítica que tenemos aquí. Entonces, la buena noticia es que todo esto eh se puede medir. La mala es que no es no nos brinda todo el panorama completo de lo que realmente está sucediendo. Entonces eh vamos eh imagínense que quieres saber todo lo que pasa en tu sistema. Tienes que estar pendiente que si te quedas sin CPU, que si la memoria está saturada, que si el GBCH Collector está haciendo su trabajo. U, la base de datos se quedó sin memoria. Tienes que estar pendiente de muchas cosas que pueden ser la causa de que tu sistema no esté funcionando bien, pero encima de eso tenemos que fuera existen distintas conexiones a internet. Además de eso hay varios tipos de browser en los que tu aplicación tiene que funcionar. A eso le sumamos que hay varios lenguajes de programación de CCS que se usan para renderizar el contenido de tu página y encima hay un montón de dispositivos que se pueden usar para abrir el sitio. Vale, tengo que tener en cuenta un millón de cosas. El problema es que a los usuarios finales no le interesa nada de esto. Yo vengo a tu sitio a hacer un trabajo. Yo quiero hacer mi trabajo y luego irme a mi casa tranquilo a a ver mis noticias. Pues entonces, eh, ¿qué es lo que podemos hacer? Lo que nos interesa es saber qué es lo que está sucediendo de lo que hay afuera hacia lo que hay adentro. Queremos saber qué está sucediendo antes de que sucedan. Así podemos validar el problema y antes de que sea muy caótico y terminamos perdiendo un cliente, poder validarlos, darle solución y luego seguir nuestro día la mayor tranquilidad posible. Entonces, vamos a hablar un poco sobre el modelo suizo. Esto es un el modelo del queso suizo. Esto es un modelo que se usa para evaluación de riesgo, es decir, se usa para gestionar el riesgo en los sistemas. El concepto general es que cadaca la podemos usar como una verificación. Si yo en cada lasca digo, voy a verificar esto, en esta verifico esto. Al juntarlas todas, cada una de ellas no es perfecta, pero si la unimos obtenemos un valor y entonces podemos validar nuestro sistema antes de que se vuelva muy caótico. Vamos a ver un caso similar en la vida real. Estamos conduciendo. Para poder verificar que estamos conduciendo correctamente, tenemos, por ejemplo, que existe el cinturón de seguridad, así no si sucede algo por lo menos nos retiene. Pueden haber espejo, están los espejos, están los límites de velocidad, las carreteras tienen límites de velocidad, están los sidebag la luz intermitente, es decir, tenemos muchas herramientas que nos ayudan a mantener una buena conducción y en caso de algún problema, por lo menos hemos hecho lo mejor posible. Pues en la vida real funciona un poco así. Tenemos varias herramientas que nos permitirán validar la experiencia de nuestros usuarios de en puntos de vista diferentes, con perspectivas diferentes. Y con esto podemos eh definir qué es lo que está sucediendo. Entonces, cómo podemos usar el stack de grafana para medir la experiencia de usuario. En el día de hoy vengo a presentarles tres productos distintos. Uno es synthetic monitoring. Con synthetics lo que quiero hacer es poder detectar regresiones en los flujos de los usuarios. en los sitios web, las aplicaciones. ¿Y qué puedo hacer? pueda hacer chequeos que cada cierto tiempo se ejecuten y me digan, "Todo está yendo bien, tus métricas están bien." Oye, hay un problema que salte una alarma y me diga cuándo, por ejemplo, eh detecte que mi sitio va más lento de lo que debería o que un flujo específico está fallando. Luego vamos a usar Front Observability, que es nuestra herramienta RAM de Real User Monitoring, para ver un panorama más amplio de lo que está sucediendo y así poder validar el problema. Y por último vamos a usar eh Grafana Cloud K6 para tener un script de carga, poder reproducir el problema. desarrollador implementa una solución, pero antes de hacer una release, yo quiero estar segura que todo va bien. Pues vuelvo a correr mi scrit de K6. Cuando todo esté green, pues adelante. Vamos a hacer el la release. Entonces vamos a ver un poco. En el día de hoy voy a usar un sitio que usamos mucho para hacer demo que se llama Quick Pizza. Es un sitio muy sencillito. Yo le digo, "Oye, quiero una pizza. ¿Qué me recomiendas?" Pues mira, te recomiendo esta pizza. Me gusta, no me gusta. ¿Qué pasa? que cuando le vas a dar me gusta te dice, eh, tienes que autenticarte primero. Entonces, un flujo de usuario normal, ¿cuál sería? Yo llego a mi Quick Pizza, hago login, le pido una pizza, uh, me gusta, o no, no me gusta. Bueno, es raro que a alguien no le gusta la pizza, pero bueno. Y entonces, ¿qué vamos a hacer? Este es el sitio que está yendo lento, ¿no? Pues lo primero que yo voy a hacer va a ser un script de K6 en el que yo puedo ejecutar en synthetic monitoring y que me diga exactamente qué es lo que está sucediendo con mi sitio. Entonces, en este script vamos a hacer una cosa. Yo voy a hacer varios clic en el sitio, lo voy a grabar, voy a guardar ese screen, lo voy a importar en synthetic y vamos a ver qué es lo que está sucediendo. Entonces vamos a pasar a la a la computadora que voy a empezar la demos a ver si sale. Perfecto. Entonces existe esta herramienta que se llama Casis Studio, que es una herramienta opensor eh aplicación de desktop que tiene esta herramienta que me gusta mucho, que es para hacer récord. Entonces tú le dices cuál es tu sitio. Espero que esté activo el sitio. Sí. No, demo es demo. Vamos a ver. Este es un clásico. A ver si me deja. Okay. Ah, ah, ah. Okay. El sitio está aquí y le puedo decir, "Dame una pizza. Aquí normalmente, ¿qué hacemos?" Venimos aquí al T de performance, vemos los números, sí, está todo en verde. Genial. Entonces, vamos a este script y le voy a decir, grábame varios eventos en el en el en este sitio de C Pizza. Esto me va a abrir un browser y va a empezar a hacer recording de lo que yo haga. Entonces le digo, "Mira, lo primero que voy a hacer es login. Vamos a hacer a ver esta password super secreta que es increíble, nadie la puede adivinar. nos autenticamos. Luego, aquí están más o menos los ratings que se han dado antes. Ahora voy a decirle, dame una pizza. Me gusta. Tenemos este widget de aquí que me permite decir, después de hacer un me gusta quiero que me muestres la palabra rating. Tienes que validar que eso existe. Cuando hayamos terminado, bueno, aquí podes ver todos los eventos que he hecho. Ahora vamos a parar la grabación. Venimos aquí y vemos que todos los eventos que he hecho en el browsers aparecen aquí. Es muy interesante porque yo puedo decir, "Muéstrame solo la SAPI porque aquí me va a mostrar asset, cosas del sitio y yo quiero ver solo las API." Esto Casis tiene una cosa interesante que si yo ahora hago una nueva versión de mi API, yo no tengo que ir al script que yo generé días anteriores y decirle, "Vale, ahora la API es API V2." No, automáticamente que hicis lo detecta y no tienes que venir aquí y cambiar nada. Entonces, yo tengo mi SAPI, veo aquí todos los eventos que salieron en el browser y digo, "Vale, voy a crearme un test del browser, le voy a llamar eh Quick Pizza, lo voy a exportar y como ven, esto me genera todos los eventos que hice. Yo me puedo copiar esto, llevarlo a Syntetics y hacer la prueba. Entonces, vamos a venir aquí, que lo tengo aquí igual, y lo que voy a hacer es agregarle cuándo empezó, cuándo va a empezar el script, a qué hora termina y voy a guardar el tiempo de duración. En una máquina normal, en un test de integración, esto no debería tardar los clics que hice antes más de 8 segundos. Clic, clic, clic. Eso no debería dar durar más que eso. Entonces, ¿qué voy a hacer? Me voy a copiar este test. Me voy a ir a synthetic. Creo un nuevo check aquí. Me lo pego, le voy a llamar, no sé, esto, este es mi sitio. Y antes que nada, déjame ver. Aquí hay otras opciones que puedes agregar. Labels, ¿en qué ciudades quieres probarlo? Mira, lo quiero probar en Naples, en No sé por qué New York está caído, pero a ver, estas son pruebas privadas, pero también eh ahí está. Eh, lo puedo probar en Londres, por ejemplo. Vamos a ver si hay suerte. Ah, mira, tenemos el Tab alerting. Le digo, yo quiero que de todas las pruebas que vas a hacer, si una de cada tres pruebas falla en 5 minutos, me mandes una alarma y me digas, "Hay algo que no está bien." Vamos a hacer una prueba a ver si funciona y ver si esto va bien. Y si veo que este script va bien, simplemente lo guardo y ya me notificará si algo sucede. Vamos a ver esto, cuánto se demora. Aquí dice que ya está bien, pero bueno, dice que duró menos de 8 segundos. Check pasado en Londres. También pasó el check. está en New York ahí todavía trabajando. A ver si responde. ¿Y qué hago? Cuando ya esto listo, lo salvo y claro, no vamos a esperar aquí a que se ejecute de aquí a 5 minutos. ¿Qué he hecho? Me he hecho ese mismo test, lo he hecho antes de venir aquí a la conferencia y aquí podemos ver una cosa interesante. Hasta este momento las tres ciudades estaban yendo bien y de momento empieza a sonar la alarma esta. ¡Uf! ¿Qué ha pasado? Veo que en Napol está bien, en New York también, pero en Londres ha fallado. Entonces, aquí tenemos dos métricas interesantes. El out time que me dice, si uno de de todos los servicios, de todas las pruebas que he hecho, solo una va bien, yo considero que tu sistema está funcionando. Pero tengo el rehability que me dice qué por cento de todas las pruebas que hice ha fallado, ha ido bien. Disculpa. Entonces, por aquí ya yo tengo una idea que si del 100% tengo un 75 que va bien, quiere decir que hay un problema regional. Entonces tenemos por acá también podemos quitarlo el alarma aquí este esta visual de de puntos que me da una idea mejor de lo que ha estado sucediendo. Si le doy, por ejemplo, ahí en Londres, aquí ya veo un error que me dice, "Ha durado más de 8 segundos." Vale, entonces voy a hacer aquí un filtro. Voy a decir, mira, yo quiero filtrar por Londres. Aquí tengo más información. Vemos que los números se han deradado un poco las métricas y el check eh ha tenido un 83% de éxito. Entonces, si ahora filtro por Napoles o New York, me doy cuenta que ya todo que todo en esta ciudad está en verde. Las métricas están en verde, todos los cheques han pasado y ya por ahí ya yo tengo una idea de que tengo un problema regional. El sistema no se ha caído, sigue funcionando, pero igual no está yendo muy bien. ¿Qué vamos a hacer? Vamos a volver a la presentación y ahora lo que vamos a hacer es irnos a una segunda aplicación. Eh, vamos a ir a Front Observability, que es la segunda herramienta que les quiero mostrar. Y yo quiero ver en ese mismo time range qué es lo que está pasando. Front Observability es una herramienta que nos permite hacer monitoreo de los user monitoring, real user monitoring del sitio web. Entonces, aquí tenemos varias métricas, las mismas que vimos antes. Según esto, dice que está en verde. Veo algún que otro error aquí, pero digo, "Vale, vamos a filtrar entonces por ciudades, como antes vimos que en Londres nos iba muy bien. Voy a buscar, por ejemplo, voy a poner Italia, eh, voy a poner otras ciudades y vamos a ver los números. Los números están en verde por aquí. A ver si se carga. ¿Qué me dice? Aquí no hay ningún problema. Los tiempos están bien, incluso si venimos acá abajo, el tiempo de carga de la página, 600, 800 milisegundos, así que todo bien. Vale, pues vamos a hacer otra prueba. Vamos a ir directamente a Londres, que es el conflictivo a ver qué me está haciendo. Y cuando vengo aquí, ya ves que estas métricas de Google están en rojo y tú dices, "Ah, vale, aquí está el problema. Ya estás viendo cuánto tiempo se ha demorado en cargar. 4 segundos. Casi ningún usuario va a estar esperando 5 segundos a que te lo página cargue. Es como gracias, me voy. Entonces tenemos otro tab aquí en en Front and Observability que te permite ver todas las llamadas HTTP que ha hecho tu sitio. Vas a ir, por ejemplo, aquí en HTTP Request, puedes ver todas las peticiones que se han hecho y vemos que la mayoría de las APIs tardan entre 100 y 200 milisegundos, pero esta en particular tarda 4.79 79 segundos, como que es un poco alto para la media. Entonces, ya a partir de ahí tenemos una idea de cuán lento está yendo nuestro sistema, pero solo para esa ciudad. ¿Vale? ¿Qué puedo hacer? Mira, vamos a hacer una cosa. Vamos a ir una tercera herramienta. Ya yo sé más o menos de qué va el tema. Ya vemos que una API está lenta y que la página principal del sitio cuando se carga está lenta. Pues yo me voy a montar un test de carga en K6. Me voy a ir a la herramienta de Grafana Cloud K6 y voy a crearme un script que lo tengo por acá. Yo voy a probar tres escenarios distintos. Mi primer escenario, ¿cuál va a ser? El mismo que les hablé antes. Chequéame que estos clic duren menos de 8 segundos. Ese va a ser mi escenario por defecto. Luego vamos a tener otro escenario que es para probar que mi sitio web por qué está ya en lo nento. Entonces en ese escenario yo voy a medir cuando voy a la página principal del sitio este, yo voy a ver por qué está yendo tan lento. Y el tercer escenario es de la API de Quick Pizza. Yo quiero ir a esta API y quiero ver exactamente por qué esa API está yendo tan lenta. Entonces, vale, me hago mis tres test de carga, le pongo que no sé, de tal hora a tal hora que aparentemente hay, no sé, 1000 usuarios trabajando. En este horario hay 100 y me hago mi script. Y lo más importante, tenemos que definir qué es el éxito para este test. Yo voy a definir unos tral aquí y le voy a decir el 90 tiene el el número de errores debe ser menor del 2% de todas las llamadas HTTP que yo haga. Además de eso, el 95% de todas las peticiones que yo haga tienen que ser tienen que ser en un tiempo menor de 2 segundos. Y de los checks que le dije que dure menos de 8 segundos, yo quiero que el 95% 99% de esa de esos chequeos sean satisfactorios. Si eso se cumple, quiere decir que el problema ha desaparecido. Entonces yo me puedo llamar a este script de aquí de que existo llamar incluso desde la línea de comando. Yo lo lanzo. A ver, lo lanzo desde aquí. Le digo a qué proyecto de casis yo quiero que esto se ejecute. No, vamos a esperar 3 horas a que termine. Y esta es la ejecución. Cuando vengo aquí normalmente yo tengo que agregar el test. Se ejecuta. Ya yo le pasé el proyecto que yo quiero que se ejecute. Y aquí ya podemos ver información ya más interesante sobre de eh ese problema. Podemos ver que hay 18,000 de las peticiones que hicimos HTTP han fallado. Aquí tenemos un poco el tiempo que se han demorado esas peticiones en ejecutarse. Y por acá ya vemos que un 77% está la métrica está como en naranja. Aquí vemos todos los cheques han fallado porque yo me esperaba que e que el 99% de los chequeos pasara pero solo pasó un 71%. Entonces, ya estos números me van dando una idea del problema y ya sé que cuando yo solucione el problema y vuelvo a ejecutar el script, todos estos números tienen que cambiar. Aquí, como podéis ver en el browser tenemos estos números de aquí de estas métricas y aquí hay una funcionalidad muy interesante que a mí me encanta, que durante el script yo le dije que tomara screenshot de cada uno de los pasos que iba dando y entonces cuando vengo aquí al browser timeline yo puedo dar clic en las imágenes que se fueron generando a medida que yo iba haciendo un evento. Entonces yo puedo ver cuando sucede esto exactamente qué hizo. Vengo aquí y veo lo que está pasando en el momento. Aparte de eso, bueno, aquí tengo los chequeos, aquí tengo las llamadas http que se han hecho, cuáles han fallado. Eh, hay un poco de todo. Es un poco jugar con esto, ver qué está haciendo y qué hago ahora. Vale, ya más o menos tengo una idea de lo que ha sucedido. Voy a hablar con el desarrollador, le doy este script de carga, le digo, "Mira, si quieres automatizar el tema, créate una JAD action. Eh, pon este tag ejecutar. Cuando abras la PR la ejecutas. Si ves que va todo bien, pues ya puedes mejarla y probarlo en producción. Vamos a suponer que ya el desarrollador resolvió el problema y va a ejecutar el nuevo script en K6. Él viene aquí y dice, "Vale, a ver, aquí vamos a ver cómo cómo va. Ahora yo vengo a mi a los script, lo vuelvo a ejecutar de nuevo, vengo a analizar los números y ya como veis el reliability ya está en verde. Vengo acá abajo, todos los checks que han pasado en los tres short, que fue lo que yo definí aquí te dice que el margen de pruebas debía ser de un 95% y han pasado el 99%, así que se ve que el problema sí que se ha resuelto. Lo mismo para los otros checks. Y luego cuando viene saltar de browser ya podemos ver que efectivamente esos números han cambiado. Solo por estar seguro. Vamos a ver en los check. Vengo aquí al synthetic monitoring. Aquí podemos ver que ya para Londres también ya la está funcionando. Venimos a front observability, ponemos el mismo time range de cuando sucedió el problema y cuándo se resolvió. Y efectivamente, si ahora vamos a Londres directamente, podemos ver que los números están en verde. Todas las métricas van bien, el performance ha bajado incluso con con una alta carga de trabajo y ya cuando venimos aquí hemos vemos que ya el tiempo de carga ha ido disminuyendo, quiere decir que el problema se ha solucionado. Así que nada, vamos a volver a la presentación. Em, y vamos a resumir un poco qué hemos hecho. Tuvo un problema, me salió una alarma. Cuando le di a la alarma me llevó al sitio de Syntetic, que por cierto me olvidé mostrarles la alarma cuando llegó. Eh, del sitio de Synthetics probamos el check de que en un flujo de usuario normal debería ejecutarse en menos de 6 8 segundos. Con esto fuimos a Kex. que sí nos demostró eh a al frontability. Ahí podemos ver un panorama más amplio de todo, validar el problema. Con el script de K6 pudimos ejecutar eh la el script de carga y una vez solucionado el problema, pues ya está. Voá, tenemos tres aplicaciones trabajando juntas para resolver el mismo problema. Entonces, em Okay, ¿qué sigue? Les he hablado de muchísimas cosas hoy. He ido un poco corriendo porque se me acaba el tiempo. Pues quería contarle un poco en qué estamos trabajando. Eh, esto es una de las peticiones que más han pedido los clientes este año, que es la aplicación de gestión de secretos, que nos permite eh token, certificados y datos e delicados e inyectarlos directamente en métricas y logs y poder usarlos en synthetic monitoring para hacer nuestro chequeo. Esto está ahora mismo en public preview, así que lo podéis ir usando y a ver qué tal va. Luego en en Syntetics tenemos en K6 Syntetic, ya no recuerdo ya. Creo que en Syntetics tenemos este nuevo gráfico de puntos que nos permite ver de una manera más fácil el problema con el gráfico con el explorador de puntos de tiempo, que es mucho más intuitivo. Y luego en Front Observability se está trabajando en esta nuevo tab de user actions que te permite definir flujos de usuario y métricas eh telemetría dedicada a ese flujo de usuario específico. Así cuando quieres ver en tu negocio dónde es que está fallando el problema, vas en el flujo de usuario específico que te interesa y analizas las métricas en base a ese flujo de usuario. Esto no sé si estará ya disponible, pero si no lo está pronto debe estar disponible, así que lo podéis probar y nos podéis mandar feedback. Y bueno, nada, eso es un poco como lo que quería, lo que venía a comentarles hoy de cómo medir la experiencia de usuario con tres de las herramientas que hay en Grafana ahora mismo. Muchísimas gracias por todos. [aplausos]

