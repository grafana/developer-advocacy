# How to run Loki at scale on Kubernetes (Loki Community Call January 2025)

Happy New Year from the Loki Engineering team. To kick off 2025, Nicole and Jay will be joined by Poyzan Taneli from the Loki ...

Published on 2025-01-24T05:15:07Z

URL: https://www.youtube.com/watch?v=lAr0GwuZHiw

Transcript: and we are live welcome welcome to another Loki Community call I am here with my awesome host with the most you didn't even you didn't even introduce yourself well sorry my name is I know sorry I feel I should say it my name is Jay Clifford and I'm a developer Advocate um on the L team nice to meet you all um and this is my colleague Nicole Nicole would you like to introduce yourself hi I'm ni ven I'm here to keep Jay in line because clearly he he doesn't buckling under the pressure and we've also got as always we've got Julie over there hi Julie hi I'm the technical writer that is working on updating the Loki docs and oop other dire oh yeah the nuring thing right today we're pretty lucky because we've got poison pelli yeah hi everyone um I'm an engineer on Lucky team and it's great to be here it's yeah everybody should know Jay by now by the way yeah yes I think so too so I guess we could start by saying Happy New Year to everyone because this is obviously our first Community call of the new year which is pretty wild to think and I've actually think we've started with a pretty cool one for the start of January or at least thinking about how the community goes like this is where most of the train of conversations go in the the community um so I think yeah just to sort of take everyone through the topic is how to run Loki at scale on kubernetes um but I thought before we get to that Nicole I think we had some cool news and stuff that we wanted to announce about graffan cons do you want let everyone know about that yes graan con is coming it is a community focused grafana organized conference it was one of the coolest conferences that I went to last year if you're at interested in any of the open- source projects that grafana that is in the grafana lab's umbrella you should really be going to this one there is a cfp that is out right now if you'd like to talk at graphicon this is one of the best conferences to go to even as a firsttime speaker because we specifically try to encourage people for that and there will be help and support if it's your first time so we are we are very keen to help first-time speakers to to get started at this conference and if you have some cool dashboards of grafana that you'd like to show off doesn't have to be professional it can be just like a cool personal use case there's also something called the golden grot awards and the best dashboards are going to win like a trip to to a graphicon and it's going to be pretty awesome so go to that link this is also in the description below and maybe we'll see you there there absolutely love it and there's actually been a few Loy community members that I've reached out to to put in cfps for so if you're on the call and you're listening make sure you get those cfps in uh I really want to see you there that' be awesome so the call um I think it's really cool that what we're talking about today because I think it sort of resonates with a lot of people as they're scaling Loki you know everyone starts off with monolithic and then you know they're starting to get to a like like a production side um and this is when they start running into to problems so I thought what we could do is sort of talk about setting the stage first about you know why are we're even talking about how to scale um Loki on kubernetes so poison um what sort of problems are we trying to address here what sort of guidance do we need for scaling Loki why can't we just put like you know everything on auto scale um first of all I think it's great that we have so many enthusiasts that use Loy at like such different scales um last two times I was an upso and I met with like so many people who do this so good with us so it's it's just so motivating to be able to work on this product and so great question what's the Dade of so if we don't really right siiz our clusters in in general like any software um we will either be running into on one end of the spectrum that things are just clanky they don't work and you can address why or if you go to your point like scale up everything give all the resources things are becoming expensive so our contexts here today how to unlock key at scale at at kubernetes and we can I think safely assume that most of us run kubernetes on the three big cloud providers AWS gcp and Azure and they all cost differently so it's not also so easy to say give me all the resources and take it from there so we going to try to cover today what's our best practices in the last five years uh with ly um look our professional I mean Cloud offering uh support we learn a lot about how to run lucky to the order of pites so we try to share that with you today um but yeah um I think that's that's um I wonder if you could tell everybody why is it that you talking about scaling why what have you what have you been doing for the lowy team for so I've been at loky um it's amazing for 20 months now it's going to be two years in April which uh did you count before the call I know um challenging two years of my career I'm I'm so happy to be here and um since last year with um a couple of colleagues of mine um we started focusing on how can we optimize um our footprint and how can we reduce our costs because yeah let's be super honest here um we are all are to make money here at some point and um we looked at our each of our components um I don't want to assume but obviously at our scale we run distributed modes of Loi for ourselves and then we noticed a couple of things that we actually have um a number of distinct patterns that we see in Lo at different sizes we we're going to touch on that again we run 40 clusters nearly at completely different sizes some of them are over uh petabytes of data a month in total volume ingested some of them are like fairly small and in this group we also have our own Dev and testing clusters and they they all show a different like set of needs and if you run them all the same either they are under scale or they cost us too much so we went through the exercise we um kind of uh looked into how we can automate um sizing of our clusters and like right sizing them um we also have um from from like some experience borrowed from our great colleagues at mimish the metric database so this is all coming from that collection of data that we now have a better understanding how ly behaves at what scale and how much it costs us and how we can optimize it um for our purposes we also really love that at grafana we have a culture of dog fooding so we use our own stuff so we use Loki in production and so when poison is telling us what the total cost of ownership is and and you know what what how we we should size our clusters it's because that matters to us you know it has a very direct impact on us before they get to C customers we we deal with our own stuff first exactly and we are um basically are all tooling our own absorbability system and we even take it a step further now we look at our metric logs and traces Footprints like how much we cost to ourselves by experimenting trying to improve and developing this software for our purposes yeah and I and I guess when we talk about the call today we're obviously going to take it for Grant granted like when we talk about all these best practices for scaling that we're expecting everyone to follow the rules of Loki and Loki's best practices in some ways so good data governance such as you know I guess well like poison do you want to take us through a few of those sort of like key aspects that they should just keep in mind before we sort of jump on to some of the the scaling best practices yeah of course um I think as we were prepping for this and like kind of getting the what should we talk about one I realized there's probably like one takeaway from this entire topic uh when things don't work let's go back to basics and what's like good practice basics for Loi so Loi is designed for um um quick like highly available ingestion uh that is super light with like minimal indexing and then brute forcing querying on on those logs so and it's also designed for people who understand their own infrastructure so our like labels the key value pairs that we index around are generally around the organic structure of a cluster we we look clusters name spaces and it's quite native to kubernetes to we do like yeah cluster name spaces General examples I'm giving of obviously these do differ for everyone um and your jobs and components right so um for example what best practices we strictly try to restrict our label cardinality to 15 in our clusters because we know you don't gain much from further if anything you lose in the both in the right path and the query pth performance if you have high guality if you don't really um Define those labels properly or um I would admit that L Key's configuration particularly on the read pth is fairly complex and like offers a lot of knobs um but uh we have a number of like resources how you can like use this and yeah like correct charting um like defaults that we use for um querying creating yeah indexes Etc I think those are like one-on ones to start with and it's a bit counterintuitive to try to force L key to do something that is not designed for ly will never be able to excel at give me everything you have and look for this particular string in the logs because it's natural W forces we call them like needle and the hstack queries right we have optimizations around that but still this is not what ly is designed for at this point um and I think users should always keep that in mind so you've been talking about the design tradeoffs in in Loki and how lowkey is very light on the injust side but it kind of offloads that complexity and difficulty on the read side you know so it's a lot more expensive to query it at scale so that's something that makes this topic a little complex and the other another reason why it could be complex is you know people might say like okay maybe we need to scale Loki up a little bit but maybe not to to the same scale as we do at grafana you know because we we run Loki um for our for our customers and maybe some other people don't have to put through two pedabytes a months so it's also like where on the Spectrum are they and that's difficult to quantify um that's true but I think where we can start as always like um how much logs do you have how much do you generate and even if you don't know this on top like just deploy and see see you're a dver of life and a month R of logs and then the next question is like what do you want from those logs is it for you like um reliability observability type of thing where you look at your logs only when you have a problem or is this something sort of a Rec like recordkeeping for you that you have like transactions and stuff that you regularly analyze over them those are like different patterns one but I would say at least in my imagination and the way I use it logs are rarely um and please keep in mind that I'm I'm a very operational heavy distributed engineer person doing this on a dayto day so for me they are generally like incident uh response type of things so I I don't get to do like uh give me 30 days of Vlogs and see for me it's about the um skill to narrow down in them so get me a set of labels and then let me find the signal in this noise is the question um does that answer your question yes that sounds good um also I wanted to ask like what are for for people who are not sure if maybe their their Loki um deployment is good enough or you know if they should scale up what are some kind of warning signs to look out for what starts to happen if Loki is too small okay um I think what I forgot to mention which uh can be like different um deployment modes we have and our current offering says we have the monolitic SSD and the distributed um I think this has been announced before and I'm sorry to say if this breaks anyone's heart but we are considering we made the decision to um decate the SSD mode um mainly because SSD mode is the simple what was it like simple scalable um deployment mode uh development mode um that combines a number of Lucky components in a subset of them so reads rights and back end um we didn't see many people gaining too much for it so monolitic imagine our free Cloud offering right we say you can get free 50 gigabytes of logs a month 50 gigabytes is something that most computers can fit right now right easily so that's a that's a good purpose for for monolitic and I think if you need a couple of order of that yeah you can put a number of them together but the the moment that you go um get close to the order of terabytes um you will need to separate the concerns between your components and you will you would want the ability to be able to scale them up and down independently that's why we do strongly suggest like if you get to the order of terabytes um I think distributed mod give you will give you the flexibility and if it sounds too complex and unnecessary I would always recommend disable everything you don't need just like um get the basic components if you don't have any rule usage disable them uh try to reduce the software to as simple as possible um I think there's a question yeah so go yeah you you hit the nail on the head there cuz I think we have a few SSD users um in the room and I think that it's quite interesting that a lot of them are raising here it's like well they don't want the overhead of using microservices versus you know SSD where a lot of these things are bundled together um and I think we'll kind of like talk about it a little bit right as we start talking about how we sort of like you know scaling the right and repath but I found it interesting what you said there poison I think like microservices gives you the option of disabling things that you don't need so that extra overhead which would always be there in some form if you had SSD because we're bundling those Services as components and perhaps maybe this is where you know a l a lot of people are scaling backend Services when they don't need to scale backend services and and I think we'll sort of cover those things a little bit later on um but I guess sort of like moving us we're talking about microservices a little bit if we talk about scaling then I assume no matter what we talk about microservices we're scaling kind of like the and right path um and that's kind of like our two focuses for today um and I think sort of talking about um talking about let's talk about sort of ingest first um when we go to customers and when we visit customers um and they talk about their their data and their logs and we touched on this a little bit earlier um what sort of the key aspects of ingest are we are we looking at um for our customers what what are we what are the key signs that we're looking for when we start to consider a scaling convers um we first ask them what is the expected volume of logs you have and it's hard to obviously 100% nail this but more or less people know and then the second question we go from there is like what is the um query load you're going to put and that's nearly impossible to guess and we're going to come to that next I believe because queries are hard to optimize but again um the first thing to think about querying is like how how many users going to use your system and what we mentioned before like what is the need that like what what do you want to get out of from your system is this is are these logs for compliance are they going to sit there until you have an emergency to go through these like and then in in that case your priority is like good retention right make sure that you set it up correctly so that all those log lines are there when you need them or like in management right you really need to make the best of the labels that you know immediately where you look at and I guess like monitoring uh over your clusters also becomes like even more important then and then yeah more of like recordkeeping or something that gets close to analytics it's it's not something ly does fantastic at scale and we we talk about that tradeoff um but it's still like possible to do that small scale we our query language is complex but also gives a lot of functionality I love that I can generate metrix of logs and stuff so um I guess those are the like that is the letter that we think of the first ingestion volume and then how many users use G it and then can we educate those users to make the best out of the platform in terms of getting the data back so so when when we're scaling in gers I guess our metrics there then is what the the amount of logs that they're potentially going to and do we how do we measure it is that like on a per day or like on a week or like how what do we expect the the customer to come with us so we can start assuming yeah we generally ask the day uh and then we go from there one caveat there though is um when we think about our um uh right path um not that we said at the beginning L key um makes a available immediately because uh ingestor hold the logs in memory for a certain amount of time two hours in general and that's a fantastic thing but also couples are read and write path so if you look at the data that is immediately available uh queriers we don't ship the data back to the queriers some of the query um retrieval Logic for example like regx matches happen in the ingestor themselves and this this is um yeah this is a tradeoff that like this coupling sometimes if you have heavy recent loads on read load on your clusters for example that's a that's actually an additional need on your right path um those are things that we also like consider um so for all components I think we're going to come to that about general rule of Tas like we look at the P90 memory and CPU utilization um and I'm also try to give the for the components who need persistent volumes that have like this supports something like ingestor and like I we we also try to be incredibly generous there because um discs are getting generally cheap and that's not something you want to be but neck that and like have an incident where oh you can't store your logs um yeah uh we also have a comment from Carla here who says I have a dashboard to check lowkey queries especially catching this needle and a Hy stack queries people scanning terabytes of chunks for no re no apparent reason yeah this is why you're saying that really matters if that's something that is part of the use case then you you might have different things to consider rather than if if you know there's just one person who who does the same query all the time like that's a very different U use case um we actually have a question that we were talking about with Julie just before call so uh Cod craft Indonesia says can promail export a logging from read log directory Julie Julie you're gonna make me say it now um we you said if it comes up it came up it just came up this this is not really the answer that you're looking for but we are going to be deprecating promp tail um in the next release and realistically promail the Loki directory for the promil client was forked almost two years ago and has been rolled into grafana alloy so our guidance has been go and you know upgrade to graa alloy and there there are information in the docks there's a lovely little um the the alloy team is really done a great job they've got a a conversion tool to make it easier for you so um basically going forward we're not going to be updating anything on promail and it's going to be slowly deprecated so I think your your best bet to answer this question is to go and see what alloy is capable of doing because all the new features are going to be going into grafana alloy which is our our opinionated distribution of um the oo collector but also has all the prom promail features on top of it so that's probably not the answer you were looking for but that's the answer but I believe the answer the answer is yes by logging from read log directory yes alloy is going to be able to to tail um log files as well you should probably switch anyway their they cooler features it's a whole pipeline now there's a there was a an office hours about that okay sorry so yes um poison we were talking about about inest and um we were this is one of the reasons why lots of people were using SSD which is which is still fine we haven't really underscored this point but we are now recommending distributed and one of the reasons is that when with SSD it's like read path write path but those comp there are many components in the read path and many components in the right path and they don't all have to scale the same way so two major components on on the inest on the read on the right path is investors and and Distributors but they're not one for one normally can you talk a bit about that yeah um um so what's the role of Distributors right like all logs come in and Distributors role is to um aggregate them in streams and then pass them on to inors so in general the rule of th what we follow and works most of the time is like you you should have half the number of Distributors um this is mostly based on experience um so if you have like yeah uh X 2x ingestor generally X ingestor should should do the trick um we can talk about in more detailed when we come to the like updated tooling and sizing recommendations um but yeah that's something um seems to work for us but for on the right pipeline two two vital components like you said are Distributors and ingestor um and on read pipeline um yeah I guess index gateways queriers um and depends on which later you look uh and compactor uh sits in between which regularly compacts uh our indexes is quite vital to the performance of ingestor because if we don't have available indexes compacted otherwise all queriers hit all ingestor to look for the data uh the log line that they need um one thing I want to acknowledge about SSD I I deeply understand the Simplicity need but um really the feedback and like the open issues and everything that we receive we don't when it works it works great but we we didn't see that people gain more about it like more from it and one inevitable thing is we support best what we do on a daily basis and we improve and learn more and more about distributed mode because we primarily use it and that's the motivation and I think Nicole your own points like the most supported thing does change for a product and a company like ours ours uh that's with this grow so yeah I do apologize but I I deeply sympathize with it and yeah I repeat like disable everything you can if it doesn't make sense um on the distributed mode MH so and I I think that's a really cool Point as well and I guess sort of like sort of tying up to the the ingested Point here I know we sort of chatted about previously before the cool when you look at monitoring your ingestor is there sort of are we looking for like you know steady CPU utilization in ingestor um in terms of how in terms of their capacity to know that they're healthy and happy how do we look at like ingestor as we we know we've we've got enough ingestor based upon our ingestion workload um this week I learned interesting stuff on right pipeline actually on that stable mod so on a on unusual not not too exciting workload what we see is like quite stable memory and CP utilization on Distributors they they do their job and I know you ask for ingestor but I have to come like my my mind in order um and then what what's a bottleneck for in Distributors if you see um unexpected CPU spikes uh in the Distributors that means they have to do a job like they and their only job is to validate the log lines that come in do they hit limits there's a detailed uh do about this like what is enfor what type of limit enforced by uh which component distributor or ingestor um but basically that's that's the first thing I look like what's wrong in the system that makes distributor do an extra job and it's the same for any component using when we come to ingestor what are the scenarios that we're looking um ingestor CPU utilization should be should be fairly low depending on like they're not quite insane unless somebody's a user is bashing them regex matches and like very heavy recent queries um and then um but memory is uh heavily dependent on the active streams so if you see an unexpected behavior on on memory do check active streams do check if there's an unexpected load like actual load coming in to your right pipeline whereas uh for CPU always question first your read Pipeline and somebody's doing that and there's I think there's a very good question on how to find um heavy queries it's not super straightforward but for success uccessful queries we have a um on dist on query front end we have a metric. goal line and we actually log the query and it's quite sorry let's just read out the com the question as well from Andre ziani any tips on where to find where a heavy query is coming from Prof find a dashboard alert or ruler and you're saying metrics. go we also talked about this with seral last time in the last community call he also shared like a query for it this is like Loki's metalog so you can query Loki exactly and yeah that I guess I guess that will be the summary of it yeah yeah here's like a I'll post in the chat the query that that we got from sirel for for an example one for how you can query metric. go we also have a comment from Maxim Maximo we upgraded from SSD to distributed but there is a deployment mode simple scalable distributed as in interim I have not found clear information how to move to purely distributed deployment mode I am I actually don't know because I haven't heard of this like interim mode um yeah I'll have to look for it to be able to answer do you guys know no we can definitely have a look to see the conversation there I feel maybe it's more around the migration guide of basically if you started off with SSD um you know what's what's the procedure of moving to microservices what's you know the the Hang-Ups there I guess sort of like we could probably make a whole another lowkey Community call about sort of you know transferring between SSD to um microservices so we could definitely I I wonder if if Maxim's using the distributed hel chart rather than the Loki Helm chart um usually when let us know which one you're using yeah because the the official Loki Helm chart should cover microservices SSD and um distributed depending on what deployment mode you you put in the chart and how you configure it yeah and I think it's just like even if they are using the official oners I guess for them it's like if they're already in production with Loki is making sure that they spin SSD down in the right way and bring up microservices in the right race so they don't have any sort of data loss or anything in the process so we can definitely look at um having a conversation on that um on another call because I think that'd be really really interesting cool um so sort of talking about um like moving across from sort of inest over to query we we look for as you sort of said poison we look for consistency and our ingest as most of time unless someone's doing a crazy query and and reaching into the ingestor um for that information um so why are ingestor so hard to scale what is it about ingestor um and kind of bundling that together sort of you know how how do what do we look to to cope cope with this how do how do we scale ingestor to deal with some of this trickery um in the ingestor side um the the thing is like um the more and more you sometimes it looks like in the right sizing is such a such a problem there's no like right answer to it you look on the day today and you feel like oh do I need to change these again um but the thing is uh it gets very expensive um when the when the balance between like when the load is not really predictable particularly on like CPU the the overhead you want to give and also it really depends if you have a dedicated kubernetes cluster to yourself that's different you can tolerate like your burst differently and you can be a Noisy Neighbor to yourself and then your resources will be all yours uh and you can perfectly like try to allocate the number of nodes you have and further like push your costs um cost optimization around that but for cases I'm imagining for any company when you have a kubernetes cluster multiple workloads share that which means multiple workloads do land on same node and then um it becomes tricky to make sure that you allocate the right resources right requests that you have with a reasonable limit on top but you don't abuse um your usage above your requests so yeah I mean um I guess like that that's the only thing I would think um I particularly complex with inors but I don't know if I'm missing something that you have in mind Sor sorry yeah moving over to to the queriers more on the querer side now so if we shift over to the ingestor we've kind of said like ingestor are quite consistent in the terms of how we use them but when it comes to querying that makes more sense because I was I'm like what am I missing yeah yeah but I think that's a great comparison because so queriers right uh quer is an interesting thing unless you touch it you don't need it so um qu we don't like we don't perfectly support autoscaling on hel now and um I think we can make a promise that we we will be investing more in that and like uh improving how a user to scaling internally uh and providing to our like H charts um but basically queriers by Nature are heavy on the CPU depending on what type of query that hit them um and they're quite spiky in their behavior and it's it's very hard to rightsize spiky workloads and the same thing from the other side I I said about like being noisy neighbors particularly when you share workloads is it's very very hard um so what worked best for us is to run as small queriers as possible and as many of them so that first um we can like there're stateless components anyways so we try to in a way try to treat them a bit fically like if we lose some of them we don't care so through moving in the further of like cost optimization what we did was um we said we can run them on spot notes uh what are spot noes they the different Cloud providers offer them under different instance types but basically they spin up faster so when you configure something like autoscale on top you can immediately go from 1X to 5x to 10x if needed and then uh but the thing is kubernetes doesn't guarantee as like it's not going to take them away it's not going to kill your workloads and like reschedule them so um it's more fit for stateless components um so that you don't lose it um but yeah uh we can sign now on most of our clusters on unless like we need an extra stability for any reason uh we run like nearly 80% spot nods and then we run quite aggressive uh autoscaling patterns and what the viota scale on on read Pat on queriers is um inflight requests combined with uh the Q size uh how many requests are queed to be processed and I um whereas for other components looking at the CPU and memory patterns are enough like how much resource they need and we add more replicas to the fleet for in comparison for querier because it's very hard to nail the CPU need and it's always like sparking a lot how much work we have is a better question uh for us to optimize around and so we have a question on on the same lines from AR ni I I hope I'm pronouncing your name right and they're saying you manage tons of clusters do you manually rightsize each of them no there was a there was a time we did we had to look like so we didn't know what to do and that was exactly what like motivated this project we we were like okay this is the cost but where do we spend the money and do we really use what we Quest and we looked at like cluster right um and that time we did okay this is let's try to tune them and one thing that we had like a debate within the team is like the the known um pets versus kettles arguments when running like large clusters you want you want your clusters to be more like kettles like they are they are the same because if they're pets it's hard to if they're all individual and different it's hard to um analyze performance across clusters when you make a change it's hard to reason about them and when something is wrong you really don't know because that cluster is unique to itself and in order to not get to that we we like I said about a year ago there was a time we looked at this manually but now we um run like scripts that does that for us and look at like long-term patterns and if we need um I would say we still heavily prefer horizontal scaling because that's what you benefit most from and if everything is okay uh like if there's no like major organic shift uh horizontal scaling should be enough to balance out resource utilization uh but periodically I would say like yeah we we we do need to look at um and now we automated yeah rolling that out now so so on on that note then talking about sort of horizontal scaling uh scaling you talked about sort of query as being sort of ephemeral in nature the way you sort of treat them so and that so is it from the query SK scheduler that you get these metrics from to know how many spot nodes that you need to basically scale out these queriers um so we so do so we don't as a practice uh vertically scale querias you should be basically um horizontally scaling these out yeah so we we regularly look at the two metrics that I say in Flight queries and and the um Q size and then uh our our scaling is like kab base so it's um I think one of them decides yeah K decides a suggested number of uh the replicas that needed um and then it's up to and then that makes that decision but it's up to the kubernetes scheduler to Pro provided for you so it's it's a fairly complex thing with horizontal pod autos and then yeah kada and then our own metrics we provide them um um and then yeah it's it's still you you that's why it's for example that's why I mentioned you need to really be careful about if you're sharing workloads in a cluster if you don't free reign in your own cluster because we always have to know uh be mindful of other workloads in the cluster and let our platform folks know we can't just say hey Max our replica to the 10x 20x what we need because then there's always a case k may suggest something but then key netes will go hey I don't have that many available nodes so it's more of a multiple nodes moving together multiple parts coming together type of thing dang that's really cool honestly I I guess that's one of those things you have to negotiate is like we always see it as like a oh yeah you just have Loki and that's the only thing that you're running in in production in this k8's cluster but realistically that's not the case you obviously have to weigh up everyone else in your k8s cluster um particularly if we talk about like at scale right like if you if you really need in in the order of like terabytes petabytes of logs and if you need to store them it it's very likely that you have other needs for metrics and other stuff and this is not happening in isolation um so then it becomes a bit of a like s domain expert uh devop expert uh talk where yeah uh you think of them holistically and then it becomes a bit of platform engineering I guess now the new tile on we have a few questions here and I definitely still want to get to your lowkey sizing guide so um just let's just go through this one uh it's a little bit long but I I will say it um this is a hard name to pronounce youa oh our user sometimes query logs for a very for very long time periods there might be multiple months 30 days at a time for services which produce logs at about a terabyte per day how can we autoscale the queriers so that the queries happen faster and the users don't complain about things being slow are there any other ways to increase query performance than scaling more aggressively o o I think I would like to I would like to say first that there's like one part of this is scaling the infrastructure but I think before that you should just look at fine-tuning the queries themselves the previous Community call that we did with sirel was all about this so even before you change any with um with the infrastructure that that's a good way that's a good place to start because he had some really cool ideas for how to change the queries themselves and then how to find those queries so we we spent a whole hour on just that yeah I think that's definitely on point because there is a type of usage that you will never be able to satisfy even if you give all the resources in the world so start analy in what's best and if it's really utilized best and if if those people really need like months worth of data for what they do um is is the way to go nice so I feel like this would be like a cool point to sort of unveil um your sizing guy Poison I know we haven't sort of talked around about backend services and stuff yet but I think we can sort of like Reveal Your guide and then we can basically sort of cover some of those components too um so hopefully we can sort of share your screen on the new and improved sizing page for Loki yes covers I've been poking to make sure it's [Music] published on multiple Bridges thank you all three of you you've been so patient with me and I will admit um I'm very sorry everyone I published like I merged this PR this morning um and it has been on my plate for for a long while but this is the fruit of the last year and um this whole exercise where we understand our clusters better um I I am sorry we took away the the tool that existed on this page before but honestly it was giving way of numbers that didn't fully make sense because it was just like multiplying by the assumed ingestion and retention and one thing you will realize is we we're not talking about retention here um retention will mostly affect your like storage mode like if you keep it an ideally you should in Object Store it it will affect your cost the bill that you pay and that's what you should look at um but it's not necessarily affecting much about L key and how it's run unless you abuse the amount of data by quing by like give me the year logs that's a different deal um but yeah when it comes to sizing um yeah uh retention is a is a completely different thing and I don't think it affects much how we size our clusters you will find everything we already talked about um here like couple of notes um yes run as small queriers as possible and then this is the way um a block post by ads like optimized query performance and then if you don't use ly rules you don't need those additional features don't even scale them up you don't need the parallel query pth um I want to talk about cash and I'm I'm now thinking we can even have a separate session on cash it's it's a whole Bas on its s um I will I will now come to the tool where it talks about like different components and their suggested signs for us I will be improving the stock in the next week or so um adding some in memory cache recommendations and like some probably like exor cache recommendations too um and yeah there's a there's a note about mcash and I know there's a note about um um me cash question I think in the nose if I correctly follow yeah um if you really don't need the scale beware you may not need the M cash but we're going to come there so what are these tabs um like I said me were quite with the right size and exercise we did um one thing we noticed is like we we actually have three distin patterns uh three distin cattles to to like so to speak um and we did ended up actually only dividing our clusters into two like generally midsize and small ones and really large ones uh so we still try to as much as possible stay away from the pet thing um but the these numbers are only based suggestions assuming your ingestion load and don't really think much about the query loads because query load is something the as the cluster maintainer is something you will kind of figure out on the way and these numbers are gathered from a quiet period assuming like base ingestion around three terabytes a day will be like less than 100 terabytes a month and you will see like the toal request and it gives you we used to give like total um only the total numbers and we realize sometimes it confuses people go like do I on one node that has like 12 cores and like has 36 gigabytes of memory no um this suggestion comes from which what we like to do uh in gors the way we run is Zone aware and we kind of find it nice to have two on each Z so we recommend six as small as possible um yeah replicas and that's the logic it goes and as you go I'm not going to go through each components but this is you will find compactor is always a singl ton and it it like in general with the load that gets bigger but you will notice query front and query scheduler particularly query scheduler you don't really need need more than two at all um and like I said queriers are a thing you need to figure out on your own a little bit when we go to a mid size we we see ingestion load getting bigger like we need more investors and more Distributors but um and so as the queriers but rest of the components are more or less the same I want to make a pose about index gateways here we assume um index gateways are are the ones that serves indexes that are already compacted and assuming you use Object Store uploaded to Object Store um and they only become a button La if you actually use a massive multi-tenant setup we are talking in the order of like thousands of um tenants because indexes are compacted per day per tenant and um then you be kind of run into this like a lot of files that needs to be downloaded type of problem um that's why these are I repeat yeah Carlo actually had a question on tency earlier any recommendations about tency I injust around six terabytes a day so I thought we could maybe use the doc for for his scenario um I injust around six terabytes per day 12 tenants yet S3 tells me to slow down when Loki writes into S3 I was hoping more tendency would help me I think if S3 is yelling I would try to increase the allowed um ingestion rate to S3 I don't think this is necessarily like something that looky side like can we slow down ingestion wouldn't think so I would look into S3 limits as S3 like does allow bandwidth but 6 terab a day is actually quite small um so I would first look at the provider configs for this one and I think it'd be interesting as well to go like Back to Basics there as well because we've had in the past where Community users have had this issue and it's come down to less about like the amount of data going through but the amount of requests going to like s3e so like the amount of like chunk like small chunks being like sent to s3e so it could be I don't know like the fact that you know you have too many labels um and they could all be writing small chunks and requesting those it in Loki at one time um and I guess like is is there any drawback poison from having lots and lots of tenants writing to the same s3e bucket could you hit rate limits from having lots of data writing through lots of tal or would you hit like a load like that you definitely would but I I I yeah I would again question if we are on like the simplest offering of s and that's like related to that one thing to your point what one might check is ingestor um provide a metrick on what's the reason of chunks being flushed and at one point and ideally they should be flushed close to full if they are two Idols um yeah uh and like ideally not forced which means the ingestor is shutting down and stuff like that so um that might be another check but with with this much data because it's small relatively um I I would check the S3 um offering and like rate limits first um because that's something we run into at the other end of the spectrum let me push to this uh yes S3 becomes a button L um yeah and then finally to to wrap this up so we can come back to questions is um as you can see when we get close to a petabyte a month um we do need significant resources and these are again from quiet times and if you do like two three petabytes a a month that doesn't necessarily mean these needs to be doubled it again comes back to Loy being correctly tuned like labels are correctly used um query is now correctly done and you may find this is relatively uh small numbers um this is a base suggestion for this type of volume but at Peak time when multiple users um in the order of tens and hundreds try to carry data back from ly these this can easily come near 2,000 we we do go up to at 2,000 and you and then there is a different very interesting like problems that come with the architecture now like all barriers touch can touch all the ingestor for recent data so we for example push the um amount of in like requests in the network for example that's like those are like different I think at quite extreme end and one other suggestion like quite humbly I would do is don't forget it's like from small end of the spectrum when moving towards large clusters there's a heavy maintenance cost that comes with it um and that's why like graphon and Cloud offering now like takes most of it away and when customers come to us and ask it's like what should be my right sizing we always this is something that we talked about at the very beginning we always ask them it's like what is your need and don't forget as it gets bigger for you that comes with a serious maintenance cost um you need to understand the software really well now the the trades off so that you can tune your components and configs correctly um yeah uh and finally sorry sorry just to wrap it up um you would also notice we moved away from the like kind of suggesting quite certain not types um any general purpose machine that works for your science they will come quite standard for four cores eight cores 16 cores uh would do um you should do your own risk analysis putting everything on a single note is a failure scenario um it's always an option but yeah having a reasonable spread um uh that's why like x large four core like yeah eight core nodes is is generally suggested but I didn't want to get too prescriptive inex because everyone's setup is different I know you're on time I yeah I mean if you're okay to stay just for a few minutes poison I mean that' be I mean like for me it's um it's actually really telling with the fact that you like in your tables that you show that like oh we scale up all of these horizontally but then as you said you look at like some of our back end components like the C the compactor um we like no matter how big you get um there's always one compactor we never have more than one replica by the looks of it in the table so is is is is the reason for that is just we should just be vertically scaling those versus rather than you know horizontally yes that's the only option because compactor is a Singleton by design it has to lock the tables it also does retention and deletes so it has to lock the tables that it touches the indexes uh that's why it's a Singleton by Design at the moment very cool and could so if someone did have more than one compact turve could that run run them into troubles along the line in terms of who's sharing what tables I don't I don't know if you have something that prevents increasing replicas I think we should but yeah it's like so they gonna try to touch the same data and one of them going to try to lock it and I'm I'm pretty sure we're gonna run like you're gonna either run into errors or very weird Behavior ideally errors that prevent this is good to know because there's a few Community issues I was responding to that might be worth a look on that so I might reach out to them and go how many compactors do you have running I want to answer question if that's okay about the not oh yeah yeah sure which one have the have the graan FKS TR running L key on arm at scale um yes as much as the because we require quite the scale um as the availability is provided by the cloud providers we do run on arm um yeah there is depending on the size of the cluster there is significant uh cost reduction I we can't say like massive performance Improvement like arm yeah as claim but yeah about 10 up to 10% cost reduction for sure we have a few more questions maybe we could just wrapid fire them for the next couple of minutes yeah I do want to call it this one Ecom Ventures posted this in our community post uh would love to go into depth okay we can go into depth around tuning of the chunks cash this seems to be the biggest bottleneck on the query side for us thoughts yeah uh I'm I'm a bit upset we didn't have time to get into cash so ly comes with okay that just means we have to have you back all about cash I need to I need to work on it a little bit but I would love to definitely uh but basically um in so in memory cache will not scale too much because it will become it it depends on the size of the available like memory you have on the on the number of replicas you um yeah you suggest I it's hard to answer this question because I don't know anything about the scale they talk about if it's something very small and that seems to be the bottleneck for them and I would still go back to basic and look at like is is everything else is performing well at all if it's like a download and a performance problem I would go into instance type and question if they're actually using a um like general purpose or like a memory there's no such thing as I think memory optimize but like a general purpose machine that suits their needs um if they close to the end of the large type of or like midsize large type of cluster we do use something called mcash egg store there's a fantastic blog post about it um by our lovely colleague um Danny uh who implemented that uh xtor basically allows mam cash to write data to the attached SSD discs um so we choose like storage optimize uh nvme um machines um and we just buy like 100 fold uh increase the capacity of our cach it does come with a lot of overhead because having discs are difficult maintaining them is a different beast on its on um but for yeah when we move to the order of pyes bandwidth limits um and like provider Object Store limits became a problem for us so that type of massive move to cach really helped a lot okay so there's a question from David who's asking I question multi tendency how can we monitor a tenant usage how many lugs are each or does each tenant push what about label cardinality can we monitor that as well um yeah so um tenant usage uh L key exposes metric per tenants how many bytes um they ingest U They al it also exposes how many bytes they query per tenant uh they can be used um label cardinality uh you may look at like active streams in general and like what they are but like to to do a label analysis log CLI is a great tool um there is a very complicated way to get that from metrics. go to but it wouldn't give like for long term it wouldn't give like accurate results and that query becomes incredibly complex I've only seen Ed do it ad um but yeah log CLI analyze labels is the way to go and I believe there's documentation on that where yes recently updated documentation and a new tutorial that Jay wrote fantastic nice we also have a few other folks talking about some S3 issues and yeah so S3 slow down messages as well here didn't find a workr yet Carla suggested increasing chunk size to reduce amount of rates to S3 yeah like Jay suggested for sure check yeah why you flush um and that yeah and do check S3 limits and Carla also said I'd like to size the chunks cash I don't need X disk men cash though six terabytes a day um yeah I'll as I said next week I'll work on that and try provide like yeah we're just giving you homework here that's what we're in general like look at in general the kind of intuition is look at how much you have to analyze your query pattern how much data your users query um at Peak time a day and try to kind of go to a percentage of it but it needs to be like reasonable it's because if you want to if you want to hold 50% if they query terabytes of data and if you want to half of it it's very counterintuitive to try to have terabytes of data in memory um so yeah but try get your go ahead so I'm see I'm just adding this to the list I'm seeing two Community calls raised now so we've got Loki Community call mcash Loki Community call monitoring Loki in real time and we'll just do entire call about like a prod cluster and we'll see what you guys do on a day-to-day basis during being one call I think that sounds great so on your back for both of them yeah that one last question from youa who posted this on our community post as well oh yeah we are running Loki in an eks cluster in three availability zones one of the main cost factors is the cross a traffic caused by the multizone architecture what are the best ways to minimize that cost there is no best way to minimize that cost um Cloud providers ask for for an arm and a leg to for that and we don't use cross a um like we we don't use cross region availability zones uh either because it's just very costly but instead ly's defense like L's kind of you know um try trade off for that is like we have um multis Zone investors we replicate data three times and yeah exactly we we are in single a because it's like freaking expensive uh and and it's working absolutely fine um like as long as you have like it's a very rare scenario that you use um replicas on all zones and basically unless you have that you don't lose data um so it's absolutely fine for us and and is suggesting to GPC compression yeah sure but I I I think it has an upper limit it's like it really depends again how much how much they to use um yeah I think it also it's just a tradeoff like maybe maybe where the cost if it's really if it's that important or or where the Lesser availability I know like on the nearest side for every Prometheus we have a grafana we have like a meta Prometheus there's they always come in ha pairs that are in different zones and it's like it's pretty paranoid monitoring but we don't do that for every single component you know just for things that are super important um okay I think that was I think we got through questions and we're already out of time poison thank you so much for for joining us here um and I'm sorry but you kind of volunteered yourself for a couple more Community calls and it's all live so we put it down yeah and it's all like it's super fun to be with you guys so yeah I'm always up for it and people are already liking the the docs update that you made so the sizing thank you yeah um and then also I want to say again that you know if you are interested in anything open source please consider submitting a cfp to refin Aon because you're the kind of person that we really want to hear from so check out this link is also in the description below and also I also have we have a a colleague am who's going to be running a different Community call and it's the gra campfire this one's about grafana and can C catch that live tomorrow you can enable notifications so that you you join it when it's live as well thank you everyone for for joining us here the the three of you and also those in the chat who ask the questions and if you get to your question or if you have any more or maybe if you just have some suggestions for future Community calls because we like giving poison homework apparently then put it in the descript in the comments of the YouTube video down below and we will do our best to to do that we'll just have to find the right people to answer your specific questions thanks everyone I guess have a good weekend thank you thanks nail the outro Ni Call

