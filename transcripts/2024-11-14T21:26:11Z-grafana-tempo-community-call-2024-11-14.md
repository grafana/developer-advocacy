# Grafana Tempo Community Call 2024-11-14

Join our next Tempo community call: ...

Published on 2024-11-14T21:26:11Z

URL: https://www.youtube.com/watch?v=f10Vl6UZwiY

Transcript: cool all right uh welcome to the Temple Community call what month is it November 2024 yes all correct I think Edition um we are going to review I have a decent little schedule today um we'll go through this I would encourage if anybody wants to present anything I don't really want to chatter the whole time so anybody wants to take any of this please do otherwise I'll just kind of work my way through um and we'll end with the AMA which we kind of do always but I think we're a little short on content so maybe a good week for more questions from the community so feel free to maybe think up some questions as we work through you can uh ask like in slack or sorry you can ask in the chat here in the Google chat or you can I zoom in a little bit I think I can um uh thank you uh feel free to ask questions in in the chat or to unmute you know whatever kind of float your vote there um all right I guess we can get started here uh agenda two Tempo 2.6.1 was released um there was really uh only this minor bug fix that I think is concerning but I want to talk about it some I mean the bug fix wasn't really that big it was just we dropped a content type header on one endpoint which somebody noticed which was interesting that people are testing Tempo that tightly um but whatever we fixed that so that's pretty much minor I did want to call out this we had some changes enhancements and a breaking change which is very odd for a patch release uh and normally we would never do that um all of these changes all of these changes were to the tempo query um to the tempo query uh uh process which is most of you are not running it's only required as the shim between Jagger and Tempo red hat does use it and they submitted all these PRS and they wanted us to get into 61 so normally don't like doing this um but since it only impacted Tempo query none of you are actually running this it was really to let get some changes in for red hat before 27 we went ahead and did it so just a heads up on some of this don't get scared by it to be honest if you don't upgrade a 261 it probably doesn't matter um it unless you really really need this content type application Jason header uh returned on I think it's the maybe the search end point I'm not 100% sure but it was a pretty minor fix and you know got some of these Tempo query changes though um so yeah 261 kind of no big deal for 27 uh you know we're kind of always working on the next release and for 27 uh I want to give a heads up it's primarily operations and performance related uh we have been slamming features in tempo for two years or so straight now three years uh and we really wanted to get focused especially on our side also on improving operations and performance and that's a lot of those changes are going to be in two7 so I've got a list of some of the ones that I wanted to call out um but I encourage anyone uh from the team if anybody has anything they'd like to mention please do uh Red Hat Andreas actually submitted PRS to get us to Hotel tracing which is cool so we've been on the Jagger client forever uh and this moves us over to the open tet Tre Tracer uh which is really neat uh nice nice Improvement there get up- to-date uh semantics Trace semantics and honestly I was doing some testing a week or two back and our traces are looking really solid better than I thought and I did a couple of small changes to connect some spans that were not connected well so I think our traces are in pretty good shape um if you do a huge query they can get a little overwhelming but for most people I think they're they serve the need well uh performance improvements everywhere Zach's been working hard on polling we got a lot of memory uh CPU improvements there Trace ql improvements I've linked two but I think there's like 10 already on tip of main not in 26 or something and SJ has been working really hard on our tags and tag value endpoints with some caching and other just general performance improvements like I said really really want to get uh make 27 be a nice performance boost um on searching uh and also resource usage overall for Tempo um and then I'm actually working on some regx improvements right now currently Tempo is using um just Ye Old go regex and that's fine um but go regex is I believe optimized for predictability more than anything and um we are switching to use the Prometheus fast regx which has all kinds of shortcuts to make some assumptions so there will be some minor behavior changes on reg X's but uh we expect Major Performance gains in exchange so it should work a lot better and it'll also work exactly like Prometheus does which I think might help some folks who already have an understanding of how Prometheus rigex is work it's likely this will not impact you except make things faster when you use rxes uh we do have some Trace Cel improvements I think mainly this has been uh fam mainly fam working on these although no my bad um hobby has also put in some good time here um but support for the otel instrumentation scope which is cool um so we've not been able to query that I think we added arrays and events uh and links into six now we're adding instrumentation scope into seven that's a nice Improvement uh a couple more functions for querying uh metrics like aage over time men overtime Max overtime and I really want to get top K in as well um so ability to ask for top or bottom K you know 10 oh my God I'm writing promethus syix uh the ability to ask for the top or bottom um series basically uh we've seen people in our production clusters running queries that sometimes return hundreds of thousands or millions of Series this makes Tempo said and takes forever to return so we're both adding limits uh into seven uh so it'll refuse to return more than say 10,000 or 5,000 are configurable limit number of series um and then also the ability for the user to see oh I have tooo many series let me just look at the top 10 or top 100 or whatever so I expect both of those in 27 as well across our figures uh cool so I mean I'm talking about how it's going to be operations and performance but we are adding a couple of features too um so expect some improvements there we also are dicating things um we are removing serverless support we put up a PR or sorry an issue a while back didn't get a whole bunch of traction on does anybody care about serverless internally it's blocking us from making some improvements and it is just complicating the code base and we'd like to remove it so we are going to um and this should actually allow us to speed up the query path a bit as well because we'll have to do less Bookkeeping on the query path that we do for serverless right now um and then also the metric summary API this was an early attempt at metrics and we believe soon the trace K metrics will completely replace this the abilities of Trace K metrics uh and so we'd like to remove this uh as well again simplify the code base get more focused on the features and allow us to improve those instead of support a bunch of things sum API is more complex because graphon calls it so I think they're removing it first and we'll probably let it live for a couple more releases to support o graas and then eventually remove it as well awesome wow that was 10 minutes I went through everything I knew this wasn't long uh Team thanks appreciate that does anybody anything to add to this I did just kind of blow through it uh I don't know what do y'all think what else do we want to deprecate everything how about V2 uh that's a good call out um B2 I'd like to put up a post to deprecate B2 um we kept B2 for you know moving from 1.0 to 2.0 for those folks who still want to trace ID search um I don't know if anybody's using it it does hold up our code base a little bit because it just can't do anything that par can do um it would allow us to clean up some stuff if we removed it uh it's tempting for sure that might be a 3.0 3.0 change um I we've been talking about if you've been reading the blog posts we've been talking about a um uh new architecture a q-based architecture uh this would allow us to reduce TCO uh increase performance uh at the cost of having to operate a queue or pay for manage queuing services from any cloud provider um and that would likely be a 3.0 cut a major rev and that might be a time to remove V2 there's a lot of internal crft uh related to that that would be nice to clean up cool all right I don't want to mention overrides I don't want to mention overrides either I would like don't that's right overrides don't mention over don't mention done no one say the word overrides uh all right we'll talk about overrides real fast thanks Zach uh we caught mid transition between an older overrides config and a newer overrides config and it's just really stalled out and I think it's been frustrating for everybody including us um I don't know I would love to get that across the line but it's a low priority and it's just a bit of a mess unfortunately it has been for a long time there's a long running issue that's like how do these overrides work why are there two settings basically and it's very confusing at the moment uh I don't know I don't have good answers there uh I think the problem is what we internally would have to we are still using the old overheads internally and we takes a lot of work to move to the new ones unfortunately so we may want to eat that cost in the next year though just guessing based on some conversations we've had about the way we want to change our deployments yeah that's PA I do kind of expect Tempo to settle a bit after this architecture uh we'll be have a really strong feature set um the new architecture will uh give us the same dur guarantees with lower TCO and better quering um and I do think it'll be time to invest heavily in operations in code cleanup and uh really taking the database to a new level of maturity uh We've really been heavily so focused on features that we just are moving as fast as possible and it would be nice to take a breath slow down with 3.0 and to just tighten things up for frankly a couple quarters it would be really nice we also increasing our investment in hel should mention that um we uh are going to start using it more internally traditionally you're to use this Json nut and qu nut um but uh we would like to move to home it is what everyone uses it's the community uses and we don't as a team internally have a strong sense of what our Helm chart does well and what it does poorly and so this quarter we're putting more time from our team into helm um submitting PRS understanding how it's used and wanting to clean that chart up and get it in better condition and I would like to thank the community for all the effort they put into it for years additionally I think we can add some docks for things that the chart does currently that um maybe come up as as regular um questions and deployment scenarios for folks so hopefully I'll have a doc PR up for one such deployment operation um yeah cool don't mention hel yeah Al okay list don't mention don't mention we don't talk about these things okay we will be talking about them in a couple quarters and we'll talk about how amazing they are and how much I'm Pro they are that's what we'll talk about all right crew anything else questions concerns anything you can ask about how to operate Tempo what we're working on what's on our minds favorite foods um how many fingers on our left hand I got five anything anything goes it is officially an ask me slash us slash us anything who's got operational challenges how's the how's the Clusters perfect in every way hi um I'm gonna jump in I talked to you a little bit um probably last month about um trying out the new traces app um I'm from integr hello um you guys were talking about Kafka Q based systems going forward um I took it back to my guys and they also raised that red panda is another alternative to Kafka just in case you weren't aware of it so I thought i' pass that along yeah it's a really good call out um I think uh there's a lot of options on the table for people who um either want to run Kafka or pay for hosted Kafka and I really strongly believe we will lower the TCO regardless by getting down to rf1 even if you're paying a service like red panda or WS manag cka or something like this um so uh that's one of the reasons that I think I'm a bit comfortable with making this change adding this dependency is because there's so many options in terms of things you can run yourself as well as things you can pay for other people to run and get what you need uh or and like kind of get this working basically so good call out thank you red panda um I mean I think there's a long long list of people confluent right don't they do manage Kafka um they just bought warp stream every major cloud provider has a money button for Kafka um and I think for small installs for single binary people like you won't even notice there'll be a little bitty kofu pod that is now running in your home chart that was not there before and you know whatever that's it it will just kind of live there so right we are thank you SJ what a good call out SJ we writing the Kafka protocol if I say Kafka I don't mean Kafka I mean the Kafka protocol and we're going to work to make use of the um most common features possible so that your options and opportunities to use other services are as wide as possible um there is a potential future where we can drop this and build our queue on object storage but we're not really willing to commit to that yet it is something in the back of our minds there is some work internally at grafana to look at building cues on object storage so we might do that at some point but that is not an initial goal the goal for a 3.0 for this re architecture is reduce tcco get to rf1 improve performance and so that is what we're going to focus on uh and then perhaps later we'll get to an object storage only kind of solution thanks Lena I like your bike little appetite there I'm a big biking fan myself thanks don't get him started yeah we could do that for a little bit do you want to talk about bikes bring out the bikes wind will Tempo no don't bring out the bikes when will Tempo 27 be released oh I got a couple questions here cool sorry I'm looking over here you're all on my second screen 27 we do we work really hard to do four releases a year um every three months and I think we've been really good about that the only time we slipped was 2.0 um because it was just such a huge release uh and I would guess given that 3.0 is going to be huge as well maybe that's the one that slips to but 27 should be no problem it should happen in I think January is that our Target is that what gets us to the three months somewhere in January can somebody double check that uh I think that's right I mean you always commit here and then do the release right so now you committed January sure I mean I think we've always hit it Haven we am I crazy I feel like we've been really good about releases the only one I can remember that was late and was frustrating me was 2.0 but whatever that was so huge totally get that that slipped a bit yeah we've been pretty good about it 26 was September 3rd for the tag oh then oh my bad ex yeah you're right it's it's not January then is it December yeah maybe December sometime in there I was thinking our weird offset quarters normal normally we release towards the end of a quarter so right I guess it would be sometime in December then could be January with the holidays and everything though we do right we have holidays uh can compaction levels can you provide some guidelines how to achieve higher compaction levels higher compaction levels effect in a good way on search performance o uh that's a great question um search uh is not generally heavily impacted by higher compaction levels or lower compaction levels what we have found is that we are not duping our um traces as well as we thought and that's part of the rf1 shift is to get to a single copy of every Trace in the back end um it was at one point the goal of compaction was to dup and it was not really happening competion really just kind of keeps your blockless low which can help with ID search and can help with polling uh but for most installs I think just trying to get to level one or two and saving money on your compaction would be wise I'd have to see what your block list length was if you're in the low tens of thousands um I would recommend not worrying about getting super high compaction levels and getting to like you know super giant blocks in fact that's probably going to negatively impact performance instead of help uh I have a strong belief that lots of smaller blocks is better than lots of or fewer bigger blocks but we've not tested that well yeah depending on backend and concurrency and stuff too y that's a good point uh fam what do you got dedicated columns what are you calling out there I'm just saying that's uh it's gonna be a great way to see help SE performance yes okay right good call for search performance uh look at our dedicated columns Docs um we see really strong improvements uh for both pulling out uh uh pulling out uh columns that you query regularly and more importantly pulling out gigantic dumb columns like db. statement and db. crazy Jason blob that's a megabyte so um look into dedicated columns that's a great way to improve uh performance as well as reduce resource usage of tempo thanks Kim always on point with the doc links there also if anybody has topics you would like to see documented for the hel chart you can add them to that doc issue that I put in our meeting doc um we'd love to hear if you have any issues or areas where you're having concern or you're finding there's gaps pretty cool all right Z any other questions concerns these are great questions um uh I think if you are having search performance issues search is the hardest thing to tune in tempo for sure uh I'd say we have a a strong a strong search performance or we have good search performance but I want amazing search performance that's part of what this rf1 change is about um and we are just constantly absolutely constantly adding uh uh imp performance improvements to tql we really really really spent a ton of time on this and now like a bunch of anded conditions and now we're getting to better or like more difficult queries what's up mat Fe um I was actually gonna ask what's the kind of Target for the re architecture with 3.0 or something like that is that something kind of two next year just general ballpark because we're in the process of migrating uh our engineering teams with logs and metrics into the kafana stack and holding off on traces until sometime next year yeah I don't know team what's a good answer for that I we've said internally we'd like six months uh to see this running in production clusters basically uh so that but we also kind of have to bundle it up a bit clean things up get it into release we might even have a release where it's their experimental like a 28 or nine and then go to 3 with it as the standard I would love to see uh right let's say q1 next year have a really strong sense that either it released experimentally in Tempo or really be able to give you all a good idea of when we expect 3.0 um yeah maybe like q1 or q and then maybe like Q2 would be a Target 3.0 I'm going Mario and Marty are not on this call hope they never watch it they will kill me for giving dates on you can never give dates just you know at least you know kind of ballpark you know type of thing so that's at least uh directional so thank you yeah thanks man no doubt I of curi we got another question here for you yeah yeah is there any you know there's um anything on the road map to you know kind of build out you know some sort of sampling component um you know because sampling is a is a big thing with tracing and you know it's um if you do an open Telemetry you can have to do that yourself and it's you know it's a it's a fair amount of work especially on across like a distri distributed team type of thing where you could actually have you know each team kind of control their own um you know uh the rules and stuff um just curious is that anywhere been thought about absolutely so uh this unfortunately perhaps for there you go thanks siraj uh we are building like you've heard possibly adaptive metrics adaptive logs these things right so our adapt tracing project is uh working or is uh is happening right now uh and the goal there is a lot of what you're talking about so like Dynamic sampling rules rules um where you can say for this tag you know allocate fairly across you know like a 100% of my traces allocated across this resource name space or team or something like that so it'll make sure that smaller teams get some percentage of your tracing budget essentially as well as um you know kind of reactive to incidents and these things so the bad news is some percentage of that's definitely going to be closed Source this is part of our like adaptive Telemetry story and Cloud um I do hope we can make some of it open source but I don't know what to say maybe I'll bring Shawn Porter on for our next Community call and he can answer some questions there um right yeah we have we just have a a challenge and with regards to uh because we're gonna have to run things in house just due to compliance reasons and stuff like that so yeah yeah I think there is one component that ya is working on right now specking out a design for that I'm going to push a little bit for open source but I can't make any guarantees there that I think could be a cool part of OSS without giving away this like core feature that they really want for a cloud essentially um so I'd say stay tuned in maybe uh next month we'd have more details there um I would love to get maybe one a member of that team on this call is youa here actually no okay maybe some other time thanks man there is one more question Jo oh my bad thanks s did I miss it uh I don't see it am I crazy question the am section oh oh oh doc thank you uh possible to figure out how much latency traces introduces to my app for running tracing and proud possible to set it up in an easy way to toggle off python I don't know much about python I'm not going to be able to talk about that I don't think uh but you should be able to turn on and off your Tracer with environment variables um if you're using either otel or Jagger they both respond to environment variables I believe you can set them you could be able maybe like do like a canary style deployment or a red blue style deployment red green red green it doesn't matter they're just colors um where um you know maybe some of your pods had the environment variable to turn it on some have it off and you could uh get a sense I would also encourage you to use profiling style tools uh we do that a lot internally in fact uh we found an issue with the otel is using a lot of resources um and we look to improve that so profiling tools are also great for this where they can show you okay 1% or 5% or whatever percent of my memory and CPU is going towards tracing um and you can make some adjustments there if it's too much or you want to give more or whatever so I would encourage you to turn on and off with the environment variables as well as use Python I use the tempo distributor just I think it said The Collector um neither so the colle doesn't matter basically um the like pipeline doesn't matter here you can configure with environment variables the client itself so the process has special environment variables you should be able to turn tracing off and then see like oh did latency increase or not uh you should be able to do that like I said maybe on part of your deployment so half deployment is tracing on half deployment off and you could compare latency metrics if you wanted to see what that is and then again profiling is a great resource to see or great like observability tool generally totally recommend profiling generally but in this case specifically to see like what resources are spent on tracing I was just going to mention also that if the reason for the question is that um it feels like the app is blocking you might double check that the SDK is configured with batching um so that you are not putting traces in the hot path and that way you're sort of batching them up and sending them into background um that could be something that will impact performance on the hot path but yeah I don't know python SDK either uh trace's explore app road map uh I don't know what to say about in fact somebody was asking about that uh explore app is total open source so if you want to participate in the explore traces app you can file issues um you can ask features you can chat in discussion well I don't know if discussions have been turned on for this Reco whatever uh you can participate here boom uh I don't think there is a Ken might be able to correct me here is there a um release schedule I don't think there's like a release schedule or Cadence for Explorer traces I don't think we've gotten to that level of maturity first floor traces as far as I'm aware we don't but if we do releases and stuff you can either check the repository for the change log or you can we'll hopefully be adding update in the what's new in graphic Cloud to provide a kind of regular Cadence Snippets of what the new features are right I think we cut this for obson which is the only reason this exists um we should talk to the team about that and get like a regular schedule Cadence I think basically things get merged here and just make their way into graphon Cloud just stream out essentially without any kind of like steady release or like kind of discrete release Cadence um yeah I don't know I I think probably as this gets more attention and help or and and effort uh and that I think we would probably right move at that point to uh maybe some nicer releases or something I think we're just moving as fast as possible on this and not worried about silly things like releases cool hey I'm on the contributor list nice all right any other questions concerns you can always ask in slack this is of course a great opportunity to get like direct synchronous feedback slack is kind of like best effort people just chat as they can um but no more questions no more thoughts uh we can move on with our days cool well I appreciate all your time thanks for hanging out uh anyone who likes biking like Lena and myself are the coolest people in the room and we will see you all next next month awesome happy writing bye everybody bye folks

