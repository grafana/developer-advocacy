# Build Scalable OSS Observability with New Features in Mimir, Loki, Tempo, and Pyroscope | Grafana

If you'r building OSS observability, you'll want to use the databases Mimir, Loki, Tempo, and Pyroscope. This video explains why ...

Published on 2024-05-14T00:01:29Z

URL: https://www.youtube.com/watch?v=K5SRjsUKjxo

Transcript: uh hi I'm Ed I'm here to talk about OSS databases for observability um I and Brian who's going to come up and do the second half this talk uh both work at graphon labs and build I build Loki he builds pyroscope um and the goal today is to try to get folks to kind of appreciate why we have four of them and why you should want to use all four of them um is anybody here using all four of these databases yeah let's go so if you are there's some new features we're going to talk about and be exciting to know what you are looking forward to if you're using them if not I'm going to start by kind of talking a little bit of a story about why why we have these four databases um starting with the best in my completely biased opinion signal which is logs um this is how everybody kind of gets their introduction to observability they build a application you know the first thing you do to see if you can even compile an application is do a Hello World example um and then it doesn't take long before you realize there's nothing I can do without or with logs right I can do I can do everything um and you know you know quite a few years ago this was the only tool that we really had for um kind of understanding how our applications were working and the result of that is that the world is very very chaotic of logs there's structure un structure trying to get people to agree on formatting you know space delimited Json it's extremely difficult and then we have applications and libraries that you know we didn't build we don't maintain um so this this is why we built Loki was to have a you know a database to aggregate logs that we could build to huge amounts of scales um and had no opinions on how that data got in there so entirely schema ingestion um but kind of back to this sort of story on how we've evolved observability um I worked at a company many years ago that had a a policy that basically said you know always have a log line at the beginning of the end not of every function but most functions that would have a consistent naming scheme and then log the variables that went into and out of that function um and this was really handy right this allowed us to troubleshoot a whole bunch of production problems um but really this is just tracing right so this was in the late 2000s I think before tracing might have had its its real origin story um but tracing is a super set of logging that gives you some really really nice features right like context is the big one you know a trace ID that can be propagated through headers um across you know h2p boundaries and Network boundaries to let you build visualizations really tiny hard to read visualizations in my screenshot sorry um but let you see like what your app was doing and where and why and how long and you can attach metadata to that and um you know do a really good job of of being able to see what your app is doing and and because this is a super set and a lot of this data is structured um this is why we built Tempo um having a place and a way to store structured data and more efficient in huge scales uh Loki is a row oriented database it's you know very very simple data model to store strings um Tempo is a column or columnar oriented databases um and uses that metadata in columns to be able to more efficiently access that additional structure um but both of these databases have a a problem of sorts um certainly a challenge um which is the amount of data that you can produce from logs and traces right you can you can in this example I looked at one of our Loki clusters for 15 minutes to see how many push request par log lines we have we log some data around the push requests we receive and it's about 30 gigs and 15 minutes 30 some thousand lines a second um you know and Loki's happy to sort of run these queries over hours days maybe a week you know if you're patient maybe a little bit longer um but you know there's just some limits to what we can do we've made it very very fast but there's a better way um metrics are the better way so what metrics gain you is a way to sort of normalize that volume away from the application's behavior itself right so by turning these into counters instead of having to log a thousand or 10,000 events and count them we increase a counter a thousand or 10,000 times and we sample it at regular intervals and so now we get a fixed rate of data and that lets us run short queries to be able to see you know error spikes um or 7day 30-day six month queries to be able to see you know real Trends over time you can do this stuff in logs and traces sometimes depends on the volume right like you know searching hundreds of terabytes of data can take a bit of time um so now we have this world we have these separate databases we built them to be good at what they're good at right like type of data they receive how they receive it how they store it how they query it and now we work a lot on how you work the best between them so metrics and logs for example um this is a that eror spike is a real thing this is a a production Loki cluster from a few weeks ago um in explore now I've turned that into a graph that I can see uh and then using Loki I pulled up the logs and did some normalization to extract the log messages to count the log messages by type in this case deadline exceeded comes to the top um if you're programming go deadline exceeded is a confusing way to describe a timeout um and so why did it timeout right so I I jumped ahead of myself here but the the the what I know about Loki building Loki this is on our right path um I got a timeout we have a pretty tight timeout you have to push request has to process in about one second and usually when we can't it's because we're CPU starred right or something is is saturating or we're we're not provisioned enough for CPU so now my sort of new favorite or maybe second favorite to logs signal is profiles right so I can use Explorer again in the same time frame and same data to go look at what the CPU was doing um and in this case uh interesting story was uh some label-based access control work that we do us as regular Expressions um goaling is not the best at regular expression work and we were recompiling the reject sort of multiple times per request and we found a way to uh remove that and essentially only compile it once per request to make those kind of air spikes go away um so this is how we sort of build and evolve our tools over time um and this is also uh why all of these things work really really well together and why you should use all of them and why we work on building interactions between them so logs can do recording rules to generate metrics if you don't have the way to instrument your app directly um if you have traces and not logs which is probably less common but there are ways to generate metrics from traces as well um all of these things can leverage profiling to understand what your apps are doing so whole big picture together and what you end up with is this toolkit and so now what I going to talk about for the next say 10 minutes or so uh a little bit about metrics a little bit about logs and then Brian is going to talk about traces and a little bit about profiles so uh apologies to all of my mimir developer friends because uh I have a lot less mamir slides than Loki slides um maybe because we just did a major announcement but maybe because I work on Loki I won't tell but the uh what mamir is is a you know open source highly scalable horizontally available U remote right backend for Prometheus style metrics that also does support graphite style metrics um what's new and really exciting that I do want to talk about um is histograms so if you're familiar with histograms in Prometheus um they basically you go into your application you define a histogram metric you give it a number of buckets um you sort of need to know ahead of time those number of buckets and then When You observe a value it gets put in one of those buckets those buckets become series and they produce crafts in the top here like the the one that doesn't have any units on the x-axis but anyway the problem here is you don't always know ahead of time what the right bucket is right or what the right bucketing strategy is and that's a code change to go change that so native histograms also known as sparse histograms or in open Telemetry world uh exponential histograms I actually kind of like the trade sparse histogram because that's really what they are is they they don't require upfront bucketing you can essentially add values to them they take the values and determine the buckets and the number of buckets you need based on the value distribution and what you get is better resolution in your histograms both for um heat maps and distributions without having to do any of that work and having to know any of those buckets so very very cool um and then you know I mentioned when we went from metrics to profiles and um metrics to traces or sorry metrics to logs there's also metrics of traces in the form of exemplars so you know given that metrics are probably the first tool that I go to most of the time because it's kind of the fastest tool that I have for narrowing down time frames and basically systems that are affected um and what you can do is you look at your graph and you get these little dots that show up all over them and those are examples exemplars that those are exact you know events that happen you can pick one at the high end of maybe say a latency Spike or something like that and pull up a trace and say this was that execution event for that high latency event and very quickly be able to find you know hopefully root causes to some anomalous latency events so thank you mimir went fairly quick there but because I want to spend a little more more time talking about Loki so Loki 3.0 announced a couple weeks ago let's go um five years in Loki turns three that's the the phrase that my peer Owen coined that it sounds pretty cool um I didn't catch this earlier our formatting is really cool on these these version releases but the story of this slide is you know what we've been building over time um Richie mentioned in the keynote we're really excited you know Loki started as an idea five years later I would say it's proven to be a popular idea more than a 100,000 active open source installations um you know as part of our SAS service for graphon cloud we run numerous in clusters that ingest hundreds of terabytes of logs serve pedabytes of queries um we've been able to build you all have a lot of logs so we've been work working to build you know the biggest most scalable system and and we've kind of you know re iterated on it a few times over the way and the things I'm going to talk about now are these uh Bloom filters in particular um and some of the other new features in 3.0 um I throw this slide in here too because um the sort of evolution of the idea has come a long way too you know Loki was built on this idea that you don't need a big index to be able to solve a lot of logging problems and instead you can lie on you know distributed systems ideas parallelism you know commodity Computing and the capabilities of object stores like GCS and S3 to obtain you know query through puts now that are in the you know hundreds to up to terabyte a second for searching your logs and the economics of this are really fascinating because it's expensive to build an index and and maintain a database with large indices and it's interestingly cheaper to brute force your way through a lot of these solutions kind of up to a point like we've found now where that that kind of trade-off is certainly for some types of queries and that's what I'm going to talk about with Bloom filters um however first I'm going to talk a little bit about open Telemetry because um this was also new in 3.0 we're pretty excited about this which is U we had kind of a missing piece here so open Telemetry has some nice Concepts around um consistency and structured metadata they call like semantic conventions um and Loki had two places to put stuff an index and then a log line excuse me and our index we want to keep really really small we just want like a few key value pairs to Define where the logs come from the small index makes Loki a lot easier to work with and this is maybe a bit counterintuitive sometimes but the Brute Force strategy does actually work quite well um but what do we do with the structured metadata that that came along with otel these resources and attributes um I guess I jumped ahead of myself we call it structured metadata so what we did is we made a place in Loki that we can store structured data which is not indexed and also not in the log line and so that's where we put this open Telemetry resource our es and attributes um it used to be I got to put these eyes out of order but what we did the exporter would serialize the metadata in with the log line into the string field in Loki and it's kind of a mess right so in order to get that data back out you had to run a Json parser initially just to get the metadata back where it belongs and then maybe another Json parser for your log line if it was Json format so now that is sort of all handled for you we can store that data in its own place and you get your log line um as you would expect so much better interaction with open infmetry and including the uh Loki has native end points now for o TLP the open Telemetry line protocol I don't know if that's what the LP stand for but it's close um oh the last thing I want to talk about though that I'm kind of the most excited about is Bloom filters and like kind of how they work and how we added them to Loki and what we're trying to accomplish with them um you know the story here looks a little bit like this right we built this you know no schema ingestion log platform and we did that because the world of logs is a chaotic Nightmare and that we like that works really well and so how do we you know sort of improve on this in some use cases because like I said we've we've been very good at building highly parallelized query workloads but you can find limits of everything and our Cloud providers tell us that we're finding the limits at least of what they want us to do without giving them more money so that changes the economics of how we approach Loki right like when you try to pull more than a terabit a second out of object storage um in a consistent way they start telling you that you're going to have to pay for more resources to do that so this is where we also realize at the same time a lot of the time our querier spend querying they're really not finding anything that fact 75% of the time that our subqueries would execute on a parallelized query they matched no log lines the filters in the query essentially excluded all of the log lines um so we have this huge amount of of time that we spend not finding any information so it's like the answer wasn't necessarily let's make Loki faster and parallelize more it started to become like how do we search less right and and what we found is that this use case that we see that that kind of drives this Behavior looks a little bit like this in most cases it's someone looking for like a high entropy string a string with a lot of variability in it um and then oftentimes over long time ranges like a lot of times like a customer support case they don't get much information other than an order ID or a trace ID and they're expected to go search to find logs to help troubleshoot and this is where people would search over you know like namespace equals all of them for for seven days or 30 days right and poor Loki's just over there looking and looking and looking and not finding anything um so the problem was how can we make that better but not have to impose more structure on ingestion right like not have to impose any anything at query time um we we really want to keep schema list we really don't want to have the user experience change around queries I say schema list I I I'm happy to always talk about schema stuff because it's a really interesting conversation um we tend to call it schema query where our query language imposes structure and things like that um so we wanted to kind of keep the same setup we have and this idea that we were kind of calling negative indexing was born um you know what we realize is 75% of our time we're looking for data that we can't find so how do we find less data um wish I remember when that started to know how much time I have left sorry Brian oh 45 okay I'm good I'm good so uh the idea of negative indexing was was kind of born here and in particular we're going to leverage a technique that I think basically every database leverages which is Bloom filters Bloom filters are a really clever probabilistic data structure and they have this unique behavior of them which is you feed a bunch of data into them and then you can take a string so you feed a bunch of strings in you can take a string and say hey have you seen the string before and it will tell you with 100% certainty no it has not or it will say yes I have but I might be lying a little bit um and this works out really well for this case where we're trying to figure out where not to search because 75% of the time we're not finding anything so we can ask these Bloom filters hey have you seen this and they'll say no and then we don't have to search that data and we don't have to worry about it or they say yes and we go search it and maybe it was there maybe it wasn't but they've removed a huge amount that we don't have to so that false positive doesn't really hurt us that much so I can't explain well how Bloon filters actually work it's a series of hashing functions everything in distributed systems is hashing functions just more and more hashing functions and then you end up with some bit map effectively a bunch of bits that you know can be used to repeatedly answer that question the way that it does and so what we decided to do with Loki here our idea is to try like how do you get the data into bloom filters so we're using an approach called engrams which is a way to tokenize the log line into smaller strings so this is how we want to approach this problem without having to know your structure is we just don't care we'll tokenize the entire line into engrams shove it into blue filters we will then tokenize your query expressions and use those the filter Expressions against then you know Eng grams those and then look those up in the bloom filters and give us you know reasonably good confidence depends a little bit on kind of your entropy levels right like so long uids work the best um you know long characters short words short strings you know I'm always amazed the number of times if you took like a five-digit number and just searched it in your logs how much it just matches everything you know like six-digit seven digigit numbers and time stamps like it's always surprises me how random numbers will match so you run into that a little bit more when you lack some entropy in your strings but very excited about balloom filters um kind of early days this wasn't three days ago but it was not that long ago but we have it working we're working on sort of making it faster this this is a kind of a progression somebody at our company maybe it's sort of wi you know make it work make it fast I think it used to be make it right there's one of those in there too but we'll assume that we got that check in there um and so that is that is mostly what I want to talk about I did have some slides in here around uh our new experience on querying and uh I'm just kind of skipping them because you got to watch that video that was made around that earlier so but the goal looks like this right we want to have um an entire query list experience around how you enter and find your log so with that I'm gonna hand it over to Brian who's going to talk to you about Tempo and pyroscope uh so I'm Brian and I want to start today by talking about um graff's distributed distributed tracing database Tempo and Tempo is really good at helping identify where where latency and errors originate across services so the tempo team has been working hard on a few new great features metric summaries structural operators and Trace ql metrics so before we dive into the features I just want to Quick give an overview of Trace ql which is tempo's query language and so here's an example uh quer Trace Quail query and so this one run will will select all of the TR that match uh each of these conditions so in this case we're looking for a service name app Fu and we're looking we want to constraint to URLs that match the SL API reix and we want only status codes in the 500 family and so with that uh I want to review kind of paint a scenario here um I'm an engineer I'm working on some stuff and I've just been paged that one of my services is reporting an elevated error rate so to begin investigating I want to find traces that have errors in them and so I can do that with a trace Quil query like the one shown here so I run this and I get a nice result set of all the traces that have an error in them and so I Can Begin by selecting a trace and kind of browsing through here looking at different spans and trying to find the Smoking Gun that's pointing to why where this error is occurring and what service is causing it and I don't don't immediately find it so I open another trace and then another trace and soon I have uh 20 tabs open and my computer's crashing and my brain is out of memory um and I'm trying to find why what's similar about all of these traces uh that can lead me to indicate where this error what service is causing this error under what conditions so there's a better way to do this I can use leverage metric summaries and Metric summaries allow me to aggregate by attributes that I suspect are relevant to the issue at hand so this service I know is an API that serves HTTP requests so I might want to aggregate by the end points that this service provides so in this case I'll aggregate by HTTP Target so I run that query and I get a nice table breakdown uh aggregated by endpoint and so I can see that I get some Metric information for each endpoint based off the tracing data and I can sort this error error column which tells me what percentage of these had an error reported them and I can notice immediately that this account endpoint is is suspect this has a much higher error rate than the rest of these end points and so this might be worth something for me to continue investigating so Tempo has uh another trick up its sleeve it can easily handle high card cardinality attributes like user IDs so here is an example of tempo's Trace qu query Builder and first I'm sorting by I'm selecting my service and Below you can see the trace qu that's going to correspond uh to the settings of this query Builder and so I want to not only aggregate by my HTTP Target but also by user IDs and so here now Temple provides me with this result Set uh similar to the ones we've seen before where we have a breakdown of account IDs oops of accounts or endpoints but also user IDs and so I can again sort this aggregation by the error column and I observed that not only is the account endpoint causing us problems but there's a specific user ID that's also that's generating the majority of the errors and so this can be super useful because this gives me a nice breakdown of of wide swath of tracing information and presents it to me in this way that I can easily respond to it so moving on uh I want to talk about structural operators so as data moves through our systems and Trac traces our built as a result of which these traces end up having these very tree likee structures so the tempo team has been smart and has worked on a way to create query operators that leverage this structure so these are called structural operators and here's two examples of them uh the first example is called the direct descendant which in English this is basically asking us what traces use service a and they immediately go and use service B and the second one is asking what traces use service a and at some point have an upstream depend Upstream descendant that is reporting an error and then finally select the service names that where those errors occur and so circling back to this investigation I'm running I can run this query asking for the service the service and this misbehaving endpoint and ask for all the Upstream spans that are have an error uh in in them and I get this nice breakdown uh which gives me a list of all these spans where an error has occurred and I can use this to then further further analyze where this error is occurring in this case we can see you know we we have a lot of common commonality in this DB name column so maybe that's worth investigating why the database is reporting these errors so these query exp query uh features are really useful and can help Engineers understand and pinpoint where latency or errors occur but often times there is a drawback which means it requires you to write a query and sometimes teams might not have new Engineers on them who aren't familiar with Trace Co and that means they need to learn the query language before they can effectively triage incidents that that are occurring or there might be a team triage and instant who is familiar with Trace coell but they're look looking at a service that they're not familiar with so they don't understand the attributes associated with that system and the behaviors associated with that system or we might just be a little bit lazy and want immediate results just tell me the answer so in this case the tempo team has us covered here too they're devel in this new tool tool called explore traces which presents this query Trace analysis experience where without having to write queries it displays how this the tracing data is oriented and gives you breakdowns of different to explore different dimensions of that tracing data so this allows us to browse through the behaviors of the system without actually having to write these pinpoint queries so with that tracing help helps us understand where latency is CA is causing issues or where the error is coming from but we want want the other half of this question where of how do I fix this problem or what code is causing these this latency or errors in this case for this we can leverage profiling and pyroscope to answer these questions so I want to Quick talk about what a profile is so in simple terms if you have a application that's running we can over some fixed period of time take point and time samples of the stack trace of that application and aggate them together into a profile so and often times profiles are visualized using flame graphs and Flame graphs have two important components they have a width which represents the Total Resource usage and they have a height which represents which corresponds to the order of function calls of a in a stack trace and so here we have a sample program which has two functions do a lot and do little and we've profiled this and we have this corresponding flame graph which we can look at and observe that the do a lot function is consuming 65% of the total CPU usage and the doitt function is only consuming 26% so that's a single profile pyroscope offers this notion of continuous profiling so continuous profiling is taking those profiles and aggregating them over time and pyroscope being a database for continuous profiling has this ability where you can query and visualize these profiles at any point in time so using a Prometheus like labeling in quering system we can select a Time range and some labels and get a nice breakdown of resource usage over time and also the canonical flame graph representation for that profile and the business value here is twofold we can both we both have proactive value and reactive value provided by profiling uh so for proactively what does this mean it means we want to cut costs early we want to improve our Roi on features we're currently developing and generally shorten the amount of time it takes to optimize code pyroscope allows us to do this by providing line level breakdowns of the resource hotspots and to kind of really underscore this idea uh years ago our applications were relatively simple um I I didn't I wasn't a live year ago but when they did exist they probably ran on a single box they're relatively to to today's standards relatively easy to understand how they behaved and the costs associated with them as time has gone on and user usage has gone up and features become more complex our applications react accordingly so now we have these complex probably distributed applications there's many abstractions our libraries and tooling for deployments we have third party dependencies like messaging cues that help facilitate the this application to work in in a distributed nature we have observability infrastructure to help us understand how this application is is operating and all this adds up and it generally means systems today cost more to run what profiling aims to do is for minimal overhead enable Engineers to go to the parts of the system they do control and ruthlessly optimize those as as a result we see reduction in the overall cost of running the a system compared to previously and I really want to highlight this idea of the resource usage required or the overhead required to profile applications so this application so this image here we can see um the grade outlines that are a little tricky to see is this is a real application that we run in grafana and the grade outlines indicate the Total Resource use of the application and this blue this yellow line that's on the xais and almost invisible is the overhead that profiling contributes to the Total Resource cost and in this table we can see that that overall adds up to less than half a percent of the Total Resource usage in this case so profiling the overhead is is is quite low so circling back to value of profiling there's this reactive component to to profiling value and that's we want to take latency issues seriously and we want to be able to respond to these latency issues quickly so broadly across many Industries revenue is inversely proportional to latency so as we see latency go up Revenue will go down and this is true for e-commerce if cart checkout times begin to rise we see users begin to abandon carts and sales are lost and if in video streaming and online gaming we begin to buffer in those environments users are going to churn to other platforms so continuous profiling helps in this case to keep businesses ahead of this curve and be able to respond effectively and efficiently to increases in latency thus Le saving Revenue another reactive component of benefit of profiling is it allows us to quickly respond to incidents so here is a breakdown of an incident that happened internally with shortly after the pyroscope team joined and this is when grafana was took a very metrics heavy approach to responding to incidents there's a lot going on here uh but the takeaway is that this blue line represents a service that's reporting 400 erors right here and we see this this recovery process broken down into three three major sections the first is the initial Spike where the system has started began to report errors and Engineers begin to triage the incident and they roll the then we see it dip down as they attempt a fix and they roll it out but as the fix begins to land spikes go back up so this this attempted fix did not solve the root cause and again we see the same behavior as a fix another fix is applied and we see again a spike in 400s as the fix again does not solve the root cause and by the third point the pyroscope team had been notified of this ongoing incident and we were able to enable profiling on the affected service while the incident was ongoing this gave the engineers the ability to immediately understand what parts of the code was causing the problem and was different and they were able to apply a fix that directly affected the the root cause of the issue and so the previous approach was really guess and check it was we think this might be the problem so let's try this fix had profiling been in place to begin with the incident would may have played out something like this they an engineer would been would have been paged they could have used pyroscope diff tool tool to select a baseline profile where the system was behaving normally and then select a profile where the system was behaving abnormally and then pyroscope will diff those two profiles and highlight what parts of these profiles are different and what parts are added and subtracted and Engineers can use this to quickly identify where the problem is is at in the code and this can help Engineers be more effective at developing the initial fix that directly affects the root cause so the modotto here is a profile a day can help keep performance problems away and to help with help with this the pyroscope team has the following road map we really want to provide a richer integration with other FAL Labs products and so this means you can go from a signal to profiling or from profiling to another signal and really play off the individual strengths of each signal and this will help help the observability Journey of everybody whether it is trying to identify inefficiency inefficiencies in the code or triage incidents we also really want to focus on user Centric uh feature development so this means we're looking to create features and prioritize features that directly impact use cases our users have and thirdly but not leastly we we want to develop this open Telemetry first instrumentation so the team has been working hard and is very active in the open Telemetry committee to to bring profiling as an approved signal in open Telemetry this allows you as as application maintainers or owners to bring profiling in as a signal to your service without a bunch of V locking code and making it difficult to move on so I want to cover some new features that the pyroscope team has shipped we've integrated with tracing with now which this means you can as you're browsing your traces in Tempo and you see a trace that looks unusual you can request a profile that corresponds to that specific span and this means if this this allows uh the workflow of finding using Tempo to find where problem is occurring and then immediately having profiling and pyroscope at your finger tips to understand why the system behaved how it did secondly we've built out an AI feature So Pro uh flame graphs can be challenging for new users to understand and often times even experienced users need to understand the characteristics and the shapes that certain of of flame graphs that certain system systems build and this can this can take some time and requires a little bit of intuition so here we can feed the profiling information to an AI model and it can give us some immediate root cause analysis performed or it can perform some root cause analysis and it can provide us with a few suggestions on you know I think these things need to should be improved or maybe try focusing on making these improvements to to see an improvement in your performance and we have this ability to interact you have this ability to interact with the AI so you can ask questions to further clarify Solutions it's suggesting or root causes it might have identified and here's my my personal favorite is pyroscope integrated with GitHub and so what this means is that if you have a flame graph you can select a node of that flame graph and then get you some source code brought up on the side and the source code has a line by line breakdown of how much resource usage each line contributes to the overall profile this is super useful because it brings a ton of source code context directly to the profile and you get this immediate feedback on how much each line is costing you additionally in the proactive sense and the integrating with other products of of in grafana pyroscope has is currently developing a feature to integrate with k6 so now as you build out a new feature and you want to load test it and understand it its performance characteristics you can use k6 to run the load test but then get a a profile in line where you can analyze the line by line behavior of that of that feature then you can identify inefficiencies and fix them before users ever see the product the the end result so in conclusion we've seen how graff's observability Suite between mamir Loki Tempo and pyroscope each bring unique strengths to the table they're individually each is very powerful and has a broad range of capabilities by themselves but together they create this holistic observability experience that's critical for understanding and scaling today's comp applications in today's complex environments and so with that I want to say thank you

