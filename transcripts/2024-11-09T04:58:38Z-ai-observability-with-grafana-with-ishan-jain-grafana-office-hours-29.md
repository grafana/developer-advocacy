# AI Observability with Grafana with Ishan Jain (Grafana Office Hours #29)

In this Grafana Office Hours, Ishan Jain talks about AI Observability with Grafana: what it entails, factors to consider when ...

Published on 2024-11-09T04:58:38Z

URL: https://www.youtube.com/watch?v=9X7M-bvnZG0

Transcript: hello everyone and welcome back to another gravana office hours yay artificial intelligence has gone really quickly from something that's just in sci-fi books to something that we now use every day at least as part of most of our Lives even if you're in Tech I think lots of us have come across it already some of us might have even tried our hand at creating some apps that are leveraging AI stks Our Guest today is going to talk about observability for AI why we should do it what to watch out for and how exactly to set it up I'm Nicole vanderen a senior developer Advocate at grafana labs and today I'm joined by a colleague of mine isan Jen a senior developer experience engineer welcome isan Hey folks uh nice to be here and talk ofly stuff that I'm really like interested in everyone's interested in to be honest well tell us a little bit about yourself like what actually is a senior developer experience engineer what do you do uh so we work on a like our role is like kind of a w wide stream where we work on like documentation we also work on like small features and we also try and improve the our main goal is to improve like the engineers overall experience like be like the ascod tools like an like uh all the tools that we have like have support for say anable or operators so people can like uh Set uh resources for like in arack Via like asport tools it's like everything is under control uh because this is what like Engineers prefer people don't really like to go to the UI create like a dashboard and do that process every time right so yeah we try to improve that process and experience we try to improve and small uh things here and there fees so it's like a wide uh landscape that we do which is nice and how long have you been at grafana actually I've been at grafana now I would say I joined like two years back oh around 2.2 years approximately uh it's been nice it's been a nice Journey I've learned a lot people are really nice uh which is a question so yeah oh so how did you get from doing what you do to doing anything with AI is that something like how how did you first start to get interested in it Alo I've been actually interested in like Ai and like ml since my undergrad days in fact fun story I uh I wrote like a research paper on Landing like Landing Rockets via AI like how the SpaceX Rockets like land I wrote like a resarch paper on that like one of the underground so I've been like in the aim space for like a long time but as an actual job I was like an SRI before so it was kind of a different thing at completely right um but always like I've been trying to play with um EML and stuff like that so when opena actually came up with GPT in fact gpt2 I was still like trying it out like a lot and once I think they released CH GPD I think everyone in the world knew what that was and just took the hype from like scale one to like scale 10 right um yeah that got me more interested in like how do we actually improve the experience for this of sorts yeah so yeah it really does seem to have exploded in the last few years like I said at the top of the shows like it went from something that's that's like maybe only for geeks or something in very specific applications and it's super high-tech to just like just another thing that we use every day it's so weird but maybe we should also talk about the terms a little bit because I feel like it's talked about a lot but what actually is artificial intelligence uh uh the basic artificial intelligence if I had to say is like giving intelligence to a computer or like a nonhuman or non- living human I guess right yeah and making them like do stuff rather than like us doing stuff right and making our our job simple at the end of the day like be um text or like writing a code writing a book of sorts like if if a computer can do that for us it's just so easy and obviously the you see robots and stuff like that which is like the next generation of AI which I feel will be there like five years um yeah just anything that's computerizing probably try U and trying to make our job simpler uh I won't say like replacing what we do but just trying to help us what we do uh is what AI should be focused on yeah it's funny the word artificial is there because you know when humans use it when we say artificial it just means an intelligence that's not like our own and is trying to trying to kind of make up for that and be as intelligent the way that a human is and that's a tall ask like I know if we're really quite there yet but we're getting close um but there are also like other terms I think that are mentioned in conjunction with AI that aren't exactly the same like machine learning and llms how does that all fit in uh so uh consider AI to be the umbrella and machine learning llms are like the subsets right um machine learning was the traditional it's it was like the hype like like I would say a few years back before llms came into picture um and llms are also if I get into like real technicality are also a subset of machine learning right LMS are nothing but just AI stuff which has like trained on a lot a lot of data to be honest that's just LMS uh text Data uh specifically nml can be um it can be like even like if you train anything like computer to write like what is one like 1+ 1 equal to two for example uh if you train U an algorithm that can also be an ml algorithm of sorts llm is like a model ml is like a concept and algorithm basis concept llm is like a model of s we just just train on lots and lots of data um and that's what the that's I think the big data has like been the thing which helped llms come into place to be honest eventually is the LM wouldn't really have existed right now maybe later on so yeah yeah I think so so for everybody llm is a large language model and I think when most people are talking about AI especially recent applications or implementations of AI they're actually thinking of llms because artificial intelligence has been around for a long time but like you said it hasn't been until recently with when open AI was able to to create to amass this huge data set of human speech and suddenly like what is actually causing us to pay attention is now we can interact with AIS like like as if they were humans and that's because of llms actually yep that's that's correct um and U like even llm have been it's not like a New Concept these I think the first I don't really exact know the exact year but I think I read somewhere that LMS have been like have been released like long time back as well it's just that now it's like close to Perfection or it was just like the amount of data too right yeah and it's just a lot more easier compared to earlier now we have like bunch of data available for people to train llms on just way too tough um yeah it's just and availability of data has improved the entire AI landscape like help the hype more and so today when we're talking about the observability of AI like what specifically are we talking about is it AI in general or machine learning or llms or all of it uh to be honest all of it but depends on the use case as well like monitoring and llm uh is to completely different from like an ml perspective because in uh machine learning you kind of build your own stuff so you have to understand how the how it's working on every training run stuff like that right llm um You most people or the most natural use case that we see right now people use the open AI uh apis to use that right and these are like pre-trained but like open has already done the work for us so what my job as an engineer is just to call the AP and use it directly right so this time I'm not really concerned about like how much um I'm not really concerned about tuning as such but I'm mostly concerned right if I write a prompt is the uh is the llm able to respond back correctly right tuning is still there but not compared to the ml tuning that we used to do U only it was just way too much uh so it's lot simpler um and ml we try to understand each run like when we train an ml algorithm right we try to understand each algorithm like how how do it perform right in llm we instead we want to understand like all right we The llm Works the API works right but how much tokens did it use how much did each request cost because opena is like an API and every API has a cost State how much dat each request cost and there's also like I want to capture like each prompt I want to capture each to uh each response how um has there been any hallucination Hallucination is if I ask like an llm what is say coffee and it replies back football is a nice game right that's hallucination to an extent um so yeah stuff like that we also need to track in LM so it's kind of different in both cases like uh and depends on which AI system are you uh working with right now so okay and also so I guess one thing that I one question that I have is you're talking about using an AI like some you know using open the open AI SDK or something and just calling it calling using that API and so how is that different from like doing API testing or something like that why do we need what considerations what extra considerations do we need to have for monitoring or observing AI apps uh this is actually a question that one of my blog post when I wrote they ask me as a uh as a question like why is how is this exactly different from an API observ to be honest right um in API observability the max things that you would technically it's LM observability in my honest opinion is an extension of API obser like but you just look at look at the data just in a different way um right in AP obility at the end of the day you look at like all right how much request I made how many errors I had and stuff like that right but in LM observability or like AI observability you tend to look at like all right I do need like the request uh the amount of request I made the amount of latency each request had but I also want to look at these prompts and completions which I said U because this is what uh defines my system the the response that I have from the LM is the actual output I'm showing to the end user at the end of the day right so I want to make sure this end output is correct whereas compared to an API generally it's not um shown to the end User it's like in a system it's being used somewhere um so just different perspectives on the collected data might still might not still be the same but um yeah just a few different things that we want to focus on in in AI obsorb or llm observability at the end day but it is an extension of AP and then I guess how how is testing or how is observing AI any different from observing another type of application right so let's consider like we have like a very simple python application roll the dice for example right uh in that like as me as an end uh engineer would just try to focus on like all right how many function calls were made how many um requests were made stuff like that right and how much CPU and stuff uh everything is being used right with uh yeah uh stuff like that in llm everything like this like how many functions were called how many requests were called how much CPU is the application using everything is important in those regards again but comes back to the same thing like the attributes that we collect like these prompts the cost I think cost was something that people were really wanted to understand like how much each request is costing to them U because it varies according to right The Prompt is long like I say hi it might be cheaper compared to I say like Hi how are you as a prompt right which is like more words so the cost would be higher it's how do we optimize that so it's like uh in application you've already go done this work and the next like have absity in there you still improve the performance of the application whereas in LM you're not only improving the performance you're also looking at like the efficiency and stuff like that uh to improve so it's just different attribute and at the end of the day observ it like one single thing it's just um for each different use case you just have to look at different things oh yeah yeah and I I guess you mentioned the cost that kind of reminds me of another thing that that was pretty popular another new technology and it was the blockchain there was also this this thought of like okay there's the blockchain but how do we test blockchain apps and that is also the difference because even if you're not testing the blockchain itself because you're usually also not creating your own blockchain you're reskinning or using somebody else's API the cost matters a lot because it's also per transaction and that's not something that we're used to when we're just hitting API endpoints there's also some rate limiting I know like with with chat GPT if you depending on your plan they might they might restrict how many times you can call their apis right yeah that's true but rate limiting is technically there in like almost every API but in like um the llm landscape it becomes more critical because again this is something that you're showing to the end user and at the end user if you show like this API has been rate limited that's that's not really a good U show right for the people so yeah uh rate limiting especially in like the llm perspective is like really really important um in my opinion and Cost U as because um in like sometime back we were like playing around with llm I remember and I had run an experiment which just the cost just blew up uh right So eventually we were like we had like the, 1500 us or something like that like a set amount and we just ran past it so if you had like cost absity set up with like llm uh it wouldn't have been the case we would have been like alerted long long way back and we we like we would have able to stop that from happening yeah I think the cost the cost also makes it difficult to test right because I'm I'm primarily a tester and when I was testing this blockchain app there was a certain point at which it just couldn't be tested in production without actually costing money and in that case observability becomes even more important because that way you're just you're not actually incurring additional costs like you said it can rack up very quickly and and you're still getting information about how your app responds and you know what the user is getting back um I wonder if there are also additional security concerns here is that something that you would you would Advocate observing for too yeah um there like how we have like uh SQL injection right there's also concept like prompt injection people can like prompt uh something that from a like U there's a bot support bot and if I like prompt something like Reveal Your revenue for for example and the chat bot is able to do that it's not really a good thing right so we also want to stop that um uh observability at the end of the day on it core cannot help with that like you cannot stop people from happening but if you still observ set up you can still see like there was a prom which was saying like reveal me your revenue or sorry right uh eventually you can set up like U blockers of sorts for that there are like different tools in the llm landscape which help you do that um yeah it's it's nice um and people should definitely look at all of these like I think the to is called guard rail or something which is Oh that's oh that's interesting I had thought of that that's another difference from apis like with with apis normally there are specific questions and specific answers you know a request will get a specific response and if you send a request that isn't understood it's just like okay there's an error you know um but with AI I mean the end points are are still the same but what like the body I guess of the request can change so much and the response can also change and yeah I didn't think about the security concerns of that I guess also the the presence of these massive data sets because you want the AI to be customized in some way usually that's why you're building an app you're giving it specific data that's a little bit more fine-tuned to your use case right and so that's another consideration when you're testing when you're observing um for AI apps is that they usually come with pretty big data sets they do um and especially like if fine tune like an llm um which we which a lot of people tend to do nowadays um in that like you do put like an additional data set over the already trained llm so you always want to keep that data like private and not reveal that data to um I mean as long as you make sure like the data that you pass to a fine tune llm is not really the most sensitive like it because at the end of the day L the llm would use that data to give like answers right so if you do put like your Revenue in there it's yeah okay so let's talk about yeah sorry go on yeah there's like U security to be honest like you to set up like security at almost every level the data and the llm uh Parts well yeah which is something that a lot of people haven't really focused on could be nice to have all right well why don't we talk about what we can monitor what we can observe what are the signals that are vital well nice uh for sure so I as you mentioned I've already wrote like a couple blog post about this um but at the end of the day um and this is like a this can obviously like change over the next year or the next six months how fast the llm world has been changing this can the things that we observe today might not be the same things that we observe or might have like lot more things that you observe in the next few months right again U current by the way just to note for viewers that um isan has written a few things about on around this topic and they will all be linked in the description below so check those out there um so yeah uh at the end of day at the end end of the day the four two main things that everyone from like a company's perspective want to look at are like cost prompt and responses right but this is from like a PM's perspective like all right there there's an application running and this is the cost of that application per request right and these are the prompts completion I can eventually improve my application based on these promps because this is what the user is asking so if I can if I can understand like a um like this is the most asked question I could probably T the llm uh to give like a proper and improved response on this right so this is the main three things I would say but over this like this is from a PL uh company's perspective but if I'm the engineer I also want to look at the attributes that I like what the temperature what is the top these these attributes technically help the uh tuning the llm response how VAR how varied it can be how controlled response you want from the LM so there are attributes and stuff like that and there's also like tokens um tokens um how do I explain like consider like if we pass like a prompt say hello that might be considered like one token people like say for symbols are like one token of sorts but tokens are something that determine the end uh the cost of each request so obviously you always want a track cost tokens prompt responses and the attributes that you pass to the to the llm and once you have this data you obviously can build like varies a lot of visualization on top uh do a lot of like all right how how does my input prompt tokens compared to the end of end cost that I'm having right and understand from there like where do you want to exactly improve your application of ss and then obviously you also track like all right am I using GPT 3.4 or am I am I using GPT 3.5 or gp4 or gp4 o and there's also1 now which was released like a few days back by open a right um I also want to see like comparing these how does my prompt my cost my response varies across these right so yeah these would be the main things I would say should be tracked but apart from this like people can track all every a lot of details that is there U but these are the like core attributes I would say definitely you should have and in terms of the Telemetry signals which ones are the most important in this situation um got um in the llm landscape like traces I've seen have been the most prominent compared to logs and Matrix traces I would say because um we have this rag rag based approach where we just don't really call an llm directly people can still have like a right if I know like people are asking like what does um what is llm observ for example right I if I know like people are asking this question again and again I might not call the llm again and again for the same thing because it's like an additional cost I might figure out like all right I I have already stored this in a vector DB somewhere right so I can probably call it from there and reduce my cost at the end of the day so tracing essentially also gives us the sequence of events that are happening um right did I make an llm call or I did not I made a call to the vector DB instead um so this helps me understand like the flow of things as well because LM is like a uh worst lands right so understanding the sequence of events happening uh and it's actually becomes like more critical when you start using like applications like Lang chain um because in Lang chain Lang chain is like an orchestration framework for like llm application is like the most popular uh we have like others as well like llama index and stuff like that but uh Lang is like the most popular so using that as an example uh in that like if you even like like a single function you still want to understand like what's happening underneath like there was an embedding API call there was a chat GPT or uh GPT completion cost uh U API call right um so the this this technically helps us build us the sequence of events that actually happened and then we can actually BR uh figure out like this is my application performance and how do I improve this eventually Okay so the traces are the most important but it sounds like you know you're talking about you know requests request information is still important so metrics would also be something that you would look at and and cost and token counters um but not log so much I mean people I've seen some people also U capture like the it depends on the people to be honest like people can capture these prompts and responses and logs as well uh but I've seen a lot of people do it like capture the within the trace itself because traces can have like strings um so I think people can set up but I think traces are the most common just to understand the sequence of events in logs you can't really understand that perspective um so yeah okay uh let's talk about how we can monitor AI um in normal observability there's always like instrumentation first and then you you know you there are different ways to do that and then at the end there's also the visualization part of it getting the the data that you get from the app um out and making some meaning out of it and it sounds like observing AI is not not that different it's also like that right yeah I think at theend at the end end of the day everything comes out to be the same thing like inum your application uh send like Tres some something some data right to a back and then then you can like visualize the data and build like visualizations on top more if you want I think um even in like AI it's the same thing like you either you can instrument your application via Hotel directly um it just it'll just just be a lot more work for you you'll have to uh do everything on your own there's Auto instrumentation which uh otel also gives out um and that a lot which a lot of people prefer as well because it's like a single line you just need to run and hotel would do like generate races on its own right so that's um there's similar stuff for like llm um as well the llm and the vector DBS there are like bunch of libraries one is like open lit there's also something called like Trace Loop um and I think Helicon and stuff like that I don't think that's Hotel specific but there are like these llm OB SDK that people can like initialize in their application and and it's a single line it's like relatively easy um and just pick up data like every function call that you make or every API call that you make uh to openi or L chain or any any anywhere to be honest like they support like 30 L of so everything is to be honest right okay so so two so two different ways to instrument there's the manual one which is sounds like it's predominantly open elemetry and then Auto instrumentation that seems to have a lot of different things but not that Hotel collector because I is that something that you could do because with otel collector you can get Trace information right but I guess it's not AI specific is that the problem you can still collect with that you like even Auto instrumentation the like base hotel Auto instrumentation you can collect traces it just won't give you the information that we need like um The Prompt won't be captured it'll just capture the there was a request made to open AI CH uh Chad competion of s but it won't capture the exact attribute that was sent the exact response that we received and yeah it obviously cannot like Calculate cost on its own so yeah yeah so I guess our flavor of The oel Collector alloy also has the same issue where it still will capture traces but not all of the metadata you want yep so which is why I think these sdks come into play and these are like Hotel specific SDK they just send generate like Hotel traces uh you can send that to any hotel uh storage back okay and that's open lit and Trace Loop you said yes there's open lit there's Trace loop I recently also heard there something called like Lang Trace but I think every all of them like generate the same Hotel traces at the at the end end of the day and yeah um the these just help us like make it simpler like it's a single line I think for most of them it's a single line open letter for sure no open letter T loop I know like it's a single line you just instrument your application and just generates Trac and stuff like that for from your application in one go it's like a one click llm OBS I would say so yeah okay so that's how to instrument it why don't we why don't we jump into the demo and see how this all comes together what are you g to show us oh I I would actually show you something that we built like late last year we built like an open integration Cloud we built like an SDK for that and it was not Hotel specific we this was a time when G I think Chad GPT just got released and people are still looking like looking to some like I was still want to look at cost at the end of the day right for my API calls I'll show that and how we've like um how the story line has went and we've built like this complete like AI observ integration in cloud and how the how that helps uh people like track this is specifically for the llm landscape for now but eventually it can be more um so and what's the stack that it's using so grafana of course is it using like Tempo Prometheus I'm using Cloud for the demo but yeah Tempo at the end of the day is like the trace backend so U Tempo and for Matrix you can use fromus MIM anything Works to be honest both the same thing to okay all right let's get started nice uh yeah uh I could show I can share my screen should be C right um so this is actually I should start from um there's this this is like cloud and we build like just to give you the the entire like storyline I've been how involved I've been in this uh from the beginning right uh so we released like this open ination it just helps you track every information but this was specifically for open end this was not Hotel specific at all were making like API calls to Cloud uh directly and this just helped people like track cost prompts tokens stuff like that right um and but the only biggest problem with this was um since this was not Hotel people were still like hesitant uh which a lot of people in the obsorb landscape nowadays are right um so I always wanted something which is like Hotel specific can generate races and stuff uh which makes it simpler for people and I really wanted something that's like a one click or like a single line that people need to add rather than like a five or six line that people need to add in their application code right um so I need to simplify that as well which is um which I'll talk about like how do we' did that uh or not yeah I'll just show the setup of sort that we do so this make your screen a little the text on the screen a bit bigger yep sure this yep um so this is like um sample very very simple actually can can you make it a little bit bigger it's probably huge for you but yeah that's great thanks um so yeah this is like a very very simple Lang chin application this this is actually something you can actually also found find in the langin docs like a quick start docs this is the same example that they use and I wanted to use this as an example because you can make like an application as long as you want um but yeah this uh at the core what it does like langin as I said is like a llm orchestration library right you can do a lot of stuff in this one I'm just making like a direct llm API call uh I've open a in initialize here and I just invoke my message and it should ideally give me a response oh that's nice uh just what you want in a demo right set my open key oh no now we're going to see it oh I mean I can remove it eventually should be a problem I can remove it like uh you remove it before this goes live yes uh so yeah let me and this is like a personal open to so does not really matter [Music] this oh my bad I mention it as open AP instead of open AP oh hope this works this time so is this a local this is just a local app yes yes this is like a Lo uh this not like I'm running this locally but at the end of the day still making like a u API call to like open and that's what we see you misspelled response by the way I did the 19 yeah there you go yeah so ideally what happened is like I made like an U API call to I passed like the prompts to the llm and it invoked invoke like oh it made the API request to opening eventually right it went to the openi servers and I got the response back which is something oh you got Chia yeah which is the translation of high in in Italian right so I know that this is working well right and now I also see that the response does have like tokens and total tokens and stuff like that right um which I can use I can track this is like important important information for me so if I had to build like instrumentation on my own I'll just capture the response take out these input tokens from the response uh these all the tokens output tokens total tokens and build like an obsorb system on my own if I have to do that um right so but that that'll be like a longer game because you'll have to do that for like every llm that you use open a is like an example but eventually if you want to shift to like any other llm right that's like more work for you right um and plus if you're do using like any specific stack and you change the stack it's just a lot of work you at the end of the day same thing right so open L something uh as said like one of the most popular llm observability libraries SD there and it generates like Hotel traces captures every relevant information that we want to and it generates a lot of stuff which uh I personally really like it can be sometimes more for a lot of people if I'm like a if I don't really care about the temperature and stuff like that it might not be useful um but they capture like almost every detail that you can capture from the request and the response uh which eventually while debugging can be like very diff um so I I initialized this um open it in my application it's like a simple SLE Library obviously you pip install open it s in your environment first right and nice and yeah this is the single line I was talking about like you just write open L do in it and it initialize the entire uh setup for you at the end of the day so once we run this um I'll also tell you what we l really pass any hotel endpoint or hotel headers where is this Trace going to be going out to right so the good thing about this library is that if you don't really pass any Endo of suchar it prints the entire trace on your console on your in your terminal so actually can see how the trace would look like right and I can see every detail that was traged uh is like and you can see the cost they do like cost calculation at the back and this did not really take a lot of time to be honest so yeah a lot of people are like also concerned all right if I add like an observ SDK in my code will that like add more latency to it otel is like relatively fast so um latency is like very a very very small issue to be honest the gains that you'll have by adding like a 0.1 Ms latency is a lot more to be honest it just prints everything and yeah you have the these are the traces these are metrics of sorts you can also see like each metric and now yeah this is the most basic example I also have like a kind of a bigger application you can see my authentication tokens here but this is fine this can be removed eventually uh that's fine uh yeah um now I I realized like right uh this my application is working and I can see like all right the cost is also not that much for each request right now I want to promote this application to like I I want to send these traces to like a cloud back end like our cloud um right so I add the hotel endpoint I add the hotel headers and I also actually can like add an application name and environment this can be like staging um and this can be like U hello world I don't know that's an application name um but yeah uh let's I can give an application name and the this is the same thing uh from the last application we just have like an llm call and I also added like a vector DB AP so that we can track like the entire landscape this not like the final application this just to give an example of the things that you can track with just like a single line of code um to be honest this like you some people might say this is side line but this is like values of argument yeah um so once I I I already have this application running which is like um sending like traces and stuff like that to my cloud back end which is this so you can see like I made 235 request since like last which is like a lot uh so my cost is probably going to go up I should stop um right um but I have this application running it's been running for a long time I can see the tokens and stuff on the console but now if I want to track it like long term uh I have this dashboard which comes out with the a Integra AI observ integration which I'll talk about uh and yeah you can actually track like the request rate that you have per second the total usage cost average cost per request this is like a very important uh like visualization for a lot of people like average cost per per cost is something that's really important and um yeah so actually tell us can you tell us about this dashboard because I think you told me that this is an open lit one that they that they created already yeah uh so it is like a very the I really appreciate the work from their team it's like an open source uh repo repository so I think it's a yeah they they build like dashboards for us which is like really really convenient because a lot of times you use these sdks but you the the data would be available right but you still don't know how do I see the data yeah yeah prebuilt uh dashboard for us so which helps me get started um my use case might not be fulfilled with this dashboard at the end of the day right but it gives me a starting point I can see like request I'm making per environment per application or like what's the average users for I might you this uh eventually change this U but yeah this is like a starting point and now I know what the exact traces attribute and everything I I am I have which I can work on right otherwise just you have to go to explore you have to see each trace the collected information and then build something of your own which sometimes can Happ um a bit of a headache to people uh right so yeah yeah uh in this dashboard like you can see a lot of stuff like each uh request that you make you can just click on it it'll guide you through Tempo um and yeah you can see this uh entire attributes here which we also saw in the console which was printed like the same attributes and the values that you can see here and this is now like track since this is in your observ system you can eventually come back like a week later and see how your uh your application is doing at the end of the day right and make more improvements if you want to um right uh let me back up a bit uh this panel I think I've been using the same model for like a few minutes but I think this should yeah this shows like compared to like I've used gp4 n times I've used gp4 or 246 which is crazy um so yeah U and this also help me compare by environment all right I had some default environment running and then I have a demo environment running um and yeah this is like an average token cost like sometimes uh you do want to understand the how does like token to each token compared to like cost is there a relation between is there not because sometimes when you tweak like temperature and topy tokens might still be the same but your cost can be go like go up a lot so still important for you to understand what's the reason for the cost going up or what's the reason for tokens going up eventually right um this is the value having a dashboard like I wouldn't have thought that that was important I would have thought why do I care about tokens I just care about cost yeah uh same like cost and tokens go hand in hand most of the times uh so it's important to always keep a track of that like what's exactly happening um and yeah we also said like ve llms and Vector DBS are like go hand in hand nowadays right people St use it for temporary storage right um and yeah here in the same dashboard you can see right how many Vector DB calls I made I made 250 Vector DB call since I started the application an hour back and this is like the same thing but if I added like more application names like all right I have one other application say chatbot running I'll see like all right how many DB requests were made by this application compared to this application and yeah same like we had these requests for llms we can also track like each request for like a vector DB and see what attributes are we tracking we we have already the dashboard built for that but if I want to go into specific request and go look at that um I can see like what was the DB filter what was the DB operation it was delete operation right which helps me debug uh why is someone running or delete operation you know application rate why this happening so I can understand the sequence of events eventually uh when I have that and yeah this is like the dashboard of s which their team has built and I also kind of like it this um the reason I uh I prefer this dashboard is kind of similar to the open a dashboard that we built like long down time back like we for people like still nowadays like cost token prompt and response are still like the most important um attributes that they want to look at so if you can highlight those up top so it's easier for me while debugging that's a lot simpler uh at the end of the day so yeah uh this helps a lot of people you can obviously the change this uh dashboard as long as you want like you can move this here if you want if you don't really care about that that's your wish right so yeah just fine great so how can people get started with this it's a grafana cloud integration you need an account a graph cloud account to do it um to use it but is it like is is there a free you know can use it for free yeah um the the good thing about this open L ISD this link uh to their repository um yeah I think do have docks and I think they build like very nice docks which does say these specific docks and these technically guide you to like deploy a collector or deploy a Loy of sorts you can send like traces and stuff like that there and then to Tempo and other backends and then visualize I think they have the similar dashboard for this as well um which makes it simpler so yeah um since it's toel you can use Tempo at the end end the day in OS um cloud is just um simpler um since it's s it's hosted so yeah um but yeah the core tool Lings is like Tempo mimir uh which you can always run in OSS and send traces there and M okay oh that's great um is there anything else that you'd like to say about observing llms uh yeah I mean uh this is like just the beginning I would say this is not the final solution at the end of the day people like still want to improve the system then this this information might still be not be the final thing like I still like even response like how I show like you can track like each prompt and each response you still don't know if the there's no automated system to tell you like the LM is like hallucinating right how is the response there's no automatic scoring done on this uh these responses compared to like say if I know if I know my chatboard should what my chatboard should answer to like say a question which is like what is llm obility it should answer in a specific way but my llm is not answering in that specific way I should have some sort of an automat automated scoring of Stu right um and I so you would have like an AI monitoring an AI uh that's actually a thing that's actually a thing people people actually use like an llm to actually score for the output from another llm like e a lot of e algorithms use another llm to evaluate an llm which is like kind of crazy yeah you can't really it's hard to do that automatically without using something that can understand it dynamically you know yeah and there's also security concern like here I I have the absor built but I still don't have that security layer built like I can't pre like stop uh people from injecting my LM from for like private information I have to set like G for that beforeand so it's like observ like one part you obviously need like security and this like automated scoring which it's called evaluations evals um so you need three of these to set like a complete uh in a spectrum EV technically sit kind of close to observability because at the end of the day you'll also be tracking those that information somewhere uh I think that's an extension that these SDK might add eventually I don't really know but that's an extension I see like evolve the evolution of llm of to also contain like EVS within there uh this tracking of s I don't know how real time it'll be yeah yeah that'll be really exciting but if somebody wants to get started with this where should they go right uh that's a good question so um if you're in Cloud uh yeah you just search for AI and you'll find this like AI observability integration under like connections connection search for AI and you see this AI obsorb it's the same instruction you pip install the open L SDK um you create like a graph Cloud token create some random policy name po token and it gives me oh yeah uh which I didn't really talk about um here I passed like the hotel endpoint of the hotel headers from the python application itself but some people prefer like in hotel it's generally recommended recommended to set like otel uh environment variable these are the standard hotel environment variables that people can set so which is the good thing I liked about the open SDK because they support like this uh the C uh the cnv and people can set this and and it's still like a single line initialization for them at the end of and you just click install dashboard and you can see the dashboard eventually this would be an empty dashboard because this is like a fresh St this is like a different St so you see an empty dashboard but once you have like start sending data it'll start to look like this so yeah um yeah this is like the um a observability uh ination that people can get started with this for now this works for generative Genera contents like llm DBS and Frameworks like L Lama index um so okay great and I'm going to leave the links to all of these things everything that he's mentioned in the description below so you can check those out all right thank you so much for coming on and showing us all of that isan that was great oh love love showing [Music] people all right and thank you all for watching this as well see you next

