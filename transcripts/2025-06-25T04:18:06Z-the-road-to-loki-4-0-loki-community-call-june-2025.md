# The Road to Loki 4.0 (Loki Community Call June 2025)

Published on 2025-06-25T04:18:06Z

## Description

In this Loki Community Call, we welcome back Ed Welch, Principal Engineer on the Loki team. We will be discussing with Ed what ...

URL: https://www.youtube.com/watch?v=LWDeIHfAC9A

## Summary

In the June edition of the Loki community call, hosted by Jay Clifford and Nicole Vanderhahoven from Grafana Labs, the team discussed significant upcoming changes for Loki 4.0. They were joined by guests Ed Welch, a founding member of the Loki project, and Charlie Bernett, the new product manager for Loki. The discussion centered on the transition from a row-oriented storage system to a columnar format to better support analytical queries while maintaining the ability to handle unstructured logs. The introduction of Kafka for data ingestion aims to decouple read and write operations, enhancing performance and scalability. They emphasized that while the architecture is undergoing a major overhaul, current APIs and user interactions will remain consistent. The team anticipates a gradual rollout of these features, with a potential release around the next GrafanaCon, likely in mid-2024. Overall, the session highlighted the evolution of Loki to better meet user needs in observability and analytics.

# Loki Community Call - June Edition

Hello everyone! Welcome to the Loki community call for the June edition, and a very exciting call at that—the road to Loki 4.0. My name is **Jay Clifford**, and I'm a senior developer advocate for all things Loki. I'm joined by my partner in crime, **Nicole.** Would you like to introduce yourself, Nicole?

**Nicole:** Hi, I'm Nicole Vanderhahoven, also a senior developer advocate. Interestingly, we all said our headlines differently! Apparently, Jay is a squiggly, and I'm an ant, or as we say in Dutch, an *apistache*, which translates to a monkey's tail. That’s a cute little ant symbol!

We have two guests with us today. First, we welcome back **Ed Welch**, who technically hasn't been in a Loki community call since Jay and I took it over, but he has been in Grafana office hours. Welcome back, Ed!

**Ed:** Thank you! I'm excited to be here. I've been working on the Loki project for about six and a half years now, so I'm one of the founding members. I'm looking forward to discussing all the exciting developments and changes coming in the next few months—well, I might say months, but it’s more like a year.

And hello to **Charlie**, joining us for the first time! Charlie, welcome to both Grafana and the YouTube channel.

**Charlie:** Thanks, Nicole! I'm really happy to be here. For everyone who doesn't know me yet, my name is Charlie Bernett, and I'm the new product manager for Loki and Logs drill down. I'm here to help with anything related to logs at Grafana and answer any questions.

---

**Ed**, it's been a year since we've last seen you. How have things been? What have you been up to? Has it been a lot of customer communications?

**Ed:** That’s always a harder question to answer than I wish it was because I feel like I work a lot, and it’s challenging to have a big body of work to show. I’ve spent a lot of time with our customers, our SaaS product, potential customers, and our community, focusing on what we need to do with the product to move it forward.

The last couple of years have been significant, particularly the last year, which has been a big inflection point for the project. The design of Loki as it exists today is impressive—we’ve accomplished a lot with a relatively simple storage and indexing model. We leverage cloud computing and parallelism, which are fundamental concepts for building a log aggregation database. 

We take unstructured log lines, store them, and take pride in testing the limits of what object storage can do. We continuously look at improving the trade-off between simplicity, economics, and performance. A big part of this is making Loki affordable to run while meeting evolving use cases.

The original design of Loki focused on helping developers and operators quickly find logs, particularly in the Kubernetes world alongside Prometheus. We borrowed the same metadata and indexing model for cataloging logs. This approach has worked well, but as we move forward, we realize that the user base has expanded significantly, with many users not as familiar with Prometheus or Kubernetes. This change has prompted us to revisit some architectural choices to better suit a wider range of use cases.

---

**Nicole:** So, what is it now that people want to do with Loki that we couldn't do at its inception?

**Ed:** I want to nuance that a bit because I don’t want to imply that Loki can’t already do many things. However, the economics of how Loki was built can be a barrier. For instance, we see more requests for analytical queries, which we didn’t explicitly design for initially. Loki was built for developers and operators, but we are now seeing more analytical use cases, such as examining log trends over time.

The nature of these analytical queries often involves looking at larger volumes of data, which can be challenging. We have clusters ingesting hundreds of terabytes of logs per day, and the nature of querying and processing this data needs to evolve to keep pace with user demands.

---

**Nicole:** That’s a big change! So, moving towards more analytical use cases means changing the architecture of Loki?

**Ed:** Yes, we are moving from a row-oriented storage model to a columnar model. This shift is significant as it allows us to handle larger-scale analytical workloads more effectively. While querying log lines will still be a priority, we want to ensure we can also perform aggregations efficiently.

We're looking into changes that will allow us to store data in a way that makes it easier to query specific columns rather than entire rows, which can be inefficient, especially with large datasets. This shift will ultimately allow for faster analytical queries, as we can reduce the search space considerably.

---

**Nicole:** It sounds like there are many changes happening. What about the query side—are we expecting any changes there?

**Ed:** Yes, we are ambitiously building a new query engine. We want to ensure that the changes roll out incrementally, so we don't overwhelm users. We're focusing on improving how queries are executed and introducing a proper query planner with execution plans, which will help with performance optimizations.

We will continue to support LogQL, and our eventual goal is to have support for SQL as well. This will make it easier for users who are familiar with SQL to interact with Loki, particularly as it becomes more analytical in nature.

---

**Nicole:** So, all of these changes are in line with making Loki more usable for both current and future users?

**Ed:** Exactly! We want to make sure that while we are enhancing Loki to meet new demands, we also maintain the functionality that current users rely on. Moving towards a more structured and analytical approach doesn’t mean we are abandoning our roots; rather, we are evolving.

---

**Jay:** Before we wrap up, can you give us a brief overview of the timeline for these changes?

**Ed:** We’re looking at a timeline of about a year for Loki 4.0 to become fully available. We will be rolling out various parts of the new architecture and query engine incrementally. We’re already running these changes internally, and we hope to provide some experimental configurations for the community to engage with as we progress.

---

**Nicole:** Thank you so much for the insights today! It's exciting to see where Loki is headed. We’ll keep our community updated with more news as it develops.

**Jay:** Thank you all for joining, and we’ll see you next month for our next Loki community call. Have a great day!

## Raw YouTube Transcript

I think we're live now and there's a little squiggle. Hello everyone. Welcome to the Loki community call June edition and a very exciting community call at that the road to Loki 4.0. My name is Jay Clifford. I'm a senior developer advocate for all things Loki. I'm also joined my by my partner in crime, Nicole. Would you like to introduce yourself?
 Other way. Is that Hi, I'm Nicole Vanderhahoven, also a senior developer advocate. And also, we found out that we all said our headlines a different way. So, you know, apparently Jay is a squiggly and I'm an ant or in Dutch we call it an apistache, a a monkeykey's tail. That's a cute little ant symbol. But we're all at Grafana Labs today and we have two guests. one that we told you about and another one that just decided to join us and we're super happy about that. Welcome back to Ed who technically hasn't I think hasn't been in a lowkey community call since we took since Jay and I took it over but has been in a graphana office hours. Welcome back Ed. Oh, thank you. I'm excited to be here. Could you introduce yourself before we move on to Charlie?
 Yeah, fair enough. Fair enough. Uh my name is Ed Welch and uh I've been working on the Loki project for I don't know six and a half years or so now. So one of the sort of founding members and and excited to talk about uh a lot that's happening and a lot that we're going to change in the in the coming months and well yeah we'll say months more like a year but we'll say months. Okay. And hello to Charlie for the first time. Charlie, welcome to both Graphana and to the YouTube channel. Thanks, Nicole. Really happy to be here. Um, and to everyone who doesn't know me just yet, uh, my name is Charlie Bernett. I'm the new product manager for Loki and Logs drill down. Uh, so anything logs related at Graphana, that would be me. Um, happy to be here, answer any questions, and just here to be helpful. So, great to meet everyone. So, Ed, it's been a year, as Nicole mentioned, since we've last seen you. How's things been? What what have you been up to? Has it been a lot of customer communications? Where where's sort of like a lot of the work for you on Loki been as of late?
 Um, this is this always a a a harder question to answer than I wish that it was because I feel like I work a lot and it's hard to have a like a big body of of work because I I spend a lot of time
 healing though or is it for real? Yeah.
 Right. Um, that's a great question because it I spend a lot of time sort of, you know, with the Loki project as a whole, like our our customers, our SAS product, um, potential customers, our community, um, you know, and, uh, you know, thinking about, I guess, like what what we need to do with the the product to kind of move it forward. Um, and it's been I mean the last couple years, but the last year in particular has been like a pretty big, you know, inflection point for the project and a lot of the design that went into it. um you know Loki as it exists today is is pretty amazing with what we've been able to do with you know a relatively simple uh storage model in model indexing model and you know leveraging cloud compute and uh parallelism and uh you know I guess th those basic concepts for building a log aggregation database where we you know take unstructured log lines and store them and you know we sort proud to say that we've you know been able to test at least find most of the kind of interesting limits around what object storage's capabilities are and and you know have made some changes to Loki along the way and and continue to look at that now and to you know getting the best uh kind of trade-off of simplicity and and you know economics you know I guess a big part of this is making Loki affordable to run um and in performance you know in meeting the use cases of an evolving you know uh I guess evolving world right like we built Loki uh original design was this tool for finding logs as a developer and operator you know there was this gap I would say that existed in probably particularly like the Kubernetes world um you know alongside Prometheus for sure like if you you know had metrics and you're like I just want to be able to quickly find my logs and that was you know why the the sort of tag lane you know like Prometheus but for logs was created because we borrowed the same sort of metadata and indexing model for for categor you know cataloging or categorizing your logs and storing them and um I think that landed really well like we we were able to uh leverage the existing Prometheus community as a group of adopters people that are familiar with the query language and the concepts and you know in a Kubernetes environment where you can use the service discovery of Prometheus we you know bolted that into, you know, Promtail at the time and um made it really easy for people to get logs into a system across, you know, you know, huge amounts of Kubernetes diversity um and quickly find and run your apps. Um but you know, move forward, you know, like I said, I guess seven years later. Um and you know the the kind of growth of the product, the adoption of the product, the customers we work with, the scales, um you know, all of that has has changed or been augmented. I guess maybe like there's still the core parts that are the same. And there's still the exact same folks that, you know, are heavily into the Prometheus ecosystem into Loki, but there's way more folks now that we run into that maybe aren't as heavily into Prometheus or at all or, you know, aren't using Kubernetes or just ask a lot of different questions around their log data. Um, and so that's kind of brought us to where we are now, where we want to, you know, kind of revisit some of the architectural choices of Loki and and you know, likely change many of them to suit, you know, more use cases and more users. And yeah, that didn't totally answer question of what I've been doing in the last year, but I guess like part of it is,
 you know, talk about that all came together. It's like uh we have grown Loki massively from its inception and people have foundationally done some amazing stuff for Loki and now we've got to a point where people have done what they want to do with Loki now they're way more excited to build upon that. Um and I think that's why we're all sort of here today and I think that's why we talk talking about where you want to go with Loki 4.0 and you kind of sort of touched on it there. Um, so what is it now that people want to do with Loki that we couldn't do in its inception and what's kind of that sort of focus on what you want to improve going forward with these architectural changes um going forward? I mean, I'm going to nuance that position just a little bit because I'm an engineer and you know, I don't know, it's what I do, I guess. But, um, it's I wouldn't say that Loki can't do it. I mean, you you could make that argument and I I again I'm I'm nuancing it, but it's it's uh you know, I think the economics of how Loki has built is what becomes a bit of a barrier. And and also like you know I guess I can put it in more concrete terms as well like um you know Loki as a as a database can do you know to answer your specific question first uh a lot of the query types that we're seeing you know more requests for what we would call analytical which um maybe I'll try to define a little bit here like you know I talked about Loki was built for um developers and operators to find their logs and but but we do analytical queries in the sort of form of of operating our systems, right? Like we look at log trends over time like we look at you know dist distributions by users or orders or transactions you know like those are all part of the troubleshooting process. Um however they're you know in a lot of cases like in a in a context of of finding problems are sort of time bounded or or scope bounded into a specific application um or maybe a narrower time range around like an incident. Um you know maybe sometimes you run longer queries to look for trends to see if you can spot like a change in a pattern. Um but you know those types of analytical queries like exist and they work you know they work well with Loki today. Um you know what happens is you you discover with which is fantastic through the growth and success of our product that like you know the scale is a really relative thing right and like um you know we've got you know I guess tweets or messages from years ago or we were excited about like you know various milestones but like um you know now we run Loki clusters that ingest hundreds of terabytes of logs a day and you know run on tens of thousands of cores of infrastructure and like um you know you can always find somebody that's like you know can you do double that right or triple that right like it's it's you know it is it is the the sort of nature of like the size of the infrastructure that our computing runs on today um and then you know combine that with you know maybe a use case that was explicitly like not you know I guess in Loki's original design doc we kind of said we weren't solving for like high volume events like like an access log or something like that and um but we want to and you can you know it's It's just where it so so so my nuance part of this is um it's sort of a volume question right like the brute force approach that we built into Loki um is surprisingly effective to very large volumes you know like like hundreds of terabytes of data can be brute forced um you know practically you know singledigit terabytes like in in reasonable queries but like it's not unreasonable to do 30-day searches on hundreds of terabytes of logs with Loki um but it does take time. Um, and you know, when you start looking at the way Loki stores data in rows and we process and read every row, um, if you think of something like an access log where a lot of times maybe you're only looking at, you know, sort of counting by IP address or a fraction of that data. Um, you know, it doesn't make sense to read that entire row for um, you know, and and so while it works, it doesn't work well enough for really large scale. Um, and that would translate down into lower scales as well, but you'd just be surprised how fast you can still do stuff like that if your volumes are kept within, you know, gigabytes, hundreds of gigabytes or single digit terabytes. Like, it really doesn't matter the kind of queries you do. um when you start getting to tens of terabytes or hundreds of terabytes like you know either additional indexing or really what we're I I'll just kind of segue into this I guess is is columner storage is generally like the way you handle largecale analytical workloads where you you know need to schematize the data in some fashion so that you can store it in columns and columns are favorable because like that IP address example you can fetch just the IP address column which is a fraction of that whole log line and it reduces this is the scope of the search space, right? From maybe hundreds of terabytes to hundreds of gigabytes and now you're back in that game where like that's very fast, right? Like you can, you know, it's already fast for Loki to process hundreds of gigabytes. So, we reduce the space. So, so the the kind of you know circle that all back around the the change here has been a lot to do with um you know us expanding into um you know larger and larger circles of the logging space starting you know where we were in this sort of Prometheus and Kubernetes world and um you know expanding that out into areas where there's maybe less Prometheus or less Kubernetes still plenty of that for sure um and increasingly larger volumes combined with you know user users and companies that use their log data for security and analytics on you know business performance and and they do a lot of support cases where like I need to go find excuse me I need to go find the you know the transaction ID from a particular transaction for something that happened you know like needle in a haststack search cases um so it's kind of an evolving situation where we just you know the the time has come that I think we want to you know revisit how Loki works to, you know, make the best use of, I don't know, make it perhaps a bit more complicated and hopefully not too much more expensive to run. But in the trade-off being that we can search, you know, smaller data spaces and effectively search larger amounts of data. Um, yeah, I think that maybe circled it back around. Let me me pause and take a break.
 Yeah. So I I think um this is a pretty big change for Loki not just architecturally but also in the direction but it's a direct it's a direction that we're going in based on current usage. So like Loki was really built for the observability use case where we need the exact lines, the log lines and so Loki was row oriented because that's what we needed to retrieve. And now we're seeing that there are more analytical use cases where it's like well maybe it's not just the the the actual log line, it's maybe we're doing sums or aggregations over all log lines over a certain period. And that's where Loki has not been too great at because when you're talking about aggregations that's columner. So if you think about a table right a row is like the log line. So that's OLTP like o online um online transaction processing and um retrieving the column retrieving by the columns that's more like if you're aggregating that would be more OLAP right so that's analytical online analytical processing and so we're switching basically from row oriented to columnar um that's a that's pretty big how
 yeah I mean there's more changes swimming as well. For sure. Um, but you're right, like the the the
 um, you know, this sort of pivots Loki more in the direction of an an OLAP database. Um, I I hesitate to say that we're going to build an OLAP database. I mean, like we're still largely just targeting um, you know, this sort of space of logs and observability, but I mean business observability is a thing you're going to hear folks say. like there's this version of like how do I you know and that's essentially like OLAP right like it's so so we're all kind of playing in the same space um you know are we looking to build a general purpose database um not necessarily like you know Loki in many ways already is a general purpose database a time series database for strings um and it can do some really cool stuff um but we didn't necessarily target the APIs around you know bulk access or machine to machine and you know there's some questions for us to answer yet about you know would we consider that now or how would we consider that um you know largely it's meant for human interaction you know with with synchronous queries and and sort of immediate results um and you know I think that will largely stay true like this is still a tool that's meant for humans to solve problems um maybe less so as a machineto-achine database um but we will we'll see on that a little bit um but yeah the the there's a number of things that um are kind of coming along with this um you know the row oriented storage of Loki and just to touch on a point like the downside of this kind of trade is like the thing that Loki does really really well which is quering log lines becomes a lot harder in a columner store so so we're you know looking at how to sort of take what we do today or even parts of what we do today just to make sure that we can have that great experience around fetching log lines um you know that's there's there's always some trade-offs when you make you know decisions on how to store your data. Um but coler storage um we are you know basically going to have to change the storage format that we use for Loki and um right now we're looking at and and sort of prototyping our own version of this. We're calling them data objects. Um it's heavily parquet inspired but it's not parquet. Um, and it's maybe a good point to sort of touch on that subject, right, of like why not park? Um, and you know, it's possible we still might use parquet. Um, you know, I'll give you the sort of short version of that answer, which is, um, parquet is the sort of layout of the data. There's there's a few things that um make it not a great fit at least at the early layers of how you build a database like this. Um it's very strictly schematized, right? So a parquet file has one schema over all of the data within it. um it was built around the idea that it would exist on a disk and in which case um IOPS were not sort of considered as a cost right so like making multiple requests into that file um didn't cost you more money because like you know doing a seek in a in a read on a disk doesn't cost you money it costs you latency you know I mean it cost you things right but like you know we build an object storage database And so, you know, if you put these paret files in object storage and you need to go reach sections of them, um you you pay per every IOP, right? You you pay in in the sense that there's increased latency because um object storage is, you know, incredibly fast, but it's time to first bite is not what they're great at, right? So and that varies a little bit between cloud providers but um you know so having to do a lot of small IO operations gets very expensive and is not very fast and so um you know parquet can work in this environment but you know our opinion is that we can do something a bit better. Um the other consideration has to do with that schema part where you know if you think about you know log data um the concept of schema is a is a bit overloaded and a bit confusing and maybe we can dig into that a little bit but the you know Loki historically like we just take these you know strings and we store them you know as they come. Um, open telemetry has, you know, through its adoption and its its ideas has sort of, you know, built more structure into observability data. Um, and you know that kind of helps because you know especially for like a columner or storage like you now have, you know, a bit of a schema attached. Now the downside is it's not a strict schema, right? Like you you have some elements that are strict, right? But then you have these kind of attributes which are are um I guess like I you know much to my like dismay like they're they're infinitely complex like they're they're kind of not today like but this just sort of seems to be the inevitability that folks really want to support infinite complexity and attributes. So what I mean by that is like you know instead of key value pairs which is you know I guess my opinion on what metadata should be it's like you know it could be an array it could be a map it could be a map of a map of a map it could be a map of an array of a map right like like you can have these you know arbitrarily complex metadata objects um uh which is very very very hard to store or represent in a database especially because it you know if it's not like if it were one of those like this is the sort of schema in its complexity and it was always the same you know you could architect a storage that you know is favorable for that but it's not right it's it's up to anyone building an SDK for open telemetry or whatever that can add so so the plus side is there's the plus side is there's some amount of structure that OTEL adds which is great the downside is that they then allow for infinite complexity and metadata which is extremely difficult to manage on a database and um you know you you you have limited options right like so what we want to do with Loki is we want to flatten out complexity or or stuff as much as we can because the more you flatten out so even remove OEL from the conversation for a minute and just think about a JSON payload and a log line it's the same you know essential problem right like if you have a just set of key value pairs um they're very flat they're very easy to turn into columns and then it's very easy to then query on those columns um if you have a complex JSON structure with multiple nested fields fields and arrays um it now becomes very difficult to flatten that out you know and and so this is just sort of the difference between like I don't know like what is the the idea of a log data right versus what is an application storing state in log lines you know
 so so Ed sort of like but I guess sort of like surfacing this in in Loki concepts a little um to for people that there might be a few people that are not part of the hotel community yet but using Loki in its current form So we introduced structured metadata and we still have this idea of um like you know a schemalist design for Loki in terms of the new storage format. Um what what does this kind of look like? How do like if we if we looked at it in forms of columns? How do how can we still be flexible with the the body of the payload and then is the structured metadata columns? What what does how does this kind of sort of bubble down into the the storage layer? Um
 yeah so so our goal and this is this will put a pin in the end of that parquet discussion right is that we want we want to support essentially very wide very sparse objects um because that's the what ends up happening like we don't um I guess we want the option for data to be schematized in a sense right like like where you have you know access log data or something that's you know very consistently the same um but what we're likely you know going do is just you know upon ingestion do JSON deserialization and log format parsing and extracting out um as well as the attributes and things that come in already structured as part of the hotel payload um and make a bunch of columns and store you know sort of create these very wide very sparse objects which um which is just hard to do in parquet. So that's why we're you know likely not to use parquet. Now that the other end of that is like when you have things that are or you you know build something that is highly structured or have something highly structured like they can be a really good fit for parquet because um you know there's a whole ecosystem out there of tools that sort of know and understand how to read paret files. So um you know I could see a world where we can like export data out of Loki into parquet because we would then be able to like know the schema and create the files um and then ship them off and and you know at that point like we're less concerned about those sort of you know access nuances of like how you read parquet from object storage because that's potentially some other systems you know problem to deal with at that point if that's the interchange format that's sort of most universally agreed on. Um, so yeah, we want to take, you know, the data that exists in your log content or perhaps it's now been pulled out into your hotel metadata and store those as columns. Um because then that allows you to do all of this sort of like I say predicate filtering, you know, like these key value filtering on your queries that um let us narrow the search scope space, you know, significantly to make analytical queries or any kind of sort of metric query, any query that like will look at specific columns like you know significantly faster because they touch way less data. So Ed, as as part of this, we're also going to be replacing the index gateway, right? With something that I believe we're calling the metas store, is that the name? And also, what are the changes there? Change it. Um maybe I'll start I I'll back up the chain a little bit because there's one change here that I'm I'm nervous will be uh you know the community folks will be unhappy about which is we are changing Loki's ingestion. Um Loki today is a is a replicated quorum system. So you send it um a push payload and then we replicate that data to a quorum of ingesttors. you know, almost everybody would be doing essentially a a three replica set a replica set of three with a quorum of two. Um, and when two of those three replicas say, "Yep, I got this." We respond back and, you know, give a 204 because we chose 204 and not 200. Um, and that right is accepted and it's immediately available for querying like the moment that you get that response code. Um, so that's nice like that part of it is is nice. you got this sort of immediiacy and you have this um you know I don't know like like sort of self-contained because the the what I'm going to say is um we're changing that and we're introducing Kafka um and the primary reason for that is the replication the way we do it um is expensive. It's hard right? So you you have you know ingesters now that have three times as much data as they actually need. Um you know and you're serving recent queries on that which are aggressive right like typically you know recording or alert rules or most rules are done on very recent data. So you have to ddup all of that replica set. Um it's I I don't I mean I guess I don't know if I have an opinion on operationally if it's sort of you know you know easier or harder to you know to be stable than Kafka like I think they're you know maybe similar in that regard. Um for us it has a lot to do with where the replication happens. Um and you know the what we have today that replication happens you know at ingestion and then we have to then deal with it essentially with all querying um and we create objects from those ingesters that go into data storage and and those are you know replicas right and there's a process in within Loki that ddupes some of them but not all of them. So we do end up with a fair bit of sort of replicated data in storage and and you can build a compactor for that that goes through and finds that and removes it. But you always have this problem of like you know that's a lagging process. So you're always going to be querying some amount of duplicated data for recent queries which is most of the query traffic systems like this see. Um so introduc maybe maybe I could show here um like I have the the diagram from the databases offsite. Uh maybe we could I could just show it on the screen while you're talking just so absolutely people can see what you mean. So this is the existing architecture. This is what Loki is like today or Loki 3.0.
 You can imagine that like each of those boxes sort of scales out you know horizontally multiple distributors multiple injusters. Um and then right so we we queries will will be served from you know both injusters for recent data and from object storage for long-term data. Um and so that's sort of another problem with queries that are fetching data from ingesters is that you create the opportunity for you know querying to have an impact on your ability to write data. um because the ingesters have to respond with a you know 200 in order to accept data and if their CPUs are very busy doing querying stuff then they can be slow to respond um or you know and we have relatively tight right timeout so like you'll you know you'll get an error back saying you write failed you have to retry it and it was like because of a query so like you know another benefit that we're going to get out of this is a better separation of read and write traffic um and so so introducing Kafka basically just changes where the replication happens. So, Kafka, it now handles the replication and sort of high availability aspects required to own that data. Um, and then we can read from it as a single replica now, right? Like we can go from a queue and say, "Give me this data." Um, and the replica is removed. And so that basically makes the replication, you know, as simple as possible, right? Like it's like just some amount of bytes that are copied within a system like Kafka. Um and you know the obvious downsides here right is like now you have to run Kafka um or something similar which we'll talk about in a second. Um and you've introduced some amount of delay here right like you you have some it's an asynchronous operation now like we accept the write and then we read it from a queue and you can do that very quickly but not as quickly as Loki could do it before. Um, so for us though in in you know I I I like for the type of database Loki is and what it does, this is the better set of tradeoffs. I I would say like I really like the system that we have now and the and what it does but you know introducing another second or two of latency on on log lines isn't as important as it is to reduce how and where the replication costs stack up. Um so um and then I'll throw sort of one element that's written on the screen there like there's another benefit to this approach um which is specific to an implementation of Kafka um with a product called warpstream um which is a object storage based version of Kafka and so um this also gains us something that's hard to get today at least it's hard to get inexpensively today which is multiszone replication. So if you if you wanted to run Loki in an in a setup where you had multiple availability zones within say Amazon um you you'd have to pay for traffic to transition between availability zone boundaries and that's very very expensive. Um, typically what happens is you set up a system, you you rack up a huge bill, you go to Amazon, you negotiate, they lower it, but like you know how much do they lower it or like will they like it depends on your relationship and you know probably your bill with them, right? Um whereas Warpstream um you know in S3 S3 automatically replicates between availability zones and they don't charge you for the bytes traveled. So like you pay for an IOP to send the data um but you don't pay for the bite. So it's it's um I guess it's a clever way to you know kind of give you multi availability zone and the durability of S3 which is really really high right like object store durability like their availability and durability are separate things but durability is measured in like 10 or 11 9 like when they receive that data they don't lose it like you maybe temporarily can't get it um but it's very very good at keeping that data so we gain those things as well. Now, Warpstream comes with some different sets of trade-offs, which is increased latency primarily. Like, so, you know, the act of using object storage is, you know, just kind of slower for the high volume ingest part than it is to write to disks with something like Kafka. So, um, all sets of trade-offs, but, you know, for us, we're finding that this feels like the right set of trade-offs for what Loki is primarily built for. So, uh, Andrea raised a good point here and I I I agree on the the log side here as well, like you know, a lot of the time we're looking back over logs rather than accepting them in near real time. So, the the delay here for me is like in the human side, we can mitigate. I guess where the the bits that we we need to work out here is when it comes to like recording rules um and you know, metric generation. Is that something that we need to keep in mind when it comes to the new ingest format with CFKA? How how are we structuring that? Are we just expecting there to always be a consistent delay or are we you know having it near concurrent when it comes to generating these sort of metrics on logs? What sort of
 I mean this is a a really good question. Um and you know the I guess in the initial stages you know things will work the way they do today and our expectation you know or our experience even in you know because we do run this in Kafka now or in Warpstream now is that um the delay is not meaningful for the evaluation of rules like we um you know there's a concept of introducing a bit of an offset well there is an offset in the query but you can also sort of delay recording rules from being executed a little bit so that the um and I say that but Loki doesn't actually do this today. Mamir can do this but um there's a config for it that we never completely plumbed through but the the I suspect we will end up having that because there's kind of other uses for that anyway like so it it's as low as to should not affect those types of use cases. um you know I guess unfortunately if you built some system around Loki where you required um you know sort of immediate available of of data like you know that's not going to work in this system for you right you're going to have some asynchronous delay here right you're going to get a 200 back that we accepted your data it won't be available for querying until it goes through the queue and is you know then into um and that was maybe in that document like when we build data like this is a bit of a tangent but like the way we separate read and write now right is we well one writes are separated by a Kafka cube but then secondly we have things that will build objects for long-term storage which will be separate from things that serve recent memory data so um we've got a complete separation of those paths but there's some amount of you know as small as we can make it but like probably about a second to two seconds delay between when we accept your write um and it's available for query ing. So, I think very, you know, acceptable for the cases that we intend Loki to be used for. I'm sorry if someone is, you know, using Loki in a way that that this breaks that that experience. Like I I wish we could sort of have it all, but we're trying to kind of do our best here with with what makes most sense for Loki as this observability tool, you know, for log data for humans largely to use. Um, so the recording rule stuff like I I the only thing I would throw on to the end of that is like it would be nice someday and I don't know when that day will be. Um, there are a lot of cases where we get delayed logs into Loki um for lots of different reasons. Um, and that's always problematic for evaluating like recording rules or alerting rules. Um, you know, I'm not sure that the alerting rule aspect is as relevant on old data, right? Like but you know systems like Mamir and Loki were built around the idea that data would be current right like they work on this concept that you get you know data to them as quickly as possible with no delay but it does happen that there is delay and there are good use cases for like why you would still want to build metrics from those logs. So um it would be nice someday to sort of decouple those things, right? Like have the evaluation of recording rules be a function of the time stamp on the logs less than but then then requires you then backfill older data in other systems too which usually has some um or might have some limitations. You know there's it's harder to make databases that can accept data from infinite time ranges. you know like it uh we we've continued to evolve Loki over the years to be able to do that like to be able to accept data from you know any time range. you know, doing so always comes with some increased complexity or or memory risk, you know, like that that you're trying to always manage these systems for the most stable memory consumption that you can and you know, those kinds of changes often, you know, introduce. But like the average, you know, or sort of the most common case folks, the data we get is current, you know, but there are cases where we get older data and we want to support that. Um um I just thought
 we have a question from
 Yeah. from Tony Woo saying regarding columnbased storage, what would happen to unstructured logs? I'd imagine one cannot switch between different storage formats.
 Um I mean the simple answer is we're just going to store that log as a column, right? Like so so if you know if in the simplest form a log line with zero structure would just be a column with a time stamp and a column with the log content um and would roughly behave the same as it does today. So you you you wouldn't gain any of the additional advantages of like using other columns to reduce your search space. it would operate like a largely like the way Loki works today where um you know the the data that you you query would be that entire column and then you you know likely would need to apply parsers at query time like Loki does today. So um you know that should all still just work like we we want for that all to just work the way it does today. um if you're able to you know I mean sometimes log content is what we call like semi-structured right like there might be some key value pairs and there's like this free form message field um you know if we can manipulate that we're going to try to do as much of that as we can on our side um you know but also like the APIs allow you to sort of do that too so if you're able to manipulate out any of that structured content um then it can go into columns separately and and give you some advantages so like you stand to gain from that but like it should basically work the way it does today. So, we've touched a little bit on the ingest side and the actual data format itself. I guess the final piece of the puzzle then is any changes coming on query side.
 Yes. So, uh rather ambitiously, we are also building a new query engine. Um we're you know the sort of way this will roll out for us. So like for us it's important that we you know sort of deliver incremental incrementally in general like so we want to kind of build these parts like as it exists now a lot of the stuff I've talked about exists in a form today in kind of various places in our infrastructure um you know mostly in our development and our what we call like our ops environment. Um, but we also have some ways that we can validate, you know, a lot of the stuff in in production with like production data that's just sort of not user impacting, right? Like we can um we can see how something will perform um in kind of an opaque way, right? Like we just let the system do and we just look at stats around um you know because we we don't look at the content of stuff folks send us, right? So we just look at statistics around like how does the you know one query perform versus another and like you know give us the ability to understand how this is going to work because it the the most sort of interesting part about building a database like this is just sort of how different everybody does everything you know like every everybody's everything is different their log content is different the way they use their logs is different the systems they use the volumes like it's it's uh I and it makes it hard to do something like this. And it's I think a testament to how generally well Loki works today. And so we want to built on abilities to be more targeted with how we can improve performance like what we're talking about here. But um and that's a lot because it's just it's kind of exciting and wild like how how folks do this. So the the back up to the uh the question was like you know sort of delivery right like um
 wait what was the question was that the question the we were just talking about the query side of things. Um
 okay um right and I started talking about incremental delivery. So, so on the query side,
 um, yes, we we Loki has a pretty simple query engine too, you know, like it was, um, I guess kind of core to how we do stuff at Graphana is start simple and prove that it works, right? Or prove that it not so much that it works, I guess, that it has, um, like a a market fit, right? Like users want to use it, it solves problems. Um and so similar will apply here where we're going to start replacing um the query engine first in in sort of the quer component. So like how we execute the query and then that will you know kind of work against data objects but then eventually in terms of how we do the planning and the parallelization um and you know look you know the the design for now we'll have a actual like a query planner with an execution plan and um you know tools that you know would be a lot nicer for debugging query performance that we lack today you know where we can sort of in in addition to like improving and adding optimizations around query planning which we we do some today but just in ways that are a a little bit too hard to scale and um you know improve. So the Yeah, exactly. That'll be you know and and kind of the nice parts about this too is is you know our you know we're going to continue to fully support logql. Um you know in fact all of the work we're doing now will largely like you know I'll make this sort of quick statement which is like promql and loql in in my opinion are really really good languages for working with time series data and building a time series result. um they're not the best languages for building tabular data and um you know sort of like a spreadsheet style result um you know SQL historically is is the you know the language here right so um so our eventual goal here in this sort of of design allows us for implementing you know a SQL query engine or query layer because like you know you essentially once you parse the language into that abs um abstract syntax tree um it gets executed the same. So, you know, support for logql and eventual support for SQL. Um, I feel like I need you to address that. I've made some comments in the past about SQL. I still don't like it, but I do agree that if you're trying to do stuff in tables and, you know, build result sets that are essentially these two-dimensional matrices. I don't know if that's actually the right way to say that, but um, SQL is a really good language for that. And so that as Loki now pivots more and more into this world like you know we're going to need to meet the people where they are and support SQL because you can do joins in logql I'm sure you've all seen them you multiply use things like on and group left and it's not easy but it does work so so that will be true now you can do um you know especially if you do in an instant query which is the concept of a sort of you know single result set like you can do these kinds of things now it just I think it'd more ergonomic to do it in SQL.
 I think a lot of the changes that we're talking about today are are really about decoupling and this query engine is also about decoupling the query the query engine from the query language. So like in this in what I'm showing on the screen here like you're talking about having a different parser right for like an SQL one and and then an logql one or whatever else. So it it will make it more futureproof. Um, I also wanted to highlight this comment from Andre. Uh, a question. Can we expect this new database system and the Kafka backend and also the query engine because I think he asked this before we talked about it for Loki 4.0.
 Yeah, I I yes um Loki 4.0 is is a bit of an ambiguous like you know you know realistically for like a Loki 4.0 that has all those things in a way that we would say go use them like that's probably you know closer to a year away than it is anything else right so um you know along the way we're going to be I mean a lot of this code already exists and we keep iterating on it and we're running it and you know in our SAS environment we're iterating on these things and and you know we want to roll out you know as the stages of them as quickly as we can um you know to get value out of them and prove that they work and things like that. So, um you know along the way here there's there's sort of this stuff exists. It's just at the moment we don't have any documentation for it or um anything sort of publicly facing. But I I would like you know some point for us to get to where whatever releases are in the 3.x series, we can sort of include some experimental configurations and documentation for folk to be able to use this stuff as we're building it. um you know with the understanding that it's still going to be experimental and you know if you're interested in helping us you know basically debug it or help us improve it then the the opportunity might exist but I I do think that like the idea for 4.0 will be like this is the you know next version of Loki. Um we thought about maybe we should do like Loki 10 or some skip some numbers or something more. To be honest I actually would like to stop using 4. I'd like to name it Loki 2026 dot you know whatever whatever and use like a date naming because I personally dislike Zenver a lot and like so maybe that'll be what it is. I don't know yet, but uh you know the generally there will be a major release I would say a year or so from now that we'll have this as kind of generally available or like it might still be in a like I don't know we don't use the phrase beta but it might still be um you know a little bit little bit more caveat mour but
 Andre also comments it's looking like a full Loki rewrite. Um it is I mean you know we're replacing the storage the index and the query engine. That's most of the components of Loki. Um you know the reality of it is like we need it to continue to work for folks today. There needs to be a path for folks to upgrade. um you know it and and so it'll it'll happen kind of incrementally in in within the project and the you know we will be you know rewriting pieces and and sort of testing them in an isolation as much as we can and and there's some nice aspects to the microservices nature of Loki. So um you know but it's it's it's a rewrite but it's like the parts that are hard here are um have more to do with the design than the implementation in some ways like and then sort of then just like proving out what you know and so like um the parts that we're kind of excited for is like you know we have um you know we have quite a bit of experience under our belts now um you know we've learned a lot we've seen a lot and then we have access to um you know both an open source community to help us you know sort of prove our theories you know hopefully as well as like you know customers that are willing to test that and also like our ability to like um you know sort of test in isolation just to see like how you know how changes work. So um it is a it is a big task and we're you know like I said we're we're well underway now and so uh you know we were trying to decide what the right time was to talk about it. Obviously, it's always nicer to say, "Hey, like here it is." You know, but um you know, there's some advantage to uh you know, sort of making folks aware now what we're doing so they can know what to expect and and and hopefully get excited about it like we are.
 Andre said, "I see. Thought that the database and query part was still an idea, not something that is already running." Awesome. Yeah, but actually we we dog food all our own stuff because we run low-key for for customers and for our internal stuff. Um there's meta monitoring going on and we always we always have like before we release something to everybody else or ask people to even experiment on it. we've already been running our own experiments and and like really using it in produ you know for our production not not for customers but um it's it's really it's also the difficult part right like we are not rolling all of these changes out all at once we are like like Ed said we're doing it incrementally in phases so you know if we change too many things at once then it's hard to tell what you know if something goes wrong which inevitably it will it's then hard to tell which part made it go wrong. So I think um Tony raised a really good question and like I think so we can clarify a bit around the storage format if you're willing Ed um but Tony raised a question around let's take the example of structured logs say in JSON format and if we do some passing at ingest time and we split them into columns how do we preserve the idea of the original JSON um log structure um and that was kind of his question of like how do we if when returning long logs, how do we how do we return the original preservation of that versus still leveraging all that Colmer has to offer for those attributes?
 Um, yeah, good good question. Um, you know, it's an interesting one that I would say the this is the hard part about moving to coler storage is this, you know, log. How do you solve the log problem, right? Because like if you have a a number of columns that now represent that log line and then you want to reconstruct it um you know you've lost some information potentially around like what that looked like. So um you know there's one path here which is we just also store the log line I know unmodified. Um and I'm I think initially there's a good chance we'll do that. um you know it's it's still likely sort of cost effective to do that and and especially for like performance of log queries. Um you know eventually it it stands to reason that you would think it would become more costefficient to break those up into columns and um and there are techniques for like how you can sort of decompose a a document like a JSON document into columns and still be able to reconstruct it. Um I'm I apologize if I'm this is like document shredding. Um but like there's way like like the way that parquet does um nested objects is is along these lines like it it you know you store some metadata with each column that indicates if it's like part of an array um you know what element it was in the array or if it's part of a map like what the sort of parent like. So there's there's ways you can reconstruct something like JSON with you know uh flattening it out. Um but you you know at some point also then ask the question of like is that actually what's more cost effective here right like reading all these columns storing all that extra metadata reconstructing it or just storing the JSON extra because like the the you know cost model of systems like Loki get a little bit complicated like the you know actual ondisk storage cost is not typically your biggest driver of of TCO um you know and then something like IO O operations where we have to go fetch all those columns. So it might actually prove to be more cost-effective just to keep the log lines intact. I see somebody else mentioning comments around like you know security and audit requirements and things like that that might also sort of drive um you know keeping things intact because you you know just have sort of external reasons to do that. Um maybe we end up you know with all of those like you could sort of indicate that this log line should be kept or we you know sort of choose that optimization based on what we learn about query patterns and things. Um it's I I would say it's not completely answered yet. Although you know I suspect we'll start with probably just storing both and then you know deciding what optimization through you know some empirical analysis is like what what ends up playing out better. I know for security logs in particular there are some people that are already using alloy because alloy um you you can determine like different data pipelines. So there is a a way where you could do it at that level. So regardless of what happens to Loki, you know, maybe maybe those um security logs or access logs um because Tony mentioned that for compliance reasons that they really need it to be completely unaltered. There is a case for just doing it in alloy and having it go down a different path.
 Yes. Yeah. And that's fairly common. and and we I don't know this is a really interesting and sort of hard question to answer because at least in my experience everybody wants it done a little bit differently you know like like some companies will say you know like this is very much like um like they they will sort of evaluate their own risks and and and requirements differently like they might have strict requirements, you know, that they've made or maybe they're through some compliance that regulatory body um where they think the only way they can meet that is by shipping the logs directly. I'm not suggesting they're right or wrong. I'm just saying that that's the the reality is like folks will have that they have to ship them and store them and sort of own them the whole time themselves, right? Whereas um we've also had companies that will be like, "No, I want you to do all of that for me, right? And I want you to maintain them and store them for as long as I need to." um that can work out if it's like you know you measuring retention in like months or years like or a year or something but if you are measuring it in like 10 years which I've seen right or 20 years I think which I've seen like you you know you really need to think about the format that they're stored in and where you're storing them and like the longevity I mean if you think about any technology that anyone has used for storage that's survived 20 years like I think maybe S3 is about the only thing at this point that's approaching that time frame that's existed that long right like you you know, it used to be tapes, right? It used to be So, very long-term retention is a I don't know, it's interesting thing. So, so I it's what's hard for us to decide is like what system do you build that meets most people's requirements and like realizing that like you just can't meet everybody's requirements like so, you know, we we still have some questions to answer there as well. like the the other part of that too I guess that's interesting is like what is the sort of other models of other of other you know companies in the space that people have accepted and like that's an interesting driver in terms of like what people want right it's like oh this is how it works with something else that I'm familiar with I want it to work that way too and like this one has always been a bit of a frustrating one for me with Loki because like we you know as our product out of the box we gave like 30 days retention right and like um you know which was great like that's longer than everybody else. It just it doesn't seem like it was as much of a selling factor as like everybody's like sort of familiar with other concepts of like you know sort of rehydration or like I don't know like it it there's interesting aspects of this kind of conversation that have less to do with the technological solution and a lot more to do with human psychology and legal bodies and regulatory requirements and like makes it harder to decide like what's the best solution here. Ed, we have only a few more minutes. Can we do a few rapid fire questions and answers do that? I think I've talked for 10 minutes on every question, but
 Quantum Q80 says, "But will it be a drop in replacement?" I'm not really sure if they're they're talking about a particular one of those one of those changes.
 So, so I mean, if you just think about like Loki 4.0, so like the APIs that you send logs to Loki with today will still work, right? So, like everything that's sending Loki, you know, from 3.0 to 4.0 0 will still work um and we will still be able to query you know old data from Loki 4.0 or in old formats um for some amount of time like maybe forever maybe we'll always keep those paths in Loki um that's a bit unlikely right like it's just hard to say you would maintain a certain code path within an app it just becomes a burden right over time um if we choose to go down the road of saying okay like you know with Loki 5.0 or whatever, it won't query um the old chunks format. You know, there'd be a couple options, right? Like as an OSS project, like you can always just run an older version of Loki to query that stuff. Um I also suspect like you know, there will be migration tooling I would like us to have, right? So that you can convert chunks to data objects so that you can still um you know, query them like so so as a sort of this is not short, is it? You know, generally I would say the answer
 more questions. Um, it should basically be drop incompatible. Yes. Is is what I would say.
 Okay. Lucas Janushitus says, "Greetings all. Ed had this blog series going on called the concise guide to graphana Loki which had a large amount of tips. How to have the best performance. Maybe there are plans for part four.
 What do you what do you want? We did we talked about this, right? We had um we actually had Sirill Tenna on. Um Jay and I interviewed him and talked about like querying performance and different ways to do that. We've also redone the graphana doc crown of low-key docs for um query tips for performance. So yes, maybe. But also there's other content for it now.
 Yeah, I mean like cash as well deployment with poison. So I feel like in the spirit of Ed, we've tried to carry on parts of the series. We can't have Ed.
 I really like those posts. I made those animations by hand. They're really really awful looking, but um yeah, I uh I mean I guess the the trying to think about like what folks really want to know like I didn't mention this and and I I'll get a little flack from my team because like we're still figuring out the details, but I really want to kill streams like the idea of streams. functionally it won't change the way you send data to Loki or query it but internally I want us to really decouple like like one of the harder parts of of using Loki in in my opinion is like getting streams correct and like um you know that's been true because we've changed the guidance over the years as well as we've added features to sort of work around some of the rough edges of the stream concept. So u I kind of want to continue down that path too of like making you know like really decoupling how we index and how we store data from how it's queried which is all relatively strongly coupled right now through streams. Um so that look for that too. I guess that will be a a big improvement here.
 Question from Dimmitri. Are you planning to move bloom filter support out of experimental? It's a really important feature nowadays.
 This is a we out of time. I got to uh the short answer to this is no. You probably won't and and I'm sorry like I you know uh in in full disclosure like this is a bit of a a bit embarrassing for us, right? We talked a lot about Bloom filters. We promoted them a lot. Um we build them and they do work. Um they only work on structured metadata and essentially what we learned was um that's really hard for most folks to make a move to. like if you're new to Loki or and and it's even better if you're using open telemetry because you'll have all these structured metadata fields that blooms actually can help quite a bit. Um but for anybody that was like sort of using Loki the kind of back to that streams concept like really requires folks to move things into structured metadata which is an agent side change and it requires them to query it in a way that's like it's like kind of a lot to ask folks. So, so what we realized right like is um you know a couple things like we realized that you know part of how we implemented the solution to blooms didn't sort of land it didn't didn't work as well in object storage was some of the assumptions that we want right so that was a bummer and then that allowed that sort of is why we pivoted to use structured metadata um and and then we also realized that you know it it it can help right and like they do work and they work when you have structured metadata and people will query it. Like I said, hotel helped a lot here. Um, but you know, this was about the same time we realized that like the real problems we need to work with here um are you know sort of the fundamentals of like how we're storing it. So like instead of putting any more time into making Blooms better, we're like no, we should fix the you know in the underlying storage and then there will be Blooms built in around data objects for sure, right? like the same kinds of things we'll use bloom filters for to um you know reduce search base and things like that. So um so we're unlikely to move it out of experimental because I don't want like you know we just don't have any capacity to sort of help support folks with using it or or you know figuring out like we we don't necessarily have great documentation like they do work like they are there there might be some documentation for them. um we we run them like they um it's just largely because like we're not you know we don't have infinite capacity on our team to support things like um we realize that we need to sort of focus more on something else. So they're probably going to stay in kind of that experimental state. So I I apologize for that. Like I you know I I think you know we learned a lot along the way and we're going to transfer all those learnings into what we're doing next. Um, you know, like I said, they they do work in some capacities if you're, you know, kind of set up well or have the ability to get your folks to kind of rewrite their queries or use their queries or you're starting from scratch. Um, but you know, our real goal is going to be to solve that problem better with all the work we're doing now.
 So,
 last question. Oh, sorry. Did you want to say something, Jay?
 No, no, no. You go for it. I think actually Andrea gave a really good answer to this, but um maybe Ed can back it up for that question you just raised, but you can do Lucas on that question.
 Sure. Lucas also asks, "Speaking of costs, is there a way to monitor how much storage or IOPS are spent for every stream or at least service name to calculate costs later on?"
 Um good question. Um and the and the
 not as a metric due to high cardality, but you can calculate it from ingesttor logs. You you yeah you can the flushing stream log will give you a lot of insight into um which streams are flushed for which reasons which can help you understand um IOPS a little bit better for sure right like anything that like the only way streams are removed from Loki is memory and an adjuster is an idle timeout right they stop receiving data and so um it it's a bit interesting to try to find those streams but you can um I mean the other thing I always point people out that's like I'm sad that we didn't build better way to do it. But the analyze series command in log CLI that argument in log CLI like point that and we'll show you your label distribution and usually can make it really quick to see what labels are introducing too much cardality and then now that you have you know structured metadata we just tell folks move those things to structured metadata. just I'm kind of so tired of the whole like that whole song and dance, right? Like I really want those concepts to be like not something we have to burden the users with, right? like just you know you send us the data we will structure it we'll index the stuff that you query on or that maybe we'll give some hints or have some um but for for today's Loki yeah you can look at that exactly you can look at the flushing um you know flushing chunks or whatever it is like flushing stream and um and uh you know check your analyze labels and see like um in terms of like query use I don't know it's a little bit harder like we um we built our adaptive logs product and which is a graphana cloud only product sorry for oss users but um we're trying really hard to make sure we can keep building this and make money um so we have value add features that are only pay only and but Loki as a database will always work as a database for everybody um you know if you're a massive enterprise and you're trying to figure out how to save a bunch of money like those features you might have to pay for but um you know for everybody that wants to run Loki and do whatever with it will always be fully functional Um, and yeah, the the stream Yeah, I guess that was my the the stream concept is is hopefully going to be way less significant kind of moving forward as well to make that easier. So, right, so we're we are five minutes over. So, let's see if we can quickly summarize in 30 seconds this this entire hour to give people the the 4.0 experience. So we're changing ingest storage and query. This on the face of it to begin with will not change any APIs. So how you actually interact and ingest logs will not change and how you query will still be logql. Um and there might be all some spicy flavors introducing new um features into logql potentially. Um in terms of storage that is the fundamental bit that's going to change quite a bit. We're moving from rowont orientated colmer. So there's plenty of questions there around what we do with columns, how we pass those logs from like ingest um and so forth. And I think some really good questions been raised by the community here around like auditing and how we maintain log structure. So that's all good food for thought as we go. Um big change here is architecture. We're introducing CFKA on inest side to decouple right from read. Um and that will be one thing if moving into the community that the community will have to consider running. Um, so a crazy amount of changes, but all exciting changes. To me, this is going to be pretty impressive to come to 4.0 eventually. Um, so yes, big changes, but for me, exciting changes. I mean, like, definitely go back and actually listen to the whole call if you've just gone for the summary at the end. I think I just
 That was a good one. Did better than I would. I thought you were gonna ask me at first. I'm like 30 seconds, huh?
 Yeah. I also wanted my mind.
 No, that was great. Jay, um I also wanted to say that a lot of these changes are not just in Loki. Um we Jay and I actually did a session with did a live stream with um D Kitchen who is our VP of engineering for databases and they talked about all of the changes across all of the databases including Loki. But a lot of these changes are are actually parts of larger um movement in all across all the databases like Mimir has already led the way on on the rearchitecturing with warpstream and stuff. Um so this is still you know we don't want to present Loki as the only as like the redheaded stepchild that's going its own way. This is actually bringing it more in line with every other database in graphana. So everything is going to be like there's a lot of cross-pollinating, you know, we're we're trying to copy homework where we can. Absolutely. Thanks for thanks for having me. That was fun.
 Final question that you should not commit to. Andre says 4.0 release date.
 I don't know when Graphonic will be next year, which will because we it's been as early as April and as like late as June. like it would be fantastic to have something for Graphfonicon next year. Um, you know, we'll see like it, you know, it's just a question of like how polished we would want, you know, it can be and like, you know, those are the kind of hard parts, right? Because we want it to be something people can just go run and we have documentation and but we got our work cut out for us between now and then. So,
 I can see the pitchforks uh in the internal Loki engineering chat. Uh, they're coming to you after that statement.
 Yeah. So I that that would be you know that would be nice. I mean it pieces and parts of it are coming sooner. So um you know I I I guess we you know in the past maybe we can get a a Slack channel just for this and the community channels for folks that are like really like really interested and like you know I mean it just it it's just going to be rough around the edges for a while. So, like, you know, it's kind of one of those like you got to really kind of understand Loki and know how to do it and know how to read Go source control or code and things to like to be successful with it. But I I'd love for anybody that like is interested in that and wants to be involved to like make it available and and that's, you know, that's one of the best parts about having community is that you find folks that are that are like, you know, just super awesome people and we love you very much. So, we want to make it as as much as we can get everybody involved. So because we we appreciate all of you. So thank you. Right on. Well, I think honestly awesome. Yeah. A year returned, anniversary ed with a big splash. So absolutely love it.
 Can't wait to see next year what we talking about
 for Loki 5.0. Well, I hope you've enjoyed. Um we'll definitely come back with more news on this as it arrives. will definitely have Ed and other members of the Loki team. Um, we'll keep going with our monthly cadence of Loki community calls with the our original best tips as it currently stands and then we will litter in as we're ready to talk more about 4.0 these types of calls where we can dive into some more of these features as they come. Um, but until next time, thank you all for joining and we'll see you on the next one. See you all. Thanks everyone. See you next month.

