# GenAI Observability in Grafana Cloud: End-to-End Agent Debugging (Demo)

Published on 2026-01-25T22:44:40Z

## Description

From Observability for GenAI Applications (Grafana ❤️‍   OpenTelemetry Community Call #4) Watch the full episode: ...

URL: https://www.youtube.com/watch?v=IeKgA6RqnVs

## Summary

In this video, the speaker, an engineer, discusses the complexities of integrating multiple agents within a software application that utilizes the Serper API for search functionalities. They express a need for better visibility into how these agents interact, particularly in tracking costs, latency, and errors during execution. The engineer emphasizes the importance of observability tools that provide detailed traces of events, enabling them to optimize performance and troubleshoot issues effectively. Furthermore, they explore the evaluation of responses generated by large language models (LLMs), mentioning the use of LLMs for self-evaluation and tracking metrics such as hallucination scores and factual inaccuracies. Overall, the video highlights the challenges and strategies of managing and optimizing AI-driven applications.

## Chapters

Certainly! Here are the key moments from the livestream along with their timestamps:

00:00:00 Introduction to the agent take run and its sub-agents  
00:02:15 Explanation of tool calls and the Serper API integration  
00:05:00 Discussion on tracking cost and token usage in observability  
00:07:30 Importance of tracing in agent execution for engineers  
00:09:00 Using the assistant to identify errors in agent calls  
00:11:20 Overview of request latency and optimization opportunities  
00:13:00 Explanation of cost parameters associated with requests  
00:15:30 Discussion on tracking vector DB calls alongside LLM calls  
00:17:00 Evaluating LLM responses and understanding failure metrics  
00:19:00 Importance of factual accuracy and context in responses  

These timestamps should help users navigate the key moments of the discussion easily!

# Transcript Cleanup

The agent run has started, which will initiate three sub-agents. I am unsure which agent will call which at any given time. Additionally, it performs tool calls using the Serper API for search functionalities. As an engineer, I need to understand how each tool and agent interacts, especially when the OLM (Open Language Model) call is made. 

To save time, I pre-run the agent. I want to demonstrate how data flows from the opioid instrumentation into any observability stacks. Typically, what you receive is a high-level overview of information, such as cost and token usage, which are useful for tracking expenses. I also want to monitor request latency and other metrics. However, as I drill down, if I am the engineer who built the agent, I need to track the sequence of events that occur during the agent's execution. 

For this, we need traces. I will utilize the assistant because there is a long span of events that occur from a single agent diagram. There are many factors involved, and as an engineer, I don’t have the time to sift through each user's agent events, especially with 100 users. I will rely on the assistant to identify issues. I notice errors in various places, but I want to pinpoint which tool calls caused these errors. I am looking for a comprehensive view of which agents called each other and when, as well as which LLM calls are slow in duration so that I can find optimization opportunities. 

The assistant proves to be very useful for explaining these lengthy traces. Previously, many of my requests were wasted on local retries, and now I know where to optimize in the long run. While the assistant often explains the entire sequence of agent paths, it didn’t do so this time, which is expected. As an engineer, I feel safer now that I have information on token usage, costs, and what my users are prompting. 

For example, each roll call contains information about the audit, including the completion status, responses from the LLM, inputs, outputs, and the cost of each request. There should be a parameter for costs that I can associate with each roll request. This information is invaluable for tracking not only LLM calls but also other vector DB calls or tools I might be using. 

I can map this information more cleanly, allowing me to drill down into which agent had issues and where I can optimize. I can also investigate aspects like usage by system, environment, and platform. I can track latencies and determine the time to the first token. This enables me to compare performance, such as how much Anthropic and OpenAI are contributing. If I require faster replies from the LLM, I may consider using OpenAI. 

This discussion highlights the observability aspect of things. However, as we discussed, we also need to conduct evaluations. I have the prompts and the responses from the LLM for specific queries, but how do we classify these? As Lucas mentioned, we use one LLM to evaluate another. I utilize OpenAI for evaluation since it exports notable metrics that are easier for me to validate. 

Many AI tools also offer online evaluations, though we don’t have that feature currently. For some, code-based evaluations are preferable because they do not require storing prompts and responses in the observability vendor for security reasons. 

With this information, I can analyze the problems in the responses. For instance, if a response is classified as a failed evaluation, I can identify factual inaccuracies or context issues. I’ll also have access to metrics like hallucination scores and toxicity scores. Based on this information, I have a solid foundation to create the second iteration of my LLM feature that I plan to release.

## Raw YouTube Transcript

So the agent take run run has started, it will start like 3 sub agents and I don't know which agent would call which agent when right? And it does tool calls as well like it uses the serper API to do the search thing. So I don't know how the integration. Happen. But I as an engineer like if this is my application, I want to understand like how each tool, how each agent when the OLM call was made, the request was made right? So pre ran the agent obviously to save time. And I wanted to show like once you have the data flowing in from the opioid instrumentation to any of the observability stacks to be honest. Alternative, what you get is like the. The most overview based information like cost and token which are like alright as an user I do want to track my cost in tokens, the request latency and stuff like that. But as you drill down if I am the engineer who built the Agent Orange, actually track the sequence of events that happened in the agent tech. Execution, right? And for that we do need like traces here and I'm gonna use the assistant as well because this is like a long span of events that that happened from the single agent diagram that I did right. There's a lot of fans in here and as an engineer, I don't have time to go through. Each and every year they ground that might happen. I have 100 users. I can't go through each 100 users agent events right. So I'll use the assistant. I'll see alright, where do I have the issue because I do see errors a lot of places, but the error happen which tool calls at the error and I do want to get like a very. Big picture of all of you of alright, which Aiden called which agent when the roll call was made, which LLM call is like very, very slow. The duration is very slow and where I can actually optimize, that's where I find the assistant very useful, like explaining these long traces, right? So like with the assistant, I obviously didn't run. As before right a lot of my requests are like wasted on local retries itself, which as an engine and now I know where I need to optimize right in the long run of things. And it does also a lot of times it does explain me the entire sequence of agent pass, but this time since its life it did not tell me that which is expected I guess. Like as an engineer, like now I feel a bit more safer since I have the information about like alright, how much tokens I've been using, how much cost I've been using, how what my users have been prompting because each roll call like let's open this for example. Have the information about the audit, what was the completion like, the the response from the other line, what was the input, what was the output and how much cost each request had. There should be a parameter for cost that goes. Yep, so I can associate like costs with each roll request and at the end of the day. And so this is very useful information for me to be asked. And also this is not just me tracking the LLM calls, this also helps me track. Other vector DB calls that I might have, all the tools that I might have, so and I can always map this much more cleanly. I can close a lot of this so that I can drill down into all right, which agent had issues where I can optimize things and then obviously I drill down into more aspects like which I've been using. Um, how much by system, by environment, by platform? I can track the latencies, I can always find the time to 1st token and alright, how much is Anthropic doing and how much is open air doing? And with that I at least I have the information. If I want a faster replies from the ILM, I might end up using open. That's all right. This is like the observability aspect of things, right? But we also like, as we discussed, we also always want to do evals, right? Alright, I have the prompts and the responses from the, the response from the room for the specific prompt, right? But how do we classify the?This is actually like feel as Lucas mentioned, like we use LLM to evaluate the first LLM itself right? In this as well, I use the open itself to calculate the evaluation since the export like a notable metric which is easier for me to validate. A lot of the AI tools also have like online evaluation. Itself, we don't have that feature as of now, but still for some people I think code based eval is still nicer because you don't need to actually store these points and prompts in the observability vendor because for security reasons sometimes some vendors cannot, right. So with that, like I at least have. The information alright, but what problems does the response generally have? Like why is it being classified as a failed evaluation? Like I have factual inaccuracies in my responses so maybe there's a context issues. I always have information already by a specific text from an ALM response was tagged as a failed evaluation. Right. And I can always see like, all right, what is my hallucination score, whatever toxicity score. And based on this information, at least I have a very good head start to build the second iteration of my land feature that I released in the.

