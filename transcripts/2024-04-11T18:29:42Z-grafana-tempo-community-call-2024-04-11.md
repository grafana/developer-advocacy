# Grafana Tempo Community call 2024-04-11

Join our next Tempo community call ...

Published on 2024-04-11T18:29:42Z

URL: https://www.youtube.com/watch?v=HS54B0IrOyk

Transcript: cool uh welcome to Tempo Community call April 2024 tax addition um today we have a pretty I I went into this morning thinking we did not have a strong agenda but it's actually pretty good uh let's get some names in here I'll start with mine anyone else is welcome to put theirs in and I'll put Doc in the chat as always uh throughout the course of our adventure today if you have questions feel free to unmute and ask type in chat or throw them in the dock whatever you're most comfortable with we'll do our best to answer this is a very informal and always an AMA so you are welcome to ask questions about anything you want uh so operating Tempo future work um what it's like to be at grafana or anything that's on your mind I suppose feel free to ask we do have a pretty cool uh agenda today again I didn't think we would but we do uh and we're going to start with Mario who went to graphicon I think he was the only person here went to graphicon from our side and I just to give us his like overview his like uh his his thing his summary I suppose I will stop presenting so he can show us all the cool stuff oh okay I don't have really anything prepared but I can share thebe this the agenda I don't know uh but yeah so this Tuesday and Wednesday so basically yesterday and the day before uh it was graphicon um in Amsterdam it was this event in the past it was more around grafana only grafana but uh this year it was expanded to include more of the open source ecosystem uh so there were presentations and announcements around Loki mimir we didn't have a big announcement around maybe if it had happened two months later maybe it would have had something but uh still it was it was really nice um I think it was the biggest event we've we've done so far we but yeah there were a lots of attendees um lots of chatting uh I mean I ended exhausted of talking that much uh sort of moving from one extreme being isolated working from pH to uh speaking for like 40 not 40 14 hours straight uh but yeah it was awesome uh it's always great to to speak to the wether Community um and so as for the content uh so nothing was streamed but I believe all the all the sessions will be available on demand um I was told the so the keynote will be available on Monday but it seems it's already here so you can go watch it uh I suppose especially the toles part when he demos a new cool stuff that is happening in graan 11 uh I think yeah that's pretty interesting it's not about traces it's more about metrics but still a very cool feature golor metrics um and yeah and the rest of the sessions will be available I believe in roughly two weeks also in demand here uh I wanted to highlight one which was about uh migrating or Migra transforming traces sorry logs into traces so where is it full agenda it happened here by uh K boss so um it was really nice because it's it it was about um really showed what the um what the capabilities and and how U powerful traces can be is it was essentially the same data uh the same source of data you had some blck lines uh but being able to represent them represent them as a trace uh was key on on the this investigation of of an incident that this person had um which would have been very difficult to get to just by inspecting the locks or that data as logs basically uh so it's really cool um and yeah it's also nice to see that we I don't know at least myself I think of traces of certain use cases I assume everyone instruments their applications with tracing but many people don't and it's still their ways of taking advantage of tracing and um yeah thinking of outside of the box how to use um tring in general This was um this session was very cool I always wanted to find cool other use cases for tracing and one that I've wanted to do for a hackathon I never have uh is find a threaded conversation board like Reddit or whatever scrape a bunch of data and turn that into traces uh because you know the you could visualize the time maybe between queries as the duration of the span the threaded nature of the of the conversation matches tracing nicely I think you write some cool Trace qel queries um to see who was talking to who who initiated questions the most I think you could do some neat stuff with data in tempo that kind of data in Tempo but I think it's cool that you found another like neat application of other data besides just distribut tracing traditional distributed tracing yeah it was cool indeed and uh I think it was coincidence but uh two more people came asking how how can I take advantage of tracing if I only have flocks and I was like why is everyone suddenly caring about this or interested about this maybe it was coincidence or maybe this um sort of feature request is more common than or use case more common than than we thought uh that was really cool um also something that was um very nice to see was that uh most of what people were interested in uh like next for Tempo and grafana we either are already actively working on it or we're building the tool rules and features uh that eventually will support those um those features so I I think in terms of databases the Highlight was explore metrics and then explore logs which is essentially instead of having to type queries uh you're able to find the data that you want just by click click click you every time you click you dig deeper into the data without having to have uh deep understanding know how prom kill works or L kill Works um and it's just a faster way of navigating and an easier way of navigating through through that data uh so that I mean one tary type was missing which was traces and people were very interested um either they they they went with an APM Focus like um I've WR app of cability and I just saw expore matrics or you doing anything of the sort or directly metrics and I believe what we're working in it's very related we be able to enhance that that experience so it was nice to see that what we're working on and our priorities are very line up with what people are demanding and more more interested in cool and yeah I don't know I don't know what else to to mention think you you did hint at a future tracing app is that gonna happen Mario uh yeah I don't know 5 PM it's not here so or any PMS well that's why people come to the community call so they G have discussions unburdened by people like PMS uh to talk about it yes we are building a tracing app in fact we almost wanted a demo today it's just it's really Dev de it's very much alpha alpha we're still like trying to arrange how things are going so I would say next Community call we'll for sure have like a tracing app demo I almost did it today but things are just not quite aligned to really show I think but expect that so an app where you're not typing Trace to queries but anyone can just click your UI and get value out of tempo exactly okay now you're in trouble I'm always in trouble my job would be in trouble cool uh any cool Community interactions any maybe like a one-off conversation somebody's excited about some thing in Tempo or grafana that you want to share uh well I would have to think um so I so I got a feedback it was this person asking for a more APM me experience with tracing uh they were using app observability and they liked it but I felt I had some raw fges uh we were discussing through that they gave me feedback even from their Engineers they like talk to them it's like like I'm with this person from the tempo team just give give me all the feedback that you have and in the end like they were they they they care about this feature they were very happy and like the last line was I just want a better um way of interacting with the traces because I got them so much and myself I'm an SRE I like I know Trace skill and I have no like no bad things to say about Tempo the database I feel like Works amazing and um and that was like very positive uh so even when there was tough feedback like I don't like these UI it was all from a very positive and constructive point of view cool okay it's good news uh love that you represented us there Mario was our was our Tempo person and I think you did an awesome job of course oh and I got my forehead tattooed with Tempo where is that tattoo sir I don't see it you can find a picture in Twitter an embarrassing temporary tattoo yes I think uh the process of multiple people getting their portraits tattooed uh will appear in the recup video or something I don't know how it'll be called but yeah very cool uh Tempo is amazing thank you for bringing that up uh let's talk about other ways the tempo is amazing in the future Zach I think has got some a long running request we've had we've never gotten to it a good security fix he worked on the past week or two uh Zack you want to talk about that yeah this was a big procrastination on our part um so yeah breaking change went in last week uh check the change log but uh the short summary is we Chang the uid of the docker container um which has the effect of you need to make an update to the ownership of the files on disk for the ingestor and the metrics generator um unless you're already handling the security context in some special way so for the Json users I included a um a bit of Json utility that will help there and I prepped a Helm chart PR today that we will include an example before release of the next version in Tempo um so if you're running main this will affect you now and so you might want to make this change immediately and if you're not then this will affect you on the next release so keep your eye out for that um yeah any questions about that cool yeah I somebody I swear somebody F that issue like six months after Le Tempo and I always wanted to get to it it's kind of a a very strong thing to be doing with your Docker security so uh I was glad you finally got to that one that's a I think it like a a thre digigit uh issue like that so two-year-old issue somebody said hey did you do this but to be honest with you I got a little nervous about changing um changing permissions on a bunch of files and so hacked around it internally but it was time to solve it cool thank you cool uh so next up I'm going to talk a bit about exposing some internal data Brum Tempo in pql that we did recently we are exposing a set of data associated with every span called the nested set values or the nested set model so this is a specific way to encode a tree which of course a traces in a set of integers like flatten a tree basically into a row which we do in our spans and this allows us to do all that structural queries like the child The Descendant the ancestor queries are done using these integer values we store in the um uh in in the span level so to show a quick example I'm running a query here and I'm now able to select that's called Left Right and parent uh this tells us where in the tree uh where in the tree uh these spans are they can tell you if it's a descendant or if it's a ancestor or a child or a sibling of another I'm I can getting all the details because I could probably spend you know half an hour hour going to what all these numbers mean highly recommend working through this page if you're interested the cool thing here uh the reason we added this is some customers were asking some very difficult questions of tempo they wanted to know the entire tree beneath specific endpoints and this quering this data out allowed them to do that so they were doing some specific things like you know API equals Fu and version equals bar and obviously this is not real choice and service sequals b or whatever so they wanted to know for very specific combinations of filters what are all of the downstream uh services and how what is the tree look like and this now lets you rebuild that uh Tree on your side so you can do some cool analysis for this the other reason we added is for the uh recently uh a mentioned super secret uh leaked uh app so the other reason releas this is because we want a part of the app to be a highlight structural analysis of traces and structural display of traces and this allows the app to query that same data out and rebuild like a structure in aggregate which we're really excited about and we're working on as we speak one last uh data point about these is they open up new and interesting queries which never existed in Tempo before like is root so you can ask for roots which I think is cool uh so if you do parent is negative one that's a flag for the sarot it also lets you ask I think this is even cooler you can ask for leaves so if nested set left minus right and again please refer to all the details I apologize I'm not going to have them off hand is it one or negative negative one there you go so this will be all the leavs so you can write a query now at Tempo that will find all of the Leafs of your Trace which is kind of a neat uh a neat ability for sure um so we I'm not sure what some of this is going to land in but I like that I like having things that allow people to be creative and find uses I think we're going to find cool uses in our app I think like I said we already have one or two uh customers who wanted this because they wanted to rebuild structure and maybe some of the community will find neat applications for these uh for these new intrinsics as well cool I think that's it any uh nested set questions I it's kind of a big brain dump this is if you spend some time with this you'll be like oh that makes total sense uh but just seeing it immediately can be confusing so spend some time with it if you want learn about how this model is created I think you'll find you can do some very neat queries in Tempo 25 and we're going to Showcase that in the app as well all right um Gad added something recently Shuffle sharding which has been present in mamir and Loki for a long time and is given us some nice performance increase I will hand the floor uh to yeah um so recently added iner Shuff charting to Tempo similar to you know how it exists in M lowy I think it was originally introduced in cortex so it's been in like the databases for a while we're just like uh catching up now um so Shuffle Shing um the idea is um if you have a multi-tenant cell you have a lot of different tenants and they're all sending data to the inors um what can happen is if one of the inors goes goes down this will impact all the tenants so what you want to do with shuffler sharding is instead of allocating all the tenants to all the ingestor you kind of like shart them in smaller sub rings and you shuffle them around so that way um not all the tenants are allocated to all the ingestor so maybe um one tenant gets ingestor one three and five the other one gets one two and six so if inest tree goes down it doesn't impact all of the uh the talents so in that way it's a reliability Improvement we also noticed it's um uh it has a big resource reduction as well and highly multitalent CES also like something we're looking into right now um I made like a simple presentation to just like share some uh cool graphs because I mean why not graphs everybodys um so it's controlled with uh two settings uh there's a runtime override ingestion tenant Shard size this is a per tenant setting and it will basically control how big um The Shard size will be of that tenant so how many ingestor are allocated to the tenant if you put in zero is just use all ingestor so like if you have 10 of them it will use 10 of them if you put in three it will only look at three inors and then um send data to those three still using the RF three like usual if you put six um you know it will use six of the 10 um so you can kind of like play with that and control it depending on like how big your tent is a a second setting we added is um like a toggle on the querer which can disable shuffler sharting and this is something you would need during roll out because if you enable um The Shard size you will like you will decrease the ingestor like the the ring use creating um and at that point until the data is flushed you won't querry um data that's on inors that are not part of the ring anymore so during wall out you should first set this flag to um balls so you don't use Shuffle sharting on a query path then decrease The Shard size and then after an hour or two you can set this flag to True again or just drop it completely because it defaults to true and then everything is fine uh but we'll should you know will be described in dogs by the time we release it so I just wanted to share some quick graphs we being rolling this out in our internal clusters um this week it's still in progress so I don't have like a ton of graphs yet but this is like in a midsize to smallish uh cluster uh the first thing you'll notice that the first graph is how many um bytes each ingestor is is receiving so thises this ingestion part at first it was really um balanced like every just was receiving the exact same amount that's because we were using just a regular run Robin like low balancing with the the sharting so every tant is sending to every ingestor once you enable shuffler sharting this will be spit up a bit and most tenants were allocated three iners only and then it kind of depends on like you know is this a big tenant then that injust will receive a bit more than the others so it's a bit more unbalanced but what you can also see is the amount of active tenant per part decreased a lot so before every inest had to allocate some memory to deal with every tenant so it was about 50 tenants afterwards it dropped to between 10 to 20 tenants so we kind of expect there if every investor is like keeping less tenants in memory overall memory usage will reduce and that's also something we noticed in um resource usage so around 2 pm is when we roll this out you can see CPU lowers a bit so just enabling Shuffle sharting reduces overall CPU in the inors and can also see here memory drops a bit and then after two hours is it drops a lot um and if I grab like the overall uh metric you can see the entire cluster inors used about 64 GB of memory then after the change was roll pull out and the data was flushed it dropped to about 40 GB so it's like a 30% reduction just by enabling shuffler sharding so like it still in justs the same data it's still storing the same data it doesn't really like change in any way and Global CPU also dropped from 2.4 to 1. 2ish so it's also a reduction in half um so it's basically happening is before every tenant was sending to every ingestor but it means if you have a very small tenant is just sending like a very small trickle to every ingestor by setting The Shard size you kind of concentrate the tenants and you know just like send more to like less ingestor do you have any guesses on that two-tier drop like there's an immediate drop on both what or on memory and then over time a larger drop any ideas there um I'm not entirely sure it's not uh correlated to uh a roll out or some other um like increase in traffic or decrease in traffic so I don't didn't really see a correlation I'm guessing um at some at some point like when you enable The Shard size some tenants will grow still they won't receive any data anymore on the inest and then it works to flush all the data and I think that's around the time it probably flushed it yeah it must be but it's like two hours that seems kind of long I thought the same like I I would have expected that more like after half an hour where it starts the wall would be dropped and would no longer be created and we start dropping complete blocks I mean I love these savings I I kind of want to dig into that reasoning though that might reveal something important about the inur that two hours is definitely more than I've expected but still an amazing an amazing resource save for anybody doing a multitenant cell very cool yeah yeah I want to see if this also happens in different clusters so I have some two more graphs this is like on on a bigger cell one of our the bigger cells um you can see the same pattern like At first they're all pretty balanced you can see the differ a bit depending on how they're allocated uh once you Nable Shuffle shutting this kind of spreads out so some tenants will use more some some pods will receive more some will receive less just based upon like how they're like allocated but the huge drop here is before every ingestor was like tracking 200 tenants afterwards it's like 20 so it's just like a lot less um do you have resource savings for that sell yet uh it's happening right now so like the CPU I roll this out like hour my her can around so CPU drops immediately from like 40ish to 30 and memory is also coming down so I'm kind of wondering if this will see the same thing that it drops a lot after two hours or not MH but yeah very nice verying um so yeah that's basically um Shuffle sharting it's mostly useful in highly multitenant cells so as a reliability and like also a resource saving um technique in smaller cells or cells with only one tent it doesn't make sense to use this because I mean you you just use injest so it's only for multi cells all right team great work uh Marty you exist I did not expect you to exist do you want to talk about compaction or no I'm just getting the know no no we're good okay next time we may or may not be trying to improve compaction I'll let Marty tell you in a month uh and you all can figure that one out cool uh any questions uh concerns thoughts uh well wishes or are we gonna wrap it up What's it gonna be I'm pretty excited to see all these changes going in I think the shuffle sharting is super cool this nested set is really cool yeah great stuff the shuffle charting and the compaction changes that may or may not be happening are great operational improvements that we've just struggled to get it put time to because we put so much time into trace ql and all these other things but they're overdue and they are going to make a massive uh impact I believe on TCO and performance for sure cool yeah one question I had on the on the shuffle sharting we saw the memory drop so I I guess we're we're kind of uh guessing a little bit that the wall replay of the tenants that are no longer sending data to those ingestor probably has something to do with that initial decrease but but then is there does the ingestor then hang on to a tenant that hasn't sent it data for an additional hour like what's the what's the time an ingestor on those in those graphs is hanging on to that data are we thinking it's an hour that's why I'm confused about the two hours because there's that complete block timeout and the job of the complete block timeout is to drop those blocks in the ingestor because they're in the back end the queriers know about them and they are queriable but I think that's only like 20 minutes in prod or half an hour PR so I would expect uh I would expect a quick drop immediately because they're just doing less taking less traffic um but then I would expect after 30 minutes to see a major decrease as it flushes all those blocks it doesn't care about anymore so that doesn't make me wonder if maybe the wall is hanging on something too long I don't know it's it's one of those things that you want to go rabbit hole a little bit I'm not sure I'm going to have time but it would certainly it might teach us something important about the investors that might help us save otherwise yeah I guess the followup there is if we restart on a cluster that we've rolled the shuffle sharting out if we restart do we pick the memory back up where we were uh no we used to have issues with wall hanging around and blocks hang around for tenis that stop sing but that should all be cleaned up this would actually be a great test G while you're in the middle of all this if you want to Shell into uh gester and confirm that all the tenants are kind of like decreasing correctly because we did have issu for probably months and I think Marty I'm pretty sure Marty kicked it around for a while I think I patched it some too I think we we I think everybody had their hand in there for a little bit to get it so it would like cleanly and correctly drop old tenants that weren't sending data nice does it do you behind like an empty directory or does it also remove it I think it removes it it's hilarious this is the same conversation as the tenant index kind of in the box but yeah I think it has a I think it has logic maybe it's only on Startup where it's like if there's nothing it does like a it's been so long since I looked that Cod but yeah it would be worth definitely checking to make sure that things get cleaned up correctly but cool all right crew I think I'm going to call it appreciate all yall's time we will see you in April May is the the month after April all our taxes will be paid correctly and we'll uh you know be happy people take care I will see you in a month bye [Music] everybody

