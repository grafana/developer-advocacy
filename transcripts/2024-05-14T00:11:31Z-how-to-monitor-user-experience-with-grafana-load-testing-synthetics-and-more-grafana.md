# How to Monitor User Experience with Grafana (Load Testing, Synthetics, and More) | Grafana

In this video, Elliot walks through user-centered observability, also known as, monitoring user experience. This includes a demo of ...

Published on 2024-05-14T00:11:31Z

URL: https://www.youtube.com/watch?v=dZtBYls-7Hw

Transcript: howdy everybody I've got a quick 2 and 1 half hour long presentation here for you to wrap it up so hopefully yall are ready for this so user centered observability load testing real user monitoring and synthetics um so we're going to be talking about here um my name is Elliot Kirk a senior software engineer here at kaana coming up on about two years of being here now which is cool great place to work and before we get started fun fact about me I like to do this fun sport called highlining it's uh it's a lot of fun have a lot of interesting stories about it so if you want to hear anything come talk to me afterwards um and what are we going to talk about here so user experience is critical um we're going to be going over why the user experience is critical what that means for us and why it's so important we're going to be talking about some solutions that grafana has in order to monitor the user experience and make sure everything is running all smoothly I'll go ahead and do some live demos of them and pray they don't break uh and then depending on how much time we have left we'll do some Q&A or I'll let you all go to happy hour so starting with something that is pretty uncontroversial uh the end user experience is critical um In the End customer experience really is everything uh I'll have some examples in a bit here but as you all would know um bad user experience is going to stop you from using something or cost you some revenue or spent developer time that's going to you know be needed to fix things people are going to file support tickets complaint on Twitter whatever um causes problems that flows into engineering and that that adds up to revenue loss firefighting loss time Etc it's annoying so we want to make sure that the user experience is solid and another thing I don't know about yall but humans are impatient I definitely know that I am in less than one second far less than one second customers start to get annoyed and in a uh at Amazon uh 17 years ago they did a study that said that 100 milliseconds of latency injected into their web store cost them 1% of sales and with a company like Amazon that obviously adds up to a significant amount of sales and similarly Google did something um like this where they injected or they caused uh 20% slower or a half second slower uh page generation for their like search results and it cost them 20% of their traffic which is also massive so we're quite impatient and these kinds of examples show just how impatient we are and really feeds into the fact that we need to understand what is going on with the user experience and make sure everything is running smooth smoothly so the performance Golden Rule uh Steve saers famously said that 80 to 90% of the user response time is spent on the front end so let's start there and we agree here um we got Solutions like real user monitoring and synthetics that try and next slide that try and minimize that uh that frontend time um but when you're working with something like k6 performance testing we see the flip where once the back endend become saturated uh it makes sense that that is the the bottleneck of our response times and without a back end there's no there's no front end to deliver so in the end uh many variables impact end user experience and it's important to handle them um across the entire stack and that is pretty inopportune timing it looks like I'm getting paged by support here there's some kind of bad release going on something to do with our well I'm just going to let them know that I'm a little busy right now but it does seem pretty perfect because I have some demos coming up that will help us to you know potentially resolve these types of issues uh getting paged on stage and whatnot and I guess what a better way to segue than um where do I start we have a lot of solutions there's a lot of aspects to this problem and it can definitely get over overwhelming so where do we begin in well makes a lot of sense to pick the low hanging fruit that is real user monitoring or Rum because I'm probably going to be saying that a couple of times so why should we start here well at grafana we've built a solution that's super low overhead to set up it's just a JavaScript snippet that you copy and paste into your application and you're good to go some of you may have heard of it it's called um farro it's an open source JavaScript agent and I'll give you some more details in a minute here but how does it work well that snippet that you embed into your application it automatically instruments your frontend app and we'll start to collect you know important data uh web performance indicators um automatically collecting your front-end errors possibly configuring front end to backend tracing if you'd like um sessions tracking user behavior all this great stuff uh right out of the box it's awesome we're very proud of it synthetics so synthetic monitoring another solution that we offer here why do we need this if we already have real user monitoring well real users are great we love you all but uh they're not always completing all of your workflows right and real user monitoring isn't proactive it's a reactive solution so we need synthetics in order to make sure that we have all of these workflows covered and I know what you might be thinking my application has a ton of workl flows this sounds like a nightmare to get all this set up the good approach for synthetic monitoring is to focus just on the critical workflows so think integration tests in production things like updating your credit card information changing your password things that users don't do every day but if they weren't to do them there'd be a problem you'd get paged someone would just stop using your product so um synthetic monitoring it just helps to detect slippage or false um failure on your critical workflows and last but not least we've got performance testing and why would I need this as an engineer we already have rum and we have synthetics so what's this for well it can be disruptive to get paged sometimes wouldn't know anything about that um but performance testing it keeps you proactive it happens in pre-production um and with our modern modern systems we want to embrace fail we want to fail early and often and catch these things before they see the light of day and they're really simple it's just uh like maintaining a delivery Pipeline with your other tests uh devs will write unit tests for production um and it fits right into your workflows I'll show exactly how you do it in a minute here and you know what all this reminds me of all these three things together swiss cheese so bear with me here uh the Swiss Cheese model as you add more layers in our case different ways to understand the user experience fewer issues get through because the holes just don't line up so logically it's best to start in prod um rum real user monitoring is the easiest to set up um and then next I would create some synthetic monitoring checks uh to check on those critical workflows and finally I would do some performance testing to shift from reactive to proactive and would you believe it grafana has Solutions for all of these things uh we've got graphon Cloud front and observability graphon synthetics and graphon cloud k6 and that seems like a good segue to do a demo for yall nice okay so just a little context to this demo I think Merl uh mentioned this telescope store that grafana helped to maintain and you know observability and all it sells telescopes so naturally we're going to want to uh make this observable more observable than it already is haha and uh so we have instrumented this with farro and we have some synthetic monitoring checks and performance testing uh running on this just wanted to give you all some context for what we're about to look at here um cool so what we're looking at here is graphon cloud frontend observability and I'm just going to click into this app here to get us into this overview page so what we're looking at here is all of the Telemetry data that is generated are sent to our endpoints from the the phoh web SDK that I was just mentioning the JavaScript agent um I guess this has been sitting open for a second and I just refresh that but looking um from top to bottom here we get some key performance indicators about our application you know things like page loads and error counts as well as something called our web vitals which is a collection of metrics that Google has has deemed important to uh you know web application stability and usability things like time to first bite first contentful paint how long it takes for the application to load initially uh cumulative layout shift how much jumping around happens after the first load and then we have this visualization below which shows all of our errors and Page loads together and we're definitely um definitely doing pretty poorly in the errors Department I have a feeling that has something to do with the incident that I just got paged for so we'll take a look at that in a second we also have this page performance table here that shows us uh automatically collected routes So within your application you have your routes and they often times will have some kind of ID associated with them um our receiver automatically Aggregates those down into into groups uh into their page IDs and you're able to see how each individual page is performing and this one's definitely experiencing a lot of errors in this new order not visible enough I have not seen this before so I'm going to want to keep an eye on that as we move through the rest of this demo here so let's look at some errors now because that's kind of the focus um this is another visualization breaks it down by the count of Errors um by the page ID they're associated with as well as the browser and if I wanted to I could click into one of these here and uh you know take a look at what's going on but before I do that let's just keep diving into the rest of this investigation so we can look at the user sessions also generated by the web SDK um a user session is defined by like a period of activity for somebody on on the web page uh after like 15 minutes of inactivity we generate a new session but this allows us to take a look and see how a user moves through the application so if I can click into one of these guys um we get a similar view we've seen on other Pages uh some kpis the web vitals again but specific to this session the total number of Errors generated as well as the user Journey so what this is is it's essentially all of the Telemetry data that gets collected um for this particular session and if we looked at one you know we could just see some metadata associated with it or if we looked at an error you can see the the error message here and we can view it um in our error visualization we can see traces that get generated um and sent to the end point but if we head to the end of this user Journey here I want to try and find that error and there it is again that order not visible enough this thing keeps showing up and it seems relatively new so let's take this into error awareness here and here we can see some more details as well as the stack trace of the error um these stack traces are about to get much better as we're in the middle of fuzzing out on screen cool um we're in the middle of finalizing some Source map enhancement stuff um that'll help to make these stack traces more usable um to end users I'm going to see if I can fix this again okay well um now that we see all this stuff um let's head back into error awareness here where it's an aggregation of all the errors that we see across our application I'm just going to add this to our watch list and hopefully that'll help us keep an eye on things as this investigation progresses now how did we get this all set up is it really as easy as I was saying it was well if we head into this tab here the web SDK configuration um depending on how you want to use the the web SDK um you know package manager or CDN really all you have to do is come in here and copy and paste this snippet into the the root of your web application and all this stuff comes out of the box for free it's really great so that's front end observability um and that's kind of the first step in this chain here um let's switch it over to synthetic monitoring cool so synthetic monitoring um this is just kind of the the dashboard looking at one of the checks that we have configured um and how did we configure this synthetic monitoring check let's take a look in here and cool it looks fine up there um so how do we configure one of these things well we have some general stuff you know like a name um whether it's enabled uh but then we have all these probe options and basically what's happening is we are picking a location from where these HTTP checks are going to be coming from um it helps us determine like the availability and reachability of our specific endpoint from various places around the world and configure the free frequency the timeout Etc um and then we can configure the request themselves and synthetic monitoring has a new feature called multi HTTP checks which essentially allows us to create chains of HTTP checks and feed the results of one uh into the next so for example um we're calling a product's API here um and we are passing some query parameters and then we're assigning some of the results of this API call into a variable that we can use in a subsequent step so we have this product ID variable and then we can pass it down along here to our recommendations API which generates recommendations for other items based on a previous item and we can see we have this product ID available and we use that as a query parameter for this next check and then we just assert that the response exists it's a very simple check just want to make sure this this API Works um and if we wanted to we could think you're some alerting don't have that set up right now but it's a nice way to make sure that all these checks are still happening and things are running in the background so what does this look like when we run a check here we can take a look at this dashboard um and this gives us an overview of how this check has been running it's been running you know every 60 seconds for a while now so we can we can take a look at whatever period we need but this is just the last hour we can see if the how the assertion is doing over time um we can see that uh you know Paris wasn't doing so hot there um and it just gives us an easy way to check and see how these different probes are behaving over time heading down a bit more we can take a look at the requests by the API themselves so clicking into the products API here we can see no error rates on this one which is great and I guess a quick Spike of errors on the recommendations API which is another thing that could explain the page that I was just getting and if I wanted to I could filter down to just errors only and investigate some of the error logs that are generated by these checks so that's synthetic monitoring um pretty easy to configure a lot of stuff uh just available for you once you get a check running and really helpful to make sure that all of your critical workflows are happening as they should so the last step in the chain is performance testing and for that we're going to take a look at graphon Cloud k6 um I have a few runs that I've already done here but I'm going to click into this guy and before I do this I want to show you how you can run it from your existing developer tools so over here I've got my vs code um and this is the this is the k6 test that I'm running in graphic cloud it's pretty straightforward um it's a browser test and we're essentially just looking for a product card on the page um adding three of them to the cart and then doing an assertion there at the end um and to run it all I have to do is this k6 cloud and then the name of the test itself and I'm going to do that um but we're not going to wait for the test to run just wanted to show you it runs um and let's click into one of these previous runs that I already did awesome so this is what you get um after running a test and visualizing it in graffic Cloud um you can see the number of requests that were made how many failures pretty significant amount of failures based on the total requests requests per second Etc um and that's just on the HTTP side of things because we are also running a browser test we can see all of the browser metrics associated with that test and this is a pretty similar visualization to what you saw in front end observability um we have those same web vitals that we talked about um but it seems like in this test they were not doing as well as they were um in our previous example so that's something to note but heading back to the HTTP side of things I mean if we look at this chart here it's a little concerning I mean this request rate went up really quickly um and then dropped as the number of virtual users users kind of plateaued and then the response time just started to climb and climb and then it plateaued until it dropped off and this failure rate and request rate were just identical till the end of the test so don't want to jump to conclusions quite yet but it seems like something hit the server really really hard and we weren't able to to load balance for this and so maybe some of these requests were getting dropped additionally we get some performance insights based on our test um um we failed two of these and so we can get some recommendations for why these are not performing as well as they could um some strategies to fix them um nice it's great when the the laptop fails and not the demo itself right but um yeah k6 provides us with some automatic insights that help us to determine how we can keep things moving smoothly um and you know prevent any of these issues from happening in the future so that's cool there's a lot more but it was just showing us the two that were failed um but it looks like we're generally doing pretty good so down here we can get some more details on the tests themselves there are 3.3k failures for this recommendations API and just like we were talking about up top it seems like towards the tail end of this test things just started failing and I have a feeling that it has to do that we need to upssize um the nodes working on this because because obviously we're getting hit too hard and we're not able to deliver so that's case six um there's some more stuff in here you know we generate traces um we can visualize more browser metrics as well as all of our logs but I think you all get the picture um and I think we can switch back to the slides now so just to summarize here um we saw graphon front end observability and action helped us to catch some errors and visualiz ize how users are using our applications uh we had graphon synthetics to give us signals if we had something failing um specifically on our critical workflows and then we also add graphon Cloud k6 to let us run some performance tests to see exactly what was going on and help us to determine that there might be an issue with load balancing um and with uh the number of concurrent requests getting too high and that's going to help us give us a lot of confidence um before we can release especially once we get those issues ises fixed so with that um I think that's time for us um appreciate you all for listening thanks

