# Grafana Campfire ðŸ”¥ - Lets talk about Alerting (Grafana Community Call - February 2025)

Grafana Alerting allows you to alert on your metrics and logs, no matter where they are stored. But why Alerting has undergone a ...

Published on 2025-03-01T03:16:01Z

URL: https://www.youtube.com/watch?v=MUiK0qO85aw

Transcript: and we are live all right thanks Osman welcome everyone to the Gana campfire the Gana Community call and today we are talking all about alerting and we get really everyone from alerting well I think um I think alerting through the generations more or less uh Carl's here with me too uh like just like last time um but we also have uh Steve Simpson and Jil deay uh who are both working in the alerting uh team and yeah I think we're GNA learn a bit about uh how alerting came into grafana um how uh it's so developed over the ages and then what we're kind of working on now right I think that's what we have so far so um we also have Osman here representing the community team um and I would say let's go ahead maybe Carl I don't know did you work on did you work on alerting first or like who who kind of brought it into I definitely worked on alerting first H before but before we dig into that maybe we could Steve maybe you want to introduce yourself a little bit more what you're doing right now and J maybe you also want to explain a little bit more than just work on alerting then I can get back to why this was introduced into grafana yeah totally um so I started working on on graffo liting two years ago and I spent the two years before that actually working on uh hosted Prometheus at graffan Labs so really deep into mimir and metrics and the last couple of years been working on Graf alerting in Cloud keeping it running J yeah I can go next um so I've been working on alerting for the past three more than three years now I think um so came in right when we were sort of transitioning from what we now call Legacy alerting to to the new alerting system uh and I've been working on the front end for that for for yeah over three years now it's been it's been exciting I think we have a lot of interesting stuff in the pipeline for the future as well but we'll get to that yeah I think I think I remember those days where because you because I used to run the alerting team as part of the data sources group and uh yeah I think did I hire you Jill I I think I you did yeah I think my first interview was uh was with you you were my first manager back then as well it's like it's so it's been so long ago it it's it's been a long time yeah but but then but then we split it out right and now it's now you're also in a different department which is an interesting kind of arrangement now because alerting was always a big part of grafana um but it's also become a very a very sort of interesting uh sort of intake to whole other processes that we have now around irm and so um kind of fits now to sort of with the future that we have um but yeah start off in the G Department that's pretty cool uh yeah maybe back to you Carl um and because you I think wrote the first parts right yeah that's that's correct so so gr pagana so 2016 uh one of the biggest feature requests for grafana was alerting like they like dashboarding they like visualization but now they also wanted to alert on the same data so we kind of figure out had to figure out how to add that to grafana we started looking at different Alternatives and the things that at that State make gra different compared to Prometheus and other Solutions were that first we were visualizations first and we Tred to cater for a lot of different use cas and the second one was that we supported multile dat sources instead of just one so Prometheus at that point was Canon new uh they had a very interesting take on label doing using labels for uh alert rules and alert rule evaluation but we couldn't really rely on that because graan itself didn't have the concept of labels and the biggest state Source back then was graphite and graphite didn't have labels so we kind of had to constrain oursel to a small feat set and the the way we attacked this was like let's just figure out like deliver something very very quickly so we started by the screen like look looking at the visualization the graph what can we do from here can we add an alert from here and you know from from like the very basic usage that that's a great approach uh and it got some initial adoption and I think we got a few things right so in instead of going for some kind of check based system we went for a Time series based system where you perform a query and then you process that query and decided if if the alert was supposed to be firing or not but what we didn't get right was the the labels or the dimensions and this uh ended up in grafana being a very one alert rule to one kind of server or pod or such scenario because otherwise you will not get alert if server two breaks down when server one is already broken and so on so so the the feature was really catered towards like small setups and not big observability use cases we did explore looking getting like Prometheus into that mix but it didn't feel natural um as the company grow and the project grow eventually we had to do something about prome because Prometheus was a bigger opportunity of us and a growing date source so I think I don't remember correctly was it 18 or 19 when we added a a plug-in on our Cloud environment for for talking to a cloud alert manager or something like that H that that's just details but the next step here was definitely focus on promethus alert Styles um um alert rules and our promethus team they started building this alert capability in our Cloud backends so we started working on this but we didn't kind of incorporate it into grafana at that point but the second version of alerting was primarily focused on on prome style alerting inana and that kind of left the the first use case a little bit behind um and and um you know sometimes these things happens you have something that Focus for one use case you want to go into direction and you kind of go too much in that direction and I think eventually we had to mix these Concepts but that takes time because otherwise you're GNA sign endlessly and I think that version three here is when I'm excited to have Steve and Y on this uh call to talk more about that um but but honestly like the first iteration was something I wrote in two months and I'm very proud to have it being Rewritten twice uh because that that means there's value in this feature right like if you work on something that's never been Rewritten it's probably not that popular or or it was perfectly designed like and yeah maybe we should have perfectly designed it initially but yeah no I mean I mean grafana wasn't very complex at that point either right so I feel like it definitely fit that model back then and and it was very visual right like you had that threshold thing that you could drag up and down so you had really this kind of like graph panel first alerting which I think was really cool and when we had the version two where we tried to optimize a bit for Prometheus as a sort of alert engine I feel like we broke that that connection right like you no longer could just sort of drag something in the graph but because you suddenly had more power with this label mechanism but there was a then like a clear disconnect and I thought like that was a bad yeah that was annoying in the interface and it was confusing to the users and yeah I remember like a great sort of time of pain um where Jil also joined yeah yeah we we very deliberately in version one try to make a simple solution we we did research alert manager with we did research boson I think was the name of another big alerting project but they all felt too complicated um and also like if you want to compete with something that already exists it's going to be harder so yeah yeah but we also hired the boson yeah just not to work on alerting yeah or I mean somehow we did yeah he's done some work uh for um for backend support right um but anyway um yeah should we um kind of move a bit forward in history because now we're at this time now where version two is out and um it's a world of pain uh the users had to select essentially when they wanted to create an alert they had to be aware if it's in a Prometheus style alert or a grafana style alert and and I I think that that just is like one extra question just just caused massive confusion do you remember those days J oh I certainly did um I think that was kind of the first question we asked users when they created a new alert rule which is do you want this to be a graphon Manish alert or a data source manag alert back then we called them um Prometheus or MIM alerts I think um and that you know brought a lot of confusion from lots of different users cuz back then I think we called it unified alerting and then the first thing we do is we ask you to choose between two different types of alert alerts right um so that is that was definitely I think a little bit confusing to the community we're now in a much better place um I think and even going forward looking into the future there's going to be a single type of alert that you create um but I think that kind of brings us to the First Community question here um Usman could you maybe bring that one up yeah definitely this is something a lot of uh Community users especially on our community form ask uh in the very beginning of alerting like what is the basic difference between a grafana alerting and a promus alerting because both can deliver the alerts but uh why would someone preferably go for the grafana alerting so maybe Giles and uh Steve you can answer in a bit more detail um yeah sure I can I can take the first bit and then maybe I'll hand it off to Steve for some of the more technical parts of that um so the first thing you should understand is the difference between the two is in how these alert rules are run and sort of like where they are being send to um so with grafana managed alerts um we really Embrace this whole you know Big 10 philosophy that we have here at grafana where you can alert on any of your data sources right as opposed to a Prometheus or data source manager alert where you need a Prometheus database or you need a memor database where your metrics live we have a lot of customers that uh don't use any of those databases we have lots of customers that use graphite which is still very popular or influx DB or any of the other uh Community uh supported databases or like I think most of them are owned by us and we maintain them actually so they're not Community maintained um so we want to make sure that you know anyone who uses those those databases uh gets to have alerting as well right so then you choose your graphon managed alert this graphon managed alert is evaluate it within the grafana binary at least as it stands and I'll hand it off to Steve in a bit to talk about what that like what that looks like in Cloud um so those are evaluated by the graphon binary it actually um queries your data source to grab all the data frames runs those through um what we call Expressions right because you're going from a set of data frames you want to reduce that into a single number to then compare that to a threshold for example um so as as far as um it or as compared to the Prometheus alerts um those were very different you just write a single promql expression um where it just evaluates that every once in a while within the Prometheus or your memor database uh and then enh that off to whatever alert manager that you have configured right um for graphon manag alerts there's an internal alert manager um that these alerts are being sent to the contact points or or I think we call they're called receivers in Prometheus world are somewhat different um we have a very large overlap in the types of contact points that we support but I think I think the configurations are a little bit different between the two um so in the future that will be one single system um hopefully and that would be less confusion uh and we wouldn't have to ask the user like do you want a grao manage one or a Prometheus um or data source managable any any questions about that because I I also want to hand it off to Steve to talk about so what we're doing with multitenant ruler and stuff no just just I mean because this is all like a bit trip down memory L is all coming back to me now so this um so essentially the Gana alert is where grafana just the binary sort of does it all right and and with the Prometheus alerting this was our attempt to have a bit of composability Right Where were essentially embraced all the various apis that the Prometheus ecosystem also gave you where you had the separation between um this is where the alerts at find this is where they're evaluated this is where the um some rules have to be met and then it fires but then there's another sort of thing that looks at what's firing and only then does some duping and then start sending out the messages so you really had this kind of alerting pipeline this first really um yeah kind of composable sort of alerting stack and for us I think this was also the uh uh really useful to break out some of these things because we wanted to operate them more at scale whereas um graun alerts yeah they it did all of these things in itself right but if you just put all this load on one binary you you you hit some issues right like I don't know maybe maybe that's a good for sort of first setup for you Steve or like what what were we trying to do at scale here yeah totally um just before we go on to that I want to just add something to the the graffo versus kind of Prometheus of um question because I I find this really interesting and the way because i' I've worked on both sides of this um and if you so if you get and what's fascinating is this the idea of kind of Simplicity is completely different depending on whether you talk to someone using Prometheus or using grafana and that was a big culture shock to me because obviously I'd been embedded in mimir and Prometheus so um you know and they say about how oh you just create your files as yo and it's it's really easy it's lovely it's simple um but then of course people using graer they want to use the UI and as you say like they want the line on the graph to to decide where their alert is and they think that's simple um so like you know unifer alerting was really a uh it was an interesting challenge I guess which I wasn't involved in a lot of it but it was definitely a challenge of getting those two worlds to meet um so yeah that's kind of my take on that but yeah so the scalability is kind of an interesting one because as you say so we unified alerting was was a victim of its own success so in graffan Cloud we saw huge uptake I think of of grafo alerting much more than I think a lot of people expected um so obviously graffan cloud has hosted Prometheus it's um uh it's you know it's big into that world and we have a lot of users and customers deep into that world but gra alerting really um took off and it's for a couple of reasons like um some people I think what really resonated people was the ability to have um you know as JS were saying you can query your Prometheus data but you can also query any other type of data so people really liked having sort of one um you know one sort of way of doing that one uh one type of alert um to do that even if they're using like disperate systems to actually store their data so this growth causes a lot of problems so the the first thing we really had to deal with is our database teams were coming to us and saying gra alerting is is is destroying our databases just causing us too much load um so we really had to kind of look in and dig deep as to what was going on um and we we noticed a couple of interesting things I'll go over a couple of them just for sort of Interest sake one of the things that we noticed grafo alerting was doing was instead of where was a Prometheus alert you will just query like the latest data point and compare that to some threshold here you say what's the latest um usage of my of my disc and say is that greater than some threshold um graan reling by default was actually looking back of like 10 minutes 30 minutes it was looking back over a range and then quite often what was happening is it was just taking the last data point so uh we had sort of unintentionally introduced this inefficiency that was actually causing us a lot of problems in the databases uh so we introduced an optimization to to pull those um essentially only query the data points we actually need another big win for us was instead of having all of so you will quite often have your alerts firing or evaluating every 60 seconds that's quite common um especially in Prometheus alerts like 60 seconds it's quite a common interval for your alerts to evaluate um and what Graf alerting unified alerting did initially was it evaluated all of the alerts at like the zero second of the minute so you had these enormous spikes of load on querying like MIM and Prometheus and L A bit like a a bit like a cron drop or almost exactly yeah yeah exactly yeah um and there was actually This Is Not only was it causing because you get these huge spikes you then get some of those queries fail because those systems have to you know they don't have enough capacity to handle them all at you know in the first in the same second so you get some queries failing but what you also get is you have to provision those systems a lot bigger so you have to provision them for that Peak load at the start of every minute so uh it becomes a lot more costly um so a very what we did is this was inspired by how the mimir and Loki alert systems actually work is just to spread those and I think Prometheus does this as well actually is to spread those evaluations over the over the minute so instead of getting the big spike at the start you get it spread over the minute so these were kind of all problems that we were sort of stumbling into uh and we we had to solve them really fast because it kind of as I say it's was like a surprise that so many of these rules were were being used um and you know the problems kind of kept cropping up it was definitely a couple of years ago it was definitely a you know it was a busy time for us yeah definitely I think uh from the community perspective uh they love the alerting component of grafana and they raised some very good issues and even some good feature requests as well and uh and for those who does not uh currently not aware of our alerting community so here's the link which you can also join and uh myself and some other Engineers are also active on the community which can also answer such question or even help you uh if there's anything related to other thing yeah back to you Steve or uh David maybe I can intercept something funny here is the the interal thing that you talk about Steve like we had so many customers and users also interal at one second uh so we we had to put like a mean interval configuration in in our Cloud environment and and people do set these kind of settings at very small numbers or has this very like weird requirements sometimes and I think that is one of the hard things to deal with with alerting as well because it's easier to think that you need to run this quare every second but you might not it's easier to think you might need to run it for every minute but maybe that's is also too frequently it's very much depends on the use case um so for most observability use cases one minute could Al almost be considered to regular um if you just scrape new metrics everye right exactly yeah we I mean that as you say we get that question quite a lot is can I set the minimum you know can I evaluate my alerts every 10 seconds or one second and my response to it is always are you going to be able to respond in 10 seconds is that going to make your response time any faster and it's probably not because it's going to be a human like looking at that alert isn't it I also had I think we had feature request for only issuing alert queries like every hour but exactly on the hour which is also kind of a different clock compared to the clock we use in alerting currently which is more like it doesn't really matter when it starts it's somewhat around the minute evaluation but if you want to evaluate it once every 12 hours that that's a very different feature then you kind of need some kind of asset transaction to make sure that that happens or some kind of locking at least yeah it's definitely important to know the use cases of what this kind of you know periodic frequent querying is is useful for because you say we we have people coming to us asking can I run this alert rule every Friday at 12 12 o'clock and send it send the result to a slack Channel I was like well that's what you've described as a report that's not really an alert right you thinking about this in the you know in the right way um I do Wonder a bit uh since we're talking about all these numbers and limits and capabilities like uh in the sort of current implementation like what what is it like what is it designed to do like what are good intervals or what do you see people regularly do or like what do you even do yourself right to to kind of alert uh on your own systems really comes down to how often you your alert is only as good as how often you collect data from your system so like a very common kind of scrape interval for Prometheus is people quite commonly use 15 30 or 60 seconds so having yeah evaluating your alert rules every 60 seconds is um it's usually more than enough and that gives you a perfectly good response time if those alerts are going to humans like there's no real need to go down to 15 seconds um unless you have all unless you have some sort of automated way of responding um to those and quite often you'll using a different kind of system for that use case and um have you figured out a bit of sort of the limits or is like could we technically support uh every second evaluation or or yeah what are the limits right now once you get once you get down to wanting kind of second or subc response time then you really start getting into into a different category of kind of uh um Solutions you really need to start pushing your uh like this this model of kind of like scraping your metrics collecting story and then running queries on them um starts to break down a little bit just due to inefficiencies like if you're scraping your metrics every 100 Mill seconds for you know think of an extreme example we running your queries every 500 milliseconds and that's actually going to be quite uh expensive um system to run it's definitely doable but they're are more efficient um Solutions like uh to do that yeah um let's look a bit at the sort of overall architecture now um because obviously when you come from angle you can still download a single binary um I'm I'm guessing alerting still works as it used to do or how does this all sort of fit together now with also the new implementations that we run in cloud like is there any sort of shared code or is this all separate no the so the the same uh the same code that exists in open source today is the exact same code that we run in Cloud uh for alerting the what we have had to do um out of necessity uh GFF in Gana cloud is build a system that's slightly more scalable than what you get from the single binary but this is this is because we're really operating at a scale that no one else is like we you know we run the grafana for a huge number of of people and therefore we've had to come up with a system um which can scale to cope with people configuring you know any sort of shape of alerts that we uh they can throw at us in in really vast numbers um but that said like that the you know the code that you run grafana yourself uh then it's all the same code it's all the same features you're getting uh it's just that we've had to build a system that can scale a little bit wider than you would um then you would need to if you're just running a single system yourself and a lot of that work has been like we've been inspired by our other system like our Loki and mimir they have um alerting systems built into them and we've been very it's it's uh you know what's the phrase imitation is the most sincere form of flattery yeah it's it's pretty much taking the architecture right out of those and essentially wrapping grafana in that and that's how we're running altic and Cloud really get to the scale that we need to to be at uh one of the limitations that car mentioned in the very early system was I think you you essentially had one alert per metric roughly right where you needed for example if you wanted to alert on I don't know like too much CPU for a certain node like you needed to run the query exactly for that node have the thresh defined for that and you only got the alert for that one right now uh I think our system now is much more capable right where you can have these kind of summary metrics and then if something in there goes above a threshold or or it might it might show you all these various parts that are passing it right like can you describe a bit that style of alerting sh do you want to do you want to take that one I can yeah I can take that one so that was a good summary David of what multi-dimensional alerts are right so with Legacy alerting you would need to know sort of what your dimensions are up front before you create your alert rules for it right so this work great for those that have um pet servers and not Kettle um as we call it so with multi-dimensional alerts you can create one query that just looks at your your dis usage of all of your notes for example and what it does it will create several alert instances is what we call one for each of those metrics that is being returned where um there might be there's there's a label in there which denotes uh for example a separate node if you or or a computer that you have right so one query and you could you can get that disc discoverability sort of for free right so when nodes are being removed they they are removed from the dimensions if you you add a new node into your kubernetes for example um those automatically get added and picked up whenever the next evaluation of that alert rule happens right so it's it's it's it's a paradigm shift um that is sort of in line with um you know the move to cloud and and sort of cloud native software um so yeah in that sense um great Improvement I think over the old alerting system yeah something else to add to that password Bingo is that it also enables platform team thinking regarding alert rules so in our case internally our platform team manages most so the kubernetes alert rules so whenever we create new software deploy new software we get a lot of alerting or standardized alerting out of the box we don't have to manage that we can of course add exceptions for our own name spaces if needed but being able to write an alert rule once and say this applies to all my infrastructure is a very very good way to enable the platform uh observability use case and I think that's something we're seeing more and more and more which is why the the mul dimensional alerting is so so good is this supported by uh other data sources now uh than Prometheus uh this is supported by all data sources there's I think one very there's there's one difference which is I think important between the grafana managed version of that in the data source managed version of it in the sense that for your Gra fom managed uh multi-dimensional alert rules uh we actually track all of your Dimensions um with promethus or with the data source managed version we only track your Dimensions once the threshold is exceeded um this is because how of of how M works and it evaluates your your um your promql expression and then only creates dimensions for um uh for metrics with labels where it exceeds the threshold with grafana managed um alert rules you get all of your Dimensions so you can see which ones are good which ones are not good and you get that full observability of of all of your noes or whatever it is that you're observing in that alert rule okay um we also got one question I think on the similar topic so it's asked by one of the user that we are all going back and forth with grafana manage alert and data source manage alerting and kind of a stuck in between to manage the alert at end so and mainly need to be seen uh by mixing the deployment so and there's other part of it which uh is also related to the cicd pipelines so maybe anyone of you can describe this in detail I can I can take a I can take a stab at it yeah um so if I understand the question decision between whether to use gra alerts or data source alerts and stuck with data source alerts at the end um because of mixins so yeah this is this is actually a really interesting uh topic because at grafana we're also in the exact same position so we are heavily investing in grafana alerting making it better all the time um but we have a huge amount of our infrastructures actually monitored with mixins and Prometheus alerts so uh we you know we also have this problem and what one of the um one of the pieces of work we're we're going to work on um this year uh is to make it really easy to take those Prometheus alert definitions um that may come from mix or may come from else and actually load them and run them as as grafo alerts because not only is there um you know a big demand for this from outside uh Gana we we desperately need this internally so we can start using the features we're developing uh when monitoring our own infrastructure um so I would I would say that uh yeah watch out for that work coming there's a community issue open and we'll keep it up updated with the progress um but I think that will what that will mean is you can carry on using mixins you can carry on using your prome alert definitions but then you can really take advantage of uh what you get from um Griff found manage to lege as well thanks all right yeah um so I think we talked a bit about um the past now and what the capabilities are right now so yeah maybe we talk a bit about the future like what what's what's currently coming up sort of both in the the way you think about the ux essentially of alerting but also maybe what something in sort of the capabilities on the backend side who wants to go first see do you want to yeah I I can yeah I can take that one um so we actually we have quite a lot of things uh in the pipeline I kind of want to pause at some interesting features that we've already have available mostly for graphon manag rule uh rules um things like I'm thinking um Alert state history for example right so you can see sort of like you can look at your alert Rule and you can see when it went from uh from normal into pending mode and then from pending into firing and then from firing back into normal for example so you can like observe that state machine for all of your alert rules um I think that was a really interesting one we've done a lot of work of on on rule based access control um and such for like all of our Enterprise customers or those that want to sort of lock down who has access to which contact point or who has access to which um alert Rules by way of um placing Ro based access control on folders for example um and as Steve mentioned our as code story there is one currently with terraform for example you could do file provisioning with the AML files um and but we're reworking that to also be able to injust these Prometheus um files um there's things like simplified routing I kind of want to call that one out we got a lot of feedback from the community that deciding or figuring ing out where alerts are being sent to was a little bit confusing mostly because we've inherited U the routing system from Prometheus where it uses label mattress to decide how to Traverse the tree to then finally end up at your receiver um so with simplified alerting you just simply choose which contact Point uh you want to send your alerts to for your alert rules um so that one that one was really exciting as well now in as we go into the future um we will be focusing more on graphon managed alerts right um there's various reasons why we do this there it allows us to develop features faster right so there's a lot more velocity behind that um uh and this is also kind of how we're going to be running things at scale um in cloud and we hear a lot of good feedback from our users that they prefer working with graphon managed rules as far as the UI and the ux is concerned um because there are certain limitations that we inherit um from the Upstream projects such as uh such as Prometheus so that's definitely a direction that that we're going in just to put more features into graphon man rules right um some of the in progress work we have right now is we are taking another good look at um sort of the scalability aspect of of alerting right maybe not exciting for uh for a large number of OSS users but for those that are working with and I'm talking about like tens of thousands hundreds of thousands of alert rules right um those kinds of users will be seeing a lot of improvements there the UI sort of works right now but it's very slow it's very sluggish um but with the pagination work that will be very smooth sailing hopefully um that's interest that's one thing I think people Overlook when we talk about scalability is it's all very well having scalable back end but the front end and back end need to really the front end and the back end need to be in harmony to really give a good user experience and when you as you say as kyl said at the start when you build a system fast um and you you overlook some of these things and it's natural that you have to come back and improve them later so you know when we start thinking about customers or users with tens of thousands or hundreds of thousands of alert rules you've really start we really got to start being smart about what you do on the front end and get it working um nicely with the back end as well so that's the ux team are putting a lot of work into that right now at these shows I believe yeah we've already done a great bit of work there as well um I think we use things like web workers where we can to sort of offload the main thread so that the UI doesn't uh completely come to a halt whenever you work with tens of thousands or hundreds of thousands of rules that we have to go search through for example um but yeah that's we continue to to invest in that area as well nice we got some more questions from the community this one is definitely very interesting one from Tomic uh the question is a as a platform team we like to store alerts in a repository based on your experience what is the best way to allow developers to add additional alerts as code as a config file either EML or Json or maybe terone I can I can tell that one so with grafana right now you um so the primary way of configuring Gra uh if you're not using fire paste provisioning is terraform like that's what we suggest to all our customers and that's really what's um I would say the happy path to provisioning right now grafana um and that applies to alert rules uh too um we do get some concerns that the writing that terraform initially is is hard U but there are features in Graff to export uh you can actually you can create something using the UI export it and they actually give you the ter form which you can then put into your uh Pipeline and then start using that as a template to build more more alert but there's definitely um some changes coming I think in The Wider graan ecosystem around uh as code um this year so that's something to watch out for as we definitely be uh following what goes on there yeah I think for the near future definitely terraform is the way to go uh and we're building out sort of a non- terraform way also to manage those alert rules but that's maybe for later in the year I think this is also a little bit organizational and depends on your philosophy right so and and most larger organization would kind of end up supporting both scenarios so internally we support both scenarios we have some teams who are very very very opinionated about everything being as code first dashboards alert r everything and while some things are not as interested in doing as code we I I personally think the as code is is how you should manage alerts and dashboards for your critical monitoring needs but the dashboard that you might not use for instance response doesn't really matter so so I think you need to think about it depending on like how will your organization be successful and how will they engage with this solution if you try to enforce something they they they don't like going to like then then it's going to be contraproductive okay we we are getting questions quite a lot which is good we got one more uh with alert rules and contact points can you use file provisioning and UI to make adjustment is there a way to do the same thing with not ification policies question steeve do you know uh the and Conta point you can use file provisioning I'm not sure alerting file provisioning right I think that's only data sources dashboards yeah I think so I think it's been if it if it did it has been I think I believe deated um but I believe definitely through ter form you can manage ter uh notification policies through terraform you absolutely can do that um but yeah perhaps not through file provisioning fire provisioning is something more going away from uh even for data sources and dashboard they're still supported but we're going to kind of move away from that concept so for alerts use terraform since that's that's much better and where the the use case that we support I want to call out that there's also just the API that you can use which is what terraform uses right so if you have some bespoke tooling go check out that API we have that documented on the website cool thanks we do have some questions actually this one already came up uh uh before we start it so uh and this is a good question this is something a lot of community uh folks ask on the Forum like why there are alert folders and other groups the same as dashboard so maybe Gils or Steve you want to take this one I I can take this one um so I think there's first I think there might be some confusion in in the question but I'll try and clarify this um in prometheus's world you have uh name spaces or in M world you have name spaces and you have groups so a group is just a virtual concept which um where you place sort of the evalu evaluation interval of all of the rules within that group um and the name space really has no significant meaning in over in grafana in grafana manage rules land we replaced the concept of a namespace with a folder there's still a concept of an evaluation Group which which decides the again the evaluation interval of all of the rules but we chose to use folders because of rule based Access Control I think that's the short answer there um you already have the ability to place dashboards within a folder and then apply permissions to those folders and say like only this particular team um has access to um dashboards within that folder and that was um so by using folders in in gon manage rules that was just kind of a natural extension of that um existing um rback model that that are our customers were already sort of familiar with thanks Gils um yeah I think uh okay car already answer one question directly in the chat so thank you car for that and should we go on question I mean we still have round about uh 15 minutes uh or we we can go on some more topics which you want to discuss especially on around the future of alerting we could um it'd be quite interesting actually follow on what J was saying about um evaluation groups this has been a kind of interesting topic with uh with Graal managed alerts um and leads on to the the topic of recording rules and this is a really um significant feature that we added last year to griffan manag to and it's again inspired really heavily from from Prometheus to the point you we didn't even invent a new name for it it's they're called recording rules because Prometheus calls them recording rules right um so that with to this concept uh to the idea of evaluation groups has been in graffin manag alerts because we've always wanted to support this concept of recording rules um and for those that aren't familiar with Prometheus I can give a very very great background the recording rle essentially let lets you take um like your metric and meeus let you take a metric and then produce another metric which is some function of of your metric so you could example for example produce a sum of a number of series and write that as a new metric to Prometheus and this is a couple of really useful use cases um for one it can be a really great performance optimization for like your dashboards and your alert rules so if you have a dashboard that's showing like the sum of 10,000 servers memory over the course and you want to show it over the course of six months that could be quite an expensive query to do whereas if you roll that data up with a recording rule um it's much faster too can be much faster too to visualize and actually and create alerts on um as well the other interesting use of recording RS is just to transform data into something that's maybe a bit messy into something that's a bit nicer to use and bit nicer to explore uh in internally we do a lot of work with cost analysis so we have lots of recording rules taking kind of raw data for like the usage and the the amount of nodes we're spinning out and putting that you know transforming that essentially into dollar amounts into we are spending this much on compute for example um so recording rules really useful really powerful feature in Prometheus um so we've bought those into into grafana uh and what that gives you is not only does it give you those um use cases you get from Prometheus but the recording rules can work on any type of data so Loki has this really cool feature where you can create recording rules in Loki and you can create metrics from log lines so you can create a metric for number of times you have an error on some system for example so grafana managed recording rules can do that but you can then extend that to any data source right because it's just it's just the you know it's the same fin alerting engine under the heric and query your SQL data it can query Trace data it can query arbitary Json files you've uploaded somewhere right it's just a gra U data source um so we think this is going to be really useful um for our users um just to be a to like because some types of alerting reallyand demand you use recording rules um either to transform the data in a particular way or to get the performance um you need so yeah this is this is one of those things where we don't I don't think we actually understand all the use cases yet it sort of opens the doors to so many kind of interesting things and interesting ways to transform data um using grafana so that's really really exciting I think what makes grafana very unique in this position is the Big T right so to being pulling things from GitHub data source and turn that into metrics or whatever Big Data solution you have and turning that into metx so it's she query and issue queries all all the time I think that's that's very promising with recording rules I'm I'm very excited to see how we can use the big 10 to create cheaper queries or how to like yeah I think you get my point take take the expensive ques making cheaper buy recoring roomes and e to track over time because that's also the one of the disadvantages of some of these bi solution is that if you want to track this over months and months and months just having that as a metrics it's going to be so much easier thanks yeah absolutely uh we got uh an interesting one as well a question and I think this also relates somewhere between like like what we can do with data sources and even with low key uh is to use explore so the question is around the explore that why isn't it possible to directly create an alert from the explor dashboard and the used and they used it as a query so uh probably the question is more related what I understand is like use explore to create an alert rule or create an alert and uh instead of like going into the alert rule ins inside the alert module I can take that one um it's a really interesting question with maybe a very boring answer but I I'll add I'll add a bit of context to it um there are various so first of all not all data sources are supported by alerting the the precondition for a data source to be supported by alerting is for a backend component to exist that knows how to query your database right so right off the bat you're if you're an Explorer and you're using a data source that isn't compatible we have to try and figure that out like if we can do something with the data frames that I uses and whether it has a backend component right um there's no technical blockers here that I'm aware of so this is something we just haven't really looked into um this also goes for data sources that do have a backend component like Loki for example so in Loki you can create uh metric queries and law queries but we only support um the metric queries and not the law queries to alert on um secondly we also see that most folks create alert Rules by virtue of of Integrations right um the amount of of users that go and manually create alert rules is is quite small compared to um the alert rules that being created as part of an existing integration like maybe you've got your MySQL you install a MySQL integration it comes with some alert rules out of the box um so I think the the boring answer that is I think we just haven't really prioritized it I don't see any technical blockers for why it wouldn't be possible though so that's why I think it's an interesting question high up on my wish list would be like a UI that makes it easier to test these alert rules over time and maybe to be able to go back to an incident and then tweak the the query and see if it would have triggered during that time period or something like that I know I know it's a quite challenging problem if you want to do this for real sense like alert like the time range of the alert Rel evaluation had would have to move as well but something that makes it easier might be good enough right so we we did have at some point um something called a back testing API in in the pipeline which I think we either deprioritize or I'm not sure what the status of that is or you can indeed just like you said Carl you go back to um maybe a known incident and you run your alert rule against that and then see like would this have caught um what was going on or maybe we need to tweak our pending period of this alert rule or maybe the threshold isn't exactly what it needs to to be or maybe it's Dynamic right um so that's definitely something that I think we should invest uh more more time in as well boson had that feature to be honest like to give the the project some credit for that oh sweet nice yeah that sounds so useful just having a sort of like a little alert lab you can play around with a playground alerting playground playround the the other side of that's interesting as well it's the it's not only the um like during this why didn't this alert fire but very if you've ever been on call the question you're often asking is why did this alert fire so dealing with the flaky alerts is also a fantastic use of that feature we should do that Jo let's do that it's a hackathon next week isn't it okay cool thanks we got another very interesting question I think this more about like uh from the receiver side for or for the re like uh it's from G it will be great to be able to preview the summary and title we sometime have to raise the alert to validate that the label the variable label or values are correct uh I was thinking actually I wasn't really seeing anything um it's hard right because anytime you think about these previews is you you really need some data to inject into the into the preview so I know for um contact Point testing we actually have a feature where you can test the contact point and the template by injecting certain like label values uh into um yeah I think so I guess it's sort of analogist to of a parallel to that in a sense again it's probably one of those things I think it I don't know if J would disagree with is technical technically possible but perhaps um just not prioritized I don't want see any technical blockers here either honestly I think it's just adding more more input fields for you know what what does the summary look like under this condition for example I think that comes back to sort of this playground that we talked about where you maybe don't only want to look at some uh some past data set or maybe some data set that you you you pulled pulled from somewhere else um but also look at other conditions right so like what what if my alert rule contained these Dimensions with these labels like what would that look like um currently the alert creation flow is very much sort of focused on sort of like what your system looks like right now we don't have that um that I guess that playground aspect that um that is being referred to in this question yeah thanks girls uh we have only like three minutes maybe we can just take one last question and uh can just wind up uh I will take this one we got one from the community uh it's a very interesting one like it's also about like reducing the alerts or saving the cost so it's from uh G if I maybe pronounced correctly but the question is very interesting so how would you analyze High noise alerts you have example alert like CPU usage is high and you have a couple of devices that produce noise but you cannot really do anything about it so you silence them and there's a second part of it is that but they still show up on the history tab what uh what you are interested is in noise from alerts that actually notify you the on call groups tell you what alerts send the most notification so I think maybe not an expert on it but uh uh depends on the interval we we can reduce and set a timer like does how long should it be triggered an alert that can somehow help here I think I understand I think I think took me a minute to grock the question but I think I understand so in our alert date history what you see is when an alert reaches its threshold it comes up as an entry then that transition to firing even though your notification policies or your silences may be set up such that you didn't actually page or send a notification to somebody and so you can't use that data to um analyze um the number of notifications set so there isn't anything built in right now that I know of she has correct me if I'm if I'm wrong um in gra the cloud we do expose I think we expose some logging around this that you can use to analyze but um that's a cloud specific feature and the a common way to do this is to build um so some people have this requirement for auditing they want to know like who has been paged and when uh and it's a bit of a manual solution but some people will build a web hook receiver that logs to wherever they need it to log to for auditing purposes um so you could build that but it would be much nicer to have something like this built in and it's something we have discussed but again it's just always out to priorities so something we have discussed is a parallel to the alert State history but for notifications so you get a notification um history which will give you that detail that you actually uh want that's definitely something that's been discussed but again you're just not prioritized yeah thanks thanks thanks Steve um I think we are already almost about end of the time uh before we end I just wanted to uh just shout out that uh for those who does not know about the Gana alerting Community we have a dedicated Al uh alerting Community form so please join it is a very good resource to ask for help and maybe uh someone from us or even some Gana Champions can help you and definitely for those who are new to Gana and alerting check out our do uh you can learn a lot about alerting we have also some killer Koda sandbox tutorial which are also integrated in this documentation so they can give you a very uh basic overview to more detail like how you can create an alert and how to set up the rules and how to trigger it so yeah um I think that's all for today I say yep that sounds good all right awesome everyone thanks everyone thanks for today see you some other time see you all next time thanks Goode have a good weekend

