# k8s-monitoring Helm Chart Office Hours - 2025-05-27

Published on 2025-06-03T13:50:42Z

## Description

In the May edition of the Kubernetes Monitoring Helm chart office hours, we discuss the recent version 2.1 release as well as ...

URL: https://www.youtube.com/watch?v=12_7y9_EdTI

## Summary

In the May 27th edition of the Kubernetes Monitoring Helmchart office hours, hosted by Pete Wall, attendees discussed recent changes and updates in the Helmchart, particularly focusing on pod logs and structured metadata. The main topic was the recent move of pod name labels into structured metadata to improve scalability and storage in larger deployments. The session also covered the release of version 2.1, which introduced the Alloy operator and new features like in-cluster tail sampling. Key points included the advantages of dynamic deployments and improved user experience through feature influence and better log gathering methods. The meeting concluded with an invitation for community feedback and a mention of Pete's upcoming appearance at Open Source North in Minnesota.

# Kubernetes Monitoring Helm Chart Office Hours - May 27th Edition

Hi everybody, and welcome to the May 27th edition of the Kubernetes Monitoring Helm Chart office hours. I'm Pete Wall, and I'll be your host today. We have a pretty great agenda, so let's dive in.

## Agenda Overview

1. **Pod Logs and Structured Metadata**
   - Recent changes to pod name labels
   - Reasons for the change and its implications

2. **Version 2.1 Release**
   - Key features and improvements
   - Upcoming features

3. **Q&A Session**

---

### Pod Logs and Structured Metadata

Recently, we moved the Kubernetes pod name labels into what's known as structured metadata. [Here is a link](#) that explains what structured metadata is. It's a feature within Loki that allows you to store special information as query-optimized indexes without counting against cardinality or active series limits.

#### Why the Change?

The primary reasons for moving these labels into structured metadata include:

- **Scalability and Storage Concerns**: In larger-scale deployments, each time a pod is deployed, it typically gets a unique name. This can lead to a proliferation of active series in Loki's storage framework, especially with deployments like cron jobs that create many pods in a short time. 

- By moving pod names into structured metadata, you can still use LogQL queries optimized for those pods without counting against the number of labels (e.g., the typical 15-label limit).

#### What Should You Do?

- If you're using the Kubernetes Monitoring Helm Chart's pod logs feature, ensure you have a Loki-type destination. This minimizes internal translations within the Helm chart and keeps everything in the intended ecosystem.

- If you're using an OTLP endpoint for metrics, logs, and traces, I recommend creating a separate Loki destination alongside it for better management.

- If you need to convert back to a label, do so only if absolutely necessary. We have examples in the GitHub repository detailing how to do this.

---

### Version 2.1 Release Overview

We released version 2.1 on May 15th, and we're already up to version 2.1.4. Key changes include:

- Use of the **Alloy Operator**
- Introduction of **in-cluster tail sampling**

#### Why Switch to the Alloy Operator?

In version 2.0, we faced several challenges with the values file complexity and the lack of templating options. The shift to the Alloy Operator allows for:

- **Dynamic Deployments**: The need for static definitions in the values file is reduced, making it easier to manage and understand.
- **Feature Influence**: Features selected can dynamically adjust how Alloy instances are deployed based on user needs.
- **Simplified Management**: New Alloy instances can be created dynamically for features like tail sampling and service graph metrics.

#### Major Advantages of the Alloy Operator

1. **Dynamic Deployments**: Reduces boilerplate in the values file by encapsulating Alloy configurations.
2. **Simplified Configuration**: The configuration process is now more user-friendly and allows for better management of resources.
3. **Support for New Features**: New features like tail sampling will automatically create necessary Alloy instances without requiring manual configuration.

---

### Upcoming Features

We have several exciting features on the horizon:

- **Log Gathering Enhancements**: Improved control over log gathering methods, allowing for a mix of volume-based and Kubernetes API-based gathering.
  
- **Opt-in for Auto Instrumentation and Profiling**: These features will switch to a model where they look for specific annotations before triggering data collection.

- **New Destination Types**: Including a null destination type to help measure generated metrics, logs, traces, and profiles without storing them.

---

### Q&A Session

Typically, we leave time for Q&A. I am currently in the Zoom meeting, so I expect this session to be short. However, I wanted to record this session so everyone can see the recent changes and plans for the future.

#### Quick Plug

On Thursday, I will be at Open Source North in St. Paul, Minnesota. If you're in the area or attending the conference, please stop by and say hello!

If you have further questions, please reach out. We value community feedback greatly, and this project is driven by your needs. Connect with us on the public Grafana Slack channel at grafana.slack.com, in the Kubernetes Slack channel, or post your issues and PRs in our GitHub repository at grafana/k8s-monitoring-helm.

Thank you very much, and we'll see you next time!

## Raw YouTube Transcript

Hi everybody. Welcome to the May 27th edition of the Kubernetes Monitoring Helmchart office hours. I'm Pete Wall. I'll be your host today. Uh and we have a pretty great agenda. So let's switch into it. Uh first off, I want to talk about pod logs and structured metadata. There's a change that we made recently to move uh the pod pod name uh labels into structured metadata. And so I want to talk about uh the reasons for that and what you might want to do. We'll talk about the version 2.1 release that happened about a week ago uh and some of the upcoming features and as always we'll leave leave time for questions and answers. So pod logs and structured metadata. So uh recently we moved the pod and then the Kate's pod name labels into what's known as structured metadata. And I have a link there that explains what structured metadata is. It's a feature within Loki that allows you to store special information uh as like query optimized indexes without them being in like truly indexed labels uh that count against cardality or active series or things like that. So why did we move these two labels into the structured metadata fields? It addresses scalability and storage concerns especially for larger scale deployments. Um, if you think about the way Kubernetes works, every time a pod gets deployed, uh, it gets, you know, typically it gets a unique name. If it comes from a deployment or a Damon set, uh, Kubernetes adds, uh, random characters to it to differentiate it from previous deployments of that same pod. And if you're storing logs from those pods and you're putting the pod name either through just pod or Kate's pod name uh into labels uh stored into Loki, that creates a whole new active series within Loki's storage framework uh for that log stream. Uh and this only gets worse if you have things like cron jobs or just fast running jobs that create lots of pods uh short in a short amount of time. So by moving those into structured metadata uh you can still use logql queries uh optimize for those pods. You can still use those uh fields within your query um and get some optimization. It's not just like a text string search. Uh but it also doesn't count against the the number of labels. Um like there's the 15 label limit that most low deployments have. Uh so why are we doing this now? We we actually if uh if some of you have good memories, we did this uh a long time ago and then we reverted it. Um we did this a long time ago when Loki was just updated uh to include structured metadata. Uh but we found it was a little too early for that. Most people who are running their own Loki instances hadn't updated yet. Uh and so it's just uh it was a bit too early. But we're doing it now because we are seeing more and more large deployments that are seeing these scaler scaling issues and we think it's just the right time to make this the default uh as it probably should have been for a long time. Um so what's important to you? What do you need to do? Uh how do you know if this is affecting you? If you are getting pod logs uh then and specifically using the Kubernetes monitoring helmchart pod logs feature make sure the the best thing for you to do is make sure you have a Loki type destination. Uh I know many people have uh if you've generated your config from the Kates monitoring config page on Graphana cloud uh it will create for you a Prometheus style destination a Loki style destination and an OTLP style destination for uh either um traces or for the entire OTLP endpoint which is metrics logs and traces. Uh the best thing is in my opinion try to keep things into your into the same ecosystem. If you're gathering logs using the Loki style uh gathering, so like the pod logs feature, the node logs feature, you try to deliver them to a destination that is of the Loki type, that minimizes the translations that happens internally inside the Helmchart and it keeps all of the things kind of in the way that they're intended to be. Um what if you're using the OTLP endpoint and you have metrics, logs, and traces and you're delivering to that. Um, I would still recommend if you are delivering to Loki on the back end. So if that is just an interface to a Loki deployment, uh, create a Loki destination alongside of your OTLP endpoint destination. Things that are gathered in the OTLP style, so like application observability will still use the OTLP destination, but things that are gathered in the Loki style like podlocks will use the Loki destination. The Helmchart's smart enough to keep things in the same ecosystem depending on where they're gathered. Um, and then if you need to convert it back to a label, only do that if if it's the the best thing for you, if it's the absolute necessary thing for you to do. Um, we have examples in the uh in the the GitHub repository that explains how to do that. Um, look for like the OTLB uh delivery destination. Um, next I want to talk about the version 2.1 that we released. Uh, we released this back on May 15th. Um, and we're already up to 2.1.4. Uh, and the major changes that came with this one is using the alloy operator and using and the ability to do in u incluster tail sampling. So, there's been a lot of questions about why did we switch over to using the alloy operator and I'm hoping to explain a lot of that here. So, in version 2.0, zero. This is kind of like on the left side a little bit about what the the Helm chart package looks like and especially the complexity of the values file. Uh which in my opinion the values file of a Helmchart is kind of like the user interface for that Helmchart. And there are three main disadvantages of the way that we're reusing uh the alloy sub helmchart as a subchart within the Kubernetes monitoring helm chart. Uh and then there is one you know pretty big advantage as well. One of the disadvantages is that because of the way Helm uses subcharts, uh you have to define everything on how you are going to deploy that subchart up front in the main values file. Uh so because we had five copies of alloy subchart into the Kubernetes monitoring Helm chart, we needed to have five sections of these alloy instances defined and then any sort of things that we wanted to pass down to the alloy subchart. So thinking about uh security contexts or uh volume mounts or um extra labels, extra pods, extra ports, things like that. Uh we needed to have defined statically in the Kubernetes monitoring Helmcharts values file. Uh over time that just added more and more and more lines to the values file. uh which means it was pretty tricky to to sort through for a user what are the things that I'm expecting that it's relevant to me if I'm just trying to deploy this to gather information for my Kubernetes cluster versus what is there because that's the default and I'm not expected to change it or you know things will go bad if I do change it. Um, also if we wanted to add any more alloy instances in the future, uh, it would only make the problem worse. If we added a sixth alloy instance or if we added more alloy instances because of new features that we wanted to bring, uh, it would add a whole new section there and it would only make it worse. Uh, I forget the exact count, but I wanted to say and in Kubernetes monitoring version 2.0, the values file is about 80% alloy config. uh which is kind of silly s considering that it is more or less just a tool that we deploy to get the the um the telemetry data and not actually something that is you know relevant to most people's actual needs. You don't care about deploying alloy you care about gathering your telemetry data and alloy is a tool that we use in um because of number three there's no templating. Uh that's kind of what makes number one and number two a problem. Number three meaning no templating meaning that I can't use things that you define elsewhere in the values file to impact how the alloy deployments work. Um for example if you say you want the application observability feature and you want to enable the zipkin receiver um excuse me immediately the next thing that's going to happen is that the helmchart's going to say oh you need to enable the zipin port in the alloy receiver instance. Um there's no way for me to say you've enabled this feature with this receiver type. I'm just going to enable the port for you. Uh and uh everything like I said everything has to be defined statically. The biggest advantage though is that it's only using built-in tooling. Um there is everything is defined up front which means everything can be revealed by doing a Helm template and it's only using Helm built-in native options. Um when we moved to Kubernetes monitoring 2.1 uh by bringing in the alloy operator uh we traded off some of these disadvantages for some advantages and you know in my opinion I think it was a good trade-off uh and I'm tried to explain uh what what uh what I feel like is the advantages here. So obviously it's a new required component. Um and with all new required components it brings some you know new changes and things like that. Um and uh also because of the way that Helm handles CRD deployments there's a manual step that's required on upgrades. uh Helm will if you have a CRD defined in the Helm chart uh will only install that CRD on fresh installs of that Helm chart and it won't do it on any upgrades uh and it won't touch it on removals and it's the right option but unfortunately it means that there's a manual step to do on upgrades. Uh so what what the change is is that we're deploying the new alloy operator project and then alloy metrics singleton logs receiver and profiles they are no longer using the Helm subcharts but they are a deployments of the alloy CRD they are alloy custom resources uh defined by the alloy CRD uh and then the alloy operator detects those custom resources and then creates alloy instances uh based off of them. Ideally, it will be a largely invisible change. Uh the alloy instances that are deployed will look identical to the ones that were deployed before in version 2.0. Uh and uh and so hopefully there should be no uh additional ramifications based on those alloy deployments. But what are some of the major advantages? So one major advantage is that we're able to do much more di dynamic deployments. We're able to take a lot of that boilerplate stuff that we've had in the values file before and extract that and kind of do for the alloy instances what we did for the destinations. Um we can put them encapsulate them into their own values file where you can see what are the options for alloy deployments. Uh and then also at deployment time based on the features that you've uh that you've selected to use, we can customize the alloy instances to use u the the the you know things like extra ports or volume mounts or things like that. Uh in it's kind of a term I'm calling feature influence. Based on the features that you've selected, what do you need to enable? Uh and then also we can do extra deployments. So the sampler deployment at the bottom there, this is an ability that we would not have been able to do before. So the tail sampling feature in Kate's monitoring Helmchart 2.1. This is brand new. Uh when you can enable that on trace supported OTLP destinations and when you do, we actually create another alloy instance to do that. Now, this isn't an alloy instance that you're expected to uh manage and connect to and and modify the configuration for like the other ones. This one is a special one that uses the powers of Graphana alloy to do the incluster tail sampling. Uh and it's really exciting to be able to dynamically create new alloy instances based on the features that you've deployed. So, we have more options or more features coming up. Um this is an example of that user uh or sorry that feature influence uh that I was talking about before. So if you uh enable application observability and you say that you want zipkin, this is all you have to do. You enable the alloy receiver like before uh and now what the helm chart does is it will automatically insert the extra port definition for that zipin. Uh it merges with what you have there. So if you've already defined that zipin port, no problem. We will we'll use the one that you've defined. uh if you've defined other extra ports, we will merge ours in with it. Uh so it won't overwrite things like that. Uh but the aim here is to just make a more user friendly experience and something that's just a little bit cleaner uh and uh and more what you expect alloy and the case monitoring helmet chart to behave. Uh I was mentioning the tail sampling feature. So yeah, tail sampling is one example of using uh the dynamic ability to deploy alloy uh to create new alloy instances based on the features that you've that you've uh desired to have. Um we have a number of new features coming that we're hopeful to to really um take advantage of the ability here. The next one is going to be the service graph metrics. Uh so this is another one where enabling capturing service graph metrics requires another alloy instance. you know why can't we do this with the existing ones because there needs to be special handling to the trace data the span data uh to understand where all of it's coming from and so the best way to do that is with additional alloy instances um if you've done this incluster before it's typically because you've paired the kates monitoring helm chart with the graphana sampling helmchart uh this is basically taking the best of both of these and combining them together uh another upcoming feature that I'm pretty excited about. So, we have a number of ways to do log gathering in the pod logs feature. You can do volume based gathering. You can do Kubernetes API based gathering. Uh we've got other, you know, exciting options coming soon. Uh the file log, which is kind of like the pod, the default pod logs, but using OTLP style components. Um we're going to do a reorganization of those features probably into new sub into new features themselves. uh with the aim that you'll be able to to mix and match these log gathering features together. So, you know, a really great idea, a really great aim for this will be let's do uh volume based gathering when you have Linux nodes and then you can enable API based gathering for nodes where you can't get to the file system. So if you've got a mixed set of Windows nodes and Fargate nodes where uh you know the EKS Fargate nodes you don't get access to the underlying file system uh you can use that to the API based log gathering for pods on those nodes. Um you can mix and match for uh Windows nodes or something like that. So I think it'll have a little bit uh much better control over how the pod log gathering uh system works. Uh next two are about how we're going to be doing uh control for BA and Pyroscope. So the auto instrumentation feature and the profiling feature, the profiles feature. Uh right now they're they're pretty wide open. You enable them and we both beta and Pyroscope tries to capture traces, metrics, profiles for almost anything that it sees. uh we're going to change that to be more of an opt-in model. So, by default, they'll be looking for specially named annotations. Uh and when it sees those annotations, that's when it's going to uh to trigger to look for those things and start gathering um you know, auto instrumentation data or profiling data. Um I think this will just be a much friendlier way to introduce these features. Um, and so look for an upcoming announcement uh in the next Helm or the Y the Helmchart office hours on how to enable those and how to change things if you want to go back to the uh the default method. Uh, and then we've got some new destination types. Uh, I don't want to hint at too many of them right now, but uh, one of the ones that I think it's going to be kind of interesting, it's one that was brought up in the last office hours, which is just a simple null destination type. you know, let's take all the data and just throw it on the ground. And why would we want to do that? Well, because what we can do with a combination of that and some other uh special measurements, we can get a capture of the amount of metrics, logs, traces, profiles that are being generated on the cluster and we can um use that to count before we actually deliver them to your database destinations. So, that's a really exciting feature that I'm excited to see. Um, and I hope you will be too. All right. So, typically we leave it up to Q&A. Uh, I am sitting here in the Zoom meeting myself. So, I think Q&A will be short today. Uh, but I did want to get this recorded so that uh, everyone can see what's been going on recently in the recent uh, release and the 2.1 release as well as uh, what our plans are for the future. So if uh if you have other questions um oh yeah quick plug so uh on Thursday in two days I'm going to be at Open Source North in St. Paul, Minnesota. Uh if you happen to be in the upper Midwest area or if you're going to be at this conference, please stop by and say hello. Uh finally, yes, if you have further questions, uh please let us know. We love the community feedback. We uh this this project really is driven on trying to make people in our community effective. Uh and we really want to know what's resonating with you and uh and what you like. Um come find us on the public Graphana Slack channel. Uh graphfana.slack.com. We hang out in the Kubernetes Slack channel on that Slack workspace. Uh otherwise uh post your issues or PRs in our GitHub repository at graphfanaates monitoring elm. Thank you very much. We'll see you next time.

