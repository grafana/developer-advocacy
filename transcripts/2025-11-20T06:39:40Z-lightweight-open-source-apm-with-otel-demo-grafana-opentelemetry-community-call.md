# Lightweight Open-Source APM with OTel Demo (Grafana â¤ï¸â€ðŸ”¥ OpenTelemetry Community Call)

Published on 2025-11-20T06:39:40Z

## Description

We're back with the second Grafana â¤ï¸â€  OpenTelemetry Community Call! Join us as we continue exploring how to get ...

URL: https://www.youtube.com/watch?v=cX74hkdI1zM

## Summary

In this second Grafana and OpenTelemetry community call, the discussion revolved around lightweight open-source Application Performance Management (APM) using the OpenTelemetry demo. Key participants included Sel Cler, a product manager at Grafana Labs, Nicole Vanderhovven, a developer advocate at Grafana, and Lud Milov, a technical committee member. Sel introduced the OpenTelemetry demo, a distributed microservices application that showcases modern observability through various technologies including Java, .NET, and NodeJS. The discussion highlighted the importance of correlating metrics, traces, and logs for effective troubleshooting and monitoring in complex systems. They also emphasized the evolution of APM practices from monolithic to distributed architectures and the significance of structured logging. The call concluded with insights on the future of the OpenTelemetry demo and its community contributions, inviting participants to engage further with the project.

## Chapters

Here are the key moments from the livestream along with their timestamps:

00:00:00 Introductions and welcome to the community call  
00:02:30 Introduction of Sel Cler and his background  
00:04:00 Nicole discusses her experience with OpenTelemetry  
00:05:30 Overview of OpenTelemetry demo and its purpose  
00:10:15 Explanation of the architecture of the OpenTelemetry demo application  
00:15:00 Discussion on the importance of APM (Application Performance Monitoring)  
00:25:00 Analyzing the APM dashboard and its metrics  
00:35:00 Exploring alerts and their significance in monitoring  
00:45:00 Correlating logs, metrics, and traces for better observability  
00:55:00 Future developments for the OpenTelemetry demo and community involvement  

Let me know if you need further assistance!

# Grafana and OpenTelemetry Community Call Transcript

**Date:** [Insert Date]  
**Topic:** Lightweight Open-Source APM and OpenTelemetry Demo

---

## Introduction

**Luda:**  
Okay. Hi everyone. Welcome to the second Grafana and OpenTelemetry community call. This time we're going to talk about the lightweight open-source APM and we will use the OpenTelemetry demo as our playground. We have an expert, Sel Cler, who works at Grafana. Sel, do you want to introduce yourself?

**Sel Cler:**  
Thank you very much, Luda. My name is Sel, and I am a product manager at Grafana Labs. I've been working on open-source products for 15 years, first on Jenkins for CI/CDs, then at Elastic on observability, and now at Grafana Labs, also on observability. In addition to my day job, I have been an open-source contributor for about 20 years, starting with Apache Java projects and then moving on to Jenkins integration with Maven and OpenTelemetry. Now, I focus more on the OpenTelemetry project, like the OpenTelemetry demo, and I contribute to OpenTelemetry Helm charts as well. At Grafana Labs, I am the product management lead in charge of OpenTelemetry and I cover other areas as well. [laughter]

**Luda:**  
That's great. It's great to have you here with us. Also, great for my internet to act up just a few minutes before this. Anyway, hi! Iâ€™m Nicole Vanderhovven, a developer advocate at Grafana. I work with Sel over there, and I'm really excited to be talking about OpenTelemetry. I'm the newbie of all of us here. [laughter] I have used OpenTelemetry in some demo apps I've built. Most recently, I used it for instrumenting an AI app. I'm excited to ask Sel all my questions because when he says he likes OpenTelemetry, what he really means is he's the guy at Grafana who's always banging on that OpenTelemetry drum. He's like, "Tell all the things!" [laughter]

**Luda:**  
It's probably my turn. I'm Lud Milov. I work on the OpenTelemetry project and I'm a technical committee member. To echo what Nicole said, nobody knows all of OpenTelemetry, and this call is where we will uncover areas that are new to each of us. Going forward, we'll probably have more of these calls. So, with that, letâ€™s talk about the OpenTelemetry demo. Sel, what is it? Why do we care?

## What is the OpenTelemetry Demo?

**Sel Cler:**  
So, the OpenTelemetry demo was born to showcase OpenTelemetry. OpenTelemetry was created in the context of cloud-native architecture. This demo is a cloud-native applicationâ€”a distributed microservices application that is instrumented with OpenTelemetry. It's maintained by the OpenTelemetry community and showcases all dimensions of modern monitoring or observability, including all telemetry types: traces, metrics, logs, and more at every layer. We started with monitoring the application layer and are now extending to show the infrastructure layer.

We demo various technologies: Java, .NET, Node.js, and nearly all programming languages, including Rust. We also show how to instrument middleware and databases, such as Kafka, PostgreSQL, and Redis, all deployed on Docker Compose to make it easy to play with on your local desktop or deployed on Kubernetes to be more production-like. 

This is a complex demo that shows everything, and what's interesting is that it is realistic and currently used by 40 observability vendors to demonstrate their solutions with the OpenTelemetry demo. So, when you get familiar with the OpenTelemetry demo, itâ€™s really easy to evaluate different vendors quickly. It comes "batteries included," with open-source monitoring and observability technologies like Prometheus, Jaeger, OpenSearch, and Grafana to showcase how to implement an observability solution with OpenTelemetry.

## Exploring the Application

**Luda:**  
Thanks for the intro! So maybe we should take a quick look at what the application itself looks like to make it more concrete. 

I'm connected to a Kubernetes cluster, and I'm just port forwarding. This is the telescope shop we have in the OpenTelemetry demo. We can go and buy things, and now we have something in the cart that we can place an order for. Oh, actually, this application might not work as expected. Well, that's why we need observability, right? If things don't work, we need to figure out what's broken and how to fix it.

Before we move on, I just want to show that even though you only see the front-end web shop, it's a relatively large application. It has quite a few services, databases, caches, a front-end proxy, and a log generator that consistently sends requests. 

We're super lucky to have all the telemetry Sel talked about. We have metrics in Prometheus, traces in Jaeger, and logs in OpenSearch, all visible in Grafana as a single pane of glass that you can refer to. With this, we can start looking at the telemetry itself. But I wanted to check with Sel or Nicole if you have any additional thoughts or anything we should cover here.

**Nicole:**  
No, I think you presented it well. It's a very complex topology because we want to demo everything.

**Sel Cler:**  
Yeah, it's almost complex by design. You could probably have made this simpler, but it's supposed to approximate real applications that aren't simple. That's the problem with some of these other demo apps; they are so simple to deploy that they work because they're monolithic or something like that. This one is much more of a polyglot architecture, involving microservices that represent the real challenges that people face in the field.

## Application Performance Monitoring (APM)

**Luda:**  
One great thing about this demo is that sometimes it's hard to understand how someone onboarded an application to OpenTelemetry. The demo has a lot of information on what services are used, and it demonstrates what's been done to instrument this application with OpenTelemetry, which can help anyone with their journey in the OpenTelemetry space. Okay, wonderful! Letâ€™s change gears and start looking at the APM dashboard. 

Before we go there, let's talk about what APM is for a second. Why do we care? What is it?

**Sel Cler:**  
APM is a very old conceptâ€”over 15 years old. Weâ€™ve had several generations of monitoring tools. APM stands for Application Performance Monitoring. I see APM as a monitoring and observability technology that helps you monitor your application and understand its health. 

When I think about it, the first thing that comes to mind is understanding if my service is healthy or unhealthy. In a distributed architecture, I need to see my service in the context of the connections between services, typically represented with a service map. If I find something unhealthy, I need to drill down to see the health of a service. This means switching back and forth between the service metrics, which are more aggregated, and the traces, which give indications of how business transactions are executed. 

I also need to access logs to verify how things really happen and if there are messages that help me understand the problem. This is how I understand my service. Additionally, my service is deployed on a runtime and middleware, so I need to be able to zoom lower to see how it's behaving on the Kubernetes pod or Linux box. 

APM allows me to switch back and forth between all layers, zooming in and out all the time across all telemetry types. This is essential for understanding health and troubleshooting. Another dimension is detecting anomalies, so I can wake people up at night when something goes wrong. This is where I create alerts and notifications. 

In the past, with monoliths, we created many alerts, but now we adopt best practices with SLOs, observing workflows, and root cause analysis. This is my vision of APM and why I think itâ€™s more important than ever.

**Nicole:**  
When you talk about application performance management or monitoring, do you differentiate that from the wider field of observability? Some people say APM sounds outdated because it focuses on the application or just on performance, which is ambiguous.

**Sel Cler:**  
The monitoring landscape has evolved a lot. With cloud-native architecture, many things have changed, and we had to change our best practices. The best practices from 20 years ago in monoliths are not the same as todayâ€™s best practices for distributed cloud-native architectures.

There were disruptions in the past by innovators who focused primarily on traces, and we learned a lot from that. But now I think the landscape is getting more stable. We recognize the value in metrics, traces, and logs, and weâ€™ve evolved our best practices to leverage the best of all three. 

Some people prefer to call it observability. I know there are nuances, but this is my current perspective.

**Luda:**  
It's good to synchronize on terms. It sounds like we are on the same page. 

**Sel Cler:**  
Now, with microservices architecture, we focus more on alerting symptoms rather than causes, centering more on the health of the application and the service provided to users than on the infrastructure layer. So, application monitoring or observability is becoming the entry point for people in charge of keeping the systems running.

**Nicole:**  
Why donâ€™t we take a look at the dashboard and go through it?

## APM Dashboard Exploration

**Sel Cler:**  
We will cover different parts of the dashboard throughout the hour. Letâ€™s start with the top rowâ€”the red metrics. You can call them duration, error, and request rate. 

**Nicole:**  
I believe the red metrics were created by Tom Wilkerson, who still works at Grafana. I think it was loosely based on the Google SRE book, even though they didn't explicitly say â€œredâ€ there.

**Sel Cler:**  
When you think about any particular service, you would think about some well-known terminology, which might be service level indicators, or just your application. Service level indicators talk more about what's important for the application, while red metrics focus more on the telemetry itself, like the raw data behind your SLI.

One thing that stands out is the alerts. It looks pretty awesome to see that something violates the threshold we've setâ€”probably high latency. Letâ€™s explore it. We see it's been firing for 17 hours. Thatâ€™s quite a duration.

**Nicole:**  
Thatâ€™s not so good. Letâ€™s see the alert on the latency. The P95 is firing for a specific operation. 

**Sel Cler:**  
We can look at this chart, which shows the latency around P99, which is like 18 milliseconds. Is it really a problem?

**Nicole:**  
No, it was for the demo. It's very situational; it depends on your services. The focus was to show practitioners best practices for creating alerts on HTTP endpoints. Typically, you have two families of alerts: latency alerts and error rate alerts. 

**Sel Cler:**  
We decided to take P95 as a commonly adopted indicator. Some take P99, but P95 is often more relevant. Something that's perhaps not noted here is that we take a time interval of 5 minutes. We wanted to provide this out of the box because it relates to the data collection interval, which is typically every minute for hotel setups and quite common in Prometheus. 

Having a 5-minute window is quite good. These are best practices we worked on, identifying good labels to help practitioners succeed by easily routing alerts to the right people and having the right notification schemes.

**Nicole:**  
Imagine if there was actually high latency. Now we can enable this view and look at the examples of operations with the worst performanceâ€”like the outliers you would see here. We use Jaeger as a store for telemetry, and if it were any other store that supports tracing, you would see something similar.

**Sel Cler:**  
This brings us to the Jaeger UI. I don't want to spend a lot of time here, but itâ€™s just another confirmation that nothing is visibly wrong with this operation. Things are just going a bit slower in the cloud, as usual.

### Alert Analysis

**Luda:**  
We talked about alerts. There are other interesting things in this picture. For example, the request rate is pretty normal. If there were a spike in usage, we would see it here. The error rate looks goodâ€”there are no errors. By looking at these three metrics, you can get a basic understanding of the health of your system and this specific service.

**Sel Cler:**  
This dashboard illustrates years of best practices that the industry has accumulated. We have layers of information. At the top, you have your service identifier, the last metrics received time, and the overview of the aggregated service red metrics. 

This aggregated view across all operations of my service will help you decide if this service is healthy or not. If you have doubts or think something is going wrong, P95 is good at surfacing outliers.

Typically, you will scroll down to the HTTP or gRPC operations, where you have the details broken down by operation. Itâ€™s likely that not all business operations have problems; it could be just a subset of them. 

Here you can see the outbound services. If you have a problem, this data will help you understand if itâ€™s happening within your service or is it a downstream service. In microservices architecture, thatâ€™s always a question. 

**Nicole:**  
Are those dashboards powered exclusively by metrics?

**Sel Cler:**  
Everything is powered by metrics, specifically the OpenTelemetry semantic conventions. Initially, the OpenTelemetry demo derived metrics from spans, which was a great idea four years ago, but it had some limits. Now, OpenTelemetry has standardized some semantic conventions for HTTP server calls, client calls, database client calls, RPC calls, and messaging. 

**Nicole:**  
I noticed during my time with the demo that the coverage of metrics is uneven across programming languages. Java, for instance, has great instrumentation, while in other languages, it depends on the instrumentation.

**Sel Cler:**  
Yes, thatâ€™s true. Itâ€™s uncommon for OpenTelemetry instrumentations to emit logs these days, but letâ€™s move on to that.

## Logs and Observability

**Nicole:**  
So, whatâ€™s the deal with logs? Why are they not as important?

**Sel Cler:**  
Logs are traditionally outputted in stdout, and weâ€™ve lost Nicole. [laughter] The challenge is that traces and metrics are captured by SDKs within your app, while logs come from external log collectors, making correlation tough. When we discuss with practitioners and tell them that the OpenTelemetry SDK can directly export logs, they often respond that their log processes are mature and they canâ€™t disrupt them just to trust the new kid on the block.

It takes time to change your log toolchain. And while OpenTelemetry SDKs have capabilities to directly export logs, itâ€™s not easy to get people to change practices theyâ€™ve had for 30 years.

**Nicole:**  
To demonstrate Selâ€™s point about correlation, the most basic way to correlate is by trace ID and span ID. When we capture distributed traces, we can also attribute logs to the specific operation. 

**Sel Cler:**  
Yes, and when we look at a trace inside Grafana, we can see logs associated with it. 

**Nicole:**  
If we open the trace view, we can see the details for this operation. There are a bunch of attributes you can correlate to understand what happened.

**Sel Cler:**  
Logs provide the details. I think the negative sentiment around logs came from the idea that they donâ€™t work as well in distributed applications as they did in monolithic applications. We had to evolve our practices to log the right way for distributed applications.

**Nicole:**  
Itâ€™s user error because logs are the signals you can abuse the most. You can put anything in a log line, which causes issues. With metrics, you donâ€™t have as much leeway, which makes it simpler to deal with.

**Sel Cler:**  
Yes, and we need to convince users that going into log analytics tools is much more efficient than tailing logs directly.

**Nicole:**  
It is more complicated to analyze logs than traces because they arenâ€™t always structured. 

**Sel Cler:**  
Exactly. We have to help organizations better structure their logs for better value. 

## Questions and Community Engagement

**Luda:**  
We have a question here. â€œWe are planning to adopt the LGTM stack as an observability platform. Is there any blueprint documentation aligned with observability 2.0?â€

**Sel Cler:**  
LGTM stands for Loki, Grafana, Tempo, and Mimir. Itâ€™s an integrated solution. Observability 2.0 is about unified telemetryâ€”having everything correlated and not just reacting to errors. Itâ€™s about understanding business impacts when you decrease response times. 

**Nicole:**  
So, itâ€™s more about understanding the overall system rather than just monitoring.

**Sel Cler:**  
Yes, exactly. 

**Luda:**  
How do I know which metrics and traces to prioritize first when instrumenting a distributed system with OpenTelemetry?

**Sel Cler:**  
We recommend starting with auto-instrumentation first. It provides metrics and traces at a technical level, and with HTTP, the endpoints are meaningful from a business standpoint. You start with auto-instrumentation and then add custom instrumentation on critical workflows.

**Nicole:**  
Whatâ€™s the current state of the OpenTelemetry demo? Can it monitor infrastructure as well?

**Sel Cler:**  
Yes, OpenTelemetry was born in the application layer, but infrastructure monitoring came a bit later. Weâ€™re working with the hotel demo folks to showcase better infrastructure monitoring done by OpenTelemetry.

**Luda:**  
If people want to contribute or try out the hotel demo for themselves, where should they go?

**Nicole:**  
I posted the link in the chat to the OpenTelemetry site. 

**Sel Cler:**  
If you want to participate in the demo, we have community meetings, and I can share another link in the channel for more information.

**Luda:**  
Great! Thank you both for being here and for everyone watching. I think that's a great way to end this call.

**Sel Cler:**  
Thank you very much! It was a pleasure to be invited and an honor. 

**Nicole:**  
Thank you!

--- 

[End of Transcript]

## Raw YouTube Transcript

Okay. Hi everyone. Welcome to the second Graphana and Open Telemetry community call. This time we're going to talk about the lightweight open-source APM and we will use open telemetry demo as our playground. Uh we have an expert uh Sel Cler who works at Graphana. Um Sel do you want to introduce yourself? >> Thank you very much Luda. So my name is Siri and I am a product manager at Graphana Labs. I've been working on open source products for 15 years. First on Jenkins on CI/CDs and at elastic on observability and now at graphana labs also on observability. uh in addition I have in addition to my day job I've been also an open source contributor so for something like 20 years starting with Apache Java projects and then going on Jenkins uh integration with Maven Jenkins with open telemetry uh on now uh I focus more on open telemetry project like the open telemetry demo and I contribute to open telemetry hand charts as well so that's only. Yeah. At Graphana Labs, I am the product management lead in charge of open demetry and I do other areas as well. [laughter] >> That's great. It's great to have you here with us. Also great for my internet to decide to act up just a few minutes before this. But anyway, hi. I don't know if I was I don't know if it was my turn and I missed it. Hi, I'm Nicole Vanderhovven. I'm a developer advocate at Grafine. I work with Le Villa over there and um and I'm really excited to be talking about open telemetry. I'm like the I'm I'm the most newbie of all of us here. [laughter] I [clears throat] have used open telemetry and anger. I don't understand all of it. I have used it a lot in some demo apps I've built. Um and I and I most recently used it for instrumenting AI an AI app. And I'm excited to ask Sirill all my questions because when he says he likes open telemetry, what he really means is he's like the guy in in Graphana who's like always banging on that open telemetry drum. He's like tell all the things. [laughter] >> Thank you. >> It's probably my turn. I'm Lud Milov. I work on up and telemetry uh project. um the technical committee member. But uh to echo what Nicole said, nobody knows all of open telemetry and this is uh the call where we actually going to uncover areas which are new to each of us. Um and going forward we'll probably have more and more of this. [snorts] So with this um let's talk about open telemetry demo. So sur what is it? Why do we care? >> [snorts] >> So uh what is the open telemetry demo? Um the open telemetry demo I think it was born like uh a demo application to showcase open telemetry and so um open telemetry was born in uh cloud native architecture. So here this is a demo of a cloud native application a distributed microservices application that is instrumented by open telemetry. It's maintained by the open telemetry community and uh it's showcasing uh all the dimensions of modern monitoring or modern uh observability as we could say all telemetry types, traces, metrics, logs more and more every layer. We started at the monitoring the application layer and now we are extending uh showing also the uh infrastructure layer. We uh demo with various technologies Java, .NET, NodeJS, all the programming languages that you can imagine almost are demoed there even Rust. We demo also how to instrument middleware on databases. So you have cafka, you have posgressical, you have radius valky uh on all these deployed on docker compose to be very easy to uh play with on your local desktop or also uh deployed on kubernetes to be more production-l like. So it's a very complex demo that shows everything and something that is is very interesting to me and why it's important it's it's realistic and it's used now by it's a de facto standard. We have 40 observability vendors monitoring vendors that demo their product their solution with uh the open telemetry demo. So it's really easy when you get familiar with the hotel demo to evaluate different vendors extremely quickly. So that's uh that's an overview of the hotel demo and something interesting is that it comes batteries included. So you have open-source monitoring and observability technologies namely Promeus Jerger open search and graphana to showcase how to uh implement an observability solution with open telemetry. So batteries included open source and also as I said 40 vendors uh showcasing how they integrate their commercial solution with it. So that's an overview of the hotel demo and why I think it's fascinating. >> Yeah, thanks for the intro. So maybe we should take a quick look at what the application itself to make it more uh concrete. So um this is actually I'm connected to serial cluster uh Kubernetes cluster. It's deployed to Kubernetes and um I'm just port forwarding. So uh this is the telescope shop right we're up in telemetry. Um so we can go and buy things and now we have something in the cart and we can place order. Oh, actually this application might not work as you expect. Well, that's why we need observability, right? If things don't work, we need uh to find out what's broken and how to fix it. [clears throat] Um and um before we move on, I just want to uh show that the even though you just see the front end web shop, it's actually a relatively large application. Uh it has um quite a few services uh and it has databases, caches, um and [clears throat] a front-end proxy and the law generator that consistently u sends some requests. Um and um if we just go and start exploring what's going on. Well um we're super lucky to have um all the information all the telemetry serial talked about. We have metrics in Promedius. Um so this is just the architecture. We have traces and yagger logs in open search and graphana as a single plane of glass that you can uh refer to. [snorts] Um and with this uh we can uh start looking at the telemetry itself. But I wanted to check with Seriel and or Nicole if you folks have any additional thoughts. Anything we should cover here. >> No, I think it uh you present it well. It's very complex topology because we want to demo everything, >> right? Yeah. >> Yeah. It's almost like complex by design, right? Like you could actually probably have made this simpler, but it's supposed to approximate real applications that are not simple because that's the problem with some of these other demo apps that are that are so simple to deploy, but like well of course it works because it's just it's a monolith or something like that. So I think this one is much more of like a polyglot architecture. It's microservices. These represent the real challenges that people face on the field. >> Yeah. And one great thing about this that uh sometimes it's hard to understand how somebody on boarded application to open telemetry. this the demo has a lot of information on what actually uh what services are used and um let's say I think it's net application but it shows what's been done to instrument this application with open telemetry which can help anybody with their journey uh in open telemetry space. Um okay wonderful. So let's uh change gears and start looking at the uh APM dashboard. Um so maybe before we go there um well this is the I don't know piece of art uh created by serial. Thank you. Uh and maybe we we should just talk about what's what's APM for a second. Uh why do we care? What uh what is it? Okay. So, who starts? [laughter] >> Go for it, Seriel. >> Okay. So, I think APM is a very old concept. I think it's more than 15 years old. Uh it's maybe we have had several generation of monitoring tools on APM Stone for application performance monitoring. I think on on now I I see APM as u the monitoring observability technology pro solution that helps you to monitor your application uh to understand the health of your application. So when I think about it uh what comes to my mind is uh we need first to to understand if if my service is healthy or unhealthy. That's the first thing. on uh now as I am in a distributed architecture I need to see my service in the context of the uh connection between services that's typically what I have with the service map and then I need if I feel that something is unhealthy I need to be able to drill down to deep dive in the health of a service to deep dive it means that we have I will have to switch back and forth between the metrics of the service which is more an aggregated the traces which give me indications on how the business transactions execute on uh also accessing the logs to verify how how things really happen on is there a detail there are message that help me understand the problem. So this is how to understand my service and also my service is deployed on a runtime a middleware. So I need to be able to zoom lower to go on the uh Kubernetes pod or on the Linux box to understand how it's behaving as well. And so this is what I get with the APM is to switch back and forth between all the layer zoom in zoom out all the time all the telemetry types. And so this is to understand the health and troubleshoot. And also uh another dimension of it is I need to detect anomalies. So to wake up people at night when something go wrong. And so this is a place where I will create my alerts notifications. All this has evolved. In the past with monolite we were creating many alerts. Now we adopted the best practice of SLOs's of observability or troubleshooting workflows or root cause analysis. We called it slice on dice and so on. But yes, this is my vision of APM on why I feel it's it's more important than uh than ever. >> So serial um when you talk about application performance management or monitoring, I've heard it said both ways. Um do you differentiate that from the wider wider field of observability? uh because some people like I I have heard a point of view that APM as a term is kind of outdated because of because of many things like the fact that it focuses on the application or just on performance and it's kind of ambiguous like performance as to of what >> um I think the monitoring landscape or the activity of keeping stuff up and running has evolved a lot because with cloud native many things have changed so we had to change our best practices. the best practices on the monoly 20 years ago are not the same best practices are on distributed cloudnative architecture today. Um and so these product have evolved on [snorts] uh maybe at a point in time there were um a big disruptions uh by some uh innovators who say I'm going to really focus focus focus on traces on to be almost trace only solutions and we have learned a lot with it but now I think it it's getting more uh stable less uh tensions uh because we uh we recognize that there is great value on metrics, there is great value on traces, there is great value on logs and we have evolved our best practices to to get the best of all three togethers on also profiles and there is no uh we didn't come to the conclusion that one of these uh type of um uh telemetry is bad but we or they can be all bad if we misuse them. If we use them as we were using them 20 years ago maybe but if we use if we rejuvenate the way we use them now um they are very useful on and I feel this we can call this APM it many people understand when we say this is APM and some people will prefer to call it observability I know there are some nuances but this is a picture I uh I have at the moment >> yeah so applications and infrastructure has people who work on them have certain needs and certain problems they would like to solve and you can call the solution APM you can call it observability 2030 but it it doesn't really change the problem itself um and uh I I do think that APM sounds like a bit old school but yeah it's also appeals to a lot of people [snorts] >> um >> okay well why don't we have a Oh, sorry. Go on, Sarah. >> Yeah, to add something. Now with microservices architecture, we focus more on alerting on the symptoms that what's really causing pain to users rather than doing alerts on causes like monitoring the CPU, the disk and so we are more centric on the health of the application of the service provided to users than on the infrastructure layer. So I feel the application monitoring or application observability get more the entry point for uh people in charge of keeping the stuff up and running. >> Yeah. I mean it's it's always good to like synchronize on on terms. It sounds like we're on the same page and the term like it it really doesn't matter if you are on the same page. But why don't we have a look at the the dashboard and like maybe and maybe we could go through it. >> [snorts] >> Yeah. So I think we we will talk through different parts of the dashboard uh through the the rest of the hour but maybe we'll we'll spend um first we will cover the top row right the red metrics. There are plenty of other ways to call them. Uh you can um like maybe we should call them there duration error request rate right or rate error duration. Anyway, [snorts] uh so >> yeah, and I think that that that it was actually created by Tom Wilkkey, right, who who works at Graphana still. >> Oh, nice. >> I think it was loosely based I mean, I don't know, maybe maybe I'm wrong. he can correct me, but I believe it was loosely based on the Google S sur book, even though they didn't actually say red there, but they talked about some of the same metrics and stuff, >> right? And when you think about any particular service, you would think about the some well the new terminology might be service level indicators or just your application. Um but service level indicators talk more about uh something that's important for the application and red metrics talk more about the telemetry itself like the row data behind your SLI or uh anything that you want to pay attention to. Um, so one thing that stands out here, uh, is the alerts, right? Uh, it looks pretty awesome to be to see that something violates, um, the the whatever the probably high latency, the threshold we put. Um, so let's maybe go and explore it. Uh, we see it's been firing for 17 hours. That That's quite a time. [laughter] >> That's not so good. Yeah. Or maybe let's see. Right. Uh so this is alert on the latency. Uh and um on P95 and it's firing for a specific operation. We can see all the details here. We can maybe go and explore it further. I learned a trick today. This shows the instant time. We can include the time range. And one thing I really love about Graphana that it shows you exemplars from different for one metrics for the traces. So we can actually take a look at specific things. Okay. So we see a chart here. It shows us that the latency is around P99 is like 18 milliseconds top. Is it really a problem? >> No, it's it was for the demo just that we Yeah, it's a very It depends on your services. Uh what we we looked at is uh if you come back to the previous uh uh screen with the PromQL request. >> Uhhu. A a big focus was to uh show to practitioners how uh some best practices on how to create alerts on uh HTTP endpoints and typically you you have two families of alerts. You have the latency alert, you have the error rate alert or maybe you combine them together and then to show people how to create an alert. What are the best practices of observability specialist or monitoring specialist on uh latency? uh so uh here we decided to take P95 which is commonly adopted indicator some others will take P99 but it's rarely uh relevant often it's more 95 um then something that maybe uh is not noticed here and I had the pleasure to work with many alerting specialist is here we take a time interval of 5 minutes as you can say see on the prom request and we wanted to to provide this out of the box to practitioner because it relates in fact to the uh data collection interval which is every 1 minute typically for uh hotel setups and it's also quite common in in Promeus. So having a 5 minute window is uh is quite good. So these are a lot of best practices that we we worked on as well as identifying the good labels to set people for success so that they can easily afterward route these alerts to the right people have the right notification uh schemes and these are all the things that we wanted to package uh in this demo on in this alert. So a lot of thoughts on it. It was a very exciting uh experience. Only the the threshold is not is crazy. It's not realistic. [snorts] You're right. >> Yeah. Yeah. But it's pretty cool. It's also um I love that it shows it's warning. So in theory, you could uh you could have some ranges for it and you can consider warning as I don't know your P4 or whatever. And I love that you have a team name here so that uh when um the on call engineer wakes up in the 3 a.m. they know whom to reach out to or if it's their alert that belongs to their team at all. >> Um >> can I add something please? And also we specify the deployment environment because maybe you use the same prime storm observability store to store staging on production data and you must filter on just production because otherwise your metric doesn't mean anything and so it was also a best practices that we wanted to push here. >> Yeah. But imagine if uh there was actually a high latency. Uh now we can uh go and uh enable um this view and we can take a look at the examples of the operations with the worst performance like the outliers you would see here. So we use Jagger as a store for telemetry. So if if it was any other story that supports tracing you would uh see something similar. But this brings us to the Jagger UI. Um, and I don't want to spend a lot of time in this view. It's just uh yet another um confirmation that nothing is visibly wrong here with this operation. It's just things are going a bit slower in the cloud as as usual. Okay. Uh so we talked about alerts. Um there are other interesting things uh in this picture. So this is a latency alert. But what uh the other things show us here that for example request rate is pretty normal, right? If you would if if there was a spike in in the usage, we would see it here. The error rate looks very exciting. There are no errors. And like by looking at the three metrics you can get a basic understanding of the healthiness of your system of this specific service right >> I wish it were in order reed [laughter] maybe that's super random >> yeah so here there is an order uh there is a lot of logic in this in dashboard and it was the opportunity for me to learn all the best practices that the industry has accumulated with dozens of years and in fact we have uh layers of information. First at the top it's quite easy. You have your service identifier. So hotel demo/cart if you can share again maybe. >> Sure. >> It work. Yeah. Thank you. And then you have last metrics receive time because when you have to troubleshoot to know if your uh instrumentation works, you want to know uh when you receive the last metric. And then we have the overview the aggregate with this uh server red metric. So duration or uh rate you have the overview of the else of your service which is intended for you to decide is this service LC or not LC. I'm investigating on the problem on a problem. Is it this service? Is this service encountering a problem or not? And hopefully just this aggregated view across all the operations of my service will tell you on if you have a doubt if you think something is going wrong on P95 is good at is identif surfacing out layers. Typically you will you will go in detail on so you will scroll down to the HTTP operations or the gRPC operations where you have the details broken down by operation because maybe it's likely to not be all your business operations that have problems maybe just a subset of them. And so here you have some logic to help you to narrow down your problem to isolate the sub population that has a problem and to tell you everybody is impacted. So maybe it's the entire service or maybe just a subset and then maybe there is a business dimension to it or a dependency that is specific a downstream service. So this is what you can see with a list of operation. It requires more time to analyze on a deep dive and then you have the view of the outbound services. Maybe uh yeah this service you have to scroll maybe to check use another one. Yeah maybe check out. Yeah, here you see the outbound operation and um we are working with the hotel demo people to to have more realistic uh details here to have inbound and outbound everywhere. If you have a problem on on a service, it will help you to understand is my problem happening within my service or is it a downstream service? Because with microservices architecture, it's always a problem. Is it is it me who is having a problem or is it something downstream? And so this is what we help you identify with uh this outbound uh data. >> And then if you continue to drill to scroll down uh you have the infrastructure view because maybe you have to check at the infrastructure. >> Yeah. Are those dashboards powered by metrics alone? Everything is powered by metrics on exclusively by the open telemetry semantic conventions metrics. So initially hotel was more used to derived metrics from spans. It was a great idea four years ago. It has some limits on now for a few years. Now open telemetry has standardized uh some uh semantic conventions metrics for HTTP uh server calls client calls database client calls RPC calls messaging on lunilla you know this very well because we are one of the key contributor to the semantic conventions and we try to use them in this dashboard as much as possible and it works well. Yeah, and maybe this is some feedback to open telemetry and myself that um as I play with the demo, I discovered that we the coverage of metrics is uneven. So for example in Java pretty much every instrumentation emits good traces and metrics. Um and in other languages it it depends on the instrumentation. some only limit traces uh and I'm yeah so we definitely can there >> oh logs wonderful um so it's uncommon for open telemetry instrumentations to emit logs these days but >> applications okay what is with that I don't understand that why why is it not as important bring back the logs >> uh Sarah I want and talk about it. So um yeah logs is a is a so first something I love in this dashboard and I love with open telemetry the metrics are powered by promeus the logs are powered by open search and we have perfect correlations okay it's because instrumentation is done well but thanks to the open telemetry standard we can provide built-in correlation between traces metrics and logs thanks to hotel semantic conventions and I think it's something extremely powerful for uh practitioners is to have um a much better uh vendor looking free because all this is standardized and they can integrate and also we know we always have uh diverse monitoring products or observability products and we can combine them and we we illustrate this in this dashboard which I love. Uh so that was the first thing I wanted to call out here on that I for me this dashboard illustrate a great success of open telemetry. Um now uh why logs are less well supported than others by hotel. Um I think one of the key reason is logs are are traditionally outputed in std out on uh I think we have lost Nicole maybe she had a network issue but let's >> I'm still here. >> Oh you are here. Okay. So logs [laughter] >> you they are typically outputed in a file or now with Kubernetes or containers in std out and so it's it's quite complicated. We have the traces of metrics that are captured by SDKs that are within your app and that we have to correlate with logs that come from an external log collector and so that makes our life quite hard. And also when we we discuss with practitioners and we tell them your hotel SDK has a capability to directly export logs the people often answer that very nice thank you but I've been doing log for 30 years we have extremely mature processes in our company and maybe logs is instrumental to our troubleshooting our own call procedures on all this and it's not something that we can disrupt like this just to trust you because you are as a new kid, new cool kid on the block. And so I think it takes more time to change your logs tool chain. It's really a tool chain with injection pipelines transformation on all this and so to make this tool chain this pipeline embrace hotel and get uh the right correlation to integrate them all together. So I think it's it's why it takes a bit more time uh for logs and also we hotel has to every programming language have their logging framework. So I come from the Java ecosystem it's log forj log back and so hotel SDKs have to hook in all each of these uh logging framework which is a bit of of work. So it take a bit of time but now with Java withnet I see many organization uh fast IT slow IT uh insurance companies banks tech companies uh using hotel logs very successfully today. >> Yeah just to demonstrate uh serill's point on the correlation uh so the most basic way to correlate uh is by trace ID and span ID. So when we capture distributed traces, we can also u attribute logs to the specific operation. And if we open the trace view inside graphana, we can see logs here. Um and maybe there are no logs for this uh specific uh span ID. >> There should be some for the trace ID. And here we go. Uh we can actually see the details for this operation. And all of this is coming well not all of this is coming from application. This one looks like something from engineix. Um but we can uh for example see the details. Uh okay. Payment went through is just the human readable uh version of it and there are a bunch of attributes you can uh correlate uh to to what happened. Um if there is something interesting here maybe some data set and name space anyway [snorts] so this u the logs give the details why I think we don't use them in instrumentations uh to echo what serial said logs have been there for I don't know 40 years 50 years and there's so much prayer art and uh when we work on observability we look into like new areas, more structured, not focused on human beings, focused on the machines, matrix first and then traces or more semantical, more structured. You can make sense of them without knowing about each specific specific format or record. Logs are still, I feel, more in the human realm than uh machine realm, but it's changing. And to to add to your point, I think um with the observability uh the rise of observability, some people have been negative on logs. And I think there was a very valid point to say logs as we were doing them 20 years ago in monolith applications. They no longer work in distributed applications. And so it was a kind of pund where we say, okay, we say very bad things about logs, maybe too bad. [laughter] But uh we had to evolve our practices to do logs the right way for uh year 2025 and they are very good ways now uh adopting more structure logging or maybe canonical logging. There are some best practices uh correlation with trace ids on span ids on all these things uh are very valuable and as you said yes they are more for mach less for machines and when we analyze traces to enable troubleshooting workflows analysis of data it's very easy to analyze traces because they are well structured on logs it's much more complicated because often they are not structured but it's us who build this obserability tool to also help organization to better structure their log to have better values on here. Yeah, structure logging. Now, we also have all these log pattern extractions, um, algorithms. Uh, and I'm Yeah, I'm confident it's logs are here to stay. >> Yeah, I am too. I think it's kind of like a user error thing because of all of the the telemetry signals, logs are the ones you can abuse the most because it's not necessarily structured. You can put anything in in a log line and and that causes a lot of issues if you know like we're we're trying to make things as structured and as predictable as possible so that you can do things like correlate um correlate different telemetry signals but when you just when people just put whatever or like they log too much or like it's so easy to overdo it with logs there. I think you need to have better discipline. Whereas with metrics, like metrics are numbers, you know, like you you don't there's not as much leeway to to there's not as much play in that structure, which means that it's simpler to deal with. >> Yeah. And I I had an anecdote today that was very interesting. I I was discussing with a fintech company on um we were discussing of adopting OTLP for logs to directly send the logs well structured and they said yes but I use cube cuttle also to troubleshoot my system and I directly connect to uh my component and I do a tail on it with cube cuttle or just tell on the file and how can I do it if I uh go do I duplicate my logs sending both to my logs analytics back end and also keeping them in stout or uh local debugging on its complex practices. it is but I think the the most difficult part is that uh there are uh best practices that tell you that logs should go to std out and then uh the way to actually get them into log management system is by scraping std out and then it's way harder to keep the correlation around but also multi-line logs it's just it's just impossible to support in this way. Yeah, >> we we have tried [snorts] hard. >> Yeah, I think also that there's so many that it has been around for so long that there are already other tools and workflows that people have gotten used to, you know, like even I find myself like with a whole Graphana Arsenal and Loki um at my beck and call there. Sometimes when I have a Kubernetes cluster, I still just use K9s and then because it's just it's just easy. It's just there. it updates and then you enter its command line. It's just like I don't need to install anything else. So um I think that can be quite difficult but as you point out siril like one of the things that hotel is great for is the correlation you know because in that way like yeah I can see the logs of a particular pod but I'm not going to be able to see like okay for for these particular logs you know what what was the what was the error rate um like what was the what was the trace that was corresponding to it or you know like you can't you can't hop signals that way because it's it's not meant for that. So, it's only by putting all of these things together that you can start to make sense and and see if it's like is it just correlation or causation or like what actually is the problem here? which came first you know >> and something I was discussing also with some um some colleagues on logs is um I think it's on us people who build this observability product to convince you that going in your logs analytics tools is 100x more efficient than doing a tail on your box because we can do log pattern analysis we can do trend analysis on your logs you can as you said correlate is help you to navigate from a log to a trace on uh when you look at a log line uh do you know if the business transaction was successful or not? Maybe your the log line says there is a problem but in fact the business transaction is successful and [snorts] if you just look the log lines in isolation you don't know if you are able to correlate with traces then the tool can tell you yeah it's not that important log line this is not the the message you are looking for because this one is a success eventually and I think it's us who build the products who who have to add so much value in our integrations in our logs analytics that you will you will prefer to to use our our solutions and just to do a a grape or a tail on on your box. >> Yeah, I think it can go the other way too. >> Uh sorry, I was just going to say I think it can go the other way in that um some I I have worked on on projects where there was really really low response response time. So the duration was very low, but then when we looked at the logs um or at the metrics like you could see that it was very low because it was erroring out. So it's very quick to serve up an error page. So like just the sheer duration is not enough. You really have to see the whole context of it. >> Yeah, absolutely. And this is where the good design on the metrics comes into play. Like when you design your metric, it should include the indication that whether the operation was successful or not and it should be something well known uh and all [clears throat] metrics that report duration uh should follow the same pattern essentially. Um but just returning back to the logs for a second, there are areas where logs or events would play the critical roles. For example, uh mobile or browser level things where like when you scroll your timeline and whatever uh social network you use, it's very hard to represent it in terms of spans like what is the spend? How much time do you spend on this page uh each individual operation maybe? But also like that you liked something. It's an event or a log. uh when you um switch to some content uh is is an event um and these are more structured and this this is definitely something that would go through the hotel rather than uh go to std out especially if it's coming from mobile application and the std out is somebody else's phone. >> Uh we have a question here from Miss Saudi. It's okay if we if nobody knows the answer, but they're asking, "We are planning to adopt LGTM stack as observability platform. Is there any blueprint documentation aligned with observability 2.0?" >> Um. Mhm. So, observability 2.0. Yeah, I think I've read uh things about it. Uh we are I think LGTM so LGTM is a Loki graphana tempo mimir. So Loki is a is a a logs u database logs storage. Graphana is a visualization. Tempo is a traces um storage on mime. M stands for mime which is a a back end for promeuse. So this is a graphana integrated solution on I think we if observability 2.0 O is uh about high context uh telemetry all unified together on also having uh trend analyszis uh computed anal analysis of your data to surface insights surface abnormal behaviors. I think it it's typically what you you will get in LGTM today. Um now um I remember with this conversation on what you will get in with just an open source stack on what you will get with a commercial stack. I think there are still some um technologies some uh in recent innovation in terms of observability related to a IML statistical analysis of your data that are that are possible in open source but you will have some do it yourself uh when in commercial products uh that can be graphana cloud which is a supererset of LGTM or that can be some other vendors you will have stuff much more integrated Yeah. And I think that 2.0 is really um it's not revolutionary, it's more evolutionary, you know. So we're already we're already there. So we've already been talking about the unified telemetry approach where it's not like should you do metrics or logs or traces. It's like no, you should do everything. You should but not just do everything, but they should also be correlated. You should be able to mix and match and like take a slice, take a transaction that's problematic and follow it through and get all of the information from it. And it shouldn't really matter what format that information comes in, whether it's formatted like a number or a string or or a span or something. You should still be able to, you know, do root cause analysis for it. But it's also this idea of like um not just not just reacting when when there's an error or when an alert fires. It's also like really understanding what the impact is. You know when you when you decrease your response times what is the business impact to that or you know it's it's thinking more broadly I think not just on the traditional monitoring level. >> Yeah. And to illustrate what you said on graphana uh with low key tempo mimir which is uh which are back ends that are tightly integrated we have some troubleshooting workflows like drill down traces drill down logs uh where you will have some kind of statistical analysis to surface insight surface uh your slow traces uh surface patterns in your logs on all these that are uh state-of-the-art monitoring practices. So maybe observability 2.0 or 2.0 plus or I don't know but that are available uh in LGTM. >> We have a another question if that's okay. >> Sure. >> Uh there's one from Jungle Jolt Narratives. How do I know which metrics and traces to prioritize first when instrumenting a distributed system with OTEL? So here what we have seen being successful is we strongly recommend people to use auto instrumentation first >> very easy if you are in Java inn net and it will provide uh metrics on traces at a technical level like your web framework will be instrumented your database goals will be instrumented um it's a technical instrumentation but in fact it's also a kind of business instrumentation because with HTTP we have adopted rest practices and so HTTP endpoints URLs are meaningful a post on your SL API/ orders is meaningful from a business standpoint get/ API/ product as well so um you start with uh auto instrumentation where you will have everything compliant with uh the semantic convention So everything will be recognized by your community dashboard or your your uh community alerts or commercial products that you use as an observability back end. And what we recommend is on top of this uh out ofthe-box instrumentation provided by SDKs you can add at a certain level of maturity on your critical workflows. You will add some custom instrumentation. Maybe adding business attributes to your spans. Maybe creating custom metrics. You are an e-commerce website. You want to monitor the revenue generated on your website to detect drops in your uh purchase or maybe spikes that suggest fraud. So you would go in an hybrid mode as a second level of maturity in your observability journey. Yeah, this is one of the things that I like the most about OTEL. Like a lot is said about the vendor neutrality and the plug-and-play thing and which like okay that's great but actually it's like when you instrument with hotel what I like is that I'm I'm automatically applying best practices that people smarter and more experienced than me have decided on because there are a lot of metrics there or there's just a lot of information there structured in a way that I would never personally have thought to to to do or maybe it would have taken me a very long time to do it and Here's the industry being like, don't worry about, you know, which ones right away. Like, here's everything that we think that we can already figure out, you definitely need, and if you need more later, you can still add them. >> Yeah. Also important part that if you find that it's overwhelming and it's too expensive or you don't need certain data, there [snorts] are usually ways to disable certain instrumentations. So for like the common example, you have your incoming requests instrumented and it's probably super important to have those. This is what are your SLIS are based on. Um and then you have your outgoing requests so you know how your dependencies are doing like if it's your failing or they are failing but then there are there could be a lot of spans in the middle. So for example your AWS SDK creates spans and it also calls HTTP and there are HTTP spans. So at some point you may decide okay maybe I need just one of this outgoing layers but not both of them and it depends on the application and usually open telemetry instrumentations provide the way to disable them or not enable. >> Yeah so um I actually have a question too serial with this dashboard uh or with in the hotel demo can it also monitor infrastructure? >> Yes. Um so yes open telemetry was born in the application layer and in traces. So uh infrastructure monitoring came a bit after but in fact um openmetry was also used a lot in the kubernetes world maybe because it's nf on when you are on um on kubernetes the the distinction between the application layer on the infrastructure layer is very seen on you don't know if a body is the application or the infrastructure layer until here uh we are working with the hotel demo folks at uh showcasing much better uh infrastructure monitoring done by hotel on how to correlate them on uh yeah on this uh dashboard as you can see there is a Kubernetes pod uh health. So here this component uh the ad services deploy just on one pod but if you had multiple you you would see them and if you scroll a bit up I think you also have the in the Linux view uh oh no we have a glitch maybe but anyway um on uh on yeah we you is very valuable to to combine you must I think today you must correlate your application layer telemetry with your infrastructure Linux kubernetes uh telemetry it's a must Open telemetry provide a turnkey solution. They are great charts for this. The uh open telemetry uh collector helm chart. Uh you can also use the open telemetry uh cube stack chart which is very interesting. Uh to combine this or you can find solutions provided by other vendors. We have the graphana kubernetes monitoring and chart which is more the promise style instrumentation of kubernetes and that also correlates very well with the application layer. You must do it in the dashboard. We have prom we have provided a full open telemetry way and what we have been very careful is to provide you all the promql queries all the queries on your metrics that you can reuse either you use a dashboard as it is today but this dashboard is Promeus Jerger open search maybe you you are doing Promeus uh low key tempo or maybe another solution and you can reuse all our promql queries we have spent a lot of time to verify them you can reuse the pan panels as well and you will get the correlation between application on infrastructure layer on all these valuable things. >> Yeah, I I'm glad that you shouted out the Kubernetes monitoring Helm chart because that's super awesome and the and the dashboard that comes with it is useful out of the box and it's run by a colleague of ours Pete Wall and those the the community call is also on on this YouTube channel. So really really good stuff. >> Uh speaking of which uh the open telemetry demo is deployed using home chart. You can actually easily if somebody who's watching us if you want to play with it yourself you can take it and you can just run it and uh play with it in your environment. And you know, we were talking about like how to customize things. Like if you if you want to add something custom, then you can just add it. Um, but what are the like what are the other knobs and stuff that you can tweak for the hotel demo? >> Uh, so the hotel demo is provided as hand chart with examples where we disable stuff. So typically uh we support disabling the batteries included Promeus, Jagger, open search uh to uh so that vendors can replace with just connecting to their obserability back end which is a great illustration also of the flexibility of open telemetry to hook to any vendor today and demonstrate vendor neutrality. So that's one thing. Then I think Ludma you have worked a bit more than me on the feature flag to showcase some problem to yeah some chaos engineering stuff. >> Yeah actually if you want to simulate failures demo comes with a bunch of chaos engineering stuff. So uh there is a feature flag service there and for example we can enable high CPU there or we can simulate Kafka Q problems and obviously you can just go into the code and uh play with it. Um so some of the instrumentations there just show showing you a quick look um they are manual. So for example, go instrumentation uh for Kafka is manual and you can play around with different attributes here here or measuring different kind of things if you just want to get a hold of how it looks like. Um and obviously you can go into the code and uh see how open telemetry is enabled at all. Um I'm sorry I'm a bit lost here. Um but you can find uh certain let's look for tracer provider. Here you go. So this is how tracer provided this provider is configured. It uses for example batch processor and if you want to see how things work uh in open telemetry you can play with with them here. Um anything any other thoughts? >> No. Yeah, I think we we are working in the hotel demo at um ensuring that uh the code snippets that we provide in the demo uh reflect the best practices um up to-ate best practices because sometimes things have changed. I'm less working on this part. I'm more working on the hotel plumbing like hotel collector setup. But uh we have some people also working on this to to ensure that it's a great uh experience to just copy paste stuff. I think it's our mindset is you should be able to copy paste on uh be as much production grade as possible. >> I mean my first thought when we were looking at that that um metric earlier that was like that was hitting alerts that throwing an alert but it was like 18 milliseconds. I my first thought was we could use K6 to make it really like a problem. Like we could we could really we could use we could do real chaos engineering with K6 um to do all of those things as well and more we can run more complicated chaos engineering tests. >> Uh >> it doesn't have to be a fake one. [laughter] Uh at the moment the hotel demo is instrumented with a load testing technology called locust >> on um so it's not Ksix it's an alternative that has an open source version and also a commercial version in locust uh on now the open telemetry uh community has this practice of vendor neutrality and so on but I know there are conversations about using Ksix uh as an alternative sometime it's being discussed. Ksix is amazing. It's open source as well. Uh now the the open telemetry community has to ensure that there is a good balance between all the all the actors of the industry. >> Yeah. Well, it's just a shame because as far as I know, locust can't control can't run cube commands for example with Ksix you can. So you can just say like what happens if I just change if I just remove this pod like that. That would be kind of cool to see on a dashboard. So here I have some things that we are doing at Graphana Labs is that we are working at providing um almost the same as the upstream hotel demo but just replacing in the batteries included replacing Jerger by Tempo uh replacing open search by Loki on we would have to check if we could switch Locus by Ksix uh so that the people who want to evolve in the Graphana ecosystem uh can also get all these out of the box as a great experience. That's that would be a wonderful idea. Uh we we have people working at the moment on the back end side but it's a great idea to consider Ksix and we are discussing with Lud Miller on on stuff related to Kix. >> Yeah, there are so many things to talk about offline. [laughter] >> There are so many things we can explore with Autodava, right? So we explored the vanilla one today like the the one that's available on open telemetry as serial mentioned vendors tend to fork and it's intended to be forked and showcase the best features for the each vendor and we have so many things to show there. We can showcase LGTM stack we can showcase graphana cloud uh and both would be extremely valuable. Yeah. And maybe more than saying vanilla, which could suggest it's feature limited, maybe it's more the upstream with pluggability to to replace stuff because all the the contributors to the hotel demo even if they have their own uh version for to demonstrate their own product. Uh we all make great efforts to to provide the best in upstream I think and that's a great collaboration that I love. Do you know what the the next steps are for hotel demo? Like what are things you would like to add? >> Uh so we have a bi-weekly community call uh for the hotel demo. Uh what we discuss at the moment we are just adding AI observability [clears throat] >> which is a exciting uh >> what are you just sorry just quickly what are you what library are you using? I don't know. I didn't look at this. Me, I work a lot. And this is also what we want to do with hotel demo is to try to showcase in real life the hotel components and if we struggle with stuff to come back to other products and typically I work at the moment at enabling Kubernetes monitoring in the hotel demo and I identified that there are some slight differences between the open telemetry collector chart and the open telemetry cube stack chart. And so uh I could reach out to this project saying there is a feature gap or there is a slight nuances that are undesired for practitioners. How can we realign all this? So this is something we do is to go back to project and to to showcase how the experience could be uh easier for practitioners. >> I think they use open AI library and open telemetry instrumentation for the AI library for open AI. I used open lit and it's like based on open telemetry and so awesome. Anyway, uh what about OB or BA or whatever we're calling it? >> Yeah. Uh [clears throat] so this is uh open telemetry ebpf instrumentation and this is the idea to instrument applications with eBPF instead or to get started or instead as an alternative completely to traditional APM agents. So for the moment the open telemetry demo has really uh hotel SDKs hardcoded in the docker images and um a reason for this is that we want to make it easy to use the hotel demo as a docker compose but also the most more sophisticated kubernetes deployment on in docker compose it's more complicated to inject at runtime the SDK stuff like this that you can do on kubernetes with the open telemetry operator which is a fantastic tool. Um so we are discussing maybe how to uh to make the SDKs more optional in the demoed application component so that we could switch traditional APM SDKs by EBPF instrumentation. Um it would be it would take some time also because what is very exciting is that there are 40 vendors uh using it. So lot of practitioners benefit of the hotel demo everything and we cannot break everything. Uh we have to be careful on how we change. >> Okay. So we're we're running out of time unfortunately but like if people want to contribute or or just try out the hotel demo for themselves for learning purposes where should they go? >> Uh Lumila did you prepare something on this or can you share? Yeah, I posted the link uh somewhere in the chat on the open telemetry io. Uh here we go. Uh and uh if you want to participate in the demo uh because serial mentioned the community meetings, I'm posting another link in the channel that uh you folks can uh find more information on. Um yeah, I also see a good question in the chat about the alloy. So maybe we are almost at time, but maybe we should uh provide some information. I want to um share an awesome series our colleague Lisa Young has created about alloy. Uh and there is a lot of information there um on uh alloy and uh Yeah. Okay. I can give a a quick uh quick answer to this. Alloy is one of the open telemetry collector distribution. So there are many distributions of the open telemetry collector provided by various actors of the industry open source or uh commercial vendors [snorts] on AOY is one of these distributions. And so with AOY the idea is you get the best of the open telemetry collector with also the best of pro traditional Prometheus instrumentation because AOY was born in Prometheus instrumentations uh which is an amazing very successful ecosystem on now has expanded to uh provide you the best of both world of the Prometheus instrumentation ecosystem like the Prometheus exporters on the open telemetry uh ecosystem the open telemetry collectors on some of the receivers. I hope this clarified. >> Yeah, that sounds good. All right. Did you have anything else to say and either of you before we wrap up? >> No, thank you. We appreciate your time, Seriel, and uh thanks for creating this dashboard so that open source users can actually get some of the APM goodness for themselves. I love this this comment, too. Just to wrap up, finally a demo where my microservices behave better than I do. [laughter] >> All right. Well, thank you both um for for being here and for everybody that's that's watching. I think that's a great way to end it. >> Thank you very much. It was a pleasure to be invited on an honor. Thank you. [snorts] >> Thank you.

