# How CERN Monitors the World&#39;s Largest Computing Grid with Grafana and Mimir | GrafanaCON 2024

The European Organization for Nuclear Research (CERN) may be famous as the home of the Large Hadron Collider (LHC), ...

Published on 2024-04-25T23:59:55Z

URL: https://www.youtube.com/watch?v=UDMoWQxW3iA

Transcript: [Applause] uh thank you and hello everyone I'm nikai and I work for the monitoring team at CERN where I'm also responsible for the for the visualization tools and in particular for grafana I'm pretty sure most of you have heard about CERN or at least the the large yon collider but I would like to start with a quick introduction and later on I'm going to talk about the monitoring service and the way we use grafana and mimir so CERN is the biggest particle physics laboratory in the world and our goal is to understand the most fundamental particles and walls of the universe it is located in the Geneva area and the accelerators are actually Crossing across the Franco Swiss border it was founded 70 years ago by 11 European countries and the number of member states has grown since currently we have 23 countries as a member states and we have also many associate States but in general CERN collaborates with more than 80 different countries in different experiments it is the home of LC the large collider that is the biggest collider ever built it is 27 km in circumference and is located around 100 m underground and it is built with super conducting magnets that are used to accelerate the particles uh with the speed close to the speed of light so as this is collider actually we have two beam lines and particles are accelerated in opposite directions and the experiments actually happen in four detectors so in the back background you can see one of our detectors um there both beam lines they they cross each other and create more than a billion collisions per second of course we cannot analyze so much data so that's why there is a special triggering system that filters out only 200 interesting events out of this 1 billion so only these 200 events combined for the for four detectors uh turn into Data make a 25 gigabytes of data per second and we need to Archive this data somewhere and this is where our data centers they come into place so currently we have two data centers uh where we permanently archive around 380 petabytes of data mainly on tapes but also on diss uh in the data centers we have around 11,000 servers with almost half a million processor cores and maybe you can ask yourself okay but how all the physicists actually access this data and how do they perform analysis on this data and this is the reason C created wcg that is the world's biggest Computing grid uh combining effort and resources from 170 institutes and universities from all around the world and providing there around 12,000 of physicists with the opportunity to analyze this data using around 1.4 million of computing cores so of course such infrastructure requires monitoring and this is where the monitoring service at turn comes into place with our M mandate to monitor the data center wcg but also the other services that are running in Our IT department So currently we monitor around 15,000 hosts and receive around 85,000 of documents every second so turned into Data this is around 3.3 terabytes of data we store every day and we receive data not only from the host but also from around 200 different data producers you may be spotted already 5,000 users up there a bit more than 5,000 users actually is the number of users unique users that used our grafana instance in the last year and um so having such wide range of services to monitor of course provides some challenges for us the first one is that we get data from the heterogenous data sources and we need to actually unify the way we get this data so that uh we can we can cope with this first we require that the data is sent to us as a j and format because we cannot deal with with many different formats we don't know actually what these producers might be using on their site and secondly we need to identify who sends the data to us so that we can uh store it in the required way and also provide them with the EAS easy way to access it through grafana data sources for instance secondly we need to provide a full tolerant infrastructure so the expectation is that actually our infrastructure is almost running all the time in order to do this uh we buil a data processing Pipeline with the different layers and we try to replicate at each layer the servers in different availability zones and last but not least it should be cable infrastructure it should be able to accept and to to get of course more hosts and more producers but also it should be easily um adaptable to new use cases and we came up with this architecture it was long time ago around [Music] 2016 uh of course it has evolved a little bit over the years but it has been this in general uh where in our ingestion layer we are Flume we are either receiving data or we are pulling data from different user systems then Flume is sending data forward to our transport that is served by Kafka providing three days of of uh buffer we profit from the CFA streaming functionality so we do some uh stream or batch processing mainly using spark but also lock star for some locks use cases and then in our export layer we consume from Kafka and we write into the different storages that are hdfs influx DB and open search on the visualization site of course graan is our favorite Tool uh we also provide access through through Swan that is a c uh service for upiter notebooks so this is for analyzing data that is stored in sgfs and of course for the data that is stored in open search there is their interface uh these are mainly locks but there are also some metrics there I'm pretty sure you have noticed mimir up there I'm going to talk a little bit more about mimir later on in the talk in terms of data types we receive metrics uh locks and alarms directly from the host on the host we are running a collected Diamond that is responsible for sending us this information but we also uh receive a service and application metrics and locks from around these 200 producers I already mentioned before for for the wcg if you're interested uh we uh monitor the data transfers uh the jobs that are running there doing their monitoring but also accounting and we monitor what is the availability of the different sites in terms of alarms apart from the host alerts uh we have a small system that is called no contact and it is used to to check if the host are up and running otherwise it is sending alert and we have also external alarm sources so the same way we receive metrix through Json we can also receive alerts and this we use also to integrate grafana alerts through web hook so that we receive all the alerts generated in grafana and store them within the monitoring service on the processing side I mentioned already we running mainly spark jobs with them we are aggregating the data for long-term storage so if it's highly granular data we want to to reduce the granularity and and also for wcg for instance um we are doing a lot of enrichment if we get a metric for a given host we just need to add extra information about uh what is the topology of this host to which um site country Cloud it belongs to so this we do in a streaming jobs on the flight so that was the introduction of the monitoring service now I'm going to Le more about the our grafana experience so it has been with us since the beginning of the architecture I showed before 2016 and we have been always very happy using grafana also our users they they love it of course they're creating a lot of dashboards uh but we found also that grafana is quite useful to be used as a unified data source you know grafana provides a proxy and now with the newer versions already a DS API so we have some systems actually that are accessing data from grafana data sources through the through the grafana apis it is quite useful uh because actually it hides the complexity of the different databases that are behind the data sources it is also very useful of course any grafana is used a lot for uh alerting tool in our infrastructure and as I mentioned already we integrated through a web hook with monitoring service so that's what we call on our site sgni so it's a kind of notification service we are we running within our service in terms of numbers I mentioned 5,000 a bit more than 5,000 users we had a unique logins in the last year in 30 days period this is 1,200 and this is more or less the constant number of users that are are using grafana 70 organizations a bit more than 2,700 dashboards and 600 uh 80 alerts so these are the numbers we have currently in in our graan deployment our grafana main graan instance is actually as I mentioned already multi-organization instance and it is behind a certain single sign on so it requires authentication it requires to be a certain uh user so that you can you can access it the way we deploy we are using a virtual machines Al Linux currently uh and we uh deployed them behind a DNS load balancer in in open Stock that we any conern so they're split in three different availability zones of course so that it can guarant is that in case we have some issue uh in a given availability zone grafana is still going to be up and running and for database we are using AOS SQL we have a second instance that is mon graan open and uh the difference is that this is a public instance it is a single organization it's public instance it does not require any authentication so it is completely open to the world and the reason we have it is that we have some dashboards we want to to share publicly with everyone who anyone who who wants to to access actually uh it's a very simple deployment we Deploy on two virtual machines again in in different availability zones we don't need anything more because it is it is quite stable it is up and running and uh the way it is also organized it is just a read only instance that um provides access to one of the uh organizations that we have in the in the mangra fun so it is using the same database underneath actually and this is just one dashboard I I wanted to show you that we have uh about the wcg site [Music] availability uh another one is uh the open stack overview actually this one is uh accessible through the through the public instance and it is created by our open Stock colleagues showing the overview of of their service and um having so many users in grafana and dashboards of course provide some operational challenges for us so the first one is that we need to manage the the organizations so they request us to create new organizations they might be public or private we support these two types currently public organizations means that all the grafana users they need to have access to this organization so we are running some scripts in order to synchronize them regularly and the private ones they are mainly used by the the service manager so they decide on their own who should get access to their own organization then we have a plug-in management so at certain you can imagine uh there are many physicist that are looking into monitoring data decide to create some plugins and this happened more in the in the past not so much in the in the last years because grafana became more and more Complete product and we are getting anything that we need almost out of the box but we still have some uh Legacy plugins that we need to maintain we need to make sure that updating to newer version uh the things are not going to break uh we still have some small patches to to apply uh even though we already manag to contribute um many things Upstream so even in the M's presentations earlier today I saw that uh she highlighted one of the Transformations was table to metrix this was actually transformation contributed by to grafana but yes uh we really need to to make sure that um updating to a new version is not going to break anything for our our users so this is a challenge for us but the biggest challenge actually we had with migration to the unified alerting framework that we completed last year but actually the process we started already with version 8 when unified alerting was uh introduced for the very first time so we expected uh that with this uh change of of concept and moving into the alert manager like concept is going to create a hype of course and it did um and more people are going to to create Alert in grafana so uh having most of the monitoring data in influ DB we um wanted to make sure that there's not going to be overloaded database we we wanted to make sure that we understand what is going to be the impact to the databases uh from the the uh having more alerts created and of course this is going to to produce more queries to be executed to the databases we also uh evaluated the impact on the on the hosting servers we had to reimplement our web hook integration because uh now the document is a alert manager like document so it had to we had to adapt to this and the last but not least we migrated actually all the alerts that we had running in parallel with the Legacy alerts and we were checking if they're going to uh trigger at the same time and uh if if they're actually matching the notifications once we felt confident enough to go forward with the migration we organized in multisteps so first step was providing our users uh with the newer version kind of R only just to uh allow them to get a feeling of the new interface to allow them to understand how they are going to manage their alerts afterwards how to configure new alerts and the Second Step was actually to to migrate the alerts but once doing this we did inam production but we also preserved the Legacy alerts in a temporary instance so that actually there were um duplicated notifications of course as you can imagine but this allowed them the time to to check if they alerts were migrated correctly and they were able to go and to deprecate uh the Legacy alerts manually afterwards and we were very satisfied with the process at the end and actually I would say the automatic uh alerting migration worked out of the box it was pretty good we did in in version 10 already and the next steps with with grafana we see around alerting uh firstly because um the new unified alerting framework provides a lot of a lot of new features I've already presented to our service managers some tips and tricks how to use multi instance alerting um how to to play with uh uh silences uh mute intervals how to create custom notations and labels and and I think there is a much more room of improvement and uh on that end then alert history overview was a feature that was uh requested by our service managers for for a very very long time and it came available in in version 10 uh we have not adopted yet but we are looking forward it requires a locky back end and we are going to provide one so that we can make the feature working and the last but not least the public dashboards as you can imagine uh having public dashboards and providing the users with the functionality to share dashboards publicly will allow us to deprecate and to get rid of the the open instance that we are running currently so this is going to be of great benefit for us just one less instance to maintain and also in many cases we have a dashboards that are duplicated in in two organizations just because we want to make them public so we are really looking forward to to provide this to our users now switching a little bit from grafana to to mimir as you saw our architecture at the beginning it was clear that the the pritu spot was missing a bit and uh we identified this there was there were requests from our users to to integrate a long-term storage for pritus metrix we started working on this and um 2020 we came up with the integration that uh we made on our own uh using influx DB for a for a back end but it was clear that it is not going to scale um very very well in the future so that's why we started looking into what tools are available on the market and of course cortex catch our attention also one of the reasons was that we saw a strong graph Community behind cortex so this gave us extra confidence later on this turned to become a mimir so we switched um into the mimir of course and uh tested uh how this would scale in our use case so for our test case we uh had uh 80 million of active Series so we played a little bit uh it took some time of course to to make it working properly but but at the end it it ended up to scale quite good so we were we are very happy with the result also like the the fact that you can scale at a different component either at the injesture side or the queerer side mimir also provides a lot of flexibility uh with multi-tenancy getting data from from different users so this would also allow us to integrate all the data into into the single storage and that's why we decided to start uh a Pilot service uh 2023 uh running on kubernetes of course our deployment with uh multim Masters and and Nod split in different availability zones we have also different not groups uh the reason is behind of course the the different requirements that we get from the components in terms of Hardware some require more memory others more processor and S3 as a as a storage with a default retention of 40 days and these are the numbers we get currently uh the quer is around uh 50 49 to be exact uh notes of course there are three Masters there so we have 46 worker nodes providing a bit more than 1.2 terab of of memory and 730 cores and currently we have 29 tenants uh producing 30 millions of active serus but our calculation is that uh on this cluster we can accommodate 150 millions of active series and being able to process around 2,000 queries per second this is a dashboard we are using for for our kubernetes cluster monitoring you can see actually how much of this it is used currently for for this 30 million of active series and here we have a just a tiny part of the uh our mimir monitoring dashboard of course it is it is a big one uh could not get it on the screenshot but down there on the right side you can see that actually it's 30 million is is active series currently okay so what is next for the monitoring service atern so being uh satisfied uh with the results we have with with mimir and also our our clients uh being very satisfied with what we can provide uh we plan integrating mimir into the standard um monitoring pipeline so this will allow us also to reduce the risk of data loss having Kafka in front so in case mem is not available for for some time or in case there is some data that is is lost in a in gesture for instance even though we are doing replication uh we can consume the data back from Kafka and we can recover it will also allow us to integrate data from uh other that are non promit data sources currently of course we cannot do this having adjacent format so we need to look into aop and to adapt aop uh we can use the open telemetric collector and we plan to replace the flum in our architecture with with open telemetric collector that scales also quite well horizontally and having cop format within the infrastructure of course this also opens the door for tracing so we are not offering tracing currently but we are looking into it and uh knowing the similarities between mimir and Temple of course Tempo is very high on our list for for um tracing uh offering and this is the architecture actually that we we plan to uh to have in the next uh couple of months and and the next year probably at latest uh replacing The Flume with open telemetric collector as you can see then a also becomes a standard format within the infrastructure we also plan providing uh our users with a tiny fluent bit agent so that they can use it on their hosts uh and then send Matrix logs and traces uh through aop format to our ingestion layer and now you have probably noticed mimir is well integrated within the pipeline so just to summarize grafana is crucial tool for the monitoring service at CERN of course providing us with uh many many useful dashboards but also with alerting functionality mimir provides the missing bit in our prit architecture and uh we we are going to use open Telemetry that you allow us to UniFi observability patterns thank you very much [Applause]

