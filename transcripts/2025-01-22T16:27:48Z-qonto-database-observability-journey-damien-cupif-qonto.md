# Qonto Database Observability Journey - Damien Cupif @ Qonto

La création d'une plateforme d'observabilité de bases de données fiable, évolutive et en libre-service (Damien Cupif, Qonto) ...

Published on 2025-01-22T16:27:48Z

URL: https://www.youtube.com/watch?v=iG1A3PK4Mq8

Transcript: ravi de vous voir aussi nombreux pour pour ce Meetup grafana sur le temps de midi au nom de l'équipe de conto on est vraiment ravi de vous accueillir dans nos locaux je me présente rapidement je m'appelle Damien je travaille chez conto depuis maintenant un petit peu plus d'un an et je travaille au sein de l'équipe SRE storage qui est une sous-équipe de l'équipe infrastructure qui compte une trentaine de personnes sur la partie base de données on est environ on est 4 4 à 5 personnes à travailler dessus aujourd'hui on va faire un partage d'expérience vraiment sur bah notre parcours dans l'édification d'une plateforme d'observalité de nos bases de données mais avant de rentrer dans le vif du sujet je vais vous parler très rapidement de De Conto euh donc conto c'est une entreprise qui a été fondée en avril 2016 par deux Français Alexandre pro et Steven AVI euh ce qui fait vraiment la caractéristique de contos ça a été une croissance extrêmement forte extrêmement rapide aujourd'hui on est à peu près 500000 clients on a 1600 plus de 1600 compur et ça ça augmente assez rapidement euh avec plusieurs centaines de personnes qui travaillent à la tech et donc une trentaine de personnes sur la partie sur la partie infrastructure aujourd'hui la croissance de compto elle se maintient dans le temps elle avoir la même tendance à s'accélérer puisque on fait de l'acquisition d'entreprise et on a besoin d'intégrer les système d'information de d'autres [Musique] entreprises donc rapidement sur la stack technique de contourne on est sur quelque chose d'assez classique la majeure partie de nos applications sont hébergées dans le CL AWS quasiment je pense c'est la quasi totalité de nos services backend aujourd'hui sont enfin tourne sur des clusters EKS donc sur des clusters CUB hébergés sur EKS et euh on utilise toute la stack d'observabilité euh donc euh euh proméus euh grafana tempo pour les traces euh euh on utilise euh loky pour euh pour les Loog même si on utilise aussi cloudwatch et euh aujourd'hui même si le périmètre de notre équipe euh vise à manager donc toute l'infrastructure de base de données de compto euh on va faire un focus sur posuse Gray parce que c'est vraiment la technologie centrale à à tous nos services mais on manage également des clusters Kafka des clusters redis euh du S3 et cetera euh on est vraiment alors je je regardais par curiosité les chiffres parce qu'on a fait une présentation en avril dernier euh sur bah le nombre de bases de données qu'on a le nombre de clusters euh donc depuis avril dernier on a créé plus de 40 bases de données de nouvelles bases de données on a à peu plus de H clusters de production supplémentaires parce qu'on a un environnement qui s'est créé from scratch euh donc on est sur un facteur de croissance qui est de 1,5 sur les sur les quelques derniers mois et donc cette croissance très rapide on est vraiment dans une phase passionnante de la croissance de compto on a cette nécessité de vraiment soutiller pour pour Pier à cette croissance rapide et manager toutes ces infrastructures donc à la fois soutiller sur des des outils qui nous aident à mettre à jour nos clusters de de façon rapide et fiable nous aider à migrer de la donnée d'un cluster vers un autre et cetera et aujourd'hui on a un nombre limité de ressources dans les au sein de l'équipe et ce qui est primordial pour nous c'est de diriger nos efforts là où ça a le plus de valeur et pour ça on peut pas le faire sans avoir bah une stack d'observabilité complète fiable et maîtrisée donc je vais parler de trois volets aujourd'hui va y avoir la partie vraiment collect de métrique donc je on va se focaliser sur RDS POS gré pour pour cette partie-là parce que c'est celle qui nous a posé le plus de challenge on fera un deuxième volet plus sur la partie alerting et un dernier volet vraiment sur la partie dashboarding visualisation bonne pratique donc RDS comme n'importe quel applicatif ne déroge pas la règle euh et pour avoir une observabilité complète on doit s'appuyer sur les trois piliers de l'observabilité qui sont les logs les métriques et les traces alors je vais passer rapidement sur les logs et les traces les logs POS gré c'est une volumétrie très importante donc on stocke pas tout on stocke uniquement les logs d'audit les logs log de connexion et les logs des queries qui on estime sont ont un temps d'exécution qui est est supérieur à la normale pour ce qui est des trac elles sont stockées dans tempo et aujourd'hui tous les applicatifs compto backend sont instrumentés et gèrent leur span gère leur span comme ils le souhaitent pour ce qui est des métriques on va se focus dessus aujourd'hui parce qu'on va voir que c'est pas sans piège de de de récolter les bonnes métriques dans un environnement managé comme RDS donc je vais vous présenter Alice on va essayer de se mettre dans la dans la peau de ce personnage d'accord donc Alice elle vient de rejoindre conto ça fait quelques jours qu'elle est là elle a un background de de SRE ça fait quelques années qu'elle est qu'elle est SRE elle a eu l'occasion de manager des clusters cubernettis on premise mais finalement l'écosystème postgql RDS elle connaî pas trop et la première tâche d'Alice au sein de l'équipe storage c'est de faire un inventaire de nos clusters et de voir si justement ils sont bien dimensionnés alors Alice elle a quelques années d'expérience et donc bah elle a les bons réflexes elle se dit bah Service Manager je vais avoir les métriques qui sont fournis par le Service Manager RDS et puis bah elle va être confrontée à certaines difficultés je prend l'exemple par exemple de ce de ce premier graphique où on regarde l'espace disque pour un cluster qui s'appelle PG1 d'accord et on voit que du coup cet espace disque est de l'espace dis disponible est de 63 go qu'est-ce que je fais cette information j'en sais rien si mon cluster il a un terra de de stockage disponible B en fait ça représente moins de 10 % de notre stockage donc ça veut dire qu'il y a probablement une action à prendre on est à risque on est sur un cluster avec beaucoup d'activités si en revanche j'ai un cluster qui a 100 Go de stockage ben en fait on est à 60 % de de libre et on a aucun point de référence et le problème c'est que ça se retrouve sur tout un tas de métriques pas que sur le stockage mais sur les métriques d'écriture l'utilisation CPU l'utilisation Iops en fait on n pas de point de référence pour savoir si ce qu'on est en train de regarder c'est un problème ou pas et on va voir que aller récupérer les informations de capacité des instances euh détient des pièges et donc on va passer au premier incident d'Alice donc Alice ça fait maintenant quelques semaines qu'elle est à conto et bah là euh elle est elle est d'astreinre de journée et elle est contactée parce que on a des alertes qui sont levées par les développeurs on a une dégradation des performances sur tout un tas de services et on observe des latences euh des latences dégradées rapidement on se rend compte que c'est la base de données qui qui qui subit de la congestion et on va observer de façon assez simple qu'on a une saturation des AOPs alors juste pour vous donner un petit peu plus de contexte sur cet incident c'est un incident réel euh qu'on a on a eu l'occasion de euh de de voir voir chez conto euh il se passait au début du mois lors de du billing cycle qui est le cycle de facturation de tous nos clients donc il faut imaginer que c'est un tous les mois en début de mois on a vraiment une pression naturelle qui va être exercée sur notre infrastructure parce qu'on a des gros batch de Processing qui arrivent et donc une des mitigations évidentes de cet incident qui serait de dire bah on réduit la consommation c'était pas vraiment possible dans ce cas-là parce qu'on a pas envie de délayer le cycle de facturation de nos clients donc la deuxième mitigation possible c'est bah d'augmenter les ail donc on on augmente les Iops et là bah stupeur on est toujours bloqué on est toujours saturé et on narrive pas à comprendre pourquoi donc sur cet incident la mitigation a finalement été bah de réduire la consommation des AOPs parce qu'on était pas en capacité de scale notre cluster RDS et on a voulu savoir pourquoi donc on a fait un postmortem et en fait on s'est rendu compte que euh bah c'est loin d'être facile mais on a des limitations physiques hardware de la machine EC2 qui euh fait tourner le serveur posgay et si on regarde un petit peu ce tableau qui est un peu caché dans la documentation ads on se rend compte que c'est pas facile et que c'est pas du tout une loi linéaire et Queen fonction de l'instance et du type exact d'instance que vous allez utiliser et ben vous allez pas avoir les mêmes plafonds euh les même plement en terme de de ressources physiques alors on s'est posé la question suite à cet incident bah comment on fait pour avoir vraiment euh une visibilité à 360° sur nos sur nos clusters et ESS de comprendre euh euh d'avoir ces ces vraies notions de de capacité euh pour pour savoir si en terme de consommation on est dans le verre ou pas et ben en fait si on veut vraiment avoir une vue sur ce qui se passe sur nos instances RDS on est obligé d'aller regarder à minima cinq sources de données on est obligé d'aller regarder euh les métriques internes à posgr d'accord via l'exporteur communautaire posg pritus pog et puis si on veut avoir bah toute l'histoire côté côté RDS en fait on est obligé d'interroger la p RDS laapi EC2 laap cloudwatch et laapi service cota quand on fait de l'inventaire alors fort heureusement pour vous vu qu'on a eu besoin de le faire bah on a créé notre propre exporteur RDS qui interroge toutes ces API et qui remonte ses données cet exporteur on l'a appelé le prometus RDS exporteur et il est Open sourcé donc si vous êtes intéressé par par ce projet vous êtes fortement conviés à à visiter à visiter le le gthub et à essayer d'y contribuer c'est facilement déployable c'est vous pouvez le tester en quelques minutes il y a un docker compose dans le projet qui vous permet de tester en local donc je vous invite vraiment à le faire aujourd'hui grâce au prometus exporteur on est capable d'avoir des graphes on a vraiment une capacité on peut voir la la capacité vraiment la limite physique de la machine qu' y a derrière et pas seulement la limite logique qui est imposé par AWS alors c'est bon là on a on a toutes les métriques qu'il nous faut bah d'un point de vue SRE peut-être mais chez conto on essaie un maximum de faire en sorte que notre STAC d'observabilité soit rendu disponible au développeurs et leur permettent bah de d'aller voir un petit peu la performance de leurs applicatifs et donc là bah nouveau challenge pour Alice elle est contacté quelques semaines plus tard pareil par un développeur qui lui dit bah j'ai l'impression mon service il est plus lent je sais pas si vous avez déjà entendu parler de en cette phrase moi plus d'une fois euh mais en fait on se rend compte queil y a pas vraiment de de factuel on demande au développeurs bah qu'est-ce qui te donne cette impression et on se rend compte bah qu'il est allé piocher un petit peu dans des traces tempo et qui bah il pense oui il observe que la latence est un peu augmentée sur sur la partie et donc finalement ce qui nous manque c'est un suivi de performance à la requête donc ça c'est un travail qu'on a fait avec l'équipe récemment qui est pas encore tout à fait terminé euh posgé met à disposition des extensions officielles notamment PG stat statements qui va permettre de traquer vraiment de façon très fine euh les statistiques du query plannerner postgray et euh d'avoir des informations sur l'exécution des requêtes donc pour chaque requête en fait on va être capable d'avoir le nombre de fois où elle a été appelée quel est le le temps moyen d'exécution et cetera donc toutes ces information elles sont vraiment précieuses euh mais euh il faut bien comprendre que c'est une vue qui est mis à disposition par posgay et que quand vous faites un select sur cette vue bah vous avez vraiment une photo à l'instant T de vos de vos de de vos performance nous ce qu'on veut c'est avoir la tendance sur le long terme on veut pouvoir prendre une querie et être capable de dire bah au bout de 2 mois en fait la latence moyenne sur cette querib elle est elle est en train de se dégrader donc on a historisé euh cette ces informations là et aujourd'hui on est capable de la visualiser dans dans nos dashboard grafana où on est capable de dire pour une requête donnée quel est le trafic moyen à l'heure quelle est la l'évolution de la latence sur sur les les dernières semaines les derniers mois et donc ça c'est des informations qui sont extrêmement précieuses pour les développeurs mais qui peuvent aller faire de la découverte sur sur leur co donc aujourd'hui où on en est on est sur une première approche qui est un peu passive c'est-à-dire c'est au développeur d'aller voir et de faire de la découverte de ces requêtes mais l'objectif c'est d'être sur les prochaines semaines d'être plus proactif dans le suivi de ses performances et d'aller sortir bah par exemple le top 10 des requêtes qui ont la dégrassation de performance la plus importante pour essayer d'aller alerter directement le développeur donc maintenant qu'on a toutes nos métriques bah on peut faire de l'alerting dessus on peut faire de l'alerting fiable où on a des informations de quota des vraies informations de capacity planning donc on peut faire des choses intéressantes et chez conto on s'efforce vraiment à ce que il y a un minimum de de false positif sur nos alertes et surtout qu'une alerte elle est toujours une action et une mitigation associée et donc chaque alerte chez conto il y a un runbook spécifique qui permet à la personne qui est d'astreinte de d'avoir moins de pression sur ses épaules et de juste suivre le runbook et de savoir exactement bah quel est cet alerte qu'est-ce qu'elle veut dire quel est l'impact associé comment je diagnostique cette alerte pour comprendre la quelle est la roue de cause et les étapes de mitigation qu'il y a derrière alors Alice si vous vous souvenez au début je l'ai présenté elle a fait du monitoring enfin la manager des des cluster cubes et en fait bah elle a une idée parce que sur les cluster CUB si on regarde le le cube promeus runbooks en fait il y a un ensemble d'alertes communautaires qui existe et qui sont le fruit vraiment du H mind de de toute la communauté Tech qui run des des qui manage des des clusters cuberntis avec toutes les alertes que n'importe qui qui manage des clusters cuberntis devrait avoir et elle se dit mais est-ce qu'on a ça sur posg est-ce qu'on a un ensemble d'alertes connu que tout le monde devrait avoir bah on n' pas trouvé on n pas trouvé euh mais du coup on a essayé de faire la même chose et on a fait euh on a on a développé ce qu'on a appelé le database monitoring framework d'accord qui est un autre projet euh idem qui est Open Source et qui va réutiliser toutes les métriques euh qui sont euh exportées via euh le l'exporteur postgrql communautaire et le prometus exporteur dont je vous parlais juste avant et donc tout ça nous donne un écosystème d'alerte qui est facile facilement déployable et réutilisable et qui donne du contexte à n'importe donc comment ça se présente il y a à peu près une trentaine d'alertes que tout le monde devrait avoir sur C sur ces clusters postgé hébergés sur RDS qu'on a mis à disposition et ça se présente un petit peu de cette façon on a nos nos deux exporteurs du coup l'exporteur communautaire et l'exporteur RDS exporteur qui collectent les métriques et le database monitoring framework en fait déploie des alertes proméus donc des pr sous forme de de promeéus rules dans nos clusters cube et chaque alerte est liée à son runbook qui est exposé sur un site web comme comme le comme c'est fait pour kuberntis où on retrouve bah la description de l'alerte son impact comment on la diagnostique et sa mitigation voilà donc j'ai parlé de la collecte de métriques j'ai parlé de l'alerting et maintenant bah vos métriques vous allez pas vouloir les consommer uniquement pour de l'alerting vous allez vouloir faire du suivi de performance vous allez vouloir faire de l'inventaire de la découverte euh et donc j'aimerais vous parler un petit peu de la partie dashboarding et visualisation et jusqu'à il y a quelques mois en fait notre stack de visualisation sur postg RDS cétait un dashboard unique qui avait pas moins de 9 sections 54 panells et les feedback qu'on avait régulièrement là-dessus parce que notre vocation enfin la philosophie De Conto c'est aussi d'ouvrir sa stack de monitoring au développeur pas uniquement au SRE c'était bah je trouve pas ce que je veux ça contient des informations trop trop tech technique pour moi je comprends pas parce que on mélange en fait on mélangeait dans ce dashboard tout un tas de de métriques à la fois des métriques au niveau de CPU memory et cetera qui sont facilement compréhensibles et puis à côté on parlait de verrou applicatif de lo pogay des choses qui sont internes à posg et donc on mélangeait un peu toutes ces informations et ça rendait le dashboard inutilisable ou en tout cas ça ça donnait pas un grand sentiment de confiance dans dans dans ce dashboard et donc pour aller vers le mieux je vais faire une petite digression et vous parler de la méthode diataxis juste par curiosité ici une petite levée de main sur qui est familier avec la méthode di taxis ouais je vois ok conto ou une personne d'accord donc c'est pas très connu et ben la méthode diataxis c'est une méthode qui permet d'organiser et de structurer de la documentation technique de façon efficace et vraiment le concept clé derrière la méthode diataxis c'est de dire quand j'écris de la documentation technique je devrais pas me focaliser sur le contenu ce que j'ai envie de mettre dedans mais plutôt quand est-ce qu'elle va être consommée cette cette documentation quelle va être mon audience et dans quel contexte elle va elle va aider la personne qui la consomme donc je vous donne un exemple si vous êtes sur une phase d'apprentissage vous voulez apprendre comment fonctionne la création d'index sur POS gr bah vous êtes en train de faire du studying vous êtes en bas à gauche vous avez envie de comprendre des choses vous avez le temps d'aller sur une documentation qui vous donne des détails exhaustifs sur comment ça fonctionne maintenant si je vais euh tout en haut à droite vous êtes en incident d'accord vous voulez une documentation technique vous en ficher de comment ça fonctionne la création d'index de prosg voulez avoir une recette de cuisine qui vous dit run telle commande run telle commande run telle commande et le problème il est réglé et finalement cette méthode diataxis qu'on utilise d'ailleurs chez conto pour notre documentation technique tout est organisé comme ça on a qu types de documents qui existent dans notre dans notre base de données de connaissance et ben on peut l'appliquer au dashboard et c'est ce qu'on essaie de faire on essaie d'avoir des dashboards qui sont minimaux et qui correspondent à des contexte d'utilisation bien spécifique donc on va avoir des dashboards de résolution d'incidents où on va avoir juste le set minimum de métrique qui est pertinent pour l'alerte qui a été levée mais à l'inverse on va avoir d'autres cas d'utilisation on va vouloir faire du capacity planning où là on vair faire un focus plus sur le long terme donc on a essayé un petit peu de voilà de sectionner enfin de de catégoriser nos dashboards pour avoir des dashboards qui sont plus légers et qui sont beaucoup plus facilement consommables donc ça c'était sur la partie organisation je vous donne quelques conseils supplément posez-vous toujours la question quand vous faites un dashboard de quelle est votre votre audience parce que c'est pas forcément vous qui allez être le premier consommateur ça peut être des personnes qui sont moins technique qui gèent moins l'infrastructure qui ont moins de connaissance dans ce domaine et documenter vos dashboard alors très bien parce que ça rebondit un petit peu sur la présentation de la poste juste juste avant le panel texte grafana c'est votre meilleur allié quand vous avez fait ce travail vraiment de catégoriser vos dashboards et dashboard qui sont légers vous pouvez vous permettre de mettre des des panels text comme ça qui vont raconter une histoire et faites en sorte que du coup votre vos dashboard racontent une histoire et soit soit simple d'utilisation donc ça par exemple c'est un des dashboards qu'on a fait sur le suivi de performance des requêtes ou un développeur il arrive sur ce dashboard et on lui dit comment l'utiliser comment trouver c aquery pourquoi il trouve pas sa query et cetera pour l'aider à le guider dans dans l'utilisation du dashboard donc ça c'est la partie organisationnelle et maintenant j'aimerais partir vraiment sur la partie visualisation comment on fait des des des des graphes qui sont claires alors je vous pose une petite question on regarde ce dashboard selon vous est-ce qu'il y a un pic d'activité à min 40 vous vous doutez j'imagine que si je pose la question c'est un peu question piège et bah en fait si je dézoome on est là d'accord on se rend très vite compte en fait que un dashboard qui n'a pas de point de référence peut vraiment biaiser notre interprétation de façon très simple d'accord donc faut vraiment être vigilant là-dessus et donc à conto on a établi tout un tas de best practice de meilleures pratique sur c'est quoi un bon dashboard c'est quoi un dashboard l lisible essayer d'apporter des conventions là-dessus donc Alice elle est super forte là-dessus moi je moi personnellement bah ça c'est un dashboard que j'ai fait d'accord il est pas lisible on comprend rien c'est erreor prô j'ai pas de point de référence mais Alice elle sait elle sait bien faire c'est les mêmes données présentées différemment d'accord là on passe de quelque chose qui était pas du tout lisible pas du tout compréhensible à quelque chose qui est limpide qui peut pas être mal interprété parce que on a notre point de référence on a notre freshold et cetera d'accord et Alice elle veut aller plus loin alors il y a des quelques conttours dans la salle on adore notion à conto et on adore les standards donc adice elle a une idée elle dit bah moi je sais faire des dashboard lisible donc je vais écrire un standard je vais écrire les bonnes pratiques sur comment faire un bon dashboard et puis elle est très contente de son gocument et puis se passe quelques semaines et sur un autre dashboard rebolote qu'est-ce qui s'est passé bah en fait pas difficile pas facile d'appliquer ce un standard comme ça je sais pas quelle est votre expérience sur grafana c'est assez c'est assez lourd des fois de faire des dashboards et utiliser bien les bons de trouver les bons settings pour faire quelque chose de clair et de lisible et c'est peu reproductible et c'est pour ça que j'ai envie vous poser cette question bah comment on peut garantir que les dashboards créés par l'équipe respectent les meilleures pratiques et je veux vous parler du coup de la possibilité de faire des dashboards à code d'accord de façon programmatique donc je vous parler de graphonet qui est une librairie Gonet qui est un langage de de programmation descriptif d'accord et qui va nous permettre de définir nos dashboards de façon programmatique donc là on est vraiment sur l'exemple plus canonique he je définis un dashboard je lui donne un titre je mets un panel dedans et je donne la query que j'ai envie de faire afficher sur ce sur ce panel donc ça ça marche l'exemple le plus simple mais bon bon vous regardez ça vous me dites bien gentil Damien mais c'est ça m'aide pas trop qu'est-ce que tu en fais alors je vais vous expliquer comment nous on utilise ces capacités là chez conto maintenant en fait il faut bien comprendre que le fait que ce soit du code ça rend ces dashboards beaucoup plus facilement distribua on a la possibilité d'isoler vraiment la partie UI de la partie données c'est-à-dire on peut extraire les requêtes dans des fichiers spécifique imaginez demain quand vous voulez faire un dashboard que c'est une revue de code comme n'importe quel autre euh comm n'importe quel autre service backend que vous écririez vous allez faire une revue de code pour savoir si votre dashboard il est il est bon bah là c'est pareil vous allez pouvoir facilement demander à des personnes de revoir vraiment la partie donnée pour être sûr que la métrique que vous mettez dans votre dashboard c'est la bonne métrique et c'est elle est valide vous pouvez lui faire confiance et puis au-delà de ça ça nous permet de définir des composants qui sont réutilisables et euh moi quand je fais un dashboard j'ai pas envie de me me poser la question de il faut que je mette dans la bonne couleur en rouge en jaune j'en sais je m'en fiche de tout ça moi ce qui m'intéresse c'est vraiment écrire la requête quelle est la donnée que j'ai envie d'afficher mais finalement le la façon dont je l'affiche bah j'ai envie de réduire un maximum la pression sur les personnes qui créent le dashboard et on va pouvoir définir une librairie de composants réutilisable grâce à graphonet et ça va vous permettre également d'harmoniser toutes vos sources de données vos variables et d'avoir le même look and F sur tous vos dashboard ce qui est ce qui est vraiment vraiment super donc un petit exemple ça c'est la définition d'un un composant d'un panel de type stat d'accord donc ça affiche un chiffre et puis bah on va pouvoir intégrer tout un nombre de comportement par défaut qu'on a envie que tous nos panels de type stat est et puis on va même en bas voyez Lig 21 W warning on va pouvoir même pouvoir personnaliser customiser ce ce dashboard ce panel à l'utilisation et par exemple on ça nous permet aussi de définir bah des conventions de couleur là nos couleurs elles sont variabilisées et on leur donne une sémantique et donc à chaque fois qu'on va avoir un frachold et ben il va toujours être de la même couleur va pas avoir d'inconsistance entre nos dashbard à l'utilisation c'est très simple on importe le composant qu'on a défini et puis en dessous bah on l'utilise et on lui passe bah les quelques variables dont on a besoin et ainsi que la query qu'on a envie d'afficher dedans voilà donc en conclusion j'aimerais inviter tout le monde présent ici à ce qu'on construise ensemble une communauté sur l'observabilité des bases de données ça peut passer par la contribution au projet RDS exporteur et database monitoring framework l'ambition c'est vraiment de dire on fait tous la même chose on a des stacks qui en général sont assez similaires et on rencontre les mêmes problèmes et finalement bah on se rend compte que il y a peu de connaissances qui sont partagées donc c'est ces initiatives sont vraiment là pour ça et et voilà je je su ravi de vous avoir présenté un peu notre parcours sur comment on est arrivé là où on est aujourd'hui il y a encore plein de chose à faire voilà si merci si vous avez des questions c'est le c'est le moment et je prends juste un petit moment pour remercier toute l'équipe avec qui c'est un grand plaisir de partager le travail aujourd'hui merci combienfin première question c'est combien de temps ça vous a pris de développer les deux projets et comment vous avez inclus ça dans dans le quotidien c'està dire que on a des urgences dans le quotidien on a des projets et des deadlines sur les projets comment vous avez pu en plus développer ça avec l'équip alors ça a pris plusieurs semaines de développer ça et en toute honnêteté ça a été un travail important de Vincent Mercier qui a également pris le temps de faire ça des fois un peu sur son temps personnel pour être tout à fait honnête maintenant c'est moins longtemps que ce qu'on pense d'accord parce que aujourd'hui développer votre propre exporteur proméus en fait c'est quelque chose de relativement simple vous avez tout un tas de librairies et de modules qui vous permettent dans différents langages de faire vos propres exporteurs et c'est pas si compliqué que ça la partie database monitoring framework gestion des alertes et cetera et les runbook c'est finalement peut-être ce qui a pris le plus de temps parce que finalement c'est un condenscer les résultat de plusieurs années d'expérience et d'avoir des incidents et de savoir qu'est-ce qui peut qu'est-ce qui peut se passer sur compte quoi on doit se protéger et cetera une dernière question c'est est-ce que vous avez une limite de temps dédié à ce type de projet dans l'équipe ça veut dire le le la limite de temps là où on abandonne le projet on passe à autre chose alors chez conton on essaie vraiment d'être le plus pragmatique possible donc à chaque fois on se donne un budget de temps pour les pour les projets si on estime que ça c'était la priorité et on se dit on a envie de passer 3 semain dessus et une fois qu'on s'est donné notre budget de temps en fait on va réduire le scope en fonction pour toujours rentrer dans ce budget c'est l'approche on a la chance aujourd'hui chez conto d'avoir une certaine maturité qui nous permet aussi de travailler sur ce type de sujet on a bien conscience c'est pas forcément possible partout j'ai une question sur l'organisation comment s'articule les les interactions avec les développeurs là de votre équipe SRI donc j'entends donc les al les alertes typiquement les actions toutes ces réponses les le runbook qu'est-ce qui qui est à l'origine de ça c'est le développeur qui définit une nouvelle fonctionnalité il code et puis il vous dit ah bah attention là-dessus il faudrait penser à Leever ou bien c'est vous c'est dans l'autre sens non alors toutes les alertes que qui sont incluses dans le database monitoring framework c'est vraiment des alertes pour le coup vraiment orienté infrastructure d' n'est pas sur des alertes applicatives qui seraient basé sur des métriques métiers ou ce genre de chos mais les dével chez conto font leur propre dashboard suivre le suivre leur propre métrique donc voilà ils font aussi ce travail là mais là c'était vraiment sur la partie infrastructure ça répond à la question donc ils ont ils ont accès enfin ils ont d'autres métriques qui remontent applicatif dans dans prometus avec il le lisent aussi avec des dashboardana ouais tout à fait ils remontent leur propre métrique donc des fois ils définissent leur propres métrique custom dans le code applicatif qui sont remonté et qui sont visu ensuite dans des dashbard et comme ça après vous pouvez faire des corrélations parce que typiquement c'est quand même intéressant dans la réponse d'incident savoir s'il y a quelque chose niveau applicatif qui s passer aussi au niveau de l'usage sie juste la bas donné pas facile d'interpréter les pics on voit que le stockage manque peut-être mais on sait pas pourquoi ouais complètement donc des fois il y a des cas où ça va être un incidence c'est un nouveau déploiement et on se rend compte que ça génère plus de pression sur la base et cet mais en général les développeurs vont être autonomes vont se rendre compte assez rapidement que c'est le dernier déploiement ils vont juste RB et il y a pas forcément d'interaction avec les avec les équipes SRE et à l'inverse des fois ça va être bon bah on comprend pas ce qui se passe on pense ça vient de l'infrastructure mais en général quand ils viennent nous voir on est déjà au courant et on a nos propres alertes qui ah d'accord ouais ils suivent le run aussi alors j'imagine là tu donner un exemple bah qui vraiment du vécu avec développeur qui dit j'ai l'impression que c'est un peu c'est un peu plus lent euh tout à l'heure c'est donc donc ça veut dire qu'ils vont jusqu'en prod aussi pour suivre ce qui se passe ou c'est ou c'est pendant la phase de développement parce que le ressenti il est clairement pendant la phase de développement mais après non non là c'était c'était vraiment l'application en production en fait bah je vais vous donner un cas d'exemple très concret sur sur ça nous ce qui peut se passer des fois c'est que on a donc plein de bases de données plein de tables on peut pas tout suivre une par une c'est impossible par contre on a des indicateurs qui vont nous alerter quand une table devient trop grosse souvent on va commencer à avoir des des sign des des signaux un peu forts du type bah un développeur essayer de créer un index sur une base et puis ça génère de la contention sur le serveur c'est souvent une indication que la table elle devient trop grosse et donc après regardez aussi ce type de métrique et venir les corréler ça peut aller ça peut donner de du levier du et du factuel pour aller pousser des projets de partitionning de table et cetera euh oou là on travaille après conjointement avec les équipes parce que ça demande en général d'avoir une une connaissance et une comension assez fine des des use case métiers de comment sont designer les tables et d'avoir la meilleure stratégie de partitionning c'est un exemple concret où par exemple on peut avoir ce type d'interaction où c'est en production l'application vie pendant plusieurs semaines plusieurs mois et en fait le la dégradation de performance c'est rarement quelque chose qui est abrupte c'est souvent quelque chose d'organique le le DataSet de données augmente petit à petit en taille et puis on atteint certaines limites physiques qui vont dégrader la performance mais c'est quelque chose en général qui est assez difficile à à à à suivre et vous savez pointer du doigt si c'est un cluster mutualisé une application en particulier alors vous descendez au niveau de la requette euh ça se rattache pas encore ah c'est justement pour ça qu'on fait ce travail c'est que aujourd'hui c'est difficile sur des clusters mutualisés de forcément être capable de suivre précisément quelle est la part d'utilisation de ressources d'une base de données spécifique euh donc c'est des challenges qui restent à résoudre et sur lesquels on on travaille donc notamment par le suivi de de performance au niveau de la requête et cetera d'accord merci eu j'ai une question sur le sur la maintenance j'ai bien vu le développement des des nouveaux dashboard euh vous les revoyez régulièrement les dashboards existants pour les corriger ou les améliorer ou intégrer des nouveaux paramètres comment ça se passe par exemple avec le les les nouvelles releases des des des produits euh sur la partie vraiment suivie des des métrique enfin monitoring de RDS post Gray on est en train de revoir les dashboards de façon assez conséquente donc en ce moment oui après on vise à avoir quelque chose de relativement stable par la suite ouais là là c'est juste parce que il y a une une ambition de s'assurer qu'ils sont bon c'est pas une une revue régulière pour s'assurer qu'il sont toujours adéquats non après vu qu'on a cette catégorisation qu'on souhaite faire sur le cas d'utilisation d'un dashboard et d'avoir des choses qui sont beaucoup plus minimales euh on peut très bien enfin moi je j'anticipe qu'on va créer de plus en plus de dashboard pour répondre à des use cas bien bien particuliers et il faut bien avoir en tête que grâce au dashboard as code on gagne énormément en terme de vélocité aussi parce que du coup on n pas besoin d'aller s'occuper de la partie ui on peut vraiment se concentrer sur la partie qu'est-ce que je mets comme métrique dans mon dashboard et comment j'ai envie de la visualiser ça ça à la fin c'est presque juste un choix de quel est le type de panel grafana que j'ai envie d'utiliser je fais ma querie et puis j'y pense plus ton alerting tu l'as fait sur grafana aussi et si oui est-ce que tu fais de l'alerting à code alors l'alerting est fait à ce code mais c'est via des promitus rules qui sont t déployé avec le l'opérateur promus sur kubernetis et pour la partie alerting enfin vraiment gestion de laastin and call on utilise [Musique] obgénie euh c'était déjà en place quand je suis arrivé je pense que on a vraiment envie de gérer un maximum de choses à ce code en tout cas c'est la philosophie chez conto donc toute notre infrastructure tout tout est terraformé on utilise de plus en plus les opérateurs on a plusieurs cas d'utilisation on utilise des opérateurs où on déploie des ressources d'infrastructure via des des manifestes CUB salut Damien j'ai une question sur les le ce que tu montrais dans l'évolution de la durée des métriques pardon des des query tu tu montrais des graphes comme ça évolutifs au niveau enfin par query euh je me disais que des queries comme ça sur les bases de données il y en a presque une infinité en fait sur si on prend en compte pas seulement les queries des des applications mais aussi les queries des des analystes qui veulent regarder enfin qui veulent faire de l'analyse tout simplement comment est-ce que tu filtres justement celles qui sont pertinente et qui le sont pas est-ce que c'est optin pour pour avoir un suivi de la mîtrique des query ou est-ce que c'est c'est po GR qui filtre pour nous c'est une configuration de la l'extension PG stat statements en fait va garder en CH les 10000 requêtes les plus récentes et qui mettent le plus de pression sur le sur l'instance c'est pour ça que sur notre dashboard pardon cétait plus [Musique] loin sur not dashboard ici on aery parce que toutes les quer sont pas [Musique] présentesalse merci beaucoup [Musique] c'est c'est pas une question mais c'est pour répondre à la question grafana alerting contre promus alors ça a été mis en place aussi avant que j'arrive mais je crois que c'est historique parce que on peut faire des alertes sur promus depuis plus longtemps que sur grafana je crois que c'est juste réponse là je crois [Musique] ok s'il y a pas plus de questions merci beaucoup Damien merci à tous

