# Uber Shares How They Cut Observability Costs With Grafana Cloud | ObservabilityCON on the Road

Uber operates at an immense scaleâ€”30 million trips daily across 70+ countries. Behind the scenes, over 5000 microservices ...

Published on 2025-03-25T05:44:40Z

URL: https://www.youtube.com/watch?v=r0oPhjIyR0Y

Transcript: hi everyone my name is uh shave R chadri I'm an engineering manager in Uber's platform engineering and I'll be joined by my colleague Milan chabby who's a senior staff engineer uh together we lead Uber's programming systems Group which is an R&D group that helps Uber stay at The Cutting Edge of innovation today we are here to talk about our profiling journey and how we are optimizing microservices at Uber scale so let's talk about Uber skilles many of you might have taken Uber to come here to this conference uh what is mindboggling about Uber is that Uber serves 30 million such physical trips every day and we do it uh across over 10,000 cities in over 70 countries right and all of this is available to you on your fingertip in the Uber mobile app but powering all of that is our Uber microservices we have over 5,000 microservices and uh over 11,000 commits land every week uh to update these microservices they are deployed over half a million times uh every week and uh yeah so let me so behind these microservices right these are touching customers but if you look under the stack there are several libraries systems containers all the way to the atom there's a huge stack and there are Myriad inefficiencies in this uh you know the software in each one of these Stacks that's room for optimization right uh however if you can't measure it you can't optimize it and that's where uh profiling comes in so at Uber our profiling Journey started with reactive profiling uh then we upgraded it to proactive profiling and now uh we are in our continuous profiling um you know and these are the four steps and I'll walk you through each of these steps um so the first uh profiler we introduced was an ondem mind U ad hog profiler we call it U monitor uh and this is a screen that you see where an engineer can go in they can select their service and say okay give me a profile right now and it'll give you a profile however this is reactive right uh we want a profile when the incident is happening so uh the second thing oops uh second thing we did was uh added Auto profiler so here is a sample configuration but uh this is essentially threshold based profiling you can say that whenever your CPU uh usage goes over a certain you know percentage 80% for example you want an automated CPU profile right uh so it gives you proactive profile but you only get Prof files you know in the moment you don't see what happened before uh and that's where we wanted proactive periodic profiling so think of this as cron based right like hey you want like continuous uh like periodic profiles um you know for your different services so these are daily one minute profiles that we collect and we Aggregate and this allows us to understand what's happening you know across our Fleet of microservices however this is again periodic it's not continuous and that's where continuous profile filing uh comes into play you know at our U team we always look at like build versus buy and when there is something that is great uh you know uh available externally that we can buy and accelerate our efforts that's the preferred solution because we can you know uh have faster time to Market so before uh enabling or you know like giving Uber Engineers a taste of continuous profiling uh like any uh engineering or what we did was we asked them we did a survey and we uh showed they shared a spreadsheet with like hey how much is is it going to cost them for their service and uh the potential Roi that they'll get and what we heard back was that mostly everyone said yes there were some Mayes but no one said no everybody agreed that it would help them improve their services and uh they estimated it'll help them you know uh between 1 to 10% of CPU savings some even estimated more than 10% uh 82% said that it'll help them better manage their incidents and many said that they will use it actively right and with this excitement we were like yeah we are we need to uh you know uh deploy this so let me walk you through some case studies of what we have learned and you can see how we used uh pyos scope or continuous profiling so the first case study was uh around reducing costs a lot of uber users go as a preferred uh backend programming language and a lot of go code typically suffers from memory poor memory allocation so pyroscope was able to pin pin point one of such top allocation sites in a backend service which was causing like 30% of uh allocation overhead so what we did was we optimized uh this service uh with an unsafe package with some guard rails and that helped us save around 10,000 course which is in an order of millions of dollars saved right now after an optimization goes in you still need to measure performance and make sure that that optimization is doing what it is supposed to do so let me walk you through how we did that so this is again the pyroscope UI what you see here in the metric chart is before the optimization the CPU utilization and then you see a gap which is when the service is getting redeployed and after that you see the lower uh metric value right uh what pyroscope allows you to do is select a section of these uh you know in the metric chart and Below you see uh you know flame charts like this right this is a differential flame chart which shows you and confirms that there was a 48% per Improvement so this is how you go for to differential flame charts and it makes debugging super simple now let me present another case study so here is uh you know the grafana chart a metric chart uh of memory usage and we noticed that for a specific service the memory usage grew to more than 90% And we had like no idea where I don't know what you see there but I see gibberish right very hard to understand that metric chart uh so once we started looking at it with pyroscope uh this is what you see right so so it's the same chart on the left and right but on the left you can select your Baseline on the right hand side you can select your comparison and Below you get like a differential flame chart as well as you know you actually get a table which points to like the line of code that is actually responsible for it right uh next example is from an incident so uh this is a timeline of the incident so we had a critical service which was crashing on boot uh due to out of memory right and then uh what we did was we enabled service Heap profiles uh then we compared the good State versus the bad State using this uh flame chart view as well and we were able to pinpoint the diff or the change that was responsible for it and revert it and we did it all in 30 minutes time to mitigate right this would have taken several days of laborious investigation and Remediation and we saved our Engineers from this pain so these are some examples and let's see what service owners at Uber are saying so these are some of our customers within Uber and uh consistently they've been saying that um you know usually there's some of these problems which can lead to outages but pyroscope helps them catch these sooner even before they become incidents uh sometimes there a slow Spike and people really uh want to debug it and before this there was no way to for them to actually systematically go in and debug those issues right and pyroscope like for one engineer they said that it was able to identify when the issues start started as well right um so everybody loves it and U another thing that we talked about was overhead right so this is a large service at Uber and first we enabled it for 5% and then 50% uh and we did not see any overhead uh with pyroscope so this was a very very critical decision for us to roll out pyroscope so in summary uh these are learnings right pyroscope uses the same default profiler that go and Java have and Ensure low low overhead uh it then asynchronously sends all of these to grafana Cloud for storage uh we have Rich set of tags to compare uh profiles across different zones deployments and so on and it gives us a rich differential comparison which helps with root cause uh identification so we did a six-month POC of U um pyroscope at Uber and deployed it across six tier one services and uh you know did not see any visible overhead it shows gave us a lot of uh rooms to optimize our services and also fix chronic problems and in 2025 we are systematically adopting it across all of uber Services uh with that I will pass it on to my colleague [Applause] Milan thanks shaik um we love profiles um continuous profiling is great but we want even more from it and that's where gen meets optimization developers are not just happy with profiles or root causes they want automated solution for code so that they can relax um this was the state of code optimization at Uber previously reactive some problem happened latency grew cores went up errors happened an outage happened that's when you would scramble to figure out what happened it's tedious it's hard to debug and find root causes of inefficiencies and As shik's Told there can be inefficiencies across the software stack it's non-scalable individual performance experts investigate on demand basis we want to change it we want to make it proactive we want to identify problems before they happen we want to make it automated so that automatically and correctly address code level inefficiencies and we want to it to be scalable so that we can democratize and allow developers of any level of expertise to perform code optimizations a lot of inspiration for this work came from a code snippet that I'm showing here this Cod snippet is um Json uh it's taking a string as source and it's filling up another buffer to meet that string to Json RFC uh uh requirements apparently it's it's doing things like hey is it a special character I have to escape it I have to append quotes and stuff like that what you see in this piece of code is a lot of appens and if you have written some go code you will see that these appens are vectors of dynamically growing size every time it doesn't fit it'll double it and throw away the old one copy the old one to new one and as this keeps happening over and over again there will be a lot of garbage collected uh this happened in a very critical service and it was consuming a lot of CPU if you are a developer maybe think about it think for a second um what would you do um to optimize a code like this jity uh which we did by the way um but it it's kind of inspiring what the generative way I did was it's first scanned through the code it said let me estimate how much bytes you need okay now that I know how many bytes you exactly need um let's allocate that much and then now that we I have this pre-allocated buffer let's simply populate it I guess I might I although I know about this one I might not have come up with this immediately I might have done hm let's allocate maybe two times more maybe five times more whatever is the maximum and still waste some memory so this was actually an inspiration that hey it can do code level optimization right uh we had seen experts looking into each hotpots and it was time consuming so gen can help gen is a catalyst it it's not a pessia it won't solve all your problems it can hallucinate it's after all a sequence predictor right it can easily go wrong so we had to develop a lot of program analysis techniques behind the scenes uh to do the code transformation and do code transformation validation and uh build a high confidence solution so that developers are open to accepting things that uh gen is automatically generating um and we had to tune um the model to produce what we want so we developed an end to-end pipeline that goes from taking these profiles that we are continuously producing to generating high quality diffs um so behind the scenes there is complicated procedure happening it all begins with having access to profiles uh it can work from any source of profile continuous profile Fleet profiles whatever we take from there we we are able to extract hot functions uh come up with a recipe of optimization come up with candidates candidate patches verify them build them test them evaluate them and then that's not sufficient you can say hey I optimize this code but the developers will ask question does it really improve so you need to synthes you need to have benchmarks we may not have benchmarks you have to synthesize those benchmarks that also we use generative AI to produce uh benchmarks and then we run those benchmarks it has to pass a lot of quality checks after that we deliver them to developers um many times in the initial steps we had experts validate them uh what's the outcome by now more than 100 automatic diffs have landed they fall into a whole lot of different categories but if we use go very frequently a big chunk comes from memory allocation related problems but we've been able to cover whole spectrum of uh categories and our developers love it um so here is Dev 25 commenting nice change I remember this optimization from he referring to something that he had seen uh on the web and then uh another person says nice to see llm code changes improving efficiency this is at scale in important services so here is our uh vision for gamifying efficiency our developers are deeply involved in code writing deploying observing them so they always have access to charts like this how their servic is behaving what's their utilization how many instances they have how many cores they have so pyroscope will come and show flame graphs where we have hotpots and then it will identify we will show them what are the top places to optimize by different kinds of metrics by CPU by memory by scale is it algorithmically scaling well or not and at their fingertips they will also have get a fix which will produce automatic optimizations for them so what do future hold at Uber for profiling and optimization we'll have low overhead continuous profiling thanks to our collaborations with pyroscope gra we will be watching Code like Hawks to and not have any inefficiencies creeping we will have automated code optimizations so we'll have expert level code optimization at scale available at our developers finger tips we are trying to shift left a lot of finding um and having automated code code Benchmark generation linting so that any inefficiency that we suspect is detected and fixed early there's a lot to lot of ground to cover in technology space and perception of how developers see machine generated code uh we need to have rigorous translation validation when code is generated by machines which is possible through technology uh we to have reliable M marking to prove that automated optimizations are uh have have real impact in production developers usually have a very high bar when code comes from machines um so we need to uh Bridge those types of uh perception problems but I believe that there are also a lot of developers very open to adopting these types of Technologies um so we have had an excellent Journey um thanks to um our friends from uh grafana pyroscope uh has been an amazing tool for us we believe this will continue it will continue to power us do a lot more optimization and um save um cost for us in our infrastructure

