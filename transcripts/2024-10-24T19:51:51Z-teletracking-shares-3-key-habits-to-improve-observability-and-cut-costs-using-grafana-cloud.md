# TeleTracking Shares 3 Key Habits to Improve Observability and Cut Costs Using Grafana Cloud

Oren Lion and Andrew Qu from TeleTracking's Productivity Engineering team share the top three habits they implemented to ...

Published on 2024-10-24T19:51:51Z

URL: https://www.youtube.com/watch?v=ZGquwsXt1C0

Transcript: Hello, my name is Oren Lion. I run productivity
engineering at TeleTracking. We're a central platform
team that looks after CI, deployment, infrastructure,
and observability. And this is Andrew. Hello Everyone. My name is Andrew. I'm a software engineer on the
productivity engineering team. Teletracking is a healthcare
operations platform. We work behind the scenes to
help keep patients flowing efficiently across health
systems and within facilities, much like Sabre runs behind the scenes to keep airline reservations
flowing within and across airlines. At a recent company town hall, I raised my hand and had a question
for TeleTracking's founder Michael Zamagias. What are three things you
want us to stop doing? And he called me the next day,
fired up and said, I've got 33. When Andrew and I thought of using
this for today's presentation, we also got fired up about
things we wish to stop doing and came up with way
more than three about 33. However, at ObservabilityCON we are super
excited to present the top three things that we found led
us to better observability recently. And these are the top three: Stop spending on unused telemetry; Stop loosely managing
beyond incident recovery; Stop building front ends without
observability. Stopping these made a big difference for us. Habit number one to stop, stop
spending on unused tele telemetry. So we started in 2018 with
an open source Prometheus and Grafana stack and by 2021
moved to Grafana Labs and came up with a budget that we
thought would be more than enough But costs skyrocketed. And we knew that we weren't making
use of all the metrics and wondered what's generating them and how
do we get them under control. In this section, I'll dive into our efforts to
identify and tamp down these costs. We migrated our metrics and logs
from open source to Grafana Labs in 2021. And I was blown away that we
were pushing over 1.5 million time series. I knew we weren't using
half that. And here's the point, I knew and we all knew that we weren't
using this many metrics but we paid for them and we needed to find
out where they came from. So we looked and it boiled down to two
groups generating all these metrics. Group A custom metrics: the metrics that we write
into the services that we code and dependency metrics: the metrics for things that we
depend on to run our services. Custom metrics, these are what we
coded into the services that we write. These show us the behaviors
we process when we process our business events. These
are super important. We think we're doing all the right things. We're building small services
and deploying them incrementally, but we keep increasing our costs. So in the top panel here you'll
see about 300,000 time series. These are custom metrics that
are aggregated for all of our, all of our microservices. And below that you'll see an example
of one team who produced 100,000 metrics. And below that you'll see the small
services that rolled up to a hundred thousand. These metrics were generated by our business services and on the
other end their dependency metrics. These what we use to support our
business services. For example, to monitor Kubernetes,
there's kube-state metrics, to monitor EC2, there's node exporter
and to monitor Kafka, there's JMX and the list goes on and on. So let me give you this example. Promtail is a dependency we
use to forward logs to Grafana Labs. It runs as a daemon set. So that's one pod running
per node and it publishes 275 times series per pod. So in a 40 node cluster that's 11,000
times series no one thinks about until they get the bill. And
take the Blue/Green example, right? So if I deploy to
a new cluster in a blink, 11,000 becomes 22,000. And the point here is one small
service generates 11,000 times series and there are hundreds more like
this and they double pretty fast too. So we're bringing these two together
custom metrics and dependency metrics. We've built incredible metrics generators. Teams will think about design and how to monitor their
services and dependencies but fall short of estimating and tracking
the cost to monitor a service and really in the reeling in the metrics
is not in the plan until you get the bill. And then you scramble. And scrambling, to reel in
metrics looks like this, find the high cardinality problem, make changes to really relabel
configs that everyone loves to do, raise a PR and get it merged,
and deploy all the clusters. The point is it takes a lot
of work to manage metrics and I wish it was easier. I wish it was like the way log
levels increase or decrease the volume of log messages. So I'd take a look at how log
levels work to manage log messages. So in logging, the problem
of controlling veracity, generating log messages is
controlled by log levels. You've got error, info,
warren debug, trace, you deploy a service and
the default log level, usually error should keep
the volume of logs low. So when debugging a problem, we may increase the log level to
generate more detailed messages. And when debugging is done we simply
dial the log level back from debug to error and decrease log volume. So here's the thing, this
is how easy it should be. Dial verbosity up and back
down. But for metrics, it's not this easy. There's no easy config to increase
or decrease cardinality in metrics like we do with logs. But
Adaptive Metrics gets close. There's detail in the labels
that we may need at some point, but we don't need all
the data all the time. Adaptive Metrics allows us to
aggregate away the detail and labels. So I don't get the bill for those
large volumes of time series. And here's the point, you still generate the detail and labels
that you may need one day but you don't get the bill and you could reinstate the
labels with the detail that you need. Adaptive Metrics does a lot for us but
we don't work much to make Adaptive Metrics work. As Jen was saying, get the humans out of the loop
and I'll come back to this. Adaptive Metrics saved us 50% on our bill and saved us lots of time since
we stopped scrambling to make client side changes to tamp down metrics.
We just let Grafana Cloud do that. Now Adaptive Metrics isn't open source. You need to be Grafana
Labs customer to use it. And this was another validation that we
did the right thing by moving from an open source stack to Grafana Cloud. So we're we're making plans to update our dependencies to
save money with AWS, right? 'cause they give us recommendations too. And they said if you use graviton
you can reduce your cost by 30%. There's an upfront cost, so thank you AWS. We are now updating dependencies
and in two quarters after we've updated libraries and
deployed, we'll have saved 30%. This is an upfront there. This
is the upfront cost to cut costs. It takes two quarters of effort. And here's the cool thing
with Adaptive Metrics. There's just about no
upfront cost to cut costs. Sorry about that. In fact, there are
no people working on this either. We've got a job that pulls
recommendations and applies them. Habit number two to stop: stop loosely managing
beyond incident recovery. When an incident is active, phones are
paging and people are jumping into help. And when it's over there's a big sigh
of relief. People want their lives back. But there's one more thing.
The paperwork, the postmortem. And the point is on top of the 3:00 AM
call and getting the system functional again and some critical paperwork
still needs to get done after the recovery to prevent the
next incident. In this talk, I'll focus on something really exciting, the paperwork beyond incident
recovery and how Grafana Incident helped us tear through. Let's look at an incident in
these three parts. Recovery, postmortem (the paperwork), and
resolving defects. The first part, recovery has many metrics to help you
measure yourself against many TTRs. Meantime to recovery. Meantime
to repair. Meantime to respond. And here's the point, recovery
to postmortem is off the charts. There's no measure that's
talked about like MTTR. So let's give you each part a goal. Recovery should be dealt be
done well within the SLA. Postmortem less than two days. Do it when it's fresh and get moving
on logging defects as soon as possible. Defect resolution. Aim for less than
two sprints for mitigating defects. Aim for less than a quarter for
preventative defects that take longer to resolve. So everyone does
a postmortem in some form. Most are based on the Google postmortem
checklist and a postmortem is a lot of work. Google has 24 questions and we have a variation of this
and it's a long list of questions. It's an eye chart so don't worry
if you can't see it in the back, just Google it. Before Grafana Incident, we easily spent easily half a day
responding to a long list of questions like this and it takes like at least a
week to get people to work for half a day. I'd like to show you where we struggled
to do a postmortem before Grafana incident. And here's where we struggled for
about four hours to write up and do the postmortem. Look at how lengthy our incident
workflow was on the top panel before Grafana incident. Each handoff between a system meant
we need to dig into another place to get the details of the recovery. It meant digging into Jira and
OpsGenie and Slack and even Grafana to consolidate it all in Confluence. And Slack was especially terrible because
there are so many comments to pull and compile into a Confluence page. And the point is the writeup shouldn't
take as much effort as the incident. So after Grafana Incident, we had a much faster flow and on top of
having the faster flow, Grafana Incident does some pretty cool things. And one cool thing is it bakes together
the recovery and the postmortem and that little robot face does the magic.
When we are in incident recovery, we're generating lots of data in Slack
and this could be in comments but also links and screenshots in Slack
to make it easy for people to see outliers and metrics. And using the robot face in Slack to
push comments to Grafana Incident, I get all the Slack data into a timeline
and into a place to organize it all. And another cool thing is to make it
easier to filter out the signal from the noise in the Slack comments
that got to Grafana Incident, Grafana Incident lets us filter
comments using these emojis. The gold star right here.
And one more cool thing, Grafana Incident templates in many
of the answers to a postmortem. So as we work through the incident,
we're already building the postmortem Thanks to Adaptive Metrics. We had room in our budget
to fund Grafana IRM and now we spend less time on our incident
workflow including postmortems, doing postmortems efficiently. And this is how we got better at
managing beyond incident recovery. And now Andrew. Thank you, Oren. So like my boss, I'm real fired up to talk about
our last habit here today. And truthfully this is an area that
we struggled with for quite a while. So you saw that we were generating
a lot of backend metrics, we even generated too many backend metrics
and yet with all of this information we still weren't able to diagnose
and detect every user problem. You see consider if a user is unable
to log in or if they're unable to load pages, those experiences
aren't being captured anywhere. So we are missing that most
direct end user point of view. And when you can't see their problem,
you can't understand their problem. And then how could you hope
to resolve their problem? So this is our last habit to stop, stop building front ends
without observability. I'm going to explain how we use Grafana's
frontend observability to add more context to issues and
therefore rapidly debug. So prior to Grafana Frontend, we didn't have a consolidated
tool for capturing user sessions. We learned multiple tools, which was a lot to learn and manage and
yet we still didn't have all the context that we wanted. A senior from our front end team told me
that they were effectively blind to the user experience. And because our monitoring and
our alerts were insufficient, often times clients were calling in
and reporting issues to us before we could detect them ourselves. Which
as you all know is not a great look. Alright, so when a bug is reported, we gotta stay online with them and
we have to collect more information, we have to know what
environment they're on, we have to know what actions they were
taking into the system and we have to know when it all occurred. Only after
that can we try to recreate the bug. So as you see, all in all this is creating a lot
of delay between when the user encounters the bug and when
they see the fix on their end. So this year we started using
Grafana's Frontend Observability and it addressed many of those issues.
Instead of managing multiple tools, we're only using the one and it comes
with better dashboards and better visuals right out of box. In this screen cap, you're looking at a specific user session
and you have all the relevant context, immediately. At the top, you can
see the browser, the platform, the app, and all the versioning. This
could already be some clues, you know, maybe there's a known defect on certain
browsers and platforms or maybe you see that they're on an outdated app version
so they're not getting the fix that you already rolled out. That could
be a pretty quick resolution. Below that are some default
performance metrics. And then at the bottom you
can see the user journey, all the different pages
that they click through, that's going to help you recreate the
issue and pinpoint the root cause. Alright, so what you saw in that previous slide
was all the default information that gets pushed up to Grafana Cloud. But what if you have a specific use
case or a particular pain point that you need to monitor? So
that's certainly possible. You can define some custom metrics that
handle your case and push those up as well. And here's our example. Some of our customers were reporting
that they weren't able to log in or that logging in was slow and we didn't
have any kind of metric about that. So we made a change to our JavaScript, we added some simple logic that would
count and time the log in flow and we pushed that up to Grafana
Cloud. And here's the result. So you're looking at a 15 minute
sample and in the top panel you can see the rate and total logins split
across two different API gateways. Previously we didn't have good distinction
between those API gateways and that added an extra step in our investigation
to find out which one you're using. But now we can compare the individual
performance a lot more easily. So from that you can write
a pretty simple alert rule, say if your rate of login is zero
in the last five minutes and fire. So we also want to time the login flow. So on the lower left you can see the
average login flow in this 15 minute period. So that involves
loading the login page, typing in your user and
password, hitting enter, waiting for our backend auth then
getting returned to the first page. So that includes a lot of human elements, like people might be slow at typing or
maybe they bring up the login page and they get distracted, they walk away for
a few hours. So that skews our metrics. So we wanna isolate our own performance. So on the lower right panel you can see
the average of just how long it takes for our authentication service to return
and now we got a better idea of how we're doing and what's creating latency
for our users and where we need to make improvement. Alright, so we saw the session view and from
the session view you're able to get information like the tenant id their
user journey and the timestamps. Using that information you can go to the
errors view and then filter through all those errors to see what was
going on during the incident. So in the errors view, all the
errors are aggregated by message. At a glance you can see if there
are a lot of 504s or a lot of 403s occurring or maybe a custom error message
that you can now trace back to your code. Let me illustrate how our front end team
can use this errors view in combination with our custom dashboards to investigate. So on the left is a panel
and this represents the
rate of logs from a specific customer on a specific service. A problem we used to have was
that when we investigated, we weren't sure if it was being
caused by a connectivity issue on their end. So if they had a
network connectivity issue, that was something they had to address, but it wasn't easy to
tell if that was the case. While looking at this time series, if there are any gaps that indicates it
was a connectivity issue and the ball was in their court. However, if instead
of a gap, the time series is at zero, that indicates a problem with our service
and we better do something about it. So looking at where the gaps are, you can cross reference those
time periods in the errors view. And here I see that a lot of 403s were
were occurring when you know those gaps were occurring, I can click on
an error message and expand it. And here you see a lot of fields. You see some of the ones from the
session view, like the app and browser. If you defined any custom metrics or
custom fields that you included with your payload and pushed up, you
would also see that here. To point out one very important
field is the context URL, which tells us exactly which API call was
being invoked when the error occurred. And to really drive it home, here's a specific example of
how we can use the context URL. So one time customers were saying
that their sessions were ending seemingly at random, they
were just being kicked out. We didn't know what was going on.
So we went and checked the errors. We saw a lot of 503s occurring and
the context URL was pointing to this get state function call. So very quickly we could trace through
it in our code and we saw that we weren't gracefully handling any random
errors in that get state call, which would just cause the whole
session to error and then kick you out. So a very quick PR to gracefully handle
the errors that was merged and shipped out, rolled out to all of our
customers and the issue was resolved. So our frontend team is getting better
and better at navigating the UI, recognizing common error patterns and
then rapidly responding to user issues. So all in all, how has our process changed now
with Grafana Frontend observability? Instead of call clients calling
into us and reporting their issues, we can get ahead of the issues by setting
up alerts on all the new metrics and logs that we get. Instead of sitting on a call with a client
and having them trying to recall all the specific details about their
incident. We can skip that. We can look at our own dashboards and
get more accurate information in just one or two minutes. And from there we know exactly where to
look in our code or in our services and start implementing a fix. Alright, so we've visited several Grafana
offerings in the last 25 minutes and eight seconds. So let me give a recap
here. With Adaptive Metrics, we slashed our cardinality
and monthly spending, our developers can more easily find what
they're looking for without all that extra noise and the savings directly
funded our adoption of other tools like IRM and like Frontend
Observability. With IRM, we are running more efficient
postmortems and we're staying on task to resolve the incident-driven
defects to work towards prevention. With every incident,
unfortunate as they are, we're having good postmortems and we're
building towards a more stable platform. And finally, with Frontend Observability, we're finally bridging the gap between
our front end and back backend filling in that missing point of view, adding more context so that our frontend
team and support teams can properly address your issues. These are three
big steps that we took this year, but they're still a long road ahead
of us until we achieve observability maturity. But I know that all of you in the audience
haven't achieved it yet either unless you're from Grafana. 'cause I've seen
the results to the observability survey, I know that there's, everyone has more
work to do. So listen to this though, the habits we stopped
made us better. So today, think of three things that
you want to stop doing, really try to stop doing them and then
we'd love to hear how your observability practices got better too. So once
again, my name is Andrew, this is Oren. If you see us around at the conference, we'd love to chat and hear your
story as well. Thank you very much.

