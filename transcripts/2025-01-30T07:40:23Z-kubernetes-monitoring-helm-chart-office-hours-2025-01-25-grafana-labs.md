# Kubernetes Monitoring Helm Chart Office Hours (2025-01-25) | Grafana Labs

In this session of the Kubernetes Monitoring Helm chart office hours, we discuss the freshly released version 2.0 and what has ...

Published on 2025-01-30T07:40:23Z

URL: https://www.youtube.com/watch?v=-cNnXO1AGOk

Transcript: hi everyone welcome back to the next public office hours of the kubernetes monitoring Helm chart uh we're excited that you're here uh we've got a really cool one today uh so today is January 24th um and the big news is that we released the kubernetes monitoring version B2 uh that happened a week or so ago um if you've seen any of the other office hours you know that this has been coming for a long time and uh we finally got it released into the public and so I'm very excited about that it comes with a ton of really cool features if you're really curious about whether the differences between V1 and V2 we've got some content on the previous office hours you can take a look at it there otherwise find me on the grafana public slack there's a link at the end I'll show you um and uh you can always ask there but that's really exciting so in today's office hours uh I want to talk about the V2 release how that went uh what are some of the things that we've been doing actually since the V2 release was done um we'll talk a little bit about the next big release so 2.1 uh what's what what what are we looking to include in that uh and um and you know talk more about that um and as always have some time at the end for Q&A um feel free if you have any questions along the way feel free to unmute and ask um if you have anything to um to ask so yeah kubernetes monitoring Helm chart V2 uh it's really exciting I'm really glad that uh we actually got it through so we released it on January 20 or January 7th um and we've had a lot of people already start using it uh giving us some great feedback um we've actually we're up to 2.0.4 already and I'm planning on releasing 2.0.5 um and hopefully that uh shows that we're really committed to making sure that you know feedback that we received issues that are reported even nice to haves uh we listen to those things that helps really drive the road map uh and the features and the even just little nice to habes like you know setting name spaces instead of having to apply rules and things like that um let us know that's what really drives the development of this project uh and so 2.0.4 we've done a lot since the the 2.0.0 release on January 7th some of the patch releases have made things like all the job labels or many of the job labels that we uh set within the helm chart are now configurable it makes it more flexible for the different visualizations if you've got different dashboards that rely on those job labels being a certain thing um this lets you adjust things to match your environment uh We've we're adding this one's uh hopefully will be released today we're adding more components to the application observability feature chain including the interval processor which can kind of like uh latch all of the flow of the Telemetry data onto certain intervals so that uh it kind of matches the the DPM of like the metrics that you get from the Promethea style scraping uh we're adding the spa span logs and span metrics connectors so we can generate log and metrics based on the trace spans that are coming in through your application data so that's a really cool feature that's coming in we've been adding lots of examples uh and most of the examples that we add into our documentation we also back with an additional automated test so not only there these examples that are beneficial for you and the community to learn how do you actually integrate this Helm chart with different uh different environments or different deployment patterns uh but we're exercising the helmet chart automatically with these sorts of things so one example is a sharded Cube State metrics what in the world does that mean so Cube State metrics is the system that that gets deployed onto the cluster uh the kubernetes monitoring Helm chart itself does the deployment of cube State metrics what it does it looks at all of the objects that are in the cluster and this is pods this is Damon sets deployments Services config map Secrets everything uh and Cube State metrics generates metrics based on all of this information uh generates metrics like Cube node info Cube pod info uh the number of replicas you want your deployment to have the number of replicas that your deployment has it's the number that are running um and so that's extremely useful for building the kubernetes monitoring experience in Graff cloud or any dashboards that you have that try to understand the state of your kubernetes cluster and the objects running inside of it at a given time C metric is amazing um it's been around for a long time uh a sharded C State metrics means multiple replicas that all split the responsibility across the different replicas of cstate metrics by default it deploys with a single pod deployment by enabling sharding you split that over multiple replicas and it turns into a stateful set um and why would you want to do this well just like anything when the cluster becomes very large hoop State metrics itself can get behind on being able to accumulate all of this dat within the typical 60sec scrape interval so when you get a very very large kubernetes cluster either by nodes or by number of PODS um just splitting out your CBE State metrics into multiple shards can be helpful and so you then have to the alloy that's scraping the metrics has to then talk to each of the individual shards to get the full context of of the of the entire cluster sounds complicated I understand don't worry we've got an example now um and so we're showing how you can deploy Cube State metrics with the sharding as well as how you scrape that properly um ISO service mesh uh this is another example that got added so ISO service mesh specifically using the sidecar pattern uh how do we deploy the alloy instances that can handle that sort of system um and uh and we're deploying an auto test with that too so ISO service mesh sets up a lot of pod Depot networking um things like Mutual TLS and things like that um sometimes that cause conflicts with the systems that are trying to talk to each other or trying to discover each other uh and so we added this to try and help understand how do you set things up so that it can uh it can tolerate in that environment uh and I see Carl Z so thank you Carl for bringing that one up um I appreciate uh the feedback on that one um also the patch releases that we've released since 20 we've haded a lot of things to help with troubleshooting understanding where your high cardinality or high DPMS are coming from so hopefully we can help resolve those things faster and then just like every patch release lots of stability improvements lots of reliability improvements uh finding ways to remove uh metric duplications things like that so there's been a lot of work on 2.0 uh and a lot more coming um I I speaking of a lot more coming I know that there's a new version of alloy that's out um I'm going to try and get that released into the case monitoring home chart today sip a coffee to buy some time um so two. one what does 2.1 look like and I want to say uh early on I want to talk about what do we think about when we're thinking about a two. one the kubernetes monitoring Helm chart roughly follows sver rules so I mean roughly we're not very super strict about it where only new features come in minor releases only patch releases have bugs but this is kind of what I think about when I think about the the versioning for this major releases like we just did from a 1.x to a 2.0 are architectural level changes um things where it's going to take a lot of effort to go from the old style to the new style and and you know we'll do tooling like migration tooling or documentation to help you with that um this could be large breaking changes um or like I said architectural level features so changing how the system actually works um and so we promise we won't do anything like that until 3.0 and that's not even in my mind at the moment uh minor releases so you know 2.0 to 2.1 what we're thinking about there are large feature releases so a substantial Improvement in something and how how the helm chart functions or or a very new feature that you was not able to be done before um in 2.1 I am allowing ourselves to have some maybe minor breaks or deprecations but this will be like we Chang the wording on one thing and either we'll tolerate it and just handle it for you or like if any if you've used the 2.0 help chart you know that if there's any misconfiguration we'll try to let you know what needs to be changed so things like that um if there's anything that does need to be changed will do give you the exact language on please change your values file from this style to this style um and then the patch releases the double dot releases uh no breaking changes in those most of the time it will be dependency updates or or bug fixes um maybe minor improvements or minor features things along the lines of we added the ability to set a nam space and an automatically generates this outlo rule so I'm not reserving patch releases only for bug fixes and patches um but uh but minor content upgrades all right so with all of this you know why am I Preamble this talking about 2.1 with two. one we have some larger features that we have planned um and not the asteris down there this is not a commitment but we're just trying to be open sharing what we have planned in our road map uh 2.1 I've talked about this a lot tail sampling I'm really excited about this one this has been something that we've been hearing about for a long time people want to set up an entire observability pipeline that gets your application data that also includes tail sampling before being delivered to your databases either grafana cloud or to grafana Enterprise or you know some other database provider tail sampling will be baked into the kubernetes monitoring helmet chart uh it will rely on additional alloy deployments to do this but I'm really excited that uh you know we have the expertise to handle that properly um other features that we're looking into service graph metrics just like tail sampling this requires kind of a special handling and deployment of alloy to handle the whole flow of data through things um but when we're doing tail sampling we can explore things like service graph metrics uh to do that I've mentioned new alloy instances multiple times and if you look at the 2.0 values file you'll know that the large majority of it is spent defining your alloy instances what I'd love to explore and this is a little bit more on the experimental side but a way to dynamically deploy the alloy instances kind of like what we're doing for destinations um where you get to be in more control about what's gets deployed with your alloy instances um it's less of a static definition of those things um and then finally I'd love to see more and more service Integrations we have uh six or seven at the moment and we're getting more and more over time but I'd love to see that number really grow uh to make it easy to add additional metric scraping and log handling for services that are on your clusters uh in a way that makes it easy to Define and you're not handling custom config all of the time one of the big things that was really an important driver for originally for the kubernetes monitoring Helm chart is is let's bundle all of this config into the chart so that as the best practices updates then it's just a Helm upgrade away to get improved config and the current pattern of using extra config really works fine but it doesn't benefit from the ability to upgrade things so adding more and more service Integrations I see as the way to uh kind of durably upgrade service Integrations and configuration over time uh in a way that makes it actually a lot easier for for you the people using this chart um just like 2.0 you're going to get used to seeing this date TBD I don't know when this is going to be released um we're looking at the planning for this now uh as things evolve you know hear about it in upcoming office hours uh any questions about what we've got going for 2.1 before I hit to the next thing great awesome um so that was my last of the main content uh now is time for just open Q&A if you have any questions about the kubernetes monitoring helmart about the project about the road map uh feel free to fire away um one thing that I'll tell you that I am working on uh today is if you're using if you are a graphon cloud user a graphon cloud customer um I am working on updating the config page for the in the UI to utilize the V 2 version of the helm chart um right now it's pin to V1 I'm working on the V2 version of that right now so that should hopefully be released within the next week or so but yeah any other questions uh yeah I have one about the autodiscovery yeah uh I am in version one still trying to upgrade my Dev environment to version two but in version one there is the auto under the autodiscovery in the default values file there are annotation defined per each type for scrape what should be The annotation that it will search in the psor services yeah and I see it moved to feature autodiscovery in version two but the the thing is we have a situation like in the past we were using the prom. iio annotations and then we start to use grafana flow and change it to monitor agent grafana kind of the one introduced by grao and now we have a mixture some uh Community applications deployed users pr. and they don't have any way to uh modify it MH uh most of our inhouse have the old Gana flow type annotations and now if I re Shuffle again for that uh the the engineering team will kill me literally yeah right and there are some applications that they don't allow to change that annotation uh for me can it be uh as far as I see from the uh templates it doesn't use any kind of regular experation or what I was thinking like I may Define for for example for scrape K test. graana.com scrape or the pr. scrape kind of thing okay right now right what you're asking so you have ponds and services that have two different kinds of annotations to to autodiscover them and you're you're seeing like yeah so currently we we can't handle two different styles of annotation that's a really great question though let me um do me a favor uh open an issue on the kubernetes monitoring Helm chart GitHub repository do you have a link to that yeah yeah open an issue there uh let me know that you'd love to be able to have multiple different styles of Prometheus uh the autois discovery annotations yeah yeah and I will we'll explore some options there um I think there's probably some things that we can do to to make that work but that's a that's a good question okay thank you I will do that awesome thank you hey Pete for the uh service Integrations is that like the core DNS thing and some other things like that okay cool so that's the What's called the Integrations feature uh it's what's got like search manager and NCD now core DNS would fit into that definitely yeah all right cool because I notied there's like a cube DNS but that doesn't satisfy the requirements of the cord DNS integration page yeah yeah okay yeah the cube DNS there's it's where it starts to blur the lines between what's you know what's quote unquote control playe monitoring and what's just the core DNS service um yeah I'll take a look at that but uh definitely I want to get the ability to get core DNS and light up the full integration on graphon cloud to make that work yeah nice thanks oh good question right on Robbie do you want to unmute and say hi and I'll I'll intro you yeah sure thanks Pete so um hello everyone my name's Robbie I'm a software engineer here at grafana as well and I'm actually going to join Pete uh as a core contributor um with the Kate's monitoring chart so uh really hoping to provide him some support uh previously worked on some of the tail samplings stuff and uh early iteration of a tail um tail sampling implementation at grafana which included a Helm chart as well um so yeah looking forward to uh being part of the team here yep I'm really thrilled that Robbie's joining uh and adding on as a core contributor to the team um you know this there's been a lot of excitement around this Helm chart and so I'm glad that uh that we're adding more development talent to it um I've worked with him in the past he's an amazing engineer I think we're going to have a great time getting you know even more goodness added into this home chart so yeah excited to have you Bor yeah thanks Pete cool all right any other questions otherwise we'll get it wrapped thanks for all your hard work no thanks Carl I appreciate it definitely all right so yeah if you have if you think of more questions if you want to interact with us you can always find us on the public graas slack g.s slack.com or we're lurking in the kubernetes channel there um otherwise you can find us on our GitHub repository Graf kubernetes monitoring Helm uh like I said at the beginning this project is driven a lot by Community feedback and um the things that are really important and resonate to you so uh find us interact with us uh open issues um and uh and yeah I'm I'm grateful that you came and that uh we can get you know make this cool stuff work together thank you very much have a great rest of your day have a wonderful weekend byebye thanks you too byebye

