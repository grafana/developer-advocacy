# Agentic AI for Observability: Grafana Assistant &amp; Investigations

Published on 2025-10-22T16:58:54Z

## Description

Grafana is moving from copilots to agentic AI. In this session, Cyril Tovena and Dmitry Filimonov demo Grafana Assistant and ...

URL: https://www.youtube.com/watch?v=mYgexXCMXeg

## Summary

In this YouTube video, Sel and Dimmitri discuss how Grafana Labs is transforming observability through their AI-powered Grafana Assistant. They highlight the increasing impact of AI in the industry, particularly in expediting the deployment process, and introduce features of the Grafana Assistant that facilitate dashboard creation, issue investigation, and integration with external services like GitHub. The presenters emphasize the rapid development cycle of the assistant and its customization options, such as rules and data sources, which enhance user experience. They also showcase a live demo of the assistant's capabilities, including its ability to analyze infrastructure metrics, create pull requests, and provide insights during incident investigations. They conclude with a vision for the future of observability, emphasizing the importance of proactive issue management and the Grafana Assistant's role in this evolution, while announcing its availability at $20 per user per month.

## Chapters

Here are the key moments from the livestream along with their timestamps:

00:00:00 Introductions  
00:02:15 Overview of AI's impact on the industry  
00:04:00 Introduction to Grafana Assistant  
00:05:30 Features added to Grafana Assistant in the last six months  
00:08:15 Explanation of Rules and their customization options  
00:10:30 Overview of MCP (Multi-Cloud Provider) servers  
00:12:00 Introduction to new data source integrations  
00:16:00 Demo of Grafana Assistant's capabilities  
00:20:30 Discussion of ongoing investigations and autonomous agents  
00:25:00 Announcement of Grafana Assistant's pricing and availability  

# Grafana Labs and AI: Reshaping Observability

**Sel:** All right. Hello everyone. I'm Sel.  
**Dimmitri:** I'm Dimmitri.  

Today, we're going to talk about how Grafana Labs is reshaping observability using AI. Before we start, since it’s the last session, I want to ensure everyone is listening. 

**Quick Show of Hands:** Who is using AI at work, whether it's a coding assistant or ChatGPT?  
*(Most people raise their hands)*

That’s super cool! Let’s get started.

AI is definitely reshaping our industry. The time it takes to take an idea to production has been significantly shortened. It’s kind of crazy; we actually deploy much more than before. And deploying more means we’re probably breaking things more often, which is why we built the Grafana Assistant. We want to ensure it matches the speed at which you’re developing applications.

The Grafana Assistant is here to help you create dashboards, investigate issues, set alerts, or learn something new. It works for onboarding and for seasoned pros—you can use rules or MCP servers to maximize its value.

### Contributions and Development

This is a graph of contributions within the Grafana Assistant repository. I’m bragging here, but it showcases the dedication of our world-class team at Grafana, working on this product. We operate like a startup within a startup, iterating rapidly and utilizing AI extensively. We release updates on average every four working days, so if something was frustrating last week, it might be fixed by next week. We encourage you to keep using it and providing feedback.

### Features Added

Now, let’s go through some features we've added since the launch six months ago. 

1. **Rules:** This is a great way to customize your experience. You can set best practices or adjust the amount of text returned. You can also provide specific knowledge tailored to your company’s needs.
   
2. **MCP:** This allows you to connect to external services like GitHub or Linear. If the assistant identifies a gap in your metrics, it can help you create an issue directly.

3. **Data Sources:** We’re adding various data sources, starting with profiling and tracing, as many users find them challenging. We also added support for multiple SQL databases, including ClickHouse, MySQL, PostgreSQL, and Microsoft SQL.

4. **Integration:** We’ve built an SDK so that each team can easily contribute to the assistant and add new integrations.

We won’t stop there; we’re continuously improving and adding features based on user feedback. 

### Future Vision

Our vision is to evolve from providing co-pilots to creating an experience that feels like having more teammates on your team. For instance, we discussed running multiple autonomous assistants tackling problems from different angles, similar to how effective teams operate during incidents. Earlier, we announced Grafana Assistant Investigations, and we’ll do more in this space.

### Demo Time

Let’s switch to the demo!

Here’s the assistant homepage. You can access it from the navigation menu. The assistant is always available to help you on every page and understands the context of where you are.

Let’s start with a prompt: "Describe my infrastructure and provide me with a diagram using metrics." The assistant will gather metrics and provide a diagram.

**Dimmitri:** Can you show us how rules work?

**Sel:** Sure! You can access the rules from the menu at the top. 

We have a variety of rules here. My favorite is the run books where we’ve centralized everything on GitHub. The infrastructure memory sometimes generates even better suggestions, especially if you have a knowledge graph.

**Sel:** The assistant just provided a diagram of my infrastructure. It shows a lot of microservices and gives a brief description of what’s running. I can generate a link to share this with my colleagues.

### Contextual Suggestions

**Dimmitri:** What are those suggestions at the bottom?

**Sel:** Those are contextual suggestions based on your query. For instance, it suggests a system overview dashboard or highlights services with latency issues.

**Dimmitri:** Could we look at MCP servers?

**Sel:** Sure! I can see a couple of MCP servers. I’ve set up GitHub, and I can add more as needed. The assistant is also indicating a region with higher CPU usage, suggesting I check profiling.

### Analyzing Profiling Data

**Dimmitri:** How does the assistant help with profiling?

**Sel:** It provides insights into the profiling data and can connect with the team responsible for that if needed. The assistant can analyze the profile for me and identify issues.

**Dimmitri:** What about the context rectangles?

**Sel:** Those represent the most used data sources. The more context you provide, the better the assistant can assist you.

### Issue Resolution

**Sel:** The assistant identified that regex compilation seems to be a major issue. I can ask it to create a pull request to fix this.

**Dimmitri:** Can it just fix the issue?

**Sel:** Yes, it will attempt to create a PR to resolve the problem. 

*(As they wait for the assistant to process the request)*

**Dimmitri:** While we wait, I can find previous conversations related to Kubernetes logs.

**Sel:** Great idea. 

*(After a few moments)*

**Sel:** We have a PR! 

### Review of the PR

**Dimmitri:** Let’s check to see if it’s legitimate.

**Sel:** It has a good description and looks promising. 

**Dimmitri:** Let’s review the fix.

**Sel:** It addresses a classic Go issue by initializing regex only once, which should improve CPU performance significantly.

### Investigating Issues

**Dimmitri:** What about the shopping cart issue?

**Sel:** The assistant can help us investigate. Let’s trigger an investigation for that.

### Conclusion of the Demo

As the investigation runs, we can see the assistant actively monitoring systems and logging activities. 

**Dimmitri:** Can we create a dashboard from this investigation?

**Sel:** Yes, it will use the context we’ve discussed so far to create a relevant dashboard quickly.

### Feedback and Future Plans

We love feedback! The changes we've made to the assistant and investigations have come from user input. Please provide as much feedback as you can.

**Sel:** To wrap up, Grafana Assistant is now generally available for $20 per user per month. If the assistant saves you just one hour, it’s worth the cost.

**Dimmitri:** Absolutely. Plus, investigations will be in public preview soon.

Thank you all for joining us today!

## Raw YouTube Transcript

All right. Hello everyone. Uh I'm Sel. >> I'm Dimmitri. >> So today we're going to talk about how graphana lab is uh reshaping obsibility using aentki. U so before we start I think you know there's uh you know it's last session so I want to make sure everyone is listening. Uh so quick show of hands who is using AI uh you know whether it's like coding assistant chat GPT at work right? Yeah. So, pretty much everyone >> everyone >> that's super cool. All right. Um, let me get a clicker. All right. Definitively AI is, you know, reshaping our industry. Um, you know, the time it takes to uh an idea, you know, to take an idea to production has been shortened, you know, extremely. It's kind of crazy. We actually deploy way more than before. And uh and you know deploying more than before means probably breaking more than ever before. And this is why we actually build the graphana assistant. We want to make sure that it also match the speed at which you are uh you know developing those application. Um and the graphana assistant is basically you know here everywhere you are in graphana to help you you know create dashboards uh investigate an issue, create an alert or just learn something new that you didn't know before. So it works you know for on boarding but not just you know on boarding also if you're a pro you can just you know um use some rules or MCP servers to get the maximum value out of it. So this is this is the graph of the contribution within the graph assistant repo. Um I'm I'm I'm bragging here definitively. Uh but this is this is the result of a stellar class world class uh team that we have at Graphan working on this but it's also a show of like the commitments that we're making to this uh uh product and we're we're kind of like working as a startup within a startup. We're definitively iterating super fast uh and we're using a lot of AI to be fair. We are actually inspired by a lot of tools that we are using um things that you know what's interesting is that we release on average you know every four working day. So things that were not working before like something you were probably annoyed that wasn't working in the assistant last week might actually be working next week. Uh so you know we really want to make sure that you you you keep using it with us uh and and and you know provide feedback. So I want to I want to do a quick uh run of like uh features that we've been adding uh since the launch like since six months ago. So you've seen the graph before. It was like six months of work uh and it's been a lot. Um so first I'm going to start with MCP. So uh sorry roles. Um, so rules are like the great way of like customizing your experience. Um, you can set some best practices, special behavior. You can tune down maybe the amount of uh uh uh text that it it returns. Um, you can provide extract knowledge because maybe some companies have like pecular way of uh instrumenting the application or maybe they use a special label in their cluster kubernetes cluster. So that this is a great way to do that and we we've listened uh to the feedback and that's how we added that's why we added uh rules. MCP is another one that allows you to you know connect to external uh services like GitHub maybe it could be linear if you if you are doing something suddenly you want to you want to maybe create an issue if the assistant is telling you well there's a gap in your metrics maybe you should have a you should have a new metric there it's going to help you to basically uh you know create an issue directly from there data sources is is another one you know we're we're like big tent so there's a lot of data source that we want to add uh it takes time to add each data source because you know we need to engineer around the format that it returns. So we started with profiling and tracing because those two are quite niche. We realize there's a lot of people that don't necessarily know how to read profile or to write uh traql queries. So we want to make sure that you can leverage those that you are sending to graphana cloud and then we also added SQL. So SQL actually support many different SQL database not just like one. So I think I'm thinking about clickcast, MySQL, Postgress. I think we have also Bigtable and Microsoft MSSQL. So it's kind of it's kind of nice to be able to leverage those tools uh within within the platform. >> Integration is also like a big one that we've been working a lot. We actually build a wall SDK internally so that each team can just contribute to the assistant and add more integration like asset did. We've seen it today. Uh there's a lot of integration within the drill down apps 2 uh and Ksix and obviously we're not going to stop there. We're going to keep adding new things. And just to you know this is definitively not not over like we added many things. I'm not going to show it now. We're going to do a long demo. So we're going to talk about this after. And you know the reception so far has been amazing. Like the amount of feedback that we're getting and the amount of users that we're getting is is awesome. But you know this is not this is not the end. Um I think I think we we're not going to stop that. Right? >> Absolutely not. >> Right. >> So in addition to that we are we're doing more our vision is to go from just providing you with co-pilots to providing experience that feels more like having more teammates on your team. So for example earlier today we talked about this idea of running multiple assistants having them be autonomous and having them attack the problem from different angles. This is what good teams already do during incidents and so we want to provide more of these use cases and so earlier we announced Gurafana assistant investigations but we're going to do more and now let's go into the demo shall we? Yeah. Can we switch to the All right, let's get started. So, this is the assistant home on page. Um, you can access it uh from the nav menu here if I can click. Right. So, it's right here. Uh, and you can open the assistant uh using this uh button here. All right. So, the assistant is like always always there to help you on every page and he knows about the page you are uh and he has the context of uh where you are. So, we're going to quickly start uh the first prompt. describe my infrastructure and provide me a diagram using metrics. >> So it's going to use tools to look at uh what I have in my uh infrastructure notably like metrics. It's going to search metric. It's basically doing like what everyone will be doing searching for metrics and then quering metrics. >> Can you maybe show us how rules work? >> Yeah. So this is uh going to take some time. So, we're going to look at rules. Uh, you can access the rules from the menu here at the top. Uh, so let me show you the rules. So, we have a bunch of rules here. What's What's your favorite rules, Da? >> Probably run books. Um, for our team, we just put everything we had on GitHub in one of these rules. Although, to be honest, the infrastructure memory sometimes generates even better stuff. And knowledge graph, if you have that, then it it it gets really really good, >> right? And those rules works for everybody. So you can set rules just for yourself, you know, because you like to have like a different tone, but you can also set it for all your team if you have like best practices and uh you you can create as many rules as you want, right? So it's done like looking at uh our metrics and it gave me a diagram. So I can open it in big. So this is like a lot of like microservices. Whoops. Right. So we have a lot of microservices. shows me a nice diagram and what's what's what's nice is that it gives me also a quick description that I can look at. So you know 20 services are running gave me how many AWS regent I'm using uh and if I like I can just go and share that to my if I like I can go and just share this with uh you know my my colleague. So I can just generate link and you know share that link to everyone and it's going to go as a full page uh view and I can share that with everyone in the company. Right, let's go back. >> What are those suggestions at the bottom? >> Right. So, interestingly, because I ask about the infrastructure and every time, you know, based on the context, it's going to generate some follow-ups or links within Graphana that are nice to go and check. Um, so for instance is is suggesting me. So, someone created a system overview dashboard because I asked about, you know, uh the infrastructure. So I can just right from here click and look at the dashboard that someone created here right um and it's also like suggesting us another follow-up like comparing uh the check out CP as you can see this the checkout services seems to like a problem with latency so I can just click on it and then off you go and again it's just going to use tools to answer the question about you know how is the CPU doing for the checkout services >> maybe we could show MCP servers Right. So, MCP servers. So, as you can see, I have a couple of MCP servers. From there, I can click on it. Uh, and I can see the MCP servers that I'm using. So, I've set up GitHub. So, we have like pull request and repository, but you can add as many uh, you know, MCP you want. Uh, and Matt said today if you want to create your own MCP server, it's also fine. And then that that's going to allow you to connect it to maybe your Kubernetes cluster or another services that you're using internally. So um already we can see the graph there's one region that is a bit outlier. There's like more uh more CPU usage. It's not that much to be fair. Uh but still something is going on and the assistant here is like telling us well there's one that is like significantly significantly higher. So I could go on and just like check maybe the the request way to verify like if one region is maybe receiving more but I'm just going to go and verify the CPU because you know I was asking about CPU is suggesting me to look at profiling. So I go in profiling and to be fair I don't know how to read profile this is definitively like a new signal for me and I think this is for you too dima. >> Yeah I usually go to the profiling team if I need you know help with that. All right. Didn't you didn't you just co-ounded Pop Bioscope? >> Oh, maybe. >> Okay. So, um as I was saying like we built a SDK and every team is basically uh you know adding integration to send you to the assistant when you need help and that's that's what we're seeing here like with this analyze flame help button. So I'm going to click on it. Um so we have it here. We have it also for tracing. We have it for alerts data source connection issue. you can we you'll see more coming up and you know we're going to keep adding those integration. So what I just did basically is ask the assistant to look at that profile for me and tell me what's going on. >> Can you tell us more about those uh rectangles is it context? >> Yeah. So the context here so there's like you should think about like this is the same as if you're using cloud desktop or another uh another LLM tool. Uh more context the better. So those pill here are the most used data source. So every time you ask something like you know if you select the data source then it's going to go faster. Uh but you can also like you know create context using the at uh mention and then you can select the the type of context you want. If you don't like keyboards uh you can also just use the menu here right so the assistant came back and it's telling us uh as you can read here that the regx compile seems to be the biggest uh problem here and compiling rig is like a classic go issue. Uh, oh yeah, >> you should not do that in the odd path. We probably have an issue here. >> Um, so I think we're just going to ask the assistant, right? I have MCP. We should give >> Yeah. Can I can it just fix it somehow? >> Yeah. So create a PR to fix this issue and give us a link to the to the PR. Right. So now is is going to is going to try to use MCP to figure it out what's going on. uh you know um all right what's going on >> where are those orange buttons >> oh >> yeah there's a warning here so yeah MCP can be destructive right we don't know like maybe depending on where it's going to connect to so what we do is we actually have like you know permission that you want to allow this this one or not I'm just going to yolo and just accept everything is like vibe uh troubleshooting I think Raj coined that one by the way um so it's gonna so now he's looking at my code. So I have a rules to tell it where is my code. So he knows about you know what which repository to use and all of this. Now he's creating a branch uh and then next he's definitely going to try to uh fix this issue. >> It's probably time to go for a coffee. >> Yeah. Do you do you drink coffee? >> I Yeah, I'm French. Yeah. >> Yeah. Do you I'm more of a matcha guy myself these days. >> Matcha crazy. Um in the meantime while it's doing that um if you want to find your previous conversation you can go go in there and just you know find conversation. So I was looking for conversation earlier about how to collect log log Kubernetes logs and as you can see it gave me like some sort of like uh snippets that explain um how to set up your alloy uh to to send those events to graphite cloud. All right let me go back if it did it that would be awesome. All right. Right. We have a PR. >> Woohoo. >> Woo. That's pretty great. Um, I think we should give a shout out to the assistant. >> Well, let's check to make sure it's legit. >> Let's check. You're right. It might it might actually be an hallucination. So, let's let's go for it. >> Maybe you just made a rule to, you know, always give you >> All right. Um, so first he gave me like a PR with such a good description. Like for sure if Matt is looking at this PR, it's going to say you didn't write that and I didn't. >> Yeah. Uh what do you think about the fix though? Let's have a look. >> Let's see. Yeah, this is a classic go issue. Instead of initializing these regular expressions once at the in it time, it looks like it it's doing it in a loop and so it's probably doing it like millions of times and that's probably what's using all the CPU and giving us that chart that we saw earlier. >> Yeah, we should measure it. >> Oh yeah. >> Yeah. So this is probably going to give us like a big uh uh gain in performance and also save cost and this is quite insane right like from in couple of minute with you guys I was able to fix uh you know an issue based on profiling um if I go back to the uh dashboard the overview dashboard here I think there was another issue dema >> oh yeah the shopping cart looks red >> looks red >> I don't know what that means though I I I'm not very familiar with this with this environment I'm new on this team >> yeah I don't know I don't know either And I want to show you how the assistant can actually take a screenshot. So that's the that's that's super fun. This is a new addition. So when you ask about what you see visually to the assistant, sometimes it's actually going to get the dashboard, but also you can just like take a screenshot and try to infer what's going on. Um, so this time I'm going to type. So what is this uh red panel that I see here? So again, it's always the same. It's going to use some tools. This time, this is a a visual tool to get a screenshot and it's going to tell us what's what's that panel about. >> Can maybe talk about the full screen view. >> Yes, that's a good idea. So uh in the menu again, you have like this uh full full uh page view. When you click on it, you actually get the chat, but as a full view. I really like this because I use it like you can actually resize the window and have like just the assistant next to your cursor or on another uh on another page somewhere >> maybe on a phone >> on your phone. It's pretty cool on the phone. So you can you have like a full page and you can just ask question directly there. >> So it's telling us the red dashboard is about like a a shopping cart SLO going on. You know we should probably uh take a look at it. So we're going to use an investigation for that. >> Oh yeah, let's do it. >> Right. So investigate this issue for us. So I'm going to click on the dependence investigation just to trigger it uh in in the background. Um something that I think I don't think we've been clear about but everything that has happened in the conversation so far has been added to the investigation. So he knows about uh the SLO. So that's why I wasn't really prescriptive about what I'm trying to investigate because it's already part of the conversation and that's already inside the context. >> Yeah. It also knows which page it's on. >> Yeah. So, this is running in the background. I can go and do something else or I can just go and check out what's going on. >> I could go for coffee again. >> I'll go for a coffee again. But you're going to be excited after. >> Going to be very jittery. >> Yeah. Do you want to do you want to tell us about what's going on this page? Dimma, let me make it bigger. >> Yeah. So, at the top we have the description of the investigation, a name, there's a date, we also have this confidence number. And that one is pretty interesting. And as the investigation progresses, it will the number will likely increase. Once the agents find evidence that supports some sort of a theory, um it will go up. If they didn't find anything interesting, the confidence number will stay low. And this is kind of a good way to tell if this investigation is even worth looking into. And so below that, we have agent activity. And this shows you in real time how the agents are exploring the problem. So right now we see loi specialists and they're looking into shopping cart error logs. Okay, that makes sense. >> Huh? >> It's looking at radius log too. >> Oh yeah. Yeah. And there's a lot of Prometheus ones, right? And they're doing all of this in parallel. And as they are finding evidence, they are starting more and more of these agents so that they kind of follow up on the previous findings. >> Yeah. Um actually I wanted to tell a story. Yesterday I was uh talking with Tom and then suddenly his ven was next to us and he got a page. >> Uh and I'm not supposed to be actually showing that but in we have a culture of like asking for forgiveness, you know, doing and then asking for forgiveness. So Tom, I'm sorry. >> I'm going to show a real investigation. So this is a real investigation that happened yesterday as Zven actually got the page you know 7 minutes after which the page I think triggered like 5 minutes after. So like almost instantly he got a report and was able to figure it out that this was just a flake. As you can see here it's talking about like false negative. So I found that like super uh helpful as you are you know receiving a page going into the incident and already getting some sort of report. Um I think this is super super useful. >> Yeah we use this a lot. I use this in bed a lot. you know, I get alerts. I can just check them. I don't have to go to my computer or anything. It's it's great. >> Um, can we maybe talk about the tree view? Um, dimmer. >> Yeah. So, the tree view shows you all the different hypotheses and it it's hierarchical. So, you know, if one follows from another, it, you know, there's going to be a tree and it will highlight the theories that are, you know, that ended up being right and it will dim the ones that are dead ends. And so, that's a nice way to kind of see which Yeah. What what's working, what's not. >> Yeah, let's look at maybe one that was done yesterday. Um, and we haven't we haven't talked about this yet, but basically this one was triggered by an alert. As you can see here, the source is a link which I'm going to click on it and it's going to take me to an alert page. Um, so this is like trigger automatically. So far we've been showing how we trigger it manually, but you can also uh, you know, we're using web hooks automatically trigger that um, uh, investigation. All right. Um, so this one is done. What's the what's the assistant here? What's the what's the great dashboard? >> Yeah, you can create a dashboard. That's very useful. you know, next time you have the same incident, you will already have a dashboard. Assistant will be able to use it as well. It's Yeah, that's that's a good one. Let's do that. >> Right. So, we're going to create a dashboard from this. Uh, and it's going to again use the context from the investigation. So, that's going to be way faster than what you you've seen at the lab because it doesn't need to search that much data. It already knows about all the metrics involved in that investigation. Uh, and so we're going to we're going to create that dashboard. We should talk about feedback. So, we have a a way to give feedback. We love feedback, >> right? >> Yeah. We all the changes we've had in assistant in investigations in the past months came from from the feedback. So, I encourage everyone to leave as much feedback as you can except for you, Jimmy. We we know your IP address. Stop sending us these mean emojis. Starts with 127, by the way. All right. Well, that's it. We have a dashboard uh ready to go. Actually, one shot that one. You know, sometimes it's going to make mistake. You just keep following following up telling, you know, you can see the dashboard, remember. So, you can ask more um you know, to refine that dashboard. Um I think that's it for the for the demo. >> Yeah, sounds good. >> Yeah. >> Do you have anything else? >> I don't know. >> Yeah, we have a >> We still have some time. Oh, wait. Oh, what's that? Oh, serial. I think there's there's something happening with the assistant. >> Yeah, I can see it from here. It sounds like uh Zven is basically >> flag notification. >> Yeah, Zven is saying that there's something wrong. Yes, he's not available. He's on the is on the train. >> Yeah, I'll just I'll just ask it here. >> Yeah. Can you ask the assistant? >> Yeah. Yeah. Yeah. >> Right. This is the the Slack integration that uh we're unveiling today that allows you to have the same experience but in Slack with the assistant. So you can just you know talk to the assistant and it's going to use the same kind of tools give you a bit of the same uh uh answer and everyone can use it in the thread as you are investigating together and trying to find uh issues. Um so now is actually looking uh at the assistant name space and checking if all the pods are healthy. So we're going to wait for that. Hopefully everything is fine by the way. This is like inception. The assistant is investig investigating his own uh infrastructure, right? Everything is fine. This is good. Um let me let me verify something. Do we have We're not going to send a message to Tom now. Um do we have a dashboard for the assistant? Yeah, I'm making mistake, but it's fine. You know, he can even he can even understand French. I should have I should have asked in French by the way. Searching dashboard. So, it's going to give me links about the dashboard that are available for my team that I can use. This is great. Uh but the cool thing is we can also ask to show me that dashboard and it uses all the context from this thread. So, if there's already a conversation and people already found some, you know, leads, it will use them as well. >> Yeah. Right. as you are as you are like discussing in the thread and talking about leads all of this goes into the context and the assistant will be able to uh you know take them into account and investigate. So this is a great integration um yeah you know to work together without actually leaving uh Slack when you have an incident and when it matters to be able to react faster. Right. This one takes a bit of time. Here we go. We have dashboard. We can look at it. Really cool. >> All right. >> All right. I think that's >> I think that's it. Yeah, we should go back to the slide. >> All right. All right. And so our big vision is to build this agentic observability platform where you don't just monitor your services, but you can understand, predict what's going to happen and act on issues proactively and um autonomously. That's why we're building all these autonomous agents, not just one, but a lot of them that will help you with all of your observability needs. These are the just some of the things that are coming in the assistant in the near future. Couple of my favorite ones is things like automatically find blind spots in your observability setup. Um we already do that in in the investigations but uh we could do more. You know it could suggest a new alloy config that you could apply and start getting logs from you know services where you don't have logs. Um the other one is um deeper integration with your codebase. Uh we have the GitHub MCP server but you know we want to do more on that so that you know it runs faster and um gives you better results. Customers tell us that they already see, you know, a lot of value from this and it's changing the way they work and they're very excited about this vision. And Grafana Assistant is generally available. We announced that earlier today and it's going to cost $20 per user per month. Just to be clear, it's active users and >> Yeah. Yeah. I wanted I wanted to say something on the $20 per month. Um, if you think about it, um, you know, like if if the assistant saves you like just one hour, it's probably already uh, you know, um, probably already good enough for the $20 because I don't know a single SR that costs $20, you know, an hour. >> Yeah, totally makes sense. Um, and and investigations are going u public preview, so please check that out.

