# Grafana Tempo 2025-02-13

Join our next Tempo community call: ...

Published on 2025-02-13T20:17:07Z

URL: https://www.youtube.com/watch?v=udHm3CH6_K8

Transcript: all right welcome to the tempo Community call February 2025 the years keep on coming they do not stop sadly um today we are talking about Tempo 271 we got a decent agenda here I think we got some exciting announcements and some boring announcements uh as always feel free to ask questions whenever not a scripted um not a scripted anything I promise as much as it might seem that way uh so feel free to ask questions anytime unmute or type in the chat whatever you're comfortable with and we will get started and we'll always end with just kind of a general ama if you do not have questions about you know the current topics all right so uh I'll put the doc in the chat in case you don't have it there we go um Tempo 271 so this has been some or there's been some discussion I guess a couple issues couple discussions in GitHub about a change we made in 2.7 and we are considering cutting a 271 to kind of revert that um what we did was remove Network compression um so internally we found the savings on CPU and memory to be worth the removal of network compression but we're getting feedback from other folks that that's not working out for them this Edward Edgar fellow um gave us a lot of good information here showing uh showing the impact on their bill basically so it's easy to change back the config is there we put it in the release notes everything's there but I think uh it was pregnant of a surprising change and we are considering a 271 just to revert it because internally we can just set the setting to whatever we want um we do attempt to generally align defaults with you know kind of our experience in Cloud because we assume those defaults will scale and will work for most people and that's why we did this made this change uh but uh this one in particular I think is impacting more people than we realized and uh I think there's a strong chance of a 271 that undoes this essentially so Edgar posted uh what their uh what their bandwidth bill was what it changed to because of 27 obviously larger um I can probably zoom in on that uh and then they tried Snappy which is which is the default in 26 and then they tried zsv which um I don't know if that's ever been the default but that's just a more CPU heavy and uh better compression like tighter compression algorithm than snapping does anyone have an opinion I know that's kind of a broad question uh I don't know if any has anyone been impacted by this uh has anyone noticed uh if there's no strong feelings here I think at this point I'm kind of inclined to just cut the 271 to help folks to make it more visible we put it in the release notes um but the 271 would definitely bring some visibility to this and help people see what options were available and maybe bring uh I don't want people to get surprise bills basically because they're running Tempo all right Network saving compression cost savings us as well okay um yeah and I think those people are paying close attention we gave them all the information they needed but I totally respect some people are just like hel upgrade and oh hey 27 exists it'll be fine uh I I think uh it costs us you know a little bit of overhead of time to cut a 271 uh but it's not terrible and I think it's the right choice for the community um so I think we're gon to go ahead and do it uh and um we'll again uh put some good information there about what options are available I think we'll just go back to Snappy but uh we'll link this discussion here so people can make the choice about if they want a z standard uh as well that's also on the list of available compression options all right very cool uh okay that was somewhat boring news I told you some would be boring and some would be uh fun this next one far more fun so uh we have had this in tempo for six seven eight months uh grafana merged a PR to support Trace fuel metric streaming I believe the explore traces app also uses streaming uh but this allows you to get your uh metrics data in over time instead of uh you know waiting 30 seconds or whatever and then all at once getting one big payload of information so uh I love this because uh it just feels improves user experience immensely um you're instantly getting feed back on the data uh instead of spinning spinning you don't know what's happening you don't know if it's almost done or not done or nowhere close you not only uh get that data back uh continuously as it's running but you also have a strong sense of how hard this query is to run so you can see it slowly uh filling up or quickly and you it gives you good feedback on the weight of the query you wrote um so in this case what we're Counting about 400,000 spans a second it's a rate over 3 hours looks like yeah three hour rate um I think this is funny we didn't know this I I didn't know this until we put in metric streaming but you can see it fills in from the oldest Tim stamp to the newest and it's because of just the way we order the block list internally in the polling code um this actually just changed with a PR that will be in 28 that we're about to talk about where it'll order at the reverso so this particular uh video was uh taken when it went from oldest to newest but actually in Tempo 2.8 uh no no my bad yeah 2.8 and latest grafana you will see it fill in most recent to oldest data and you can get this really nice uh streaming experience on your metrics uh very excited about this as well as how it impacts explore traces I think it's going to be great for both um technically trace or metric strey exists in 27 but there's some nasty bugs in there um that we did not realize until we put this in grafana and so uh it really is not going to be fully functional until 28 but yeah pretty pretty psyched about that one cool oh hey yeah I would like to take a quick poll is is anyone using streaming for search that's been in there for a while and that feature's been out there I was wondering what has anyone like enabled that or used it Co um not sure what steps would be required in the graffo side but it would be nice if everything was kind of enabled by default yeah uh I don't think it's too bad there's some docks out there we've been able don't use it much yet cool um I think uh there's a dock that details it it's in temp or in graphon is just like a toggle on the data source uh in Tempo you have to do that http2 over HTTP thing um there is there is a nice doc somewhere out there that describes it um if somebody wants to find that uh that would probably pretty cool uh it's a simple setting a Boolean setting uh that's required for uh Tempo to work with Gana for the streaming cool back to boring news I I I think I did a good job here I went back and forth uh back to Borg news serverless and has been uh deprecated was deprecated in 267 was deprecated in 27 has been completely removed from the code based in 28 so heads up on that thank you to SJ Sid our Duke of deprecation um he has been working hard to kill all dead code and Tempo and I am very happy for it uh this particular feature we tried out it didn't really work for us it was more impactful back with the older search styles with paret we are finding it more faster just to uh use queriers in addition removing this is going to allow us to make one of two performance improvements we haven't decided which uh in the query path so the front end was generating uh when it was generating jobs had include a whole lot of extra information for cist to work that it no longer has to require and so we have some choices there about how to reduce that that should deliver you know a little bit better performance because we just not uh the the jobs are smaller the everything's easier and Timothy wford Wolford gets the Community member award for the day for finding the doc good work thank you appreciate that um finally back to fun news ordered results uh so this has been on ask forever I think since like the first couple months we had Tempo um uh for the results to be ordered the most recent results I suppose so Tempo just kind of creates thousands or hundreds or hundreds of thousands of jobs shoots them all to queriers gets results back and as soon as it hits your limit quits uh that's the default behavior and Will Remain the default behavior in 2.8 but we did add a query hint uh with most recent equals true that will force Tempo to exhaustively search the time range um in order to uh return the absolute most recent results in reverse order so I did a query before we started here here's a simple query from our Dev itself um if I do this the old way a resource service name pyroscope Gateway uh you can see uh just random results which is kind of the way Tempo behaves now uh with most recent it's going to require some extra resources because we have to um search a bit more exhaustively but now we have the absolute most recent the time range so you can see my time range up here was an hour uh on this particular day you know 1212 212 the first hour of the day and then you can see my results are in Reverse time order uh based on the trace start time so we can uh we can force Tempo to return to us the absolute most recent results for any query uh for a lot of queries this has almost no impact on latency uh we were really tempted to make it the default I really wanted it to be the default Behavior the problem is uh this query in particular empty no no conditions basically a no conditions query with most recent equals true is extremely expensive because it has to check there's you know basically every single trace within a within a Time range in order to confirm what the absolute most recent are it's not quite that intense but it is still significantly more intense this is actually the absolute fastest querium Tempo now and with most recent it becomes the slowest and most painful querium Tempo so that really surprising um shift is one of the reasons we didn't make it default I think we're still considering it uh it will likely require probably an iteration or two on the blocks themselves how we store data Maybe with some changes there we could make this default but we wanted to provide this as an option for people who um uh as an option for people who uh who wanted this feature so this will be in 28 but it's just a way to ask tempo for the absolute most recent results cool oh shoot did Edgar K show up we're just talking about your discussion Edgar really appreciate the work here uh we are going to cut a 271 because of your post and some other people bringing up a similar issue uh we're going to cut to 271 just to go back to Snappy but it'll bring light to this uh light to this kind of issue that you've highlighted we'll link to this discussion and that'll help people see the impact of trying the different uh comp options so thank you for that I appreciate it all right folks it's quarter after we did not or quarter till I suppose we did not eat up a lot of time but we are open for questions so uh it's kind of an open Forum here if you have any questions about operating Tempo features road map um we are here at your service so feel free to ask anything you want uh and we'll do our best to answer I got a question sure we noticed in a future 2.9 releaser including kofka as part of that process maybe you could talk a little bit about that kind of the decision making around that and uh yeah I think if Marty does not mind uh taking this one he's himself and Mario and hobby have been doing good lot of work here hey yeah um sorry I didn't get a good slide or something like that but Ju Just to talk generally um what we're kind of doing is aligning Tempo um to a different architecture that will allow us to fix some things that are kind of like fundamental so um I would say the biggest thing is right now to run it in production with high availability end durability you have to run it with replication factor three which means the Distributors write the data to the inors three times the inors flush independently we kind of allow the compactor to clean that up uh by duping blocks um but the problem is that's not very effective right so it's eventually consistent it's best effort um what bringing Kafka is doing is allowing us to move the replication Factor just to the que of data that's coming into the distributor and then the other components Downstream um can read that uh just one copy so for instance the the inors um won't have to flush three copies of the data to the back end anymore we can actually just read from the queue deterministically flush one copy um so in that sense it's really good for TCO it'll be really good for reliability because if there's any failures like data is not discarded we can Replay that data um so it's kind of like a fairly large change um I I definitely were aware of the operational complexity overhead um but I think that savings will be worth it right uh did you have any other or questions in particular or things like that uh I'm not sure how much information we have out there before maybe if we scroll down in this community doc there's probably some links and slides down there sorry I didn't get anything prepared for this one no you're okay I realize it's in the future right so but I think we're actually really excited about it and it sounds like partially this has been implemented as part of mmir and Loki as well right so um yeah I think um for us you know we've been seeing a lot of those context errors um uh sorry cancel context in errors right that could be potentially uh making false metrics right so we're getting internal errors where there might not be internal errors right so um that was a big thing for us because our SLO is dependent on that you know so that that we're actually applauding it I'm more excited I'm kind of curious is this based on the scheduling of how you're doing releases would a 29 be in the fall of this year or would you think it would probably take longer than that I'm more Curious um so I'm not sure where that was written down as on the 29 road map I'd like to see that I think we're probably going to tie this to a 3.0 release um but I'm not sure um as far as timing um definitely like I think that's like in the fall is probably close on timing um but there I think there's a couple things that we need to figure out as far as a migration plan to make upgrading easy um for instance if you have a problem with the que maybe we want to turn that back we want to support the old architecture that doesn't have any operational overhead um so I think there's some things that we need to figure out there thank you yeah Co great so there should be a 28 this spring at 29 this summer and then 210 fall 3.0 fall I think that's probably the earliest we could start talking about 3.0 we really want to make sure we deliver a um solid product basically a solid open source project because it's going to take a lot uh we have to fix a lot of documentation we have to educate a lot of people about this new uh way to operate Tempo we have to rewrite the helm chart because it's going to have new components so it's a big lift and we want to make sure we do it right and we do believe strongly this is going to deliver um an overall cheaper faster Tempo uh with the additional operational complexity of some kind of Copa compatible thing so that is the cost for paying as operators okay maybe you do manage Kafka AWS maybe you pay any of the million Services maybe you run Kafka yourself um but we believe the cost of that will be massively offset by the improved performance as well as the lower TCO of tempo what's up rben oh regarding to the C CL this is going to be a heart dependency or there will be an alternative like for example an internal queue or something um so oil or what other the plan regarding to this I don't know if we really have the bandwidth to give the old architecture as well as the new are int right now is to completely swap over to the new there may be and this is not on the road map at the moment but there may be an opportunity where we essentially write our own queue on object storage uh that would be potentially six months or year after we even deliver this that's something we've talked about um but we really just want to deliver this uh and then maybe think about later of dropping that dependency with some kind of clever like bespoke CU that we wrote but I will say that's lower priority uh primary goal is to deliver the new architecture with all the benefits um and we know this is uh going to discourage some folks but we just believe it's the right path for the project and we need to make it okay thank you but we are currently running this in Dev and I think really coming over some stability humps this week and seeing a stable product or project that we're looking can move to what we call our operational or staging maybe for some people cluster in the next week or so not to put a time out on Marty's work here but uh right so we're we're operating it now and then right uh we will hope to get to bigger clusters soon and then honestly our internal goal is to start rolling this out to our production clusters this quarter uh pav what do you say yeah thank Joe um if there's interest from open Source Community would you um consider sponsoring that work externally to support the old architecture with the 3. [Music] zero um yeah yeah we have a number of features that this community has um that the community has support and put in that we don't run and we make it clear we don't run so uh we do our best to balance what we need with the community I think there's an opportunity here we should really discuss this though uh we would want to make sure however we did that it would lay cleanly next to what we want to do so I think if uh the community or red hat specifically really really wanted this older rf3 and Jester style architecture what I will tell you is certain features and Tempo will always be a little wonky like Trace ql metrics um we'll we we're not calling that GA specifically because we need to go to rf1 to call out so it would be a it would be I think we would consider it like uh an unsupported way to run Tempo but I think we could probably work something out we should probably have a design doc maybe have a syn meeting like that would be a big enough lift we would want a little bit more than just in the past has done great work just through PRS but this would I think take a bit more I understand thank you very much y I'm on call and just got paged but it's fine acknowledge uh yeah anything else team any other concerns I I recognize this is a big shift I really appreciate all feedback uh positive and negative um uh but I do think overall it's going to be the right the right path forward and we're all going to enjoy it cool questions concerns um personal stories I know you all just want to talk about anything cool all right I want everyone to take care uh thanks for coming it will in a month we'll have every every one of these Community calls will be more updates down this path where we are uh what it's looking like um is there going to be a 29 or a 210 we don't know when do we cut the three so stay tuned as we continue to provide more details I'd love I'm gonna ask Marty's team to do this I'd love to hear um maybe some good benchmarks like TCO savings or cost savings at different components so we can kind of show why why we're interested in this um not for next month but just generally those kind of updates I think would really help uh everyone kind of understand why we're going this Direction all right thank you all for your time and we will see you in a month

