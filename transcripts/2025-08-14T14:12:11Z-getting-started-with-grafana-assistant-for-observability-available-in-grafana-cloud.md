# Getting Started with Grafana Assistant for Observability | Available in Grafana Cloud

Published on 2025-08-14T14:12:11Z

## Description

The pace of software delivery in 2025 is unprecedented — cloud-native apps, microservices, and AI-generated code are shipping ...

URL: https://www.youtube.com/watch?v=UtZkFYUmjrM

## Summary

In this video, the speaker discusses the rapid evolution of software development in 2025, emphasizing the emergence of "vibe coding" and the increasing reliance on large language models (LLMs), particularly one referred to as Claude, for generating code. The focus is on utilizing the new Grafana assistant to enhance observability for a Python API that publishes events to Kafka and stores data in a Postgres database. The speaker demonstrates how the assistant can quickly create dashboards and alerts based on real-time data without the need for extensive coding knowledge. Key features highlighted include optimizing telemetry configuration to reduce unnecessary metrics and costs, showcasing the potential productivity boost for both seasoned and new Grafana users. The video concludes by encouraging viewers to explore the Grafana assistant, which is currently in public preview, and hints at the term "vibe ops" possibly becoming more mainstream.

## Chapters

Sure! Here are the key moments from the livestream along with their timestamps:

00:00:00 Introduction to the changing landscape of coding in 2025  
00:01:30 Rise of LLMs and rapid software development  
00:02:45 Overview of the Grafana tool and its capabilities  
00:03:50 Explanation of the setup: Python API, Kafka, and Postgres  
00:05:20 Introduction of Grafana Alloy for monitoring  
00:06:15 Using the assistant to create a dashboard for Kafka and Postgres metrics  
00:08:10 Dashboard generation and context-aware capabilities of the assistant  
00:09:35 Creating alerts for Kafka consumer lag and Postgres query latency  
00:11:00 Optimizing telemetry data with Grafana AI Assistant  
00:12:30 Final thoughts on the benefits of using the Grafana assistant  

Feel free to let me know if you need any more information!

# The Changing Landscape of Software Development

Okay, so life as we know it has changed. As we dive into 2025, we've been fully submerged into the deep waters of vibe coding, and we're now shipping software at an incredibly rapid pace. It's super exciting, but also terrifying. With this pace of innovation, a new reality is emerging: a lot of code isn't even written by humans anymore; it's generated by large language models (LLMs), often by a French guy named Claude.

Services that used to take months or even quarters to prototype and deliver to users are now being rolled out in just a few weeks, and sometimes even days. However, despite the rise of vibe coding, one aspect remains constant: we must maintain our code. We need to know when it breaks and why it breaks. If you’re watching this video, it’s likely that you or your team are hoping for an easier way to manage this.

I understand your skepticism; let's be honest—every SaaS product claims to offer the next groundbreaking AI solution. In this video, I’m going to do the exact opposite. I won’t hype anything up; instead, I’ll show you the new **Grafana** and how it can help you achieve real observability coverage for a new service much faster. This will give you a bit more of that vibe coding energy when it comes to operations. Let’s get into it.

## Setting Up the Environment

Alright, so what are we working with? We have a Python API that simulates a service with the endpoint `/produce`. It publishes events to a Kafka broker and also writes them to a Postgres database. All of this is wrapped in a Docker container and hosted in Kubernetes. It’s all open source and doesn’t rely on any cloud services.

We’ve also added Prometheus exporter and Kafka exporter containers so we can monitor them in Grafana. Additionally, we’re running a load test through **K6** to generate traffic. We’re essentially piping those metrics from the load test into Grafana Cloud using **Grafana Alloy**. This is an open-source, open telemetry, and Prometheus-compatible collector, which will be used for our pipeline generation to scrape those metrics and send them into Grafana Cloud to our managed databases.

At this point, everything is up and running, and we can see that in Grafana Cloud. Now it’s time for some fun! It’s time to engage with the essence of vibe coding or vibe ops in this case. We’re going to use the assistant to help us onboard this new service.

## Creating Dashboards with Grafana

First up, I’m going to ask it to create a dashboard for my Kafka and Postgres setup. The prompt I’ll be using is: “Can you generate a dashboard showing key Kafka and Postgres metrics for correlation of my services?” 

As this work is being done in the background, you’ll notice on the right-hand side that the assistant is context-aware. It already knows what data is available because it’s scoped to your Grafana Cloud environment. It’s not guessing; it’s building with live data sources and the data flowing into them, such as logs in Loki, metrics in Mimir/Prometheus, Tempo for traces, and Pyroscope for profiles. It will also reference our documentation, so if we have any best practice questions or need to integrate with other features of Grafana Cloud, we can easily ask.

You’ll see that the assistant starts doing the work in the background, navigating us to the dashboard and picking the best visualization types for the data we want to display. 

Right on! The assistant just scaffolded a dashboard for me with panels for broker throughput, consumer lag, connections, locks, query times—everything I really care about—without me having to dig through a dozen metric names or write the queries myself.

## Setting Up Alerts

Now that we’ve got full visibility of these two services, the next thing I want to ensure is that I know when things go sideways. We need alerts. Dashboards are great, but I don’t want to write PromQL queries right now. So, I’m going to ask the assistant for help again. My prompt this time is: “Can you create alerts for Kafka consumer lag and Postgres query latency?”

This targeted prompt is where the assistant really shines. It provides context on why the thresholds it sets actually matter. Instead of just saying, “set this to 200 milliseconds,” it explains what normal looks like for that metric and what a spike might indicate.

So, I have a dashboard and alerts created for monitoring my stack, which is awesome! However, now I need to clean up the telemetry that I’ve been sending in. We’re sending quite a bit of data, and not all of it is necessary. This is one of the biggest challenges with telemetry—exporters from Prometheus and OpenTelemetry can often flood you with too much data, and you have to decide what to keep, what to aggregate, and what to send to object storage.

I’m going to feed my current configuration into the Grafana AI Assistant and ask it to optimize based on best practices. My prompt will be: “Given my Alloy config, optimize it to remove the non-critical Kafka and Postgres metrics.”

The assistant will help optimize the config by filtering out noncritical metrics while keeping the essential ones we just prompted for to create our dashboards and alerts. It will search over Prometheus metrics stored like Kafka and Postgres, identifying high cardinality metrics that are problematic for both cost and storage. Based on that analysis, it will provide a proper config that I can copy and use to update my configuration in my IDE.

The assistant will also give me key optimizations made, detailing what it’s removing, what it’s keeping, and what it’s dropping. This is crucial because you’ll want to review these configs carefully. You never want to accept something at face value without understanding what’s going on. It’s noteworthy that this can reduce ingestion costs by 60 to 70% in both Kafka and Postgres metric volume, which can lead to massive savings as you scale. 

## Conclusion

To wrap things up, the Grafana assistant is pretty sweet and has personally saved me a ton of hours. For long-time Grafana users, it will rapidly increase your productivity. If you’re new to Grafana, the assistant will have you feeling like an expert in no time. That’s vibe ops!

And if that term ends up in a conference talk, just make sure to add Grafana in the footnotes. I don’t know if it’s going to catch on, but go check it out. The assistant is now in public preview, and we look forward to seeing how it saves you time. 

Take it easy!

## Raw YouTube Transcript

Okay, so life as we know it has changed. 2025 has fully submerged us into the deep waters of vibe coding and we're now shipping software at such a
rapid pace. It's super exciting, but also terrifying. And with that pace of innovation is
coming a new reality that a lot of code isn't even written by us humans anymore. It's written by these LLM's and
usually a French guy named Claude. The services that used to take us months
or quarters to prototype and bring to users are now taking just a few
weeks in, sometimes even days. But even with the rise of vibe coding,
one aspect is still remaining constant. We got to maintain it. We have to
know when it breaks and why it breaks. And if you're watching this video that
someone is probably you or your team, and you're maybe hoping there's
finally an easier way, however, I understand you're probably
skeptical because let's be honest, every SaaS product right now claims to
have the next groundbreaking AI solution. So in this video, I'm going
to do the exact opposite. I'm not going to hype anything. I'm just going to show you the new Grafana
assistant and how it can help you get real observability coverage for a new
service a heck of a lot faster and give you a little more of that vibe coding
energy when it comes to operations. Let's get into it. Alright, so
let's see what we're working with. We've essentially got a Python, API that simulates a service with
the endpoint /produce it's publishing events to a Kafka broker and also
writes them to a Postgres database. All of this is then being wrapped
in a Docker container and hosted in Kubernetes. It's all open source. It's not using any cloud
services or anything like that. We've also added Prometheus
exporter and Kafka exporter containers, so we can
monitor them in Grafana. Now we're also running a load test through
K6 because we need a way to generate traffic, and we're essentially piping those metrics
from the load test into Grafana Cloud using Grafana Alloy.
This is an open source, open telemetry and Prometheus
compatible collector. So this is sort of going to be used for
our pipeline generation that's going to scrape those metrics and then send
them into Grafana Cloud to our managed databases. Now at this point, everything is up and running and
we can see that in Grafana Cloud. So now it's time for fun. It's time for actually giving into the
vibes of what Vibe Coding is or vibe ops in this case. And we're going to use the assistant
to help us onboard this new service. So first up, I'm going to ask it to create a dashboard
for my Kafka and Postgres setup. The prompt I'm going to be using is can
you generate a dashboard showing key Kafka and Postgres metrics for
correlation of my services? Now while this work is being
done in the background, you'll see on the right hand side with
the assistant that this is all context aware. It already knows what data is
available because it's scoped to your Grafana cloud stack or the Grafana
Cloud environment. So it's not guessing, it's building with the live datasources
and the live data flowing into them. So things like logs in Loki
metrics in Mimir/Prometheus, Tempo for traces, Pyroscope for profiles. And then it's also going
to look at our docs. So in case we have any best practice
questions or how to integrate with some of the other features of Grafana Cloud, we can easily ask that and we're going
to go through some of that process here. But you'll see that this is going to
start doing the work in the background. It's going to navigate
us to the dashboard, and then it's going to essentially pick
the best visualization types for the data we're looking to actually show.
And we'll see that here. Right on, so the assistant just scaffolded out
a dashboard for me with the panels for broker throughput, consumer
lag connections, locks, query times, all the stuff I really care about
without me having to dig through a dozen metric names or write the queries myself
and be an expert in promQL. So now that we've got full visibility
of these two services, the next thing I want to make sure
is I know when things go sideways, we need alerts. Dashboards are great, but I don't want to handle
promQL writing right now. So I'm going to ask the
assistant to help out. Again, prompt I'm going to use here is can you
create alerts for Kafka consumer lag and Postgres query latency? This
is a more targeted prompt, but this is where the assistant
really goes above and beyond, and it gives me context on why the
thresholds it sets actually matter. So it doesn't just say, set
this to 200 milliseconds. It explains what normal actually looks
like for that metric and what a spike actually might mean. Okay, so I got a dashboard and an alert
created for monitoring my stack, which is awesome, but now I need to do some cleanup of
the telemetry that I've been sending in. You can see here that we're
sending quite a bit in, and it doesn't necessarily
mean we need all of it. So that's one of the biggest things with
telemetry as a whole is these exporters from Prometheus and Open Telemetry can
oftentimes throw the kitchen sink at you and you have to kind of make the
decision of what to get rid of, what to aggregate, what to keep, or
just send to object storage and whatnot. And these are all questions
that weigh on you and your team. So I am going to actually feed
my current configuration into the Grafana AI Assistant and ask it
based on best practices to only keep what's crucial. My prompt's going to be given my Alloy
config optimize based on best practices to remove the non-critical
Kafka and Postgres metrics. What's awesome here is you can see that
it's going to help optimize the config, but it's going to filter
out the noncritical metrics
while keeping the essential ones that we just prompted for to create
our dashboards and alerts. So you can see that this is searching over Prometheus
metrics that are stored like Kafka, PG and Postgres, and then it's going to look at the high
cardinality metrics that are really problematic for both cost and
storage. And then it's going to, based on that analysis, give me a proper config that I can
copy and actually go back into my configuration in my IDE change
that and make the proper updates. You can see that it's going to give
me the key optimizations made as well. So it's going to tell me exactly
what it's getting rid of, what it's keeping and what it's dropping, which is super crucial because you're
going to want to walk over these configs anyways, right? You never want to
just go with something at face value. You're going to want to
understand what's going on, but it says that this can reduce
ingestion costs by 60 to 70% in both Kafka and Postgres Metric volume.
This is huge because you scale that out, it's going to be massive savings. So the assistant has been really handy
in this regard. So to wrap things up, the Grafana assistant is pretty sweet
and has personally saved me a ton of hours. Now, if you've been
a long time Grafana user, it's going to rapidly increase
your productivity, and
if you're new to Grafana, the assistant is really going to have
you feeling like an expert in very short time. Yeah, that's vibeops. And if that term ends
up in a conference talk, just make sure to add
Grafana in the footnotes. I don't know if it's going to
catch on, but go check it out. Now, the assistant is now in public preview
and we really look forward to seeing how this saves you time. Take it easy.

