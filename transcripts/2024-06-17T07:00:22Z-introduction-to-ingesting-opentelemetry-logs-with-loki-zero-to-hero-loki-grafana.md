# Introduction to Ingesting OpenTelemetry Logs with Loki | Zero to Hero: Loki | Grafana

Have you just discovered Grafana Loki and are planning to use OpenTelemetry as your instrumentation tool of choice? Or looking ...

Published on 2024-06-17T07:00:22Z

URL: https://www.youtube.com/watch?v=snXhe1fDDa8

Transcript: open Telemetry what exactly is it and how does it relate to logs and Loki my name is Jay Clifford and in this episode of Zero to Hero we're going to break down what exactly is this open standard and hopefully clear up some ambiguity around orchestrating your code The otel Collector and of course how to write these logs natively into Loki [Music] let's start with the fundamental question what is open Telemetry well at its root it is a standard and a standard's role within any industry is an attempt to unify a subject matter to create a single source of Truth within its execution and components for example let's consider an industry that has lots of standards Healthcare the international classification of diseases or ICD for short is one of these standards and it is globally recognized for diagnosing and classifying diseases ICD provides a Common Language if you will which promotes interoperability between components and actors essentially if someone discovers a new illness in the UK and reports it using the ICD standard then medical profession Professionals in other countries who follow this standard will understand what the new illness is and its qualities like IC open Telemetry attempts to standardize communication within the observability demain open Telemetry or oel for short attempts to standardize the instrumentation generation collection and exportation of telemetry data such as traces metrics profiles and logs this promotes the idea of a vendor neutral standard which gives Engineers the power to swap and replace their observability platform with another solution without rewriting large amounts of underlining instrumentation or combine different vendor components into one solution since they all follow the same standard now you might be asking at this point this sounds great but how how do I Implement a standard into my observability architecture luckily open Telemetry is more than just a standard it provides a set of apis language sdks and an agent which implements this standard providing a FastTrack solution to achieving it so how does this relate to logs if you recall the episode on the structure of logs one of the inherent challenges is the variety of components that that can make up a log entry and the many ways that it can be formatted otel attempts to standardize this with a consistent set of attributes and formatting that all being said some of these attributes are still optional here is a highlevel overview of the log entry typically these log components are compressed for transportation across the network when unpacked however these log entries usually appear in ad Json structure format incorporating the components you see in this example why don't we see this in practice by revisiting our carnivorous Greenhouse application and integrating the open Telemetry SDK to generate otel logs I will be using kiler codo once again but you can find both the sandbox and the repository Linked In the description below let's start by setting up our python development environment first we'll create a new virtual python environment and then install our application dependencies using the provided requirements file next we'll install the necessary open Telemetry packages to orchestrate our application the first is open Telemetry drro this contains both the open Telemetry API and SDK the second is open Telemetry exporter OTP this package allows us to export these logs within the OTP format otherwise known as the open Telemetry protocol let's take a look at our main.py file to configure our current logging framework to use the newly installed otel packages the first thing we need to do is include our dependencies this includes a function for setting the global logger to the open Telemetry logs module the OTP logs exporter which will export our otel logs via jrpc to The otel Collector and then a few classes from the logs SDK to handle the processing of our app logs next we'll create a log provider using the log provider class this will act as the logging mechanism for our application within its creation we have also provided the resource parameters and created some static resources including a service name and instance ID as static tags these will become a little bit more interesting later in the demo we will then use the set logger provider to set our new otel log provider as the global logger we can also configure some attributes of our logger as well for instance we can create an exporter to attach to our log provider for exporting our logs within the OTL format and also the batch log record processor for creating batches of these log entries before they are sent lastly we'll initialize a default logging Handler with our new Handler that we've just created now since we have configured the primary log Handler to act as the otel logger we actually do not need to change any more of our code we can simply call the standard logging class along with the type of log we would like to raise for example we could do something like login. error login process failed unexpectedly and then this log is triggered it will automatically be converted to the OTL format and exported so we're exporting our newly formed open Telemetry log entries from our application but where are they all going the open t R collector is a vendor agnostic agent that can receive process and Export open Telemetry data the collector's role allows you to offload aggregate your Telemetry data enrich it and then send it to an open Telemetry compatible endpoint such as a storage provider like Loki the interesting concept within the otel collector is that although passing information within the pipeline is open Telemetry formatted both input and output components can be of different data types for example there is both a Prometheus input and output component in our case however since Loki has a native open Telemetry endpoint we do not require any passing within the open Telemetry collector the open Telemetry collector is configured using a yaml formatted configuration file let's take a look at a basic configuration for log collection our configuration can be broken into the initialization of components and then the creation of the Telemetry pipeline let's start with the components in our case we're initializing the OTP receiver with both a jrpc and HTTP endpoint this allows our application and infrastructure to push otel Telemetry data directly via these protocols by default the arpc M pointer is exposed on Port 4317 and HTTP on [Music] 4318 next we have our exporter this outputs open Telemetry formatted Telemetry data to a chosen endpoint our endpoint in this case is pointing directly to the Loki native open Telemetry endpoint more on this in a moment lastly we build out our pipeline this can be broken down into metrics logs traces and profiles within this tutorial we are only focused on ingesting and outputting logs so our pipeline is actually quite basic when we start to look at more advanced topics in this series we can start to consider how other Telemetry data types can be used for in collaboration with logs to extend this config the last piece of the puzzle is preparing Lo key for open Telemetry based logs the primary feature we need to enable is to allow structured metadata I'm going to pause it right there and go down one final rabbit warrant what is structured metadata so remember that Loki's index comes in the form of labels labels essentially add metadata to our log streams so that we can improve query runtime and identify different log streams however there's a problem one of the primary label rules is that we should be bounded and used sparingly that's a problem for all of the metadata that comes with our open Telemetry formatted log entries so what do we do with attributes such as Trace ID Trace flag attributes and so on this is why we created structured metadata structured metadata is a way to attach extra metadata to our log entries with without indexing them or including them in the log line content itself examples of useful metadata are kubernetes pod names process IDs or any other label that is often used in queries but has high cardinality and or is expensive to extract at query time now you might be asking can I decide what goes into structured metadata and what becomes labels the answer answer is yes my colleague Ward will be creating an upcoming video covering this in more depth we do this by setting the allow structured metadata to True under the limits config within our Loki configuration file all of our pieces are now in place so let's spin up our demo and start producing some logs first we will spin up our observability stack using Docker compose up this includes Loki grafana and of course the open Telemetry collector next we will run our app locally since we've already installed the otel packages to our local virtual python environment then we can find our carnivorous Greenhouse app on Local Host Port 5005 let's create some logs as we usually do turn on the bug mode to make some more interesting logs finally let's jump into our grafana instance and take a look at our logs through the explore tab we see two labels based on the static resources we created earlier service name and service instance ID this is because both attributes are part of the default list of metadata that automatically becomes labels rather than structured metadata I highly recommend checking out the documentation for the full default list and of course this list is also configurable let's run a query and take a look at our log entries if we expand our log entry we can actually see far more metadata Fields based on those generated by The otel Collector such as severity number Telemetry SDK language and scope name lastly let's run a query against our structured metadata in this case we we have a metadata attribute called Cod line number this tells us a specific line from where the log entry was called let's filter by a specific Cod line and check the result this gives us the ability to treat our structured metadata like labels even though they aren't indexed this has the benefit of not having to apply line passing first to look for these attributes saving in overall performance per query and with that we'll close the book on open Telemetry for now however I don't think this will be the last time you'll hear from open Telemetry make sure you check out Ward's fantastic tutorial on its release and there is going to be plenty more content from me on otel in the future for now we'll move on to our next injest video all about the fluent D and fluent bit plugins for Loki we are also cooking up some deployment videos to be released in the not too distant future and until then my name is Jay Clifford stay curious [Music]

