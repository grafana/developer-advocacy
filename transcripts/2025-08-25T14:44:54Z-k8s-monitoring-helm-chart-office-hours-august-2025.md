# k8s-monitoring-helm Chart Office Hours (August 2025)

Published on 2025-08-25T14:44:54Z

## Description

In the August edition of the Kubernetes Monitoring Helm chart office hours, we discuss the version 3.3 release as well as the plan ...

URL: https://www.youtube.com/watch?v=GPverf9LpIo

## Summary

In the August edition of the Kubernetes Monitoring Helm Chart Office Hours, hosted by Pete Wall, a principal engineer at Grafana Labs, the discussion focused on the recent release of version 3.3 of the Kubernetes monitoring Helm chart. Key highlights included the introduction of a new destinations map option, which simplifies the management of destination settings, and the addition of Helm hooks to improve deployment and uninstallation processes. The session also covered upcoming features for version 3.4, such as enhanced front-end observability, improved log gathering capabilities, and plans for more dynamic deployments using the Alloy operator. The meeting included a Q&A session where participants engaged with Pete, asking about specific features and functionalities, demonstrating community interest in the project. Overall, the session emphasized community contributions and ongoing improvements to enhance user experience.

# Kubernetes Monitoring Helmchart Office Hours - August Edition

**Hello everybody!** Welcome to the August edition of the Kubernetes Monitoring Helmchart office hours. I'm Pete Wall, a Principal Software Engineer at Grafana Labs and the primary engineer for the Kubernetes Monitoring Helmchart.

## Agenda

On today's agenda, we will discuss:

- The release of version 3.3, which we launched at the end of last week.
- Key features and changes in this version.
- Upcoming features planned for version 3.4, which is expected to be released in the next couple of weeks.
- As always, we'll leave time for questions and answers.

## Kubernetes Monitoring Helmchart Version 3.3

We released version 3.3 on **August 15th**. This release was exciting for me as it included many community-contributed changes, not just small bug fixes. Here are some notable updates:

### New Destinations Map Option

One major addition is the **Destinations Map** option, which runs parallel to the Destinations List. The difference is that the Destinations Map utilizes objects instead of an array of destination types. This allows for a more straightforward reference by key (name) rather than by index, enhancing usability in tooling like Terraform or Argo. 

The current method has been an array with name and type, along with details for each destination. The Destinations Map sets the name as the key, simplifying overrides for specific deployments. 

### Helm Hooks

We have also added **Helm Hooks**, which allow you to run tools before or after the installation, upgrade, or removal of the Helm chart. This will ensure that deployment processes are smoother, particularly for the Alloy instances:

1. **Deployment Delay**: The deployment of Alloy instances will now wait until the Alloy operator is up and running to avoid race conditions.
2. **Pre-Delete Hook**: Before removing the Alloy operator, the Alloy instances will be removed first, addressing uninstallation issues present since version 3.0.

### Prometheus Scrape Protocols

The **Prometheus scrape protocols** can now be set on individual Prometheus scrape targets within the Helm chart. This feature was previously available in earlier versions and has now been brought into 3.3.

### Cron Job Metrics

We've added **cron job metrics** to the kube-state-metrics scrape allow list. This allows for visualization of cron job statistics when paired with Kubernetes monitoring in Grafana Cloud.

## Upcoming Features

Here are some features we have planned for future releases:

1. **Front-End Observability Feature**: We're reintroducing this feature, which was removed in version 2.0 due to compatibility issues. The new implementation will support Alloy components for better telemetry data handling.
  
2. **Log Gathering and Reorganization**: This feature will enable multiple methods for gathering pod logs, making it easier to work with heterogeneous environments.
   
3. **Alloy Without Cluster Roles**: We are testing Alloy deployment without cluster roles, which is a security requirement for some environments.

4. **New Destination Types**: We are working on debugging destination types and cloud provider label enrichment to improve tracking based on cloud platform attributes.

## Q&A Session

Now it's time for the Q&A session. If you have any questions, feel free to ask!

**Carl**: About the new Faro destination, will it allow microservices with the Faro SDK to send data to the local Kubernetes service of an Alloy pod instead of the Grafana Cloud endpoint?

**Pete**: That's a great question! Yes, this feature will allow you to send data into the cluster first before it goes out to Grafana Cloud. This change is aimed at environments that require internal connections without exposing data externally.

**Rob**: I wanted to ask about the change to move pod names out of the Loki labels into structured metadata. Would it be possible to append an additional list to include more labels without taking ownership of the whole list?

**Pete**: Absolutely, that's a valid concern. In the upcoming log gathering and reorganization feature, we aim to address this by allowing easier management of labels, so users won't have to redefine the entire list.

**Additional Questions and Comments**: 

- **Persistent Storage**: A question regarding whether there will be a single setting for enabling persistence across all Alloy types was raised. We are working on a new feature called **collector common** that will allow for setting common configurations, including persistent storage, for all Alloy instances.

- **Operator Hooks and Tolerations**: There were inquiries about adding tolerations for operator hooks. This is something we can consider for future updates.

## Conclusion

Thank you for joining todayâ€™s session! Your feedback is crucial, and we appreciate the community contributions that help enhance the project. If you have further questions or suggestions, feel free to reach out on the Kubernetes channel on the Grafana public Slack or check our GitHub page.

I hope you have a great rest of your day and a wonderful weekend! Take care, everyone!

## Raw YouTube Transcript

Hello everybody. Welcome to the August edition of the Kubernetes monitoring Helmchart office hours. I'm Pete Wall, uh principal engineer at at principal software engineer at Graphana Labs, uh and the primary engineer for the Kubernetes monitoring helmchart. Um on today's agenda, uh we talk we're going to talk about the version 3.3 that we released uh at the end of last week. Uh what's in there, what sort of things that you might be interested in. We'll talk about some of the upcoming features. Uh we do have some stuff planned for 3.4 which will actually be coming out uh maybe not next week but probably the week after. So we're trying to get some of these new features out uh sooner and sooner. Um and as always, we'll leave time for questions and answers. Uh so Kate's monitoring Helmchart version 3.3, what's involved? So we released this back August 15th. Um and this was really exciting for me because this one contained a lot of community contributed changes. uh and not just little small bug fixes here and there, although we do really appreciate those, but uh some some notable larger changes as well. Um the first one that I know a lot of people have been looking for and was really excited to to bring in is uh there's a new destinations map option. This one runs parallel to the destinations list. Um and the difference here is that the destinations map will load destinations uh using objects, right? So rather than an array of destination types, so this one can use uh objects. And if I can show this one here uh on the main readme for the Helm chart, you can see how they are uh how how you can use this. So this is the current way or the way that we've been doing destinations since 2.0. Uh it's an array with name and type and then all of the details for that destination. Destinations map sets the name as the key and then everything else is inside of that key. This should hopefully make it really easier to uh to to incorporate this into uh tooling especially like Terraform or Argo or things like that. Um any types of place where you have uh you want to set your defaults and then you want to override for particular deployments one or two particular uh settings or something like that. Um, being able to reference the destinations by key, by name, uh, using the key, uh, is going to be a lot simpler. Um, you could do this before on this method, but you would be referencing it by index and just trusting that the ordering of those destinations is consistent uh, across all your different deployments. This way makes it just a little bit more explicit about I am setting, you know, a setting on this local Prometheus one. Um, so this is pretty exciting. Um, I'm kicking around. We'll we'll see how popular this is and how people like it. I'm kicking around making this one a default or even making the destinations uh type this field here be able to support either or. But uh I wanted to get this out there. Um and uh and yeah, I'm really happy that we've got that. So give it a shot. Uh let me know how it goes. Uh if there's uh rough edges, let me know. We can try to get that fixed. Oh dear. Okay. just present. Uh, are y'all seeing the slides still? Give me a thumbs up. Okay, cool. Thank you. Uh, all right. The next thing that we've added, Helm hooks. So, uh, Helm hooks. Uh within Helm charts there is the idea of Helm hooks which are uh tools that you can run before or after the installation or upgrade or before or after the removal um of the Helm chart and they allow you to do essentially some kind of you know presetting of the environment or or finalization cleanup of the environment after removal and something like that. We haven't had Helm hooks. um I've been trying to uh not avoid them, but I just I'm trying to do things, you know, so that they weren't required. But there were two instances uh that came in that I felt like were worth uh investing the time to put into this. Um one of these was a contri was was a community contributed change. Uh and again, thank you for that. Um the first one which is the one that came from the community was uh what we're what we do now is what we delay the deployment of the alloy instances until the alloy operator itself is up and running. Um and so this makes sure that on install uh the alloy operator has had a successful running pod before we drop the alloy instances. That just makes sure that uh there's enough time for things to kind of get up and running and uh you know the alloy operator's leader election is underway and things like that before we put the alloy the actual alloy custom resource instances onto the cluster uh so for the operator to resolve them into actually running alloy instances. Um there uh the reporter of this said that there was a race condition uh and so this resolves that sort of thing. Uh the other Helm hook that uh that I added was a pre-delete hook uh which kind of does the opposite, right? So before we remove the alloy operator when you're in uninstalling the Helm chart, uh we're going to remove the alloy instances first. So before the operator pods go down, before all of the resources like the arbback rules that it uses uh to manage alloy instances go away, we're going to remove uh we use this pre-delete hook to remove the alloy first. let those go down, let them drain appropriately, uh, and then we'll proceed with the rest of the uninstallation. This should hopefully fix some of the uninstallation problems that we've had since 3.0 where, uh, there were chances where if the alloy operator was removed before the alloy instances, then there could be some lingering artifacts left over. Um, hopefully this will address that. Uh, another change in 3.3, uh, the Prometheus scrape protocols is now something that you can set. This is uh something that you set on individual Prometheus uh scrape targets within the Helm chart. Uh some people uh wanted specific scrape protocols. This was something that we were able to change in uh or you could set in the 1 something versions of the Helmchart and we finally brought this feature up into the 3.3. Uh and then last um cron job metrics. We were added cron job metrics to the coupube state metrics uh scrape allow list. Um I call this one out too because this one lets me highlight that uh cron job monitoring has been added to graphana cloud's kubernetes monitoring feature. So if you are pairing the kubernetes monitoring helmchart with kubernetes monitoring within graphana cloud uh you should be able to have uh visualizations of your crown job statistics too. Uh so check that out. Um, as always, the Kubernetes bounding helm chart works great with open- source versions of all of our databases. Uh, it's not required to be used with the Graphana cloud. Um, but we do obviously heavily work with them together. All right. Uh, briefly, I want to talk about some of the upcoming features we've got planned. Uh, like the disc disclaimer says here, this isn't commitment to working on these ones, but these are just kind of opening sharing of the road map that we have. Um, some of these are coming sooner. Some of these are still, you know, in the in the horizon, but these are the things that are top of mind for me that I'd really love to see coming in. Um, the first one, this one actually bumped up to the top from a long time ago, but the front-end observability feature, a proper front-end observability feature. Um, if uh users of the Helm chart from ages ago may have noticed that re remembered that in like the 2.0 0 time frame. We had a front-end observability feature, but that was removed. Um, the reason for that is that that feature actually was not compatible with the Graphana Cloud front-end observability, and I thought it just added too much confusion. And so, before we really fully released 2.0, um, we removed that. Well, now we're finally bringing it back. And the the reason why now uh and not earlier is because there are alloy components that were added. Um, I think it's like hotel call.exporter.faroh and receiver.faroh uh that do the proper uh telemetry data handling to be compatible with the proper front-end observability feature. It should work just fine with open source pharaoh. Uh but this is the one that you want to use if you are also using graphana cloud front-end observability. Uh this one will include also a specialized pharaoh type destination. Um, and so to properly pair this, you'll have a special destination called out just for Pharaoh. Uh, but keep an eye out for this one. Uh, log gathering and reorganization. This is something I've been talking about for a long time now. Uh, it's still in the works. Um, but the basically the idea that we'll be able to um enable multiple paths for gathering pod logs. uh you'll be able to mix and match things like gathering logs from the file system as well as from the Kubernetes API because sometimes you have clusters that have heterogeneous node types or different access policies or things like that. Uh this should make it easy to work with those kinds of environments. Uh, another one that I'm excited to to take a look at, uh, I need to do some more internal testing before I know all the ramifications of this, but uh, alloy soon will be able to deploy without cluster roles, only with namespace roles. Uh, this is a security requirement of some of the environments that uh, I've worked with. And so I want to test the Kubernetes monitoring helmchart with alloys deployed with no cluster roles. uh Alloy right now is the only component within the Kubernetes monitoring helch chart that still has to use cluster roles today. Uh everything else can be limited by namespace to just rolls. Uh but I want to make sure that all of the features work or if there are features that only have to use cluster roles, uh I want to know which ones they are so I can highlight them. Finally, some new destination types are in the works. Um, one of the ones that I'm planning for the 34 release kind of on the same timeline as the Pharaoh release, uh, is, you know, some debugging destinations. So, uh, there's a, uh, standard out destination. Uh, but, uh, being able to pair that with some of the other ones should hopefully, uh, make it simpler to find the, uh, you know, just get a quick snapshot of the logs without having to first store them into uh, like a logs database and then read it there. it should be just it's easier to to get a snapshot of things. Um and then the last one uh is cloud provider cloud provider label enrichment. The idea here is if we detect that you're on a certain cloud platform uh can we use that to add labels, add attributes based on the uh what we know about the cloud provider. So things like region or node labels, things like that are tags. Um, all right. That is the uh that's the end of the uh upcoming features or at least the planned upcoming features. Uh, and so now it's time for Q&A. And Carl, I saw that you dropped one into the uh into the chat. Um, do you want to just ask that one? Otherwise, I'm going to be it's going to be some silence while I Oh, yeah. That's all right. No, I was just um about the new Farro destination. Is the idea that the microservices that have the Farro SDK in them currently because they're currently using the Graphana Cloud Farro endpoint is the idea that this would now go to the local Kubernetes service of the of a alloy borrow pod or yeah that's a great question. So the the topology of deploying the front-end observability feature to work with you know graphana cloud frontend observability um definitely the ability for just adding the SDK into the web app and sending the data directly to graphana cloud um that still will work and you can totally continue to do that. Um if there are environments or requirements where you don't want to have uh an external connection out to graphana cloud directly um this feature would allow you to send them into the cluster first and then send them out to to graphic cloud. Um, notably the change for for this topology deploying with the Kubernetes monitoring helmch chart likely would mean that you would need to deploy an alloy or maybe it's alloy metrics alloy receiver something like that or it's a new alloy uh that you would deploy um that would need to be deployed with maybe some sort of ingress or something like that because it would be uh connections initiated by the the user's browser that would then need to bring come into alloy before being captured there and then delivered out to uh to Pharaoh. So, so that would seem to make it a more complex um deployment than what's currently using. You're not wrong. Um there are, you know, but but everyone's got different requirements and different deployment types. And so uh you know it and it's potentially less so it could be used for people who aren't using you know publicly accessible web apps but maybe internet apps or something like that. Um where the idea of you know bringing all of the data together into a a common source before exfiltrating the data out to graphana cloud is more of a requirement. So, um, it is, you know, regardless, it's been something that people have been asking for for a long time. And previously, the the way that we've been handling it is here's a chunk of extra config code that you can add to your deployment. And so, it's always been possible, but this way makes it a lot cleaner. Yeah, Rob brings up a good point. It's probably more for the the open source community um than people using Bonic Cloud. Sure. Okay, it makes sense. Thanks. Yeah. No, good questions. Um, I did want to highlight uh, so we talked a little bit about this before the recording started, but uh, in the application observability feature, we have uh, the interval processor. Uh, interval processor uh, is still within Alo. I think it's under the experimental um, uh, release level or something like that. Um, uh, we found that there's a bug there that it will try to send traces to that and the traces aren't compatible. Um I'm I'm aware of that and so I'm going to try to get a patch out to fix that real soon. So if you are using the interval processor uh be aware that uh we'll we'll in the 33.2 release likely coming out later today uh we'll have a fix for that. Um other questions? Um I do know that there's been a bunch of questions on the Kubernetes channel on the public Slack. Uh I am planning on getting through those ones. Uh but if you have a question directly related to that, yeah, please uh let me know. Hey Pete, I had a question if that's all right. Firstly, thanks for running these are great. Um I had a I was wondering about the recent change to move the pod names out of the Loki labels and into the structured metadata sort of segments. Yeah. Um, it would be super useful to be able to rather than have to take over the list that you have in the values chart of all the pod labels or all the log labels so that we send to Loki to be able to like append an additional lists if we do want to include rather than having to take ownership over the whole list which might you know update in the future. Yeah. And and I guess for any place where you have a predefined list of stuff that somebody might want to add something to it without having to, you know, maintain the whole original list as well. Y uh it would just be super convenient kind of in those uh cases to be able to just append it to some other list and then merge them together in Helm uh rather than Yeah. taking control. Yeah. Yeah. Yeah. Yeah. Um totally agree. Um, Helm and arrays are always a little bit tricky. I think it's I mean it's honestly it's YAML and and Kubernetes and all these kinds of things, but Helm definitely is the way that a lot of people see this problem. Um, for those who don't have the context in the pod logs feature, uh, we have two lists. We have a list of labels. It's called labels to keep. Basically, we we turn a ton of things into labels and then we have this labels to keep list uh that filters them down. Uh and then we have a second list uh of things to turn take from labels and turn them into structured metadata. Um the the thing with Helm is that if you want to change an array, you have to redefine the entire array because Helm doesn't know are you trying to add one element or are you trying to set the existing contents of the array? Um and so Helm just says, "Oh, you're probably trying to set a whole new array." Uh which means um like Martin said that when you want to just change something, you have to take ownership of the entire contents of the array. And you know, one of the big downsides as a maintainer of this chart is that if I add an element to that array, anyone who's redefined it doesn't get it because they will have to append that to the list that they're maintaining. And I don't like that idea. You know, if you know, I would want the users of this chart to trust that, you know, updates that we bring are in the user's best interest. Uh, and so they shouldn't have to keep aware of these sorts of things and maintain that. So yes, I totally agree with that. Um, if I'm going to go back one tab to the um, upcoming features, this log gathering and reorganization feature will contain a fix for that. Um, what I'm trying to avoid doing is introducing breaking changes for people who are using the podlocks feature today and have things set up the way that they like it. Uh so this feature the re log gathering and reorganization feature will likely uh softdeprecate the pod logs feature but then intro introduce new features within the Helm chart to do uh pod log gathering based on volumes pod log gathering based on API um other methods that we don't even have today. So like gathering via a Loki receiver. So you can actually push logs into into alloy or gathering with uh special log setups like if you're on an open shift environment, you can configure the open shift cluster itself to just automatically send log data and so you don't need to do anything else other than that. So there's some really cool things to do that with those features. I definitely am going to take a strong pass through it with the usability framework in mind. Um, one of the reasons why the pod logs feature and those two lists kind of became tricky to manage is that the that one feature, the pod logs feature is trying to do three different styles at once. It's gathering via the the volumes which gathers logs in the Loki style. You can turn on the file log gather method which does the same thing but then it's an OTLP format or you can do the Kates API based method. And so I'm trying to that labels to keep list was trying to be tolerant of all sorts of different gather methods. Um with the new features that are coming up for this log gathering um it really should be a lot clearer. Uh and definitely we'll be avoiding lists of that nature. So, um, yeah, thanks for highlighting that. You know, I've I've definitely felt the pain of, uh, you know, a bunch of people asking about that and having to tell them, "No, you have to give us the entire list back again." Um, and that's not a great experience. So, that's definitely high on the list of things to improve. Amazing. Thank you. I had one more question if that's okay. Oh, brilliant. I was looking at the the doc uh the yeah the documentations and there was a page around setting the uh persistent storage in the uh the writer headlog kind of stuff. Yeah. Um, and I was wondering with the move to the alloy operator, is there an option to have like a single setting where you say true enable persistence in alloy and then that would permeate down to all the different types of alloys that you deploy because the I guess the docu the example in the documentation has you having to redefine the persistent volume effectively for each type of alloy. Yes. Uh individually. That is a great question and that is something that um we didn't talk about in I think last time's uh office hours. Uh but another another usability change that came up um and maybe I just missed this from the slides or something like that, but um inside of the main values file, there's a new section called collector common. Uh, and then inside of collector common alloy, you can put anything and that will apply to all alloy instances that get created. Um, asterisk on that. There's a bug that I need to fix. Right now, it does for all of the the kind of the main alloy instances even though I'm lying in saying that it works for tail sampling and service graph. I think that it doesn't work for the tail sampling alloys. So, that will come. Again, that's on that's a bug fix that will come in 332. Uh but if you put you know if you have uh well let's just look at um you know alloy receiver often you set uh or something that you wouldn't want to set on all of them like a pod annotation pod labels the storage like you mentioned um if you want to just turn on live debugging for all of them or set the the log level within this collector common you can do that and it will apply to all of the alloy instances. Um I like I like showing the screen and showing the the examples here so you can actually see what I'm talking about. So inside of I think the private image registry um this one has an example of using that. So uh rather than setting you know all of these you know individual services need to be directly told to use a private image registry. uh but down here I'm not doing it for alloy metrics or alloy receiver because I'm doing it for collector common. So that means that all of the alloy instances that are deployed with this are going to pick up uh this data here for the image registry stuff. So this is one example of using it um rather than having to define it you know one two sometimes five different times you can just deploy it once and it will apply to all your alloy instances. But nice I I guess though in the case of the persistent storage where I think some of the examples have like variations for each type of alloy whatever deployed. Yeah. So the collector storage here uh or maybe it's the same across all of them. They're roughly different because so alloy metrics uses some storage for its write ahead log. uh Alloy Logs is using it for log positions. The difference between these two top sections here is just the name of the the storage mount. So that's fine. We can name that differently. Uh the the the part down here, this will be a little bit different. Uh but those anything that's different you could put into the individual alloy section. Um and then anything that's common, but again it have to be common across all of the alloy instances. Yeah. you could put into the collector common object. Cool. Awesome. Thank you. Thank you. Great questions. All right, I see more here. Are there specific reasons why operator hooks are not inheriting the operator tolerations? Um, no. We can add that. Um, it makes sense uh to a degree. I mean, it needs to run the the the Helm hooks that we've added in 3.3. Um, they aren't running the same code or obviously not the same code, but they're not they don't need to be running in the same name space or the same node as the H or the alloy operator itself. Um but uh u but I can understand that you know you might want to run it on this you know you know you're going to be running alloy operator. I think maybe something that would just help this is just to be able to set the tolerations fields for those hooks. Um so that's something that that's an improvement that I can make. Um second question is can we also define tolerations as well for all of the collectors globally? Uh yes, the collector common uh applies anything that you can set into the alloy sections. Um anything that you can deploy or you can set into an alloy instance itself uh you can put into that collector comment. So that would include tolerations. So this is going to be I'm I'm really excited for that feature. It's going to be huge for things like setting environment variables which are common across a lot of different alloy instances. uh the image registries like I showed uh tolerations uh labels annotations all that sort of stuff. Um also highlighting how it's nice to be able to set things by uh object and not by array because then I'm not having to redefine the array every single time but uh defining the object itself. You can put anything that's common inside of collector common and then anything specific inside of that uh one alloy instance. Hit that button again. Good questions. Any other questions? Um, I have one more question, Pete. Okay. Um, I I just curious, um, with the move again to the alloy operator, are you thinking about moving any more of the Helm chart to be kind of wholly CRD based rather than, I guess, generating lots of config maps and then passing them to the alloy instances. Do you see yourself kind of moving it in that direction or you have it kind of where it is? I'd say um that's a good question. The what I have in mind, especially long-term when I look at like 4.0 or even beyond, I don't have any new CRDs planned. Um so I don't have anything in the works that adds wholly new functionality to the way that we're deploying things. One thing that you you call out config maps. Um the Kubernetes boundary home chart specifically creates a distinct config map for the alloy instances next to the actual alloy CR custom resource that gets deployed. Um and that's intentional for a couple reasons. Um but mainly today the biggest benefits that I see with that is that if you redeploy the helmchart and all you're doing is you're updating the config then you don't need the alloy operator to reconcile that we can just update the config directly. Um that also helps with a little bit of you know it does it minimizes the churn of an alloy instance itself. Um, it also keeps config inside of a config and not um in larger and larger alloy custom resource definitions or not definitions but the custom resource instances. Uh, we don't need to embed the config into there. Um, where I do see potential benefit and things that we may invest more and more time into in the future with with regards to operators is util utilizing more of what the operator gives us for more dynamic deployments. So in 3.2 2 and 3.3 uh well no 30 and 31 I think um the ability to deploy tail sampling and service graph metrics those required additional alloy instances but we're kind of we're not calling them out we just we're using the operator to deploy these alloy instances uh to really deploy a pretty complex topology without making it feel like you're having to to manage out this large topology um where I could see and this maybe tip a little bit into what I have planned for 4.0 and I'm not really ready to fully commit to it, but I'll tell you kind of where I'm thinking about this. Um, the idea with 4.0 is that we can do for the alloy instances what we did for destinations in 20 uh where you there are less hardcoded instances of alloy in the Helm chart. There's not going to be alloy metrics, alloy logs, all the things, but we'll be able to to give us the list or, you know, a map of alloy instances and based on what you're wanting to do, we can deploy that. Um, one of the things if I can go even further with this, this is totally speculative. This is not me saying this is how we're going to do it. But could we have a thing in which all you tell us is this is the feature, this is the data that we want. We want to gather infrastructure metrics uh about these things. We want to have application ports opened. Um and then you just devel deployed the alloy footprint that makes the most sense, right? Could we make it such that we can just say we know you know Grafana Labs the the central maintainers of the alloy project know the best practices for deploying to gather all of this data and we just deploy an optimal topology out of the gate uh based on what sort of data that you're trying to get. This is again it's it's like way off in the future kind of idea but it's one of those things that I think about when we think what can we do with the alloy operator beyond just deploying stat static instances. Can we actually make it more reactive um more proactive with uh you know making sure that things are set up in the most optimal way. Nice. Thank you. Yeah, really interesting. Um but to to more directly answer your first question, yeah, we don't have any plans to add new new custom resource definitions. Um I specifically made the alloy custom resource definition very broad. I mean if you look at it, it just is like spec and open close brace. uh but I made that because I wanted to minimize you know churn and updates of that CRD directly um so because I know as operators of of clusters and operators and runners of the Helm chart um CRD changes can get hairy so I'm trying to make sure that that shouldn't have to happen anytime soon. Awesome. Cool. Good questions. Excellent questions. Any others? All right, great. Uh, but again, if you do think of more questions, um, or if, uh, you've got something that you just want to chat about, talk about, uh, you can always find us on the Kubernetes channel on the graphana public slack, graphfana.slack.com. Um, I'm in the Kubernetes channel a lot. Um, I've got a little bit of backlog of questions there to catch up on, but I will be uh, working through that today. Um otherwise if you have issues uh or you have feature requests you can also find us on the GitHub page graphana gates monitoringhelm. Uh this project is hugely motivated and built based on community feedback. Um and we want to make sure that whatever we make is something that is helping your jobs go easier and uh it helps you be more effective. So um you know the feedback is super important to us and the contributions. Oh my gosh, again 3.3 some major communitydriven contributions. Um it really means a lot and I was really excited to see all of those come in. So um if you're uh if you want to help u contribute and make the project better, you know, we're we love getting those PRs, um I would be happy, you know, more than happy to work through some of the process of getting uh any of the changes or the automated unit tests and things like that working. So um please reach out. Excuse me. Thank you so much for joining. I hope you have a great rest of your day uh and a great weekend. Take care, everybody.

