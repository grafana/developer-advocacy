# Mimir October 2025 Community Call

Published on 2025-10-03T03:08:42Z

## Description

Let's talk about discuss Mimir! Specifically we'll discuss Mimir 3.0, which is coming soon.

URL: https://www.youtube.com/watch?v=54BmaGkwvwY

## Summary

The video is a community call for the Grafana Mimir project, held on October 2nd, where participants discussed updates related to the upcoming launch of Mimir 3.0. Key speakers included Dimitar and Nick, who presented on significant architectural changes, particularly the decoupling of the read and write paths using a Kafka-compatible system, which improves efficiency and reduces downtime during outages. They also addressed various breaking changes and improvements, including the requirement of the query front end and the introduction of the new Mimir query engine. The session included a Q&A segment, discussing migration strategies, potential challenges with high cardinality, and autoscaling capabilities. The team anticipates releasing Mimir 3.0 by late October or early November and encouraged community feedback throughout the call.

# October 2nd M Community Call Transcript

Welcome to the October 2nd M community call! I think we should be live now.

**Zo:** Do you want to post the link to the stream in the community Slack?  
**Dimitar:** Do we have the link? I don't know where I can copy the link.  
**Zo:** Oh, I have it. Okay, I got it. Let me post it. Do we have to go to the Google Hangouts just to let people know?  
**Dimitar:** Hi there, everybody. I've gone into the Google Hangout and posted the YouTube live stream link there as a pin. So, anyone who joins should be able to see the link. I'll also post it in the community Slack in case anybody happens upon it there. 

I think you're good to go ahead. 

**Tal:** All right. Do you want to present?  
**Dimitar:** Sure. Yes, I can share my screen. One second. Thanks. 

I think we can get started now. People can jump in whenever they see the link. Welcome, everyone, to the Grafana Mimir community call for September and October. We're really excited to try out this new streaming format because we think it will improve your experience. Feel free to share your feedback during or after the call. 

For today's agenda, we'll share updates regarding the upcoming launch for Mimir 3.0, and we'll have Dimitar and Nick share more updates on what we have done in the past one to two months. 

We'll start with an overview and key changes, and then we'll have a Q&A session. I'll hand this over to Dimitar to give us an overview of the changes. Go ahead, Dimitar.

**Dimitar:** Sure. Should I share the slides?  
**Tal:** Yeah, go ahead.

**Dimitar:** One second. 

The Mimir 3.0 launch includes a lot of changes, some of which are breaking changes. The most significant change is the new architecture for the in-storage system. Previously, you would be familiar with the read and write paths on the left here, with ingesters shared between those paths. They receive incoming writes and serve reads from queries. 

In the new in-storage architecture, we're decoupling the read and write paths by having CFKA in the middle. This could be CFKA or anything Kafka compatible. Distributors write to Kafka, and ingesters consume from Kafka. This way, the write path only involves the gateway, engine, and the distributor, along with the Kafka brokers. 

Ingesters operate on the other side of Kafka. If there’s a write path outage, the read path can continue working, and vice versa. If ingesters fail, we will still be able to continue accepting reads and writes. This decoupling also changes the minimum replication factor; in the old architecture, it was three, but in the new architecture, you only need a replication factor of two in the ingesters and two in Kafka brokers. However, this is a trade-off you need to consider based on your requirements.

The new architecture also contains a new component called the block builder, which is still experimental. Currently, ingesters ship blocks to object storage after consuming metrics from Kafka. The block builder will consume from Kafka but will handle deduplication of samples, ensuring blocks are not uploaded multiple times to object storage. This is still in the experimental phase, and we're exploring the architecture of the block builder.

With Mimir 3.0, this will be the default architecture. The old or classic architecture will remain in the code and supported but will be deprecated in a future release, potentially around version 4.0.

As for migration from the current architecture (classic) to the new in-storage architecture, there are several ways to approach this. One easy way is to set up a new cluster, which can involve downtime. The format in object storage remains the same, so blocks will continue to serve after migration. However, this approach does involve downtime for the entire cluster, which could last from minutes to hours.

There are more complex migration paths that can achieve zero downtime, but these are not trivial to implement. For example, you could run two clusters in parallel, one with the new architecture and one with the classic. More details will be provided in our documentation.

**Nick:** With 3.0, we're also taking the opportunity for a major version bump in Mimir and the Helm chart to implement some backwards incompatible changes. We believe these changes won't significantly impact users because they codify how we recommend running Mimir.

I'll go through the changes briefly, excluding the in-storage architecture that Dimitar covered. The first is requiring the query front end. If you are using Mimir through the Helm chart, this has been the default. It is technically possible to avoid the query front end, but that will be removed to simplify the code path.

Another change requires the query scheduler component, which complicates operations when embedded within the front end. We are now mandating the query scheduler component. If you use the Helm chart, this has already been the default for several releases.

We are also defaulting to the Mimir query engine, our streaming query engine, instead of the Prometheus query engine. This unlocks future optimizations. The Prometheus engine will still be available for fallback if you encounter issues with the Mimir query engine.

The read-write mode is being removed as it was an experiment that did not receive significant support after its initial introduction. We recommend using the Helm chart for deployment, which offers a better experience.

**Tal:** All right, moderator Matt, do we have any questions from the chat?

**Matt:** I can check. I'm excited to see what questions folks have. So far, we have some thumbs-up emojis, which is great!

Feel free to post your questions in the chat. 

While we wait for that, we have a couple of questions that we thought might interest folks. For instance, do you have to use in-storage with Mimir 3.0?

**Dimitar:** You do not have to use it; you can still use the classic architecture. There is a values file in the Helm chart that switches back to classic architecture, but the default values file is in-storage now.

Yes, people are looking forward to the migration document. The approach Dimitar mentioned, where you spin up another Mimir cluster alongside your existing one, is a nice middle ground between downtime and migration complexity.

**Dimitar:** Internally, we followed a complex process that was labor-intensive and prone to mistakes. If aiming for zero downtime, double writing to a new cluster running the new architecture while pointing both clusters to a single S3 or object bucket is a decent middle ground. Just be aware of the cost implications during that period.

**Matt:** That's a helpful overview. We should document this for clarity. 

Is there a way to start exploring the new architecture, especially regarding AutoMQ?

**Dimitar:** We haven't explored AutoMQ yet, but the code for the new in-storage architecture is in Mimir 2.17. It’s not enabled by default, but you can configure it and start experimenting. The configuration docs are available for version 2.17.

Another question is about whether there’s any intention to use ISO ambient mode instead of NGINX in the Helm chart.

**Dimitar:** That's likely a question about ingress. I don't think we have explored that yet. It might be beneficial to post that in the community Slack for further discussion.

Are there any changes to the format of the data written to object storage? If you migrate from 2.x to 3.x, can you migrate back?

**Dimitar:** Yes, the object storage format remains the same. Depending on how you migrate, there may be extra steps to ensure that ingesters upload all metrics they’ve collected to object storage, but the format is still Prometheus TSDB.

**Max:** Is this a problem with Mimir regarding high cardinality?

**Dimitar:** Can you clarify? I didn't see the full question. 

**Matt:** There was another question about queries falling back to the Prometheus engine from MQE. How does that work in Grafana Cloud?

**Dimitar:** Currently, MQE can handle all Prometheus stable features and is 100% compatible with everything not experimental. If there's a difference, that would be considered a bug. 

If a query can't be handled by MQE, it will automatically fall back to the Prometheus engine. In the future, we may change this behavior to allow users to specify which engine to use.

**Matt:** Regarding reducing the number of hops and cross-AZ traffic generated, do we have any recommendations?

**Dimitar:** This was a major motivating factor for the in-storage architecture. With the classic architecture, having distributors writing to ingesters in different zones was costly. Writing to CFKA helps avoid those costs. You would need to configure multiple zones for distributors to eliminate cross-zone hops.

**Matt:** Does the community have more questions for Nick and Dimitar?

**Dimitar:** I’m not sure how much lag there is in the live stream. 

**Matt:** It seems like we’re up to date with the questions. 

**Dimitar:** We can give it another minute, but if there are no more questions, we might end the live stream.

**Tal:** I have one more question. When can we expect Mimir 3.0 to be released?

**Dimitar:** We plan to release it in about a month, so the community should expect to see the first version at the end of October or early November. 

**Matt:** There's a question about autoscaling queryers.

**Dimitar:** I'm almost sure queryers can be autoscaled in the Helm chart and JSONet. We autoscale everything stateless by default, but I think there may be some missing documentation.

If there are no more questions, we can probably end the stream. If you have more questions, feel free to post them directly in the Slack community channel, and we can follow up from there. 

Thanks again to Dimitar, Nick, and Matt for joining the stream and for all your answers. That wraps up today's call. We look forward to seeing everyone in about a month. Thanks, everyone, and see you soon!

## Raw YouTube Transcript

Yeah, welcome to the October 2nd M community. I think we should be live. >> We should be live. Uh uh Zo, you want to post the link to the stream in the community slack? >> Do we have the link? I don't know where I can copy the link. Let's see. Oh, I have it. Okay, I got it. Let me post it. Do we have to go to the Google Hangouts just to let people know? >> Hi there everybody. I've uh I've gone into the Google Hangout uh to uh and and posted the YouTube live stream link there as a pin. So, anybody that joins there should be able to see the link. I'll also post it in the community Slack uh in case anybody happens upon it there. So, I think you're good to you're good to go ahead. >> All right. Uh Tal, you want to present the Sure. Yes. My screen. Okay. Um I think we can get started. So people can just jump in uh whenever they see the link. Um, welcome everyone to the Grafala Mir community call for um, September October. Uh, we're really excited to try out this new format uh, as streaming because we think it's probably going to make your experience better and feel free to share your feedback or your experience um, during or after the call. For today's call, we're gonna share more updates regarding the upcoming launch for Mimir 3 and also um we'll have Dimitar and Nick to share more updates regarding what we have done in the past one to two months. Um and this is agenda for today. We'll go over some overview and also key changes at every we'll do some Q&A to get started. Um I think we'll I'll hand this over to Dimitar to go over the probably the overview for the changes. Go ahead. >> Sure. Should I share the SL? Um share slides. >> Yeah. >> Yeah. One second. Um, oh, thanks. Um, so the mirror tree launch is we're building a lot of uh changes in it. Some of them are breaking changes. Um I want to talk about the maybe the most important bit we're we're shipping which is the new architecture the new in storage architecture. So previously most of you would be familiar that we have the read and the right paths on the left here um and how the injusters are the component which is shared between the read and the right path. they receive incoming rights and they serve uh reads they serve from queries. Um in the new inest storage architecture we're decoupling the read in the right path by having uh CFKA in the middle. So it it could be CFKA or anything Kafka compatible. Um in this case distributors write to Kafka and injusters consume from Kafka. So in this way the right path is only the gateway enginex or and the distributor and the the cafka brokers. Um injusters are pickable on the other side of cafka. So if there's a right path outage the read path can continue working and and vice versa. If injusters fail me will still be able to continue accepting reads and rights um rights I'm sorry. Um this yeah this this decoup this also changes uh the minimum rep replication factor in the old architecture it used to be three in the new architecture you only need a replication factor of two in the injusters and two in cafka brokers obviously that's up to you and it's trade-offs you need to make on how on whether to keep cafka replication as two or not but it's something that changes um the new architecture also contains a new component which is still experimental called the block builder. So for now injusters still ship blocks to object storage after they consume metrics from from Kafka. We're working on the block builder which can consume again from Kafka but instead of but um takes care of dduplicating samples or rather takes just doesn't upload blocks multiple times like injusters do and and puts them in object store. uh is still experimental and we're still playing around with the architecture of the block builder and whether we don't need more coordination there. Um but yeah, so this is something which will be coming to which will be coming with the mir 3.0. Um this will be the default architecture. the old or sort of classic and current architectures uh will will remain in the code but it will be and and supported but it will be deprecated in a future release. So perhaps maybe 4.0 would have the classic architecture deprecated. Um, we still don't have clear dates and plans for when the duplication would happen, but um, like as a as a best practice, if you're setting up a new mirror cluster, it's best to to set it up with the new architecture. Um as for migration between the current architecture so what we call classic and the new inest storage architecture um there are there's one easy way which is to set up a new cluster and um well uh there are multiple ways. So you can shut down the the current uh mirror cluster have some downtime and then set it up with the new architecture. The the format in object storage is the same. So blocks will continue being served after after the migration. Um the the downside is that there is downtime with this migration. So the whole memory cluster would not be able to serve reades until we finish the migration which may be minutes to realistically hours. Um there is a there are multiple other ways like there's a much much more involved migration with zero downtime which we're still it's supported um in our JSONet but it's not something uh trivial to pull off. So but this time it's it's we're we're kind of not giving advice on that. Um, you can also spin up maybe two clusters at the same time and migrate data between those. One with the new architecture, one with the with the classic. Um, there will be more details on this in our docs. I think I've covered everything. Um, so with 3.0 though there'll be also a lot of um some some other changes apart from from the just storage architecture and Nick will will say a few words about those. Uh yeah so uh we're taking the opportunity of doing a major version bump in Mamir and a major version version bump in the Helm chart to do some uh backwards incompatible changes. Um these are backwards incompatibles. They're breaking changes, but uh we're hoping there won't be a lot of impact from them because uh they mostly are just uh codifying the way we've wanted people to run Mamir uh for forever basically. Um I'm going to go through them uh a little bit except for inest storage which Dimitar uh just covered. Um so the first one is requiring the query front end. Um if you are using Mamir through the Helm chart, uh this has been the default the entire time. Um it is only it's technically possible to not use the query front end which dates back to before Mamir even existed back uh in Cortex when you could just query a querer directly. Um that will be going away because it uh it dramatically simplifies the uh the code path and allows us to do a lot of cleanup. Um but we don't expect that to affect anyone using Mamir. Um another backwards incompatible change uh requires the query scheduler component. Um there was a there is was a way to have a a uh a queue of queries embedded within the front end. um this didn't have great scalability and made the uh sort of made operations a little more complicated than they need to be. Um so now we are requiring the queryuler component. Uh if you're using the Helm chart, this is already the default. Um I think it may have been the default the entire time. Uh but it's definitely been the default the last last several releases. So that should be pretty minor. Uh another another cool thing is that we're defaulting to the mamir query engine which is our streaming uh query engine as opposed to the the Prometheus query engine. Um the querers already use the mamir query engine by default and now the query front end will be using it by default. Um this unlocks a lot of uh future optimizations uh for us. Um Prometheus engine will still be uh a possibility if uh you hit a bug in the Mamir Corey engine. Um it'll be possible to switch back to the Prometheus engine. Um but mamira core engine will be uh the default going forward and will be where we put uh a lot of uh optimization efforts. Um inest storage uh dimitar covered um read write mode is going away. This was an experiment that we introduced uh about two years ago. I think it was an effort to make mirror easier to run instead of having distributors, ingesters, queryers, query front end, store gateways, etc. Um, we had three components, read, write, and backend. Um, we've we ran a few uh few of our production environments in this mode. Um it uh it was shipped and it was an experiment and it it didn't get a lot of effort uh a lot of work put into it after the fact. So it just kind of became this odd duck. Um and in the meantime we introduced the Helm chart and started recommending that that's how people deploy Mamir. Um and so that's how we recommend uh people deploy Mamir and get an easy easier experience uh running mamir these days. Um some of the lessons learned from readwrite mode might end up getting integrated back into mamir but for now it's being removed. Um it's an experiment and uh yeah um it's an experiment but uh it's not going to be the way we run Mamir or recommend people run Mamir going forward. Um all right uh next slide. That's all that's all I got for here. Q& A. >> All right, moderator Matt. >> Yeah, I can probably Okay, he's here. >> I'm excited to see what questions uh folks have for you uh in the chat. So far, we have some thumbs up emoji, which is great. uh like to see like to see that. >> Questions feel free to post it there. >> While we wait for that, we have a couple of questions that we could uh that we could uh address. uh some questions that we thought maybe uh folks might be interested in in knowing about. So I think you already addressed this one. Uh do you have to use uh injust storage with Mamir 3.0? >> You do not have to use it. You can still use classic architecture. Uh I believe there is a values file in the Helm chart that switches things back to classic architecture that you can use. Uh but the default values file is inest storage now. Yes, people are looking for looking forward to the migration document. I think uh Dimitar the approach you mentioned where you spin up another Mamir cluster in parallel to your existing one is probably going to be the the nice middle ground between downtime and complexity of a migration. The way we ended up doing it internally is uh there's a lot of steps. It's very uh it's very labor intensive. >> It's also easy to to to get wrong. Um so if you're aiming for zero down time, it could be the the double writing is probably a decent middle ground. um which would be to spin up a whole second cluster running the new architecture. Point both clusters to a single S3 bucket or single object bucket. Um disable the compactor in one of the cells. Compactors assume they're the only ones running against the the um the bucket. So they might interfere if you have two cluster clusters running each their own compactor set. Um and then as you as data is aged out of the old injustice after um 13 hours by default, you can shut down the old cluster and um only leave the new one running. Uh this would require you set up your auto collectors or your Promethei to send to two places to sort of remote write to to end points. Yeah, has an obvious cost implication for the duration of the of of those 13 hours. Thanks, Dimmitar. That's a that's a that's a nice nice high level uh explanation of that. And uh I think that yeah that's the that may be one of the ones that we want to uh write down in the documentation uh to be a little bit more uh clear and uh uh but we we have not done that quite yet. There is another question here um about is there a way that we're able to start exploring the new architecture? We need to look at see how autoMQ works in the new architecture. I'm not sure if we've done any exploration with AutoMQ. >> We have not, but the new architecture inest storage, the code is in Mamir 217. Um, it's just not enabled by default. And I think the corresponding version, the Helm chart, it's not enabled by default, but it's it's all there if you if you want to configure it and turn it on and start poking around. Yeah, I think the configuration docs for it are already published if you look at 217. Um, so you should be able to configure it. Um, maybe if you open the docs, we we publish uh what we call next version of the docs, which is an unpublished version. um that might have some some details around um what exactly we need out of CFKA the CFKA protocol and more more details. Um they're still incomplete but they will be by the 3.0 release. There was another question that uh came in. Uh is is there any intention to use iso ambient mode instead of engine x think we're talking about in the hound chart? Uh yeah that's probably a question about ingress right which I don't think so we haven't uh we haven't explored um so uh no as far as I know there may be some benefit to post that as a question in the community Slack to follow up on that uh to talk about the use case of of that or if anybody else in the community has had some experience uh uh with with any kind of modifications to the Helm chart to to use. Um, that might be good to further the conversation So another question um is are there any changes to the format of the data written to object storage? So if you migrate from 2.17 or 2.x to 3.x could you migrate back? Uh yes yes the object storage format is the same. Um depending on how you do the migration, there may be extra steps where you have to make sure to that the ingesttors upload all the uh metrics they've collected so far to object storage. Um but yeah, the format in object storage is still the same Prometheus TSDB. That's not changing. Um Max asked a question. Is this not also a problem with in Mamir with high cardality? I'm not sure what that's in reference to. Um, can you clarify Wait to see if uh Oh, I'm not sure I see it. I'm not sure I see the full question. Uh Max if you want to ask that one again. Um in the meantime uh there was another question uh around uh queries uh falling back uh to the Prometheus engine from MQE. Uh curious about uh whether MQE can handle all the queries, some of the queries, 50% of the queries. How does that work uh in graphana cloud? Uh so right now MQE can handle all Prometheus stable uh features. Um it is 100% compatible with everything that's not experimental. Um if there is a difference that's that's a bug. Um there there are a few like uh cosmetic differences like the the format of error messages looks a little different sometimes. Uh but yeah, MQE can handle everything that's not experimental. There are a few experimental functions in Prometheus that haven't been implemented in STO yet or notto MQE. Uh like I think Prometheus has an experimental info function um that is not in MQ yet. So right now uh when Mamir detects a query that MQE can't handle, it will automatically fall back to using the Prometheus engine. It might be the case in the future that that doesn't happen automatically. It might be the case in the future that it's just a setting on the quer which engine to use and you have to pick that when they're starting up. Um but for now, yeah, it'll automatically switch the Prometheus engine if it can't handle it. >> Okay, thank you for following up, Max. Maybe this is a good question to get your thoughts. Um I think this is from Matt too. um regarding how to reduce the amount of hops and cross easy traffic um generated. Do we have any recommendations? Um so that was actually a major motivating factor for uh inest storage was um with the classic architecture you have distributors writing to ingesttors in three different zones. If those zones were in different cloud availability zones then it was quite expensive. I think writing to CFKA you don't incur those uh cross aaz costs. Um, so the new architecture will help with that, but I think you have to uh, Demitar, you might have done some work with this, but I think you have to explicitly configure multiple zones of distributors and have them uh, >> yeah, I think um, I'm not 100% sure how it works with Apache Cafka. I I think there's some possibility that Apacha will still have to replicate the data between the zones. >> Um >> yeah, we have experience with warpstream which use which stores data on object store. So the data doesn't cross um availability zones. Um I think there may be other implementations that you can use. Um but to to answer your question, you do you would need multiple distributor zones and uh perhaps multiple copies of the RET path to like completely um get rid of crosszone hops. Um it's possible it's possible to set up um multiple distributor zones in theory. It's not something that Helmchart supports. um how the read path separation happens is still to be determined. We don't have a good um we don't have a way of of directing queries to to like a preferred zone uh for example from querers to to injesttors and from query front ends to queries and so on. Still TBD but like it's it's something we're working on something we want to solve. Cool. Thanks for the answer. Um, does the community have more questions for Nick and Dimitar? >> I don't know how much the lag is in the live stream when they would hear your question. >> Yeah, I think I was checking the live stream too. I don't see more questions. So this is probably the the thing was saying here is probably up to date. Okay. Um we can give you another minute. If there are no more questions, we can probably just end the live stream. Um and if there are more question, we can always follow up offline. Yeah. Let's just give you one more minute. I have one more question. When do we uh when do we roundabout expect Mamir 3.0 to uh be released? Um I can probably chime in and probably Nick and Dar feel free to um join too. I think we plan to release it uh in about a month. So the community should expect to see at least the first version end of October, early November. Uh Matt has a question about uh autoscaling querers. Um I'm almost 100% positive that querers can be autoscaled. Um in the Helm chart and in the JSONet um I think we just auto I think we autoscale everything that's stateless by default. Um dimitar do you know? Um I think I think everything stateless is autoscaled. Um there should be a kada scaled object which then in turn creates an HPA but yeah it's not 100% complete. I think it's missing some docs if I'm not mistaken. Uh but it's it's basically the same query we're using internally and it's been working well. So you should be able to use it somewhere in the quer block. Quer auto scaling. Oh, not 100% sure right now. Okay. Um, if there are no questions, we can probably end the streaming. Um, if you have more questions, feel free to post it directly in the Slack community channel and we can probably follow up from there. And thanks again, Dimitar, Nick, and also Matt for joining the streaming and really appreciate all the answers you have given to um, all the questions in the channel. And I think that's pretty much everything for today and we'll probably have um another one a month or so and look forward to um see you guys next. Thanks everyone and see you soon.

