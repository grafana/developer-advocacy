# Grafana Agent Community call 2024-02-21

Highlights - Agent Load Balancing Traces 0:00 Presentation Links ...

Published on 2024-02-27T15:11:24Z

URL: https://www.youtube.com/watch?v=7c7iawdIPZY

Transcript: welcome to the February edition of the grafana agent Community call today we have Paul in here who's an engineer on the grafana agent team and he's going to talk to us about tracing deployments and I'll let him introduce it further um so Paul and take it away hello uh thank you Eric so yeah uh today I'd like to talk about uh tracing load balancing traces across multiple agent instances both in static mode and in flow and how static mode and flow are different and how flow can give you more options um and also after that we can go into some pitfalls um so uh yeah there are some Edge case with traces where people really need to be careful that the for example ta sampling samples correctly um um yeah otherwise it's very easy to configure a tail sampler and then later realize that it actually sampled the tracing correctly because the load balancing wasn't set up properly and then we can look into some limitations uh where really even the best load balancing strategies uh can't really help us right now uh with the collector um you know the old the um uh all the agent components for choices come from the uh open tary collector so any limitations of the agent in this case would be limitations of The Collector uh yeah and and and and so forth so yeah let's um again I prepared some slides so let me share my screen okay okay so this is just uh uh let's start with static mode loow Bing but I'm just sharing one tab how can I how can I share the whole window one second oh yeah okay I found it yeah okay I want to jump back and forth uh between tabs so okay so let's start with static mode uh this is maybe what some people who have used agent in the past are familiar with uh how is uh how are traces ingested uh and no balanced in static mode um so on the left here we have uh the application which sends the choices the instrumented application and it's instrumented with open Telemetry because the graph Tempo is open to Native the agent ingests and sends uh traces in theop format um and really here we have a cluster of Agents uh which are uh receiving via open Telemetry receivers tra uh spons so that's the first step uh the second step is is all those agents will then send those PS to each other so they will hash the trace ID normally and they operate sort of as a hash ring so um different uh spans will go to different agents depending on the trace ID and the goal of this is to make sure uh PS for a certain Trace ID are gener are processed by the same agent right so if uh Trace has let's say two spans both of these two spans have to go to the same agent we don't want to have one span for that Trace going to One agent another going to another agent so that that is what makes lad balancing tricky the reason why we need to do that sort of thing is because if you want to do T sampling for example uh the tail sampling just won't really work uh if it doesn't have the whole Trace um U yeah so for example you might have a rate limiter step in your T sampler where you want to uh limit the amount of spons uh for a certain trace and you limit that to 50 obviously the agent doing the tail sampler we limit those to 50 but if the same Trace goes to also another agent then that will also do another limiting of 50 and then your limit won't actually be 50 it will just be um you know something greater than that so uh so this is the second stage where agents distribute the work amongst each other then they sample it uh in the third stage and then they send it to Temple and this is configured in static mode via this slow balancing uh config option so the right now we're looking at the traces config of static mode and again this is the load balancing option it's a little bit uh it's it's an interesting configuration because it's almost exactly the same as the collector one it's almost exactly the same as the open collector but it has a receiver port and and that is the port on which agents receive spons from other agents right so in here uh this agent will receive let's say from this agent uh on that Port that's something that the collector doesn't have um right so let's go to flow actually does anybody have any questions so far okay um there will be time for questions after the presentation but let's go to flow so uh firstly I should say flow can do exactly the same thing that the starting mode does um if you want to configure your flow agent to do what static mode does you can no problem uh but it's not how most people do load balancing in The Collector uh and I would argue you probably should use flow a little differently from how static mode does so what does flow do uh I mean what can you do with flow that I would recommend I would recommend splitting uh into two uh agent clusters if you will so on the left we have the load balancing agents which are the ones running the low balancing Sporter on the right you have the tail sampling agents and the reason why you want to do that is to scale uh the different sets of Agents independently of each other so if you need a bigger set of tail sampling agents then you can scale that without impacting the low bouncing agents uh too much um and so forth and I think it's also just kind of you know um it's not the simple thing to configure it adds maybe a little bit Textra configuration but it's um I think simpler to reason about uh you get more separation of responsibilities okay so uh yeah again you can configure flow to do exactly what static mode does if you enjoy it uh you know there are benefits to it so uh you know um it's up to you what you like are there any questions about the part as well okay so um let's go to the interesting part which is the actual goas and there are quite a few of them and again uh this is the same with the collector the when we talk about any um um uh hard things to do with uh the agent in terms of T sampling traces it's basically the same with the collector it also can be a little bit difficult to tell sample um and Vol balance in a reliable way in a scalable way so um okay so first of all uh let's say let's say you're configuring um let's say you want to uh uh scale the number of sampling agents so these agents here on the right um You Want to increase uh or decrease the number of agents that you have there so if you go back to the documentation for the load balancer um but this time for the flow low bouncer uh you see that the low bouncer can accept a resolver uh that's how it f you find the agents to low balance too and an issue with the resolver is that when you have all these agents on the left side while the number of Agents on the right side is changing the numbers on the left might have a differing view of the agents on the right so one agent here will think there are five agents here on the right and another one would think there are four because it just didn't get updated yet right so you really want to minimize um that situation so I would say probably shouldn't be scaling too much uh try to minimize the amount of time um that you spend scaling and also if you're in kubernetes there is a kubernetes resol which you can use it's a very quick resolver because it doesn't do any polling there's no polling interval the agents get notified by kubernetes whenever uh a new agent pot came up so um the chances of uh these agents on the left having a uh or inconsistent views of the agents on the right are are much lower um but you know otherwise you have to use usually a DNS resolver and as you can see there is a resolver timeout here sorry resolver interval here um which is set 5 seconds so there's a chance that if you use the a 5sec interval in those 5 Seconds uh different agents will be exporting a little different and and in the case of a t sampler a span for a given Trace May no longer be going to the same tail sampling agent um right um another problem is that um let's say you are scaling the agents on the right uh but they still haven't finished sampling right so uh I I think in this case um I think the something might end up being a bit incorrect as well so ideally we should we should have a situation uh in the collectors where when we scale uh we probably want to let those agents finish sampling uh before shutting them down or before allocating that Trace ID to another agent and that still doesn't exist yet it would be pretty great in my opinion if it does uh but yeah that that's just something we need to do so these are limitations which are in the collector in the agent uh and hopefully things will get better I'm optimistic that we'll fix this uh soon but at the moment uh that's the current state first so you just need I don't think this is a showstopper for most people but it's just something to be mindful of so you when you are scaling keep in mind don't just scale all the time scale more rarely right um I've even heard of people say that they're only scaling up they they don't even scale down they just scale and that can work if it works for you then great because uh you that way you're also minimizing the amount of times you're scaling right so it it also helps keep things uh in a consistent state so those are the two main uh uh issues that you need to be aware of um now let's go into some more advanced cases uh we added a section to the auto Co exporter load balancing uh docs uh which is uh I think very helpful in helping you choose a load balancing strategy uh and here we explain how you can couple the low bouncing Sporter with other components um and those are all stateful components so here we have the tail sampling span Matrix Service graph actually sorry they're not all stateful um but tell sampling is uh [Music] and um and service graph is also stateful so when I what do I mean by stateful uh I mean that they need to persist some amount of information in order to make a decision in order to output something right so it's not like they just get some information and evaluate and output straight away uh they need to ingest some spans leave them on in memory for a while uh then wait wait for some more spans wait for some timeout or something like that and then make a decision based on that so um that's certainly what the TA sampling does again if you want to uh configure a TA sampler uh um you need a low bound answer with the rotting key of Trace ID uh for the same reason I mentioned so the sponds from the same Trace go to the same agent that's what this routing key of Trace ID means it means that um you get some consistency in what agent processes what Trace ID um now we already talk too much about those sampling so let's go into spawn Matrix span Matrix is is a way to generate metrix from spans those are so-called red metric she stands for rate uh rate error and duration so um so you get three different types of metric the rate metrix the error Matrix the duration metrix and um this I think uh it's not really a I don't think it's really a stateful component um um so the important thing to know about this component spawn Matrix component is that to be accurate it needs to go before tail sampling um so why is this it's because if you already sample some of the M spans if uh in other words if you already removed part of the spans then the spam metrix uh component just wouldn't wouldn't have enough information to work with right um you know it might be okay if you sample some useless spans like maybe you have a health check service generating spans and you don't want span metrics for that he check service go ahead You Know sample them out no worries uh but if you have a a real service and you want to generate span metrics for it uh definitely do not sample uh spans before the span metrix can do its job um right so um yeah I think that's the most important thing uh for spawn metric uh you also uh if if you read the docs I'm not going to go into every single thing that's in the docs uh there will be links under the description below uh but um basically with with span metric you might have uh different you know the service ID is a label in the metric and and so different agents might produce identical metric I mean identical metric series because there'll be although they'll be getting the same um well okay let me start from a little far right so let's say you configure load balancing with the trace ID different um agents will process different Trace IDs but they may have the same service name in those different Trace IDs and that means they will produce identical Matrix because the service uh name is part of the spa Matrix labels so this will generate an error if you remote the metrix because uh you try sending overlapping uh you know samples for the same Series so uh the backend database will get confused because at one point you say the value is one and uh then the value might be uh what uh you know something else so what you want to do to make those uh series unique is put a collector ID um label on your spa Matrix uh that will generate um obviously more cardinality the more the agent instances the more the bigger the cardinality uh but you can actually get around the cardinality problem there is a documentation page in the application observability section uh which can guide you to using a collector ID uh label with adaptive Matrix adaptive metric is a way to automatically aggregate all those SP metric from different collector IDs so uh then you you wouldn't incur extra costs uh for for the different you know through the increasing cality yeah S I can't explain every single thing in simple terms so I hope that this is clear enough H you can read the dogs for more information it's it's all in there and this is just an overview right so let's go to service graph a um service graph is very similar to spawn Matrix because in order for it to work you also uh want to generate Service cfts uh before any sort of tail sampling or any other sampling really so uh yeah it has similar limitations and again you can use a collector ID label because service graphs uh the service graph component generates metrix so both the span metrix component and the service graph component they both generate metrix um right so this collector ID label we just go to the service gr Magics yeah okay so I talked a lot about uh the low balancing Sporter but TR you don't even have to use low bouncing Sporter sometimes if you don't do any um tail sampling for example you may not need to use it uh for example if you choose to sample traces using a non stateful component like um let me there should be a [Music] probabilistic yeah okay this one so this probabilistic sampler um it just samples traces uh so response at random so um oh yeah actually yeah it samples uh traces so yeah as long as every agent has the same high seat then uh you can just sample uh perfectly fine without any low bouncing supporters okay so I think I already mentioned the main points are there any questions about anything yeah thanks Paul um yeah one one question I had is are there other you know components not that we need like a deep overview of but just that would be useful for doing Trace sampling in general or just you know Trace Telemetry in general yeah yeah I'll say so so probalistic sampler is a useful one if you can do sampling just based on that I would say go for it um certainly is possible you could also do head-based something instead of uh tailbase sampling which is um I'm not to Big expert on this but I think we can use for example a jagger remote sampling extension I think your the instrument application would connect to this and then um pull the remote sampling uh uh config and I think to choose to create or not create Trace uh based on that but I'm not entirely sure how that works to be honest um yeah so you could use that you could also use generally you could use anything in hotel call uh well sorry not anything uh but as again tracing uh in grafana uh products is open toage native so for uh uh tracing pipelines I would say you need to use Hotel Co components generally but not every Hotel Co component will work with the traces right there are some that just work for metrics and so on so read talks but if you're wondering what you can do with traces then it's AEL Co and the other one that you might need to use a lot is the transform processor which gives you a way to rename resource attributes add resource attributes um and and so on so really we could just work a lot with attributes so I think oh yeah okay uh the uh kubernetes attributes one is interesting the filter one is interesting but two so let's go to this as well uh so the transform one as I said it's for uh mostly just working with attributes adding them removing them modifying them and so on you could also promote a uh span attribute to Resource attribute and so on uh kubernetes attribut this can enrich a um um your uh resource your your resource attributes with texture kubernetes attributes like the kubernetes name space that you're running on and and so forth the futter one can uh I believe this one will um just remove spans so it it just drop certain spans based on your filter so this is also a good thing to do if you want to avoid Inta sampling let's say you have again a he check service generating spans then you can filter out all those spans right no using the auto Coos there's no need for a t sampler in this case and the batch processor is a very popular processor um probably most pipelines have this it can bundle up um dmetry not just traces but also logs and metrics uh in a batch so that then you could uh send it over to the database uh in a big batch rather than a smaller batch and this typically should go after any sort of sampling because you don't want to be spending all that time making a bdge only for the tail sampler or some other sampler to sample out half of the things in the batch right so uh you want the batching to be at the very end of the pipeline after any sort of filtering or something yeah any other question questions no other questions I will mention one thing but um before I do if there are any questions uh bym in the in the chat or raise hand and last call for those well I do want to mention oh oh we do have a question um we've got a question that says I have a question about using Tempo in multi-tenant mode I want to set the excope orig ID by the otel off headers component but it can only get the value from Context there isn't an option for set header value from attributes okay let's go to that component that's an interesting question so let's see it's this one all headers yeah I see what you mean is from attributes newish it might not have existed when we first implemented this yeah let me see um if that exists in The Collector sounds sounds like you should make an issue so we can track it and get that implemented for you because it if it exists Upstream um then we should have it in the agent yeah for sure sure good question um let checking that what I what I did want to uh mention is if you are working in static mode traces today we are actively uh doing development to to create converter support we do have that from static to flow mode but not four traces yet um so we are excited to get that out the door but it's under development so no promises on timeline but we're looking forward uh to that to make it easy to jump from static to flow mode to take advantage of the uh more flexible and uh additional features find Paul in any yeah so I where was that yeah I think we have the same features as The Collector yeah I think it's just a missing feature um and I think I was wondering about the same thing actually recently it does sound like a useful thing to to have uh and yeah I can I can double check after the coin raise an issue but I I don't think that it makes sense to uh to have such feature I I don't know how urgent it this but I think it makes sense if I understand correctly this is mostly for selfhosted um Tempo instance where you might want to include that header because I think normally on graan CL there's a Gateway which inserts that header but in a selfhosted uh temp mere uh you kind of have to insert the header yourself because there's no Gateway so that's why it mostly works with headers uh this they call headers just because the agent was designed in a way to take the header from the input from the incoming connection and put a similar header on the outgoing connection um but yeah uh there was no immediate need for getting the header from uh attribute inside the tetri so yeah I'll double check this and raise an issue thanks for raing it okay any other questions all right unless there's anything else you want to you want to close us out here paen but uh I think that's everything and nice work thanks very much uh yeah that's of me I uh would like to emphasize again this is a a very busy area of work right for constantly making Improvement so uh the main thing to take away from this meeting is really the links in the docs where you could find further uh further improvements and uh yeah happy sampling and happy tracing suggestion thank you not so all right thanks Paul thanks everyone and we will see you next month bye

