# k8s-monitoring Helm chart Office Hours 2025-04-25

In the April edition of the Kubernetes Monitoring Helm chart office hours, we discuss the recent updates to version 2.0, We also ...

Published on 2025-05-01T10:00:57Z

URL: https://www.youtube.com/watch?v=prFhW_vOR2w

Transcript: Okay. Hello everyone. Thank you for joining to the April edition of the Kubernetes monitoring helmchart office hours. I'm Pete Wall, uh one of the engineers that builds the Kubernetes monitoring Helmchart at Graphfana Labs. Um and I am excited to uh to show what we've been up to. We skipped uh the March edition. Uh so we've got two months of stuff to cover and uh the agenda looks actually quite similar. Um we've there's been a lot of work going on behind the scenes. Uh but I'm excited to share a little bit of what's happening in the 20 branch um or you know the main branch for releasing 20. Uh and then the version 2.1 that's coming up very soon. Uh give some updates on that including some more internal details about how we're going to be building that and how that's actually going to look when you deploy that onto your clusters. And then always we'll have time for questions and answers uh at the end. So Kubernetes monitoring helmchart v2 what's been happening in the last two months. Um this is a very short summary but I wanted to just highlight a few of the big things that have happened uh for the last two months. The pod logs feature we introduced an experimental uh option for gathering pod logs using the file log receiver. Flog receiver comes from the open telemetry community uh and it has support inside of graphana alloy. So using file log has a couple advantages or a couple options that people really enjoy. uh it gathers the log data in the open telemetry format and so if you want to keep all of your stuff in the hotel format this is a way to do it from the very start. Uh the other advantage that is that you can connect the fi the logs that come from the file log receiver with the kates attributes receiver. So that means you can enrich your log data with a lot of information about uh the kubernetes artifacts that generated them. Um it is experimental. there's still some things that we want to refine and change about this one. Uh so feel free to use it. Uh but we do reserve the right to change uh some of the way that that gets implemented and potentially even break some of the interfaces for that. But uh a lot of people have been excited about that. So uh let me know if this is something that's that's interesting to you and uh if you use it, please send us feedback. The next one for cluster metrics, we added the easy way to get prober metrics. So the kublet uh also exposes metric endpoint for probes. Uh think of the livveness probes and readiness probes that are on your kubernetes resources. If you want to capture metrics about the health and and success rates of those metrics, uh we have something inside of the cluster metrics feature that can automatically find those, scrape those, deliver those up to wherever your metrics destinations. Um, another change that we did uh in I think this was just a few weeks ago right now uh for the longest time in 20 we tried to be smart with placing the destination components. So with the Helmchart you can define your destinations and it's an array. Uh we tried to be smart with placing the the alloy components the component config for those destinations onto the alloy instances that were using it. And so the metrics destinations went onto the places that were generating metrics. The logs destinations went onto the alloy instances that was generating logs. And we tried to not put destination configs on things that weren't using them. Um often in computer science, the more you try to be clever, the more it actually causes problems. We decided to just say all destination components are going on alloy instances. What's the big advantage for this? The main advantage is that if you use the extra config section or if you do some other config modification that you now reference a destination that wouldn't have been placed there before. There was nowhere no way to force the Helm chart to say I really want this destination config to be on this alloy instance. You just had to redefine it I guess. Uh so now all destinations are available to you. Uh what's the overhead of this? There's going to be a minimal memory overhead just to load the config components, but if they're not being used, there should be no CPU hit or anything like that. They'll just sit there idle. Uh if you run into issues, I know we've seen some people had an issue with an upgrade. Uh if you run into issues, please let us know in the Kubernetes Slack channel, uh or file an issue in our GitHub. Um we've been really hard at work adding lots more integration and platform tests. Uh I strongly believe in test automation. Uh not only does it does it create living examples of how to use the Kubernetes monitoring helm chart. Uh not only are they great examples, but they are examples that we exercise all the time. Uh several times a day in fact. Uh but these tests help us verify the development that we're doing so that it actually works under the configurations that we expect. So we added an integration test for application observability. So using the app feature uh generating the host infometrics and making sure that those are generated and delivered correctly uh using uh Argo CD to deploy the Kubernetes monitoring Helm chart and so as an alternative deployment without using the Helm CLI uh we added an integration test to understand better how OTLP translation works. So you gather your metrics with the Prometheus style, you gather your logs with the Loki style and then you deliver them through the OTLP destination through like the OTLP gateway. What happens to the labels when they get translated into attributes? Um we added a new integration test to do patch level upgrades. So this is uh you know zero you know something n minus one to the current version. And so we wanted to make sure that uh upgrading from a previous patch release to the current release uh there's no surprises there. When 2.1 releases will also include one that goes from the uh the minor upgrade release. So something n minus one to 2.1. Uh and then finally we've added uh platform tests for EKS Fargate. So uh clusters that use Fargate nodes, how do those uh interact with the kids monitoring hel? And then also deploying using the EKS add-on. Uh we do publish an add-on to the EKS marketplace. That one is based off of version one. So just be aware of that. Uh but we wanted to add some test automation so that we know that deploying using the add-on is as reliable as as any of the our other deployment methods. As always, tons of dependency updates. Uh the alloy helmchart itself just released 1.0.2. So we are including that um lots of updates to KSET metrics and node exporter and Windows exporter and all of those things and we stay on top of those usually a day or two after they get released to the public. Uh and then also lots of bug fixes. All right, so version 2.1, where is this going? Uh we've been talking about it for a long time. Uh and I just want to give an update on what uh what we have planned. Um, I was really hoping to have it released this week, but uh, we're just we're taking a little bit more time to make sure that everything is working the way we expected to. So hopefully by next week we'll have it released. Uh, but I want to highlight some the key features that we're going to be including in 2.1. Uh, alloy operator. In uh, two months ago when we talked about this, it was called dynamic alloy deployments and now we're calling it out. There's a alloy operator. Uh, and I'll talk a little bit about that in the next slide. uh also in 2.1 tail sampling service graph metrics and then we're going to be adding some more service integrations into the integrations feature. So what is the alloy operator? Uh so on the left side this is how we deploy alloy instances today. We have the kubernetes monitoring helm chart which is a helmchart and it actually bundles the alloy helmchart and and deploys five copies of it potentially based on what features you've enabled. Uh this has been working great, but by the nature of how Helm works, uh we have to define all of the things we want for the alloy helm chart at the very in the values file. So if you look at the current 2.0 values file, I want to say it was something like 80% of the lines were dedicated to just defining alloy uh of the different alloy deployments we have there. And that's a lot. And especially when the effort, a lot of effort went in when making 2.0 to reduce the values file into just the things that you care about. Uh there's a lot that's going into defining alloy instances. And I feel like that kind of overwhelms when people start to look at it. Um, and so one of the things that we're doing with 2.1 is trying to do for to a to an extent do for the alloy instances what we did with destinations. Show what you need to know. Show what you are interested in in being able to do. Don't remove any of the functionality. You'll have still all of the the ability to customize and tune the alloy deployments, but not have to show every single switch and feature at the at the top level. So instead in 2.1 we are not going to be using the alloy helmchart directly. We're going to be using the new alloy operator helmchart. It's a a new project that we're building. Uh and what this allows us to do is it creates a custom resource definition for alloy and then the kates monitoring helm chart will then deploy alloy kinds. So instances of the alloy kind. Uh and that then with the operator will deploy the alloy instances basically identically to how they were before. My goal is an upgrade from 20 to 21 will look seamless. Uh it will result in the same pod footprints and everything like that and all the things will work exactly the same. Uh it's just the method by which these alloy instances get deployed will look a little different. Um so why are we doing this? What's the advantage? First off, like I said, it reduces the overhead of the values file. It kind of condenses it down to the things that you care about as users of the Helmchart. It also allows us to do a little bit more dynamic deployments of Alloy. And here's a great example. One of the things that we're going to include in 2.1, if you are using the application observability feature, you know that one of the things you do, you enable the feature, you know, Appali enabled true, then you get an error message that says, hey, if you want to use Appal, you got to turn on a receiver. So you go apply okay receivers OTLP or gRPC enabled true and then it goes hey if you want to use gRPC you need to open up the gRPC port on the alloy instance and you're like oh my gosh this is now the third error message so hey alloy receiver extra ports you know gRPC is on now what we do since we have much more control over the alloy deployments when you enable the gRPC receiver in the appoly feature we'll automatically open the port we have the ability to do that now so there's there's going to be much more of a rich uh interaction between the features that you enable to the deployment that actually enables those features to work. So, I'm really excited about this one. This one's really cool. Uh one of the other main reasons why we wanted to do this is that for the next big feature, so for tail sampling, um it's really going to come in handy. So uh I wanted to show a little bit of how tail sampling affects the deployment uh of the kates monitoring helm chart and of the topology on your clusters. This is kind of how it is today. If we enable the application observability feature, you get alloy receiver uh apps on your clusters will send their traces to alloy receiver which we do some processing in our pipelines and then we deliver them to tempo uh to store the traces. When you enable uh the tail sampling, what we'll do is we'll automatically this will be invisible to you. We'll automatically deploy another alloy instance called uh called the tempo sampler. And this will be tied actually to the destination. So you can enable tail sampling rules per destination if you have different ones based on where you're sending them to. uh an and a potential use case for that would be if you have tempo oss locally deployed on your cluster you want to send all your traces there but then you want to send just subset of the traces to offsite storage or something like that you'll be able to define different rules for different uh trace destinations so in this case we will enable the tail sampling on the temp tempo destination and so automatically the case monitoring helm chart will deploy the sampler uh we'll configure the pipeline in there to do the sale sampling rules and then we will rewire the destination from alloy receiver to the sampler and then the sampling rules will take effect there and then finally forwarding off to the original destination that you specified. Um I didn't have the values file snippet that would enable this at the moment. Uh but for the most part it will be two or three lines. Tail sambling enabled true. Uh and then here are the policies that I care about. So we're hoping to bring the same level of uh ease and uh delightfulness of uh of enabling this you know previously complex feature um into Kates monitoring 21. The other big feature that we're adding to this was service graph metrics. This will behave similarly but just slightly different. So service graph metrics will be an additional feature for the application observability feature. It's not tied to the destination. So in this example uh app you know connector service graph enabled true or something like that for the values and so then what will happen is traces will be both sent to the destination as well as to a service grapher alloy instance that again we're going to be deploying because we'll have the ability to dynamically deploy alloy instances. Um and then service graph looks at the traces that are generated from all of the apps on your cluster and then it generates metrics about them. And so you can see then what will be really interesting is that the al the new alloy the service grapher will accept these traces. It'll generate metrics and those metrics will then be forwarded to a metrics destination that you have in your list. And so in this case it's you know mamir on graphana cloud but it could be a locally deployed mamir or locally deployed prometheus or anything like that. Um definitely you can use both of these features together. So you could deploy service graph metrics you could also deploy tail sampling together. Uh and we'll automatically wire all these things together in the way that we know how. Um I'm really glad to have Robbie Langford as a part of the Kate monitoring helmchart team. uh if you've used the graphana sampling helmchart that's in our graphana helmcharts repo uh he is the original author of that one and so I've got a great support and knowledge in Robbie to help make sure that all of this stuff is implemented um the the correct way. So I'm really excited about that. And again, because both of these features deploy new alloy instances, we what we really didn't want to have is have a sixth and a seventh alloy in the values file and it's just going to get ridiculous. And so getting the ability to use the alloy operator to dynamically create and deploy these alloy instances really gives us the flexibility to deploy these features with the ways the simplicity that you expect as well as the full featured and scalability that you expect to. Uh so again here's the summary of what we have planned. Uh alloy operator tail sampling service graph metrics and a few more service integrations. Um, again, we're trying to target this for next week. So, keep your eyes open. Uh, and that's the end of the prepared stuff. So, I'd love to open up for questions and answers at this point. So, just feel free to unmute, ask your question, raise your hand, or ask it in the chat. Hang on. All right, I'm going to take a drink of coffee. So maybe I have a question, Pete. Um, hi Busty here from solutions engineering team. Um when we do sizing for customers um often struggle that they send the data to us. Have you ever thought about a configuration where we would just do like some sinkhole configuration that we have where no metrics are sent to Graphfana cloud but we would be able to get the information from from the alloy instances running about how many metrics would we be sending um and we just have a sinkhole configured that where we just have a dry run and we don't need to do a full-blown POE with the customers to to get the the sizing done correctly. I see that's a really great idea. So, let me let me restate just to make sure I understand what you're saying. You want to be able to deploy the Kates monitoring Helmchart with all of the features that that you eventually want to use with the customer, but rather than actually delivering the data, the metrics, logs, traces, profiles to a destination for storage, you want to send them to some sort of intermediate like a metering thing that measures, understands the full active series, the full DPM, and then you can get a an estimate of that and then you can decide, okay, do we need to tune this further? or do we need to uh or is this good and we just turn it on for real destinations? Actually my my my thought was about um alloy will have the information how many metrics it has scraped. So this internal metrics they can be forwarded but everything else just like have a re label rule just drop it. Mhm. Yeah. you know, you know the Prometheus relay rules that you that helps you just dropping the data before sending it and just keep the stuff the the internal like scrape sample scraped all that stuff and um just have it like in a dry run mode or in just an estimation mode or whatever we call it. So yeah, that's a really that's a cool idea. So basically like a null destination and so all things can go to that but then you can use the alloys own internal metrics to get the information that you need. 100%. Yeah, I like that. Yeah, we should definitely be able to do something like that. Um let me make an enhancement uh ticket on the GitHub and then I'll send it to you and make sure that capturing your ideas right. Uh and then it shouldn't be too too too tricky to implement something like that. It's a great great idea. Thanks. Yeah. Any other questions? Um, I'm just starting to to use the auto instrumentation with BA. BA. Mhm. I don't know how to pronounce in English. Got it. Yeah. Ba. And I I was wondering if uh because I've just started just 30 minutes ago and I'm testing um does the OLT um do I need only one destination which is OLTP and so it can send the the metrics the logs and the traces or if I because at the moment the way I have um defined my destination is I have one destination for trace one destination for metrics and one for logs. If I at the moment have given him the three destination, will it work? Because I don't know yet if it's working. I'm sure. Sure. Okay. Yeah. Okay. No, that's a great question. So, um and and there's two things that I'm going to highlight from the Helm chart based on that question. So, thank you for the question. Um what does BA need? What sort of destinations does it need? So, BA when you pair it with the application observability chart or a feature sorry um it will generate both metrics and traces based on the the the applications that it finds on the cluster. It'll automatically find applications and scrape those or or generate information automatically instrumented. That's what it does. Um if you uh so what it will do is it will generate metrics. those metrics get scraped by alloy metrics uh and then those metrics are in the Prometheus format. So my recommendation is to keep uh a distinct destination in the with the Prometheus type. Uh that way you don't have any issues with translation from the Prometheus format metrics into open telemetry. Um it also generates traces and those traces are directly sent to the alloy receiver instance. uh and those ones are in open telemetry format from the beginning and so I I would recommend keeping your three distinct destinations uh one for the metrics uh the logs um I don't believe that there's application logs that get generated from from bail uh but one for the traces uh and so that way everything kind of stays in the same ecosystem that uh where it was generated. Um, the other point that I wanted to make, and I think this is a great time or a great segue into talking about this, uh, is there's a little bit of a subtlety to the way that the Kubernetes monitoring Helm chart assigns the destination to where to send the data based on where it came from. Um, if in your in your Helm chart in your destination list, you define I've got a Prometheus type destination, I've got a Loki type destination, I've got an OTLP destination for traces, and then I also have an OTLP destination for metrics, logs, and traces, all three at once. Uh, maybe it's a OTLP gateway or something like that. The way that the Kubernetes monitoring hels by default is it will try to match ecosystem to ecosystem. So if you have metrics that are generated from Prometheus style things, so a Prometheus scrape, it will deliver them to the Prometheus type destination. It will try to keep it in the same ecosystem and not do translations. If you have metrics that are pushed to alloy receiver uh and so they come the metrics come in the OTLP format, we will keep them in the OTLP format and we will prefer to send them to the OT the OTLP type destination. um we won't multiplex uh in this circumstance. There are for every feature in the values file you can manually specify the destinations uh and overwrite the default destination. So for cluster metrics or something like that you could say I want to send all metrics to both the Prometheus destination and the OTLP destination and then we will do the translation to OTLP for that delivery. Um so there's a little bit of a subtlety into that. uh there is a documentation in the Helm chart that explains uh the reasoning for that. It's trying to avoid the uh the translation which uh often is imperfect. Um so just be aware of that. All that to say my recommendation is keep the distinct definitions, the destination definitions. Um and I think you'll have you'll find you'll get more out of the data that gets eventually stored into your destinations. Okay. So from what what you've said I just got that Bailey just a scrape but it doesn't send the data. He send the data to the alloy receiver in OLTP. Yes. Yeah. So BA generates metrics and traces. The traces are pushed to alloy receiver which then gets processed and pushed to your destination. The metrics sit there and they're scraped like any other metric source that you have on the cluster. So alloy metrics will find them, scrape them and then deliver them to your metrics destination. Okay, thank you. Good question. I still I still need to have a look of the example with all the traces. I try to understand what what is done from traces to metrics but uh maybe that's would be for another time. Sure. Yeah. And and you take a look and feel free to to ask questions on the Kubernetes channel on the public graphana slack. Um, okay. And uh and we'll be glad to help you out there. Thank you. All right. Any other questions? Good questions today. Great. Okay. Um my hastily added slide. Uh if uh anyone uh here in uh on the call live or is watching this later, uh if you're in the Minnesota area, I will be at Open Source North in St. Paul on May 29th. Um and so Grafana will have a booth there. I'm actually giving a talk. Uh and so feel free to stop by, say hello, bring your questions there. I'd be glad to see you. And then finally, the wrap-up slide. Thank you everyone for joining. Um, again, we build this project based on community feedback. Uh, this project is meant to make your jobs easier. It's meant to deploy a really fullfeatured experience and solution for gathering observability telemetry data. Uh, and we try to make it so that it actually solves the problems that that you're facing. Uh, so please bring us feedback. Uh, bring us comments, bring us the things that you want. Graphano.slack.com is our public community. Slack and we're often in the Kubernetes channel. Um otherwise our GitHub graphana monitoringhelm file issues uh send PRs that would be great too. Uh but we really thank you for joining this community call. Uh and uh have a great rest of your day. Take care

