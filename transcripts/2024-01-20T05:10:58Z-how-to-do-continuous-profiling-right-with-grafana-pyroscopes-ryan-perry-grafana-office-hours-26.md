# How to do continuous profiling right with Grafana Pyroscope&#39;s Ryan Perry (Grafana Office Hours #26)

Ryan Perry, co-founder of Grafana Pyroscope, talks to us about how to do continuous profiling right. Ryan is also an Engineering ...

Published on 2024-01-20T05:10:58Z

URL: https://www.youtube.com/watch?v=pzNRvMQzrJ0

Transcript: 35 hello everyone and welcome to another Gana office hours I am Nicole vonu I think this is our second episode of the year is it is that right is it just the second feels like it's I think it is okay well today I've got my co-host hello I'm Paul bog another developer advocate here at grafana and also for the first time we've got Ryan Perry yay welcome to the stream thank you for having me excited to be here who are you anyway yeah who am I uh yeah I mean I'm I'm Ryan uh yeah that's a deep question how deep should I go but uh but yeah no I'm Ryan I'm one of the founders of pyroscope and uh yeah we started pyroscope a while back obviously um you know we have since joined grafana there's I'm sure we'll get into a lot of the reasons why and all that stuff but yeah I've been been working on continuous profiling for a while now um been worn a bunch of different hats software engineer product person um salesperson Etc and so uh yeah I've kind of looked at profiling from a lot of different angles and uh very excited to talk with you guys about it today so why is that how did you get into continuous profiling was that like always something that you were into or is that just a recent evolution of things that the path that you were already on yeah I mean when I was seven I actually no I'm kidding so okay I thought this was a prodigy yeah a true Prodigy I just wanted to be a profile right always my call yeah exactly um no I um so I kind of got into continuous profiling so I worked with uh Dimitri who's my other co-founder um at a at my first job in the software world I did like a coding boot camp I guess if I go way back I went to business school and then I was like I don't know software seems cooler more fun whatever and so then I did a coding boot camp ended up in the software World got my first job at this company where uh we did a lot of performances intensive work and that was where we started using profiling Tools in order to cut costs and um improve the performance there and that was where we kind of the seed was planted there of like huh this is like pretty cool we built like a very hacky version of what now exists as pyroscope at that company and um it's called sensor Tower they do like mobile app analytics stuff and uh and yeah and so we we built a hacky version there and then we were like you know we saw that like lowkey had a database for logs you know Tempo had a database for profiles I mean sorry for traces and so then we were like okay there's no profiling database that exists if we could figure out how to kind of collect this data continuously we could kind of bring the benefits that we got to everybody and uh you know it's kind of every uh engineer's dream to have a really cool open source project that that becomes popular and so we kind of took a crack at it and um you know kind of the chips fell in the right direction awesome and all at seven years old that's crazy yeah right all this was at seven years old no this was uh post [Laughter] College real Prodigy um so you actually built what became pyroscope just to solve your own problems yeah yeah definitely so um so yeah like I said so there's you know there's different kinds of profiling right so it's like there's a lot of Open Source profiling Is Not A New Concept right it's like you know something that has been around since like you know since I was seven even before then and um but it it existed in a much different form where you would kind of maybe like SSH into a machine that wasn't you know functioning the way that you expected it to you would kind of like grab a a profile uh which I guess we haven't explained exactly what it is it's like a flame gra it's a snapshot um a way I often describe it as like a very fancy pie chart that tells you you know which lines of code your application is working on and so you would kind of do that in an ad hoc way occasionally but like let's say something happened yesterday you know you got page for it you wake up this morning everything's on fire if you didn't grab the profile at that time you know you you've missed out on like the information you need in order to resolve the issue and so that was kind of the you know Innovation or enhancement that we made to this is making it so that you're always storing this data over time so you can like go back and you know compare last week to this week or go back and see what was happening yesterday or or whatever it might be that's cool yeah I know back in back in my day I had to depend on a jvm heap dump and uh you know Analyze That and hopefully you caught it at the right time right which you never did you know like so I I come from the testing world and what I would do before running a performance test was I'd have to like ssh in manually to each one and be like okay start because we can't leave it running forever like I remember once I forgot to stop it after the test and it basically just froze because of all of the data that it had collected I'm like well that's gone great now I have to restart the test I guess you know and that's why that was part of what the pain was for me and there was also significant overhead so it was very I always tried to to make sure that the collection itself didn't cause that component to to overload um there's so many issues with it but like how did you solve that yeah so that's actually um yeah that was kind of the key piece was it was like okay can we find a way to get this information in a like low overhead way right and so I would say historically you know yeah you know back if you were doing those Heap dumps a long time ago and and even even as recently as maybe like five or six years ago the most common way of profiling was yeah you would you know hit a button you would start it would maybe like hook into like the method invocations and say you know start the stopwatch stop the stopwatch you know this code was running for this amount of time and that yeah if if you're doing that all the time that can be very expensive the way that um the the kind of improvement that was made with a lot of these open source profilers that didn't store the data but at least got the data in a low overhead way is that they from a separate process they look at the stack trace of your application um you know x times per second usually like the default something like a 100 times per second and it's able to then basically collect that data without blocking your application you know if you do leave it running forever you know it's just kind of a very minimal overhead of it just kind of looking at the stack trace and saying what is at the bottom of the stack Trace right now and what does the you know the full chain of functions look like and then it kind of you know morphs that into a a tree and then a human readable form of flame graph and so basically a lot of these on the client side uh you know really low overhead profilers exist and then becomes a matter of how do you store this data efficiently and how do you query it efficiently which we did a bunch of things to uh to make that more efficient as well well you mentioned the VIS visualization of it and that was also the problem because even when I did get the metrics it was like okay what exactly am I looking at am I looking at the highest point or am I trying to get the mean in some way or like what is it should it be by time because this script might be doing and touching different components at different times so you have to make sure that it's at that specific time that you're measuring it or do you plot the whole thing on a logarithmic scale to reduce the influence of outliers and now it's just completely different now it's like flame graphs that you're comparing how how does that work I would say it's not completely different I mean I still think that there is I mean yeah there's still kind of that element of time but yeah so you're getting these flame graphs in different time chunks and then basically we store them at their most granular level and then at query time we'll then aggregate them into the you know time period you're looking at so if you're like oh we were experiencing this issue for a full hour or a full day you can select a day worth of data and get one flame graph for that entire day and then maybe you care about you know server a versus server B you know similar to Prometheus or Loki or whatever you can you know add these metadata to the profile so that you can say not only do I want to see the last day but I want to see the last day for this specific server this specific you know uh region or pod or namespace or whatever uh labels or metadatas that you want and and so you know it still does have that time element a little bit but um but then it at the the end of the day it kind of boils down into the actual flame graph itself and you know understanding the performance attached to uh you know those two Dimensions either time metadata or both yeah the flame graph as a visualization um method I guess wasn't really something that was on my radar so it always felt like either a snapshot or something that was so heavily aggregated that it lost a lot of the detail flame graphs seem to be like a compromise between those yeah yeah it's and so it's so yeah in this case it's yeah that that's the nice thing about flame graphs and profiling is that you can get that granular level of detail or you can zoom out and so that's kind of the the key piece that's why we called it pyroscope you know it's like pyro flame grab scope you can kind of like it's like a you know microscope you can zoom can zoom out and so so yeah so you can get that that code level detail you know for it anything down to even a specific API request that lasted you know let's say like 200 milliseconds you can get a profile for that or you can get a profile for your entire system you know like we we use it at grafana I can get a profile for every single line of code that runs at grafana which is a lot of code and see it in one giant you know honestly very hard to read flame graph at that point you know but we we've done it before and and sometimes it actually is useful because there's some like libraries for example that we use across the entire company like obviously um or maybe not obviously like lowkey mamir Tempo pyroscope we all share certain pieces of our architecture and so you know for something like that looking at a flame graph for the entire you know of entirety of all of our databases we can see like oh if we want to you know cut cost 10% there's this chunk of the collective flame graft that's consuming a lot of resources maybe we should go look at that and be able to improve that which which we have done a couple times yeah there's I don't know it flame grafts are just those visual representations that are like the coolest thing ever I mean they're even great on t-shirts oh wow I have that well you know you had to be a cubec con uh North America I was there at the booth and I was hanging out with Dimitri quite a bit enough man yeah yeah so I snuck one of these These are the old ones I think so I I gotta get you the the updated swag we we've got a couple of uh yeah that's been one of the best parts about joining graas they got some or we got some crazy designers so we got some really cool yeah swag hopefully you make it out to the next cubec Con in France or the next time I'm with you in person I'll make sure bring me bring me one I would like one cuon I just have the the gr one right now there you go the lights I still need to get one of those I I have yet to get one of those yeah we need we need to get a combined one because uh that was our uh I don't know if you guys saw that was our hackathon project was H flame gr AI so it was like we had gr nice for flame GRS and so I want to do one that has uh you know flame grab so tell us about grafana like how how did you come to join grafana and what has that changed yeah um so yeah it's been interesting so yeah so we so once we started pyroscope we started it back in January of 2021 and uh we joined y combinator so we were like going through that like accelerator and stuff and and yeah we we had uh big Ambitions of you know building like a really comprehensive observability Suite you know obviously we were starting with profiling because that's what we used and were familiar with but we eventually you know the initial plan was like okay we'll start with profiling and then we'll expand into Lo or expand into traces expand into logs expand into metrics and then as time went on honestly we just like we were like we really like profiling and there's a really good solution out there that already does logs metrics and traces and uh and so we you know we've been talking with grafana from the beginning um because I mean even when we were just pyroscope on our own uh everybody would ask like okay I have all the rest of my data inside of grafana like where you know is there some way I can get my profiling data in there too and so we had a grafana plugin and and all that stuff and so you know we always knew that there might be a chance and then yeah honestly the um you know everything kind of fell the right way where um at a certain Point uh grafana had released flare there uh which was like you know basically a uh similar thing to pyroscope a profiling database and a lot of things were like really aligned and we were like okay well you know if we're all working on the same thing like why not work together and so we um graffa had reached out to us and you know we were super interested and so yeah we we kind of got together with the flare team liked what they were doing they liked what we were doing things complemented each other in a really nice way like we had focused a lot on the instrumentation side on getting all this profiling data into the database and then they had you know kind of that um architecture that was inspired by mamir and Loki and Tempo that you know was really proven at massive scale and so we kind of just combined the two of those and then we had the visualization and put all of it together and and it all just like worked out everything um you know fit together very smoothly once we joined to the point where yeah we were able to release pyroscope 1.0 released the ga of uh graffan Cloud profiles all within like a couple months because everything was just so compatible already yeah no that's cool yeah and I thought I had heard something about that that uh you know you there was that uh little modification on the back in for the storage of the uh the actual profile data to kind of bring it in line with the rest of the uh the lgtm stack that's cool that's that's good it was uh it was able to be done I mean because because originally where you guys was using like uh was it influx DB on the back end or something we were using yeah we were using an open source um like the key piece was like our object storage and so we actually had like two different architectures like one was separate for like the cloud one was separate for open source and essentially it was called Badger DB and Badger DB does not scale horizonally so basically you could only grow pyroscope to as big as a you know hulking instance you could get from AWS or whoever you use so that's like nice but yeah it's like obviously you know at a certain point if our whole thing is like we want you to do continuous profiling and then you're like well now I can't scale my database like that you know that's a huge issue and so yeah um that was the biggest thing that we were able to pick up once joining Gana is that now it's horizontally scalable um you know it will literally scale to effectively any any amount of data that you're willing to send to it yeah that's awesome yeah and that's definitely been proven it seems like as far as the storage capacity of those backends oh yeah absolutely um yeah we've seen some pretty uh both in in our cloud and then also just people running at open source we've seen some pretty um massive deployments of continuous profiling um yeah you know certain industries use it more than others I've noticed like ride share like there's a lot of ride share companies that are using it pretty heavily um fintech you know areas where like latency is super important um you know those are ones where we see people really uh leverage continuous profiling super heavily huh so Ryan I'm going to ask some basic questions here and you can always um correct me if I'm wrong because I I really don't know that much about this that's why I wanted to have you on here to ask you my questions um let's do it it's it sounds to me like continuous profiles are metrics is that an oversimplification are you collecting metrics with certain tags they are I think at its very I mean yeah philosophically at its very core you know everything could be a metric I mean I would say yes say ultimately what a what a profile is is a metric you know at the the line level for some sort of resource usage and so you know if it's CPU for example when you're looking at a profile and um you know I can show some if you'd like but yeah when you're looking at a profile the metric that it's it's counting is you know what amount of time for example was spent on this specific function and so you know you're getting that just in a very condensed way where you can see it all at once so you know the same way that a pie chart is technically you know the underlying metrics of the data that informs that pie chart the same is true for a flame graph or a profile it's you know the underlying metrics of the data that populates the flame graph I guess my follow-up question is why do continuous profiles have to be a separate Telemetry signal why does there have to be a separate database for continuous profiles instead of just pushing everything to Prometheus which I believe pyroscope can do right yeah so so yeah so that is because the if you just pushed everything the amount of data that you would be sending is like pretty astronomical so in storing it and saving it as a collective profile so um as a set of metrics you are basically saving you know saving it in the form that ultimately it needs to be rendered in right like if you were saving all these individually to them build a profile you're going to have to query a you know a bunch you know some profiles can have thousands of different you know metrics all in one profile right and so if you did put all that into like Prometheus or some metrics database now you have to not only query all of those separately but then you have to spin the CPU to you know piece them together and merge them and and you know make them look pretty and color them and all that kind of stuff whereas if you store them as a set you can kind of maintain the relationships of like a to b b calls c c calls D and it allows you to basically just keep things much more Compact and and make a database that is specifically oriented towards you know this kind of like relation you know yeah kind of like relation based data where where things are related to each other okay well I would love to get to the demo for how to how pyroscope works and then maybe after that I'll see if I still have questions and we can Circle back around to it let's see some flame graphs yeah um let me share my screen here um yeah okay yeah there's a bunch of things in here yeah so before I go like too deep I will show just like real quick like what you know we've been talking about flame graphs in case anyone has not you know seen a flame graph or or knows how they work you know so basically you know let's say you have a function here that uh you know is main that's going to be the top of your flame graph and the way that you read this is that the width like I said is like 100% And so if the you know the width of this you know everything starts in the main function so no matter what's happening that originated from this function and then you have you know do a lot do little and then each of those call the function that was uh prepare and so you know here you main calls do a lot do a lot calls prepare and so the way that shows up in a flame graph is that main calls do a lot which calls prepare and then it also calls do little which calls prepare and then you know here so for example let's say you're spending $100 on your servers 65 of those dollars are spent running this function and 26 of those dollars are spent running this function and so or if you're looking at it from a Time perspective 65% of the time is spent doing something doing a lot and 26% of the time is spent in this function where it's doing little so uh I just wanted to show that of how the flame graph itself works but yeah so I'll go through like a quick demo um you know there's a bunch of slides here but um you know for this demo I I just mentioned that um that ride share companies tend to get a lot of value from profiling so for this demo I I'll show kind of in a ride share World um you know what what this might look like but the same you know it's kind of generic so like the same thing could be applied to anything and so the way pyroscope works is let's say you have three different servers one in the East one in the north one in the South and you add the region label attached to to those and then you have three different routes you can either order a car a bike or a scooter and you add the vehicle label to that and so the way continuous profiling works is it's going to collect all of this um all of these profiles periodically um every 15 seconds by default but you know that's configurable and it's going to store them on the server again with this Associated metadata where it can be you know queried and um yeah where it can be queried so the way that shows up inside of grafana is um so there's there's basically three views I'm going to start with this one just because it's uh most closely related to what I was just showing you so again there were those three regions you have east north and south and so what this does is is now you can start to get more insights as to where your resource utilization is going for these different um you know for these different regions for these different servers and so here what we're seeing is that the north region is we're looking at CPU we've um you know we can Group by a number of different labels here uh right now we're grouping by the region label and here I can see that 63% of the CPU for our whole application is being spent on the North Region um 16% on the east region 20% on the south region and so if I look at you know each of these I can see the flame graph associated with each of them and so not only do I just know that 63% is spent on this region but of that 63% I know that most of the time is spent in this specific function 87% in this case and so that's really useful again for if you're trying to cut costs or maybe there's an incident happening and these all should be equal but for some reason there's an incident causing this to consume more res resources and so one of the big benefits of pyroscope as I mentioned before was is being able to um let me hit refresh here um being able to then kind of compare and uh Analyze This profiling data so let's say I want to see what is the difference between you know this blue east region and this green North Region I can hit the two of these hit compare tags and basically now what this is going to allow me to do is sort of you know zoom in I can select different time periods and different labels so what we're looking at here is now we're seeing those flame graphs side by side on the left side we've selected you know 1030 to 1035 for the east region on the right side we've selected you know 1050 to 1055 for the North Region and I can start to look at these profiles side by side and kind of get an idea of comparing one to the other um you know here I can see that this function is consuming more resources you know widthwise than this one is um when this is your code obviously you recognize it more um but one of the really nice benefits is I can also go to this View and so now let's say I do the same thing where I select the um whatever it was the South Region I select the uh North Region I hit execute and so what this is going to do is now oops uh what did I say North Region um is so now I'm compare taking those two flame graphs that we were looking at side by side overlaying them on top of each other and similar to how you get like a git diff it's going to tell you okay this is what the difference is between these two functions and so or between these two flame graphs and so in this case that flame graph on the left that was you know functioning and consuming less resources was spending 160% less CPU on this specific function and so if this were an incident you know now I need to figure out exactly what I need to do to go resolve this incident I jump in here I see you know what's shining in bright red and I'm like okay clearly the check driver availability function is causing an issue that's caused by a mut text lock and now I know exactly where to go to fix that issue whereas before profiling you might just have metrics and know that something's wrong but not exactly which lines of code are associ with that issue to then go fix it um yeah let me pause there does that make sense yeah that looks good um can you can you talk a little bit more about the this diff view uhuh yeah so trying to explain it because you know flame graphs are already difficult to to explain but you have to read the flame graphs and also learn to read the diff of a flame graph it's little meta yeah so so yeah so the flame graph itself like I said so the width is representing the amount of resources that are spent on a certain function and so what this is doing is taking those two side by-side flame graphs and then you know yeah putting them on top of each other and so really the way to read it is um you know kind of relying on the colors um green means good in in the sense that you've um slimmed down the amount of resources you're using on the green areas and red means bad in the sense that from the original query you know this like Baseline query here to the comparison query you've added more you know CPU time or memory or whatever it might be and so um you know typically how I read this is I look for the areas that are in red and I go in here and I say okay so you know uh looking at the tool tip here the Baseline was was consuming 30% of CPU on this function and the comparison was spending 79% of CPU on this function and so what we're calculating here is the diff between the two of those and you know kind of highlighting that I can actually also show a real world example of this um that might also kind of show it so um so yeah so this is an example of an actual incident that happened at grafana um this was like a while back when we had just joined grafana and grafana had just started doing um whole system continuous profiling all the time so this was an incident we had a customer that was very unhappy you know incidents happened um but yeah and so in this case you know there was a specific uh let's see this was you know these this blue line here is status code 400 you know obviously we're using metrics we know that um you know things started going wrong but yeah I would just focus on this blue line here from 2120 to 2135 and then again we tried to fix something but then it broke again from 2140 to 2150 and then again and so you know while this is happening you know again this is just purely metrics when you have the diff so let me show you what this looks like with continuous profiling is you know you see these same spikes on here I don't know if we're looking at the same thing actually oh oh oops I just realized I'm only sharing my screen and not the uh full screen um let me let's see stop screen let me present share screen share screen live demo for you yeah that's always the way though like I want to is it is he purposely being vague because it's a real production Incident That's what I was exactly that's what it was so what do you see now now do you see a full screen we do yes yeah sorry so that's what I was explaining so yeah so this was um you know an incident that was happening we had a bunch of uh you know errors like I said you know I kind of just went through this but yeah the blue line here bad blue line here worse and blue line it's getting better but still not great and so you know this is sort of without profiling you know this is kind of you know where things end off and so to answer your question about you know the diff and where the diff becomes useful is that you know I kind of want to know you know this is before everything was working fine and then after everything started breaking and so what the diff allows you to do is you can select this before period where things are working as you expect them to be you can select this after period where you know now we've selected this Spike here and now you don't just know that there was you know a spike here and a spike there but you specifically know that the difference between when things were healthy and when things started spiking is that we spent an additional you know 62,000 per on a specific uh in this case a async QBE process Loop and you know some Loop gone wrong doing something shouldn't be doing and we were able to then you know go in and figure out exactly what was wrong and fix the code related to this issue because again this diff is showing in red what the problem is and so you know we can go in and fix that wow that's really cool and you're saying I also actually didn't know this do we just have continuous profiling running on everything in production at all times everything on and like all apps all uh all services all apps I think we um you know yeah in this drop down there's just like you know five or six because this is our uh you know just like a demo application but in um in grafana I think we have like 900 things in this dropdown and so every team you know again when something's going on when they're doing a performance Improvement or when they're um you know trying to debug an incident you know they go into this dropdown and just select whatever service it is and and the way we do that is um we actually do two different types of profiling we do um whole system profiling with ebpf which you just kind of add it and it just profiles everything automatically adds all the uh we use kubernetes adds all like the kubernetes metadata to all of the profiles automatically um but we also have uh you know our we use go for most services and so we also have a the graphon agent which scrapes those go in point so we kind of have two different versions of the profiling data to look at depending on the different situation okay so it's also like what you're talking about instrumentation so you talked about how on the binary side we have grafana agent and then the external instrumentation we have ebpf but there's also a source instrumentation for some languages right there's a their sdks for pyroscope yeah let me show my screen if I can do so successfully this time so so yeah so this is kind of the the two paths that you can use so actually ebpf is done via the grafana agent and so we kind of describe it as either SDK implementation which is going to be your you know gy install pyroscope uh pip install pyroscope goget pyroscope npm install pyroscope you know adding a library to your actual application code and you know typically I would say developers tend to enjoy that route because they have control over the code they can easily add that we find that um often like sres or um more like infrastructure focused Engineers tend to prefer this method because it allows you to without changing your application code um get this profiling data and so the graffan agent sort of goes to your application uh in the case of ebpf it like adds a you know ebpf program that will send that back to the grafana agent which then sends it back to the pyroscope server um or for like go for example it will go look at your PPR endpoints scrape those endpoints for the profiling data send it to the pyroscope server um whatever might be via the grafana agent and so it just kind of depends on which you know which situation is is more fitting for uh various companies use cases we see probably honestly equal usage of both how did you decide that for grafana because I imagine we have both types of personas and more so what made you decide for one or the other so yeah that was one of the nice things is that we actually built this uh before you know a lot of so most of the um the sdks existed before grafana or before we joined grafana because the graffan Asia wasn't an option at that point because uh we weren't part of graffan and so um what was nice is we got a lot of users who um were like I said more like developer focused um in the open source world uh we adapted most uh the most popular open-source profiling libraries from each of those different languages and made it so that they can properly send data to pyroscope and then once we join grafana obviously there's like many grafana users who are uh using the grafana agent pretty heavily and so we uh basically you know made it so that the grafana agent is capable of doing things the graffo way and so we kind of just merged our two communities into one and and now we support both of them and it shares there a lot is shared between them in the way that um you know it makes it easier to maintain but um but yeah we just kind of support both but I guess for for internal use if there were a new a new team or a something new that needed to be profiled how would you go about that decision I imagine people would be pretty open to whichever way you suggest yeah so we often you know yeah so it depends on the uh the use case for that as well so at grafana we have like I said we automatically profile everything that happens in kubernetes at all and so basically just by existing you know in the grafana infrastructure your workloads would automatically be profiled and we see that a lot of customers do it that way as well and a lot of Open Source users um just have you know yeah systemwide profiling they profile everything um if you want more specific control because so that's going to automatically add labels like I said of of namespace um pod painter all the various like kubernetes metadata it will add those using like the kubernetes API but if you want to you know uh add a specific label for endpoint or um you know something that's more unique to your application that's where we recommend using the sdks because you can similar to like adding a trace span you can choose what you want to say hey I want to add a label to this specific code here and and sometimes people opt for that if they if they're really digging deeper into into into their applications right they have a suspect query or something in a class yeah okay yeah um yeah I I would say that's one thing uh one thing that uh I didn't show as well as so yeah so we've talked a lot about the um you know the profiling in the sense of yeah like systemwide profiling something that we just recently added I think I flashed it on the screen for a second um is the ability to let's see if I show an example here so yeah so so another thing that we have support for is basically um you know execution scope profiling and so this is coming out uh in a couple days I think in the next version of grafana um basically if you're using traces a different kind of profiling than just profiling all of your servers getting a a 15-second profile periodically over time you know sometimes you do want to know specific uh like you were saying for a specific query or something along those lines and so basically this will allow you to you know if you have a a 400 millisecond Span in a certain Trace you can also see a 400 millisecond profile or if it was five minutes you could see a five minute profile or 10 seconds or whatever it might be and so this is also a new pattern that we've seen that a lot of people are interested in getting a profile per API request or a profile per Trace span and so this is something that we've added for that use case as well um but this one again you would have to add the uh the SDK version to be able to get this because it's doing a very specific label for every single trace span it's adding you know a profile for that specific Trace span and so um that's another use case where you would use the the sdks over the uh like ebpf approach okay and it sounds like you're advocating for a a hybrid approach it kind of sounds like you would recommend for a new company and and correct me if I'm wrong that they consider if it's a possibility going the ebpf rout for like a systemwide thing so that you don't have to individually set things up as new projects come online um and then maybe add add the SDK support for things where need a little bit more information than what you already have is that right yeah that's that's perfect I mean yeah we we definitely recommend um and like you said it depends on on how familiar your company is with profiling I will say for a lot of uh go users uh in particular since profiling comes like native inside of the go runtime a lot of go people are already familiar with profiling and so you know sometimes with that we recommend just going with with the uh you know going with the graphon agent or the sdks or whatever but uh I would say for the if I had to you know group you know give some general advice I often recommend going with ebpf first just because it's really easy to get up and running and get some data and uh particularly if you're using kubernetes um it's going to add all those labels automatically that kind of thing um and then you kind of get an idea of okay I I think I want to dig further here I want to have this extra capability or add these labels and then you can add the SDK and you can do both side bys side yeah yeah that definitely makes sense kind of the ebpf method for the lowest barrier of entry and that way then you can start seeing how you can really utilize that that tooling and how it can benefit you before you start going ahead and adding everything in via the SDK yep yeah that's that's really cool yeah I also like that approach because it's like it follows gal's law which says that it's just better to start with the simplest possible system and then add on complexity if you need it because if you start with all of the things right away then you know you would need an entirely different system to make it simpler y yeah exactly yeah that's the that's the tricky part um I I will say one nice thing about profiling is that it is really good at giving you that view of you know in a meta way of your system so as you can optimize your system so like one thing that we see a lot is um that we didn't expect that we've seen over time is people who use profiling to optimize their usage of logs metrics and traces because now they can see you know a lot of people think about the overhead of profiling but not about you know there's some log line that someone added 10 years ago that is just logging mindlessly and then it's like you has spent logging something they don't care about and so it's not profiling where you start to kind of you know get how meta do we get I mean do you like continuously profile continuous profiling abely if if we want to confuse everybody like turtles all the way down I was just gonna say that Turtles all the way down yeah like those little like those little like Russian dolls where there's like dolls yeah vka or moska um but I I wanted to ask about the the sampling you mentioned somewhere that it was by default 15 seconds it was a 15c interval how does I imagine that would affect the the requirements like in terms of storage but also Computing if you make that shorter or if you make that longer right so how do you how would you suggest people start with that and how what do we do as well like is is all our stuff 15 second ones so we do so yeah that was one thing that we did alter um was so we initially yeah have everything by default 15 seconds like I said we we do go profiling so we have a lot of there's multiple different profile types for go um you have uh CPU profiling allocated space in you space um you know allocated object objects in use object so there's like several different profiling types and um and so what we did was CPU we found is most frequently used by people and some of the memory profiles were used a little bit less and so what we did was um to decrease the amount of data that we were ingesting we changed the uh interval to like every I think it's every minute instead of 15 seconds for those and then you know you decrease the amount of ingested data by four because you go from 15 seconds to a minute um and so it I 15 seconds tend to be really uh we find that to be The Sweet Spot just in terms of um you know obviously like I said it's different depending on who you are but we find that to be granular enough where you catch things in a very specific time frame but wide enough we're you know we're not getting a profile every second for example or something like that and so uh so yeah that's kind of what we recommend but yeah you know you can there's a lot of levers that we've built in you can change the amount the the scrape interval that you're scraping the profiling data you can change the um sampling rate that you're you know you're looking at the stack Trace 100 times per second you could change that to 50 times per second and then now the overhead that it does cause is cut in half and so there's a lot of levers that we have to be able to change that but at the end of the day I would say you know most of that Bec com s sort of not irrelevant but a very minor consideration because you know if you're talking you might be decreasing the overhead from 1% to 0.5% or from 3% wow it's that and so at that point you know let's say you're operating a bunch of servers most of your servers are probably not operating at 99% and so the actual impact of changing that is probably very minimal compared to you know when that incident does occur and you know that you know you're about to miss your SLA or slos or whatever um you know having that information is going to be much more valuable on that side than you know the 0. five% that you saved by you know switching the the interval or whatever but but we still offer that just in case you know um we like I said we did it ourselves so we know that th those cases do occur but but for the most part we we we recommend people to focus on like the benefits that they get on the opposite side versus the overhead that they incur on the front end yeah because you definitely need it when you need I think Ryan just said one millisecond intervals for everything that's what I heard that is not what I said but that's what I heard one single knob that goes to 11 right turn it all the way up yeah turn it all the I mean we do see people I mean some people do really want like very very granular profiles and not that I I don't think I've ever seen one millisec but um you know yeah like that's what I want Ryan like high frequency trading like we we have some people who have used pirate that and so it's like that's something where you do want to know this specific you know uh millisecond time range you know what was going on and so um yeah you know we tried to make it as flexible as possible to support even things we don't recommend uh support those to so so you're providing a foot gun to people right where they can shoot themselves in the foot yeah if they want to we try to warn them we have all the warnings on the foot gun that if they want to shoot themselves in the foot you know okay so a couple questions okay so Andre ziviani said maybe more like traces instead of metrics and so this was when I was saying that like continuous profiles are essentially philos ically metrics but this comparison to traces kind of makes me think of how when we had um Joe Elliot from Tempo come on and he was talking about how there is head sampling and tail sampling where you can either decide upfront what percentage you're going to trace or do it after the fact and is there something similar for continuous profiles so if you're using the grafana a not right now that is on the road map to Via the grafana agent be able to control this we're also doing a lot of work with open Telemetry as well and basically trying to match the same patterns for other things but um but yeah as of right now it doesn't exist it's not as important for profiling and the reason is because tracing can scale more volatile more vol I don't know the word that I'm trying volatile foring that like if you get a million requests you're going to get a million more traces whereas for profiles if you get a million requests if you have a big enough server that same server is still going to just generate you know a single 15-second profile so because of the way that it scales it's like less um important to have that ability but again like I said we we introduce this uh ability to do these traces or uh profiles for Trace spans and so that will actually be attached to your you know sampling and traces where if you sample the trace then you sample the profile as well and and vice versa and so yeah that that is a good question or a good comment that it is much more like I would say profiling is almost like if you added a trace span for every single function call that would be effectively what profiling is and the reason you can't do that is because the overhead would be so wild and so you know profiling is kind of an extension of tracing it just gets the profiling data through that you know sampling the stack Trace method versus tracing which when they label the spans it is that sort of start the stopwatch stop the stopwatch method but at the span level and the trace level that's much less taxing on your system then if you did that for every single you know millions of functions you're doing it for you know thousands of spans instead which is you know obviously fine and we have another question here here from Olympus Biz who says can you add a few example alert rules for gra redus plugin I want to display most Keys hit on redis grafana plugin so that would be I guess the answer is yes I think that's a little bit less related to profiling in this case like Trace yeah yeah it would probably be more on the tracing side that being said one of the things that is on our road map as well though is you know and one like I said one of the reasons we joined grafana is that now you know logs metrics and traces exist alongside profiles not only that but we can leverage some of these other signals in order to enhance profiles and have profiles enhance those signals and so one of the things I showed some you know there were some like lines there when I was showing the continuous profiling stuff is that we can now use the like Prometheus mamir um you know kind of backend in order to set up alerts for profiles so let's say you did have a specific function that was doing something related to reddis um in this next like couple months you'll be able to uh kind of create a metric from the usage of that function that's extracted from your profile and then be able to set some alert rules based on that and so it's like a little bit removed you know obviously depending on I would say the reddest case is probably better done in traces than profiling um but you know I don't know you could also be profiling the reddis like Library itself and looking at things there could you talk about the pyroscope open Telemetry Library yeah I would love to is that further muddies the waters between profiling and tracing I think this actually uh does the opposite and that it really and that it yeah it does provide a nice you know uh delineation between profiles and traces as you know tracing is obviously a signal within open Telemetry along with logs and metrics and profiling is sort of the the next one and so um this is a uh data model that we've proposed we've been so me and Dimitri um some of the folks from data dog from elastic from uh let's see red hat uh various other um you know people who are doing things around profiling and then some end users as well have been in a Sig for the past two years I think I think we started after the Valencia cucon we were talking about earlier Nicole but um but yeah after uh it's been about two years we've been working on building a data model for a that all you know different profilers whether it's Ruby python go ebpf Donnet Etc can all create profiles in the same format so that you know again we we uh you know we're vendor neutral and all that kind of stuff and so um yeah this has been proposed you can see there's lots of comments on it even as recently as just you know two days ago we are still getting a couple comments but uh it's looking like we we're aiming to have this data model um which is similar to PPR it's not exactly pprof but similar um has some added extens I and benefits to make to bring bring PPR sort of in theel context so adding things like the ability to link from a trace to a profile adding that as like a field on the profile to make that more efficient in that kind of thing and so um so yeah we've been working towards it uh we're hoping to get this merged in the next um you know couple weeks and then once this is merged we'll move from the uh draft stage into the experimental stage which allows for all of the language instrumentation groups within open Telemetry to now start exporting profiling data in this format that is proposed in this PR um and then yeah we start to add some of those things like the the tail sampling head sampling all those things that exist within open Telemetry in The Collector we can now add those for profiling as well it's awesome seeing all the big names getting together with the open Telemetry project to add that in there because yeah that being being able to stick with vendor neutrality and all that good stuff that's uh that that's a big deal yeah yeah so it's been um you know as with anything in open cementry it's been uh you know definitely a a long haul I'll paste uh paste the link in the chat um we are definitely looking for feedback so yeah if people one of the things that's most useful here is if people you know like the direction this is going feel free to you know add some emojis to the uh to the pr if you have questions comments add those and if you if you support it uh feel free to add a review saying hey I approve of this um the more reviews that it gets the more quickly we can move and get this you know into the open Telemetry world so that people can start getting their hands on it so um yeah if I had one ask I would say uh yeah add some emojis add some uh some reviews approvals and if you have any questions or comments add those too yeah and whatever you do don't suggest that there should be an open profiling project we don't want to do that we don't want to split it yeah no no it's I mean yeah if once you read through this it makes sense the uh the format that it's in now so um we we explored that you know even early on and definitely have explored all of no no stone has gone unturned and we definitely are pretty confident that this is the best route for um you know for the otel community great we are almost out of time but it occurs to me that we didn't actually talk about installation options milky asks Ryan can you please do some demo and show how we can make profiling I think they mean you know installation and configuration what options do we have let me show um so yeah so I would recommend I'll send a link here you can go to our examples folder and this will I mean obviously our docs we have uh you know many docs that will explain this but um you know whether you want to do youra agent go the push method go the pull method node Java Etc um we have all these here that rer example that I went through today we have sort of a a bloges you know kind of walk combination walkth through SL installation instructions version here um um obviously you know everybody here is using different languages so I can't go through all of them right now but basically for each of these we have an example of how you set up pyroscope so really it's just this line let's say you're using go you just add this code snippet here name your application tell it where to send the data um add any labels if you want to add labels and that's the bare minimum that you need if you're using go um yeah we have the evf example Etc so I would say I would recommend just going here um like I said this is python you know it's just pip install pyroscope and then same thing you add this snippet to your um actually let me just show an example to your let's see like if you're using flask I believe it's in what server.py so yeah so like this is all you would need to add pyroscope to a flask application um but yeah we have this for all the examples I would say I'd recommend just checking out the documentation and um we just did a giant docs rewrite so hopefully it should be pretty easy to uh to understand and get through but yeah we'll have all the information there all right great thank you thank you so much for coming on the show and telling us all about continuous profiling and answering my questions no problem thanks for having me and uh yeah looking forward to seeing you all in person next time I see you getting you some pyroscope swag um yeah we're I'm counting on it yeah he's such a show off it's because he doesn't he has it and I don't yeah you don't have this Paul yeah last thing I'll say is yeah we're we're um you know active on GitHub we're on the grafana public slack like feel free to uh in the pyroscope channel feel free to reach out if you have any issues or questions and we would love to help you get started um or do more advanced things if you've already gotten started so just let us know and yeah I think that's all I got awesome awesome thank you and thank you everyone Paul do you have any announcements um well I I think this was my last grafana office hours um yeah I'm leaving Nicole has has abused me one too many times so I am leaving way around it was it something I did you know I just got here actually it was yeah sorry wow now decided yeah exactly now I'm GNA be uh putting on my engineer hat again and uh going and uh actually being a heads down coder uh and architect and all that again so but yeah I've I've loved my time at grafana though definitely all right well especially this right exactly and but I still do want a the updated pyroscope t-shirt because I plan to uh no no no you leave the company you leave the swag yeah I'm gonna take that shirt back right I'm kidding I'm kidding I'll un invited I'll still get you the uh the updated swag awesome all right well maybe we'll still you know if you're nice maybe we'll still have you on in another capacity yeah that's right I want to become a grafana champion so yeah wherever you go good luck with that exactly I wonder which team does the approvals for that oh wait all right well thank you Ryan thank you Paul for being a decent co-host sometimes relief that's that's what I'm here for gy yes yes thank you everyone for watching and we'll catch you next time bye all bye

