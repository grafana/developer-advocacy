# Grafana Tempo Community Call 2025-03-13

Join our next Tempo community call: ...

Published on 2025-03-13T18:04:15Z

URL: https://www.youtube.com/watch?v=2c2uLcxMFr4

Transcript: all right welcome to yet another Community call for Tempo March 20125 um I was not present for the majority of this agenda creation it's going to be a bit of a surprise to me I can ramble through this but I imagine other people have a better um uh sense of what we're doing in fact I will start with the top one because that's easy and exciting and then I'll hand over to whoever like to handle the second one first of all we're going to thank uh our new team members russin started a couple months back Matt is I don't even know if he's technically on the team yet but he's really freaking close if he's not it's like this week or next week or something yes and no at the same time I don't know what that means but uh both are great developers super excited to have um impactful people and uh we've seen a lot of growth in Tempo internally we're getting a lot more headcount because of that and to see Heavy development in Tempo or increasing momentum on our development in Tempo very excited about the future of where we're headed um uh given the amount of attention we've been getting it's been growing I know been last few years have just been very very uh strong growth internally on Tempo um also externally community's been picking up everything's been picking up very excited about it uh so Russell and the Matt are new and I'll go ahead and cheese this one forward also on team m announcements Carlos Carlos somewhere down here uh where did I it just says new maina new maintainer and I just searched for maintainer and went oh I ruined It spoilers it's not cars oh there it is right there thank you uh cars uh started a few months back um in this link you see the pr to make them a maintainer there's some links off of that into a lot of their work and Tempo done they've been doing great uh work and open source Tempo also great work internally helping us do set up our operations and other things that are not visible to the community but uh very excited for Carlos uh just been a fantastic team member proven themselves the last few months and uh feel free to dig into their PR uh everything's quality for sure all right team is growing we're excited on our side uh what's up next who would like to discuss new Trace Q features I think each of these features has an author so I can talk over them I have a question for some of the time okay like to know what's the difference between sum over time and rate and increase when should I use each one only jav can answer that question jav can you answer about it um yeah the the rate is essentially how it grows uh the metric and the Zoom is essentially you you take uh all the values and you zoom all together um rate is the count of spans per second right uh increase is the count of spans but over the step so like if your step is five minutes it'll be the amounted increased over that five minutes right rate I think is more EAS I'm sorry your time count over time we don't we don't have are you sure it's count every time the number of spans in that step rate is the same thing divided by however many seconds thank you but yeah clearly we know Trace uh some over time is unique in that it takes a parameter the other ones just count the spans uh some over time takes any parameter attribute any number numerical parameter and we'll just add those up over the step basically uh and step is not setable in old versions of grafana but I think current versions of grafana have a little block where you can type in a custom step if you'd like um the past versions let Tempo pick whatever step it came up with basically all right um topk bottom K is and flight if Sage exists uh they could talk about it yeah so right now if you write a query you get bazillion serieses out you can uh with this you can only get top or bottom K series so you can write write a query like you know rate by client IP and only give me the top 10 or the bottom 10 client IP so this allows you to do second stage operations on top of Trace metrics uh this behaves almost as uh Prometheus top K and bottom K so functionally it's similar so it will look at top K items per step so you if you say top K 10 you might get more than 10 series and that's by Design that's it thanks Raj that's a little bit paired with another PR that's in Flight um to constrain the max number of Series quable in tempo right now that doesn't exist um which allows for you know you could technically write this query in Tempo feel free to do it if you like to crash your local Tempo um so you can rate by some of infinite cardinality Trace ID span ID whatever this is going to create hundreds of thousands perhaps millions of series it will make Tempo quite Z and so this top K along with another change um to constrain Max series will help oh hey I didn't know if you're here fam will help um fix that fam's been working on the max series because uh in Cloud people have occasionally just straight blown up the query front ends and the querier by executing queries like this bam anything you want to add maybe only through emojis oh no hey I mean you've kind of said it all but um I mean some uh we we a few iterations ago we added you know you can search by span um you know by span ID and I love when people write by span IDs it's great that makes us happy here on call that's right so yeah these are kind of I'd say that's been a um major theme of the past five to six months of tempo development is kind of the growth in cloud has pushed us to find all these operational gaps things that you know uh Loki and mamir already found years ago but uh we didn't quite realize and finding new query patterns new data patterns that blow up Tempo but we're just kind of patching that I'd say Tempo is operationally most sound today that that's ever been um but we still have a like a list of these kind of U things to improve uh the way you operated and the stability in the face of you know the insanity of users right um new intrinsic I think spam did that too did you do par ID P I did nice just going to confirm that y okay no emo I need I need your emoji feelings about spam par and ID that's right um so the parent ID is um oh you have a presentation for Mario cool uh the parent ID I can the parent ID is pretty simple um uh intrinsic span parent ID equals you know what we still have not built this um it is something I really want to build parent name equals food uh it's might require an iteration on the backend format uh at the least if we shoe hard it into our current one it would be uh costly a very expensive query to run so this is still on the list um it's still very valuable to me uh but it is harder than we expected when we made the original Trace ql spec and so it's still just kind of on the back burner uh but like I said tempo's getting a lot of people a lot of momentum to to handle a lot of projects and so I think suddenly things like this in the next maybe six months to a year are becoming addressable where before uh we just didn't have the bandwidth and we were just keeping the wheels on and adding the most obvious features so uh this is cool this is way more cool but way harder and I'd like to see that in the next U you know maybe year or so would be awesome don't hold me to that moving forward project Rhythm I was invited to open a somewhere where's slack there it is yeah presentation a presentation by our own Mario uh yes speaking of three days after Mario's Day March 10th is Mario Day we always celebrated it on Tempo we're very excited for Mario on his day um so let's all give him Round of Applause thank you Mario uh and we'll give the floor over to him to talk about Trace ql metric apparently uh right yeah uh so I wrote project Rhythm it's the internal name for the change that we doing into tempos architecture and I thought of using the same slides that uh we used in May last year when we fast floated this idea time FES um and back then we were presenting some issues that we have now with tempo's current architecture and we were thinking of introducing um a Q based architecture uh potentially Kafka uh it was be undefined uh but since we've made a lot of progress in in this project um and we're currently uh working on it developing and testing the the new changes uh we can confirm that we've gone for um the CFA API for uh for this change and yeah we we've switched to a Q based architecture yeah back then we only had this we didn't know how it was going to look like but right now we're currently yes on that step so so far we've been focusing on uh introducing Kafka into the right path uh and this and this has uh created a new component uh in Tempo which is the block Builder which is now building blocks in the future it will replace the injusta as um the main component building blocks for our object storage uh but so far currently the responsibility is still shared um between both um I don't know if we want to go over the the regional questions we want to go into more detail what the future plans are what we're currently testing and because I can talk about Rhythm for a couple of hours at least that's not yeah to focus on I think uh where we are currently would be great uh and I also think this end goal would also be great I think like sh telling people where are uh you know where our state is so they know kind of the progress and then what things we intend things to look like when we finally cut this thing uh with documentation Helm support everything uh what they should expect when they actually operated all right so the main goal is um like two things uh we want to separate read and and right path um that's a necessary change for the other goal uh which is we need R of one replication Factor one for all the data in Tempo uh Tempo currently runs based on a replication factor three architecture uh that means that each right it's replicated into three Injustice and after the introduction of Matrix uh we realized that that um idea didn't work very well for for metrics U mainly because of how jobs are shed for metrics uh but that's going too much into detail basically we want to run with replication Factor one to only have one copy of the data so for that we in introducing uh a cube based architecture that adds the durability guarantees uh are now provided by by the queue basically so um yeah so the the previous slide this is what we are currently we've introduced this new architecture to the right path so there's this new component uh and also the metric generators are also uh reading from uh akka thejus are untouched so basically right now we have two injection paths uh one for like the old architecture and one for the new architecture the next step and the future form of tempo will be this one in which uh there is a single right path and the rate path is migrated to sort of like the new architecture to only read these new rf1 blocks so the inors will stop generating blocks they'll receive a single copy uh of the data which they'll hold in memory and answer queries for recent data and the responsibility of building and shipping data to OB storage for long-term torage uh will be in the new block Builder component and we hope uh uh that this will basically enable uh uh trade skill metrics to run at a much higher performance and will be able to carry longer time ranges and Temple will just work better and faster that's right uh so once this is accomplished um we'll provide uh migration guides um it won't be different to what we're doing cuz we're migrating uh Tempo deployments ourselves so um I think by then we'll have a lot of experience of what can go well what can go wrong and all the necessary steps to do a a step byep migration without any downtime um we'll maybe cut a new version of tempo I think that's still undefined when and how uh we also of course will introduce board in all the deployment options that we offer I think the most common uh used is the home trats will um will update the hel TRS to support I don't know make CFA compatible uh solution like red panda or something that's open source um and yeah we basically expect to Sunset classical Tempo or Rhythm Tempo uh EV um cool Edgar has a question if you want to field that oh oh yeah so block Builders are they going to be a replacement uh for the compactors uh Mario you can feel that I can answer what do you think you just muted yourself I'll go okay so sorry uh yeah no no there they're not going to be a replacement um you can think of block Builders as an exis of the Injustice they they're taking some of the responsibility from the Injustice into another component right so compactors is missing from this compactors will remain uh we expect there to be significantly less pressure on the compactors and part of our kind of like TCO prediction is that you'll have to run significantly fewer compactors and they'll be doing significantly less work uh there's memory issues in compactors people file um issues with this all the time and Tempo and uh what a lot of people don't know is that primarily the memory uh consumption of compactors is due to combining the traces and that's going to a lot of that's going to go away with the rf1 architecture so we expect impactors to remain um but we expect there to be significantly fewer of them right the block Builders just turn into blocks that are in object storage the inors just allow querying of recent data is is what those two components are going to do um about Kafka do you consider workflow tool like temporal instead of job Q I don't does anybody know what temporal is or can speak to that I'm not sure what temporal is um and Pavo you raised your hand if you want to ask your question while I go figure out what temporal is uh I think I answered my question already okay thanks okay um so I don't know what this is we are going to support the Kafka API uh and we're going to try to use a minimal set of the API uh so that it will be as portable as possible we are not going to guarantee it can work with anything because uh we will only be operating against uh Kafka basically or we're going to use warp stream but that doesn't matter um but we expect anything that is C compatible to work now we say that about object storage too S3 compatibility works and then people try to use it on like some other hosting provider like digital oceans S3 compatible thing and it doesn't work so uh we're going to do our best to keep the API surface that we use minimal which should improve the portability as much as possible for using other queuing things like temporal uh we will likely put very little to no effort into that what I would like to see from the team that's working on this though is a interface uh where maybe community community members can uh Implement their own if somebody loves temporal or I don't know what temporal is but some other queue that we don't support then we would accept a PR to uh to add support for a new queue but it would be behind an interface and it would be kind of up to community to maintain those because we wouldn't use those so that would be my stance on any of the billion other queuing options that exist out there right now I think the the at least the initial implementation will be tied to the CF kaipi uh but I think yeah we're not um using a lot of Kafka so as long as whatever other implementation um supports some basic requirements uh like some Concepts that we're using like um like it remains open for for the future but yeah for now I think we're going to be very T also to mention something that I uh forgot um we're also expecting Rhythm to help with um TCO relability durability in general um T we still don't have numbers because we've not done the entire migration uh but even with the added uh cost of um the Capco or whatever other the system you use uh just the smaller injustes flushing less data will be flushing a fourth of the data that Tempo is Flushing right now uh we expect it to be uh quite a huge uh savings and also in terms of reliability there certain issues with the current architecture with the Injustice being too big too stateful uh being read and write path components um there are many challenges that um these changes aiming to to fix or alate yep paval asks is the data still sharded in the inors uh I'll answer that real quick yes um so block Builders generators and ingestor will still like uh collocate a trace they all have the same needs to do for instance the structural queries right as well as in metrics we do service graphs we have to look for two edges in the same trace the block Builders we want the same kind of like um uh capabilities in Trace through all to exist on the blocks and storage as in the adjustor so yes all of these are going of behave the same they're going to pull from The Q's the partitions are sharded by Trace ID just as they are now uh Andre asks can you share which implementation of Copco we will use internally the answer is uh I said it earlier warp stream we'll be using work stream internally um I would be surprised giving given what we're using it would not work with a lot of the other options that are out there digital ocean S3 is not compatible I have no idea it probably is I just picked one out at random but every once in a while we get this issue that's like tble doesn't work on this and it's some other S3 compatible thing and then you go to their docs and it's like we're S3 compatible for these apis and then here's the long list of things we don't do that S3 does and it always kind of you know gets into the details so when I say C compatible I mean we can't guarantee it's going to work with all of the ca compatible things out there uh but given uh given the amount of the API we use I do expect it to be mostly compatible across the board uh thank you Jai linking the workstream uh website I think it might be might be a good idea to list what endpoints and like what's the surface of the c k that will be using that way people will know what's compatible and what's not but yeah so far it's very little yep thank you um that's a great idea if we just list these are the three apis we use these are the features we use on those calls that' be it really help people uh find out what it's compatible with I think I mean I don't know wear ways from this but I believe my goal is in the helm chart to ship a tiny kfka thing and for most people running like single binary it's going to work they notice it's just going to be this little little Kafka thing sitting there uh but we should also provide the ability to to drop in right like an end point because you chose manage Kafka over here over there or you running your own like Kafka cluster somewhere else near or uh thanks Raj also linking to maybe another option I'll tell you the thing we've struggled the most with with Tempo is when people have some random on Prem S3 compatible uh like Dell or something Appliance and Tempo just crushes it and they don't know why and it's because it just can't keep up with this three um all right anything else about Kafka rf1 Tempo Rhythm um we are running this in our Dev cells we are pushing it into our op cells now not quite there but we're close and we're going to try to be rolling out to prod in the coming months uh the phase one and then phase two following that um so there will be continued information here in this community call about TCO impact and um migration how it's going we'll keep everyone up to date for sure all right we'll move forward but this is a very uh Oops I meant to close the other thing this is a very loose presentation feel free to continue to ask questions uh and in chat there or unmute uh anything is fine um and at the end we'll ask for an a oh my goodness there's a lot of questions already uh oh no my bad these are some of the questions that have been asked thank you for whoever is keeping drink all right so project Rhythm nice compaction changes I believe Zachary Leslie will be discussing this but I don't want to put him on the spot if he's eating a sandwich no not for another like hour probably um yes so we've got some compaction changes coming up um this is sort of driven by the Rhythm project we are noticing that um compactors duplicate a lot of data and this is for a number of reasons there are just edge cases scenarios those kinds of things but um basically blocks can get compacted multiple times and end up with duplicate data which is going to affect metrics um and so we want to be a little bit more consistent so here we have um yeah some data for sure um a small percentage yes thank you um we have a little bit of a design dock internally we've been going back and forth on I've got a PR up that is a work in progress um tester passing at the moment I expect to be having this run in our Dev environment in the next uh week and then we'll go forward from there I'll probably mark it ready for review by the end of the week uh depending on how review goes H you will see how we take it U one step at a time it introduces two new components there's a scheduler and a worker component the workers will uh replace the compactor component so we expect to be turning down all the compactors um and what happens now is that jobs for compaction will get scheduled which means they there will be a central location we don't have to rely on the ring propagation delay Etc to determine the jobs which uh end up creating the compaction and we just want to be really consistent about that we want to reload jobs from the cash uh we want to make sure that we only hand out jobs which we um are not currently working on other blocks Etc there will be potentially future jobs jobs that we want to do on the back end like Trace redaction and I imagine retention uh tenant retention will also go into this so anyway there's a PR up feel free to review leave comments if you care um if not Joe's got plenty of comments so um yeah that's a little bit where we're at uh any questions uh I'll point out that I know this sounds complicated but this will be seamless in comparison to the um the Kafka changes right the the rf1 this you you might might even notice this changes if you were to deploy like one Helm chart to the next it's kind of just shuffling the responsibilities a little around of competion um so that we don't duplicate data which happens occasionally we found uh and this is basically creating like a central source of the jobs that then a series of workers which basically are the compactors pull from and the reason we chose this is for a lot or there's a lot of reasons and I think to sh light on why we're kind of adding a new thing this block schuer whatever this I it's not block schedule myad the uh the scheduling component is we do see some potential growth here so a common request that we struggle to meet in cloud is like I would like to delete all traces that match this query it's it's very hard for us right now we basically have to run a batch process to do that and something like this might start filling that role is kind of a longer term vision of this piece is adding capabilities to Tempo for your users where like oh man we push this attribute and it has pii this might have the ability to submit a series of jobs to rewrite blocks to redact all those attributes so there's uh it is kind of a step forward I do think the complexity is way smaller than it may sound and I also see um I also see a lot of potential here for growth and capabilities and Tempo which is why we chose this path yeah definitely and we'll see it might even reduce TCO it'll give us a little more indication of um you know what we're doing as far as compaction and we could scale these separately way that doesn't that doesn't you know multiple jobs don't get run at the same time which conflict on block idas Etc yep uh on that note compaction is like one of the oldest processes in Tempo and it's had very little attention this is the first major change in years and so like Zack said I do think we're going to this along with the change in the blocks that are created are really going to reveal to I think a lot about how compaction Works what it does and so I do think we're going to find some savings here to through this work cool I think this is great stuff thanks Zach appreciate it all right I in a panic right before this meeting threw a bunch of PRS of note before I knew there was any agenda because like I said I was in a bunch of meetings right before I walked in here um so I I'll just kind of quickly point out some of these we've been through quite a bit um I won't go into all the details because that's probably a lot but these are interesting things if you want to keep up with Cutting Edge Tempo tip of M this is the these are the big things coming through uh first I'm going to highlight some from the community Edgar restored some logging that we lost in 27 uh somebody just today I think uh added the ability to do IPv6 on the Distributors um we due to Edgar's work also fantastic Community member Edgar thank you uh pushed us to uh revert some changes to our compression so that is in 271 and of course will'll be in 28 there's the pr there uh and then somebody in the community is maintaining a nomad example and our examples I don't we have almost no expertise of nomad inter terminally we use kubernetes but I think it's cool the community's grown so much that somebody who's super into Nomad has provided this so if you love Nomad if you want to deploy there please help us maintain this and gain some information from this gain some knowledge from this too uh we've seen a lot of performance improvements in the last month even uh right here there's one two three four five six PRS and all are going to start shaving uh performance personally I think we've done a great job at the low level I think we can improve of course but at the low level I think we've done had a lot of attention uh in my opinion the next performance big performance gains except for rhythm will come from job throughput and that's kind of a personal focus of mine to figure out how to take tens or hundreds of thousands of jobs and get them to individual queriers bring all that back recombine it's a lot of work um and trying to reduce those jobs and improve the performance of farming those out is a big big uh or something that's in my mind I think at the moment but yeah a lot of cool performance improvements there feel free to dig into those um there is a panic it's in 27 there's the fix right there there's a little grimacing face so you know how I feel about it um the Panic really only impacts people larger installs no one has reported this from the community we do see it in our um in our cells if you are seeing this in your Tempo clusters uh Raise Some Noise about it and we will Port it back to 27 at the moment no one's saying anything and so we're just going to put in 28 but if suddenly a lot of people start showing up and are showing this they're seeing this instability we will absolutely get it in a 272 uh but it really does only impact uh particularly large cells with lots of reads lots of read traffic um and then finally uh Carlos is doing some work here we are looking to improve the um uh the consistency across all endpoints of what we log and what we metric in terms of how many bytes are at all those little metrics and statistics that come back with a trace qall query Trace by Aid query we want that to all be the same across everything so that's what this final link here all right and we have quite a few questions but I think have we answered all these are there anything is there anything else we can do for you all any other questions um they can be about Rhythm they can be about uh an O you saw last week uh feel free to ask whatever you'd like we're here this is a great opportunity to connect to the people who work on this every day and to get some insight and help um if not I believe we basically have everything answered is anybody afraid when they see the little Kafka symbol in a diagram do they like start Zach is okay Zack your job is to rep Andre also Andre C you and Zach need to hang out apparently I I would like to see that interface I think the interface for public contribution of other cues would be cool I understand that's a later thing though no big deal right we do not have the band with for this if someone wants to work on this it's a possibility but there is an option for basically like low to no durability where we just put the cues in the Distributors like we don't have time to work on this but uh it would basically be cue everything up in the distributor and pull perhaps I don't know uh but if you are that allergic to Kafka um submit a design doc talk about ways to implement this queue in different in different ways I see as a potential of us owning that down the road but it just can't be for this initial thing like we have an enormous number of of pressures um and we are working very hard to deal with like the growth we've seen that I've said and so we a little bit have to do what makes sense for us but we do long term also want to do what makes sense for the community um and so we respect that this is like a scary thing for some folks like uh and some people chose Tempo because it's like oh object storage only right so we're trying to be very upfront about why we're making this change what the dependency brings and then maybe long term uh you know well after we deliver the first thing we might have the ability to restore that kind of object storage only um stance anything else Team all right super appreciate everybody showing up this was a great call a lot of good chatter um keep posted uh I love to show TCO met TCO improvements the next couple uh Community calls so that you all get a sense of um what you're gaining in for throwing cka somewhere in the mix um and yeah thank you all for showing up take care enjoy your March and we will see you in whatever's after March April four that's right bye everybody all right take care e

