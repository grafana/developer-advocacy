# Pérennisation des stratégies d&#39;observabilité chez D-EDGE avec OTel, Loki, Grafana, Tempo et Mimir

Clément Boudereau, ingénieur principal chez D-EDGE, présentera comment D-EDGE a tiré parti de la norme OpenTelemetry, ...

Published on 2024-12-13T16:23:39Z

URL: https://www.youtube.com/watch?v=iXs5O7sDVvo

Transcript: euh ben merci tout d'abord euh de venir sur cette session euh pour vous expliquer un peu la la la stratégie de pérénisation d'observability chez dedge euh donc moi je suis Clément Boudreau euh je suis principal ingénier chez DH euh j'ai eu un un défi on va dire sur la partie observability qui a commencé il y a 2 ans euh et je vous vais vous raconter un petit peu euh l'histoire du projet où est-ce qu'on en est et quelles sont les prochaines étapes et surtout en terme de stratégie qu'est-ce que qu'est-ce qui nous a permis de de pouvoir aller vite euh si vous avez la moindre question hésitez pas à scanner le QR code qui vient juste de s'afficher euh ben j'y répondrai en fin de de session ou après la session vous pourrez me rejoindre à côté d'un café ou à côté du du stand des experts euh alors qu'est-ce que c'est di je suppose que la majorité d'entre vous connaissent même pas la société hormis les dagers qui sont présents ici euh en fait on est euh on fait du sas pour les hôteliers et on interconnecte des canaux de distribution donc quand vous passez par exemple une réservation sur un système il y a très fortes Chan que vous passez par notre système euh et globalement à l'intérieur c'est ce qui s'appelle une CRS central reservation system euh et donc en fait il y a énormément donc il y a plusieurs produits le produit cœur on va dire c'est la Central reservation system il se trouve que dedge en fait il y a très grosse histoire sur on a on a un historique donc il a eu plusieurs systèmes d'information tout le monde n'est pas au même degré de maturité ou dans le même use case sur la partie observability et move to cloud je revient sur sur le move to cl parce que c'était un élément très important pour améliorer notre stratégies et Ré les coûts opérationnels et on a plusieurs produits qui sont bah comme je le disais donc il y a la CRS pour vraiment gérer toutes vos stocks de chambrees ainsi que vos réservations distribuer dans le monde et ainsi que différents produits par exemple comme les moteurs de réservation les booking Engine et cetera qui là aussi peuvent être disponibles dans le monde entier donc on a plus de 500 connectivités c'estàd qu'on travaille avec plus de de 500 partenaires et bah en quelques chiffres en fait ça peut se représenter en des millions de transactions et en terme de volume de log typiquement ça peut se représenter en tabte de log par jour donc je vous laisse un petit peu imaginer pour du sas dans l'hôtellerie où finalement la marge elle est quand même assez relativement réduite par rapport à d'autres produits il faut le défi il est là en fait c'est de trouver les meilleures solutions les solutions les plus simples dans le domaine de de l'hospitality pour euh répondre efficacement au aux besoins de nos clients nos clients sont les aôteliers et donc en quelques mots euh DH et ce qu'on fait c'est qu'on simplifie l'hospitality c'est vraiment un monde qui est très complexe il y a beaucoup de systèmes interconnectés euh et malheureusement en fait on HNE pas tout le système il faut all aussi analyser les dépendances externes et trouver très rapidement euh le problème est-ce que ça vient de notre système ou est-ce que ça vient d'un système extérieur et dans ce cas-là il faut sortir le log et donc euh c'est la raison pour laquelle observ est très importante dans notre domaine donc on a commencé euh quand je suis arrivé sur le projet on avait on devait répondre à plusieurs contraintes qui étaient un problème de scalabilité c'est-à-dire que les les coûps pouvaient être potentiellement exponentiel alors la raison de de cette cause là c'est d'une part il y a une explication qui est simple c'est le nombre d'hôtel mais aussi les combinaisons d'utilisation de produits comme les revenus Management System qui permettent d'aller optimiser les prix en fonction d'un événement il y a un concert qui se déclenche le prix va être modifié automatiquement et cetera donc ça en fait multiplié par le nombre d'hôtel on arrive très facilement à des facteurs multiplicatifs qui commencent à devenir important et et en fait quand on a commencé à lever le capot sur ce système d'information là donc là aujourd'hui je vais vous parler vraiment d'un retour d'expérience sur un système d'information de d'Edge et quand on a levé le capot on s'est aperçu que tout était é basé sur du log et quand on exploitait en fait les les les données souvent on partait pour du graphe donc c'est des solutions qui ont été bâties vraiment de manière historique étape par étape mais il y a un moment donné quand il a fallu vraiment répondre à la problématique de scalabilité on a été confronté à des problématiques de coût qu'il a fallu répondre et donc c'est pour ça en fait qu'on a mis du des logs des métriques et des traces et surtout avec Open télémetri on va voir pourquoi open téléméri pr ça a été un un bon élément euh dans dans l'approche et dans notre dans notre cas particulier de migration euh donc on va revenir en fait sur les les les fondamentaux qu'est-ce qu'on a quels quels étaient pour nous vraiment les éléments importants euh pour que ça soit euh efficace pour le business et que ça soit opérationnelh que ça soit très opérationnel pour nous euh tout d'abord c'était les coûts parce que bah comme on l'a vu on pouvait pas continuer euh sur les solutions existantes parce que on est sur de l'exponentiel donc on va pas faire de l'exponentiel sur les coups ça ferait pas de sens si à un moment donné on gagnerait plus d'argent euh il faut que ça soit scalable c'est-à-dire que on dans quand on a commencé à lever le capot de l'existant bah il se trouvait que par exemple quand le système était en souffrance on avait plus de log donc on pouvait pas bâtir décemment en fait des métriques ou ou avoir un degré de confiance sur l'état du système quand notre propre système de monitoring est d'ARM c'est juste pas possible euh il faut avoir des données reliaable donc comme comme je l'ai dit bah si on a un problème sur le pipeline de transformation ou à n'importe quel endroit dans notre télémétrie ça devient très important en fait d'avoir des données qui sont relativement fiable parce que bah si on a plus de système de monitoring on est juste aveugle et on sait pas on connait pas du tout l'état de santé de notre système donc on peut pas décemment reposer tout tout notre système sur sur ce genre de de de système là euh bah des SLR parce qu'à la base en fait ces solutions là étaient plus pour des US case d'analytique p security Siem euh pour pour analyser on va dire des traces utilisateurs et cetera mais qui ont été utilisé sur un autre scope et malheureusement le contexte a pas été changé donc comme toutes des techniques en général la solution elle est bonne à la base il y a des dérives on revient pas sur le contexte on revient pas sur l'existant et malheureusement on arrive à à beaucoup de techniques qui est difficilement absorbable euh et il fallait des intégrations smooth c'est-à-dire que dans l'existant en fait on est parti avant le move to cl sur AWS sur des architectures on premise qui tournaient sur des machines avec des agents préconfigurés et cetera et ce pattern on va dire était assez global et on pouvait d'un point de vue observability vulgariser dire qu'il y avait un seul style d'architecture à monitorer et donc le CASE était relativement simple là ù sur un move to cloud c'est beaucoup plus complexe on peut avoir des short la Task on peut avoir des lambda comme j'ai entendu parler tout à l'heure ou de le CS vraiment des des styles d'architecture qui deviennent quasiment complexes à intérêt ou incompatible avec un système existant il y a on a eu un problème aussi c'est que les le requirement aussi c'est que c'est bien d'avoir tout que ça soit que les cûts soient corrects que les dat les Data soient RBL mais il faut que les dashboard aussi soit très très efficace et qu'on on nattende pas la minute ou 2 minutes pour avoir nos résultats donc bah comme je le disais l'observability c'est bien mais il y a des pièges de partout nous notre cas ça a été vraiment pour les coups et que ça soit vraiment très responsive et que ça fonctionne relativement bien euh donc les problèmes qu'on a eu donc comme je le disais c'est qu'on avait une approche vraiment qui était logcentrique là ce qui a fait ce qui a fait surtout mal en fait c'était un un un un manque de cadrage sur les tradof d'usage avec une solution d'observabilité on va pas aller faire de l'analytics typiquement c'est d'autres solutions alors peut-être que ça peut marcher pour des petits usqu mais à grande échelle très rapidement s'aperçoit que c'est c'est pas le bon c'est pas forcément le bon outil euh quandbi même grafana permet de visualiser de la donné la la donnée mais si vous avez un backend qui n'est pas du tout dimensionné ou en capacité de de pouvoir le faire ça va devenir compliqué euh donc il fallait que bah les coûts en fait soient quand même relativement réduits et ajustés par rapport au US case euh et que euh les les les là notamment ce qui avait fait mal dans le système existant c'était les cooupts en CPU euh parce que bah par exemple pour charger votre gr vous avez besoin d'aller chercher des logs et pour aller chercher des logs en fait vous avez des coûts suivant la solution que vous utilisez donc c'était sur un sas et sur ce sas là en fait il quand on regardait sur la facture on a voyait apparaître en fait un haut cout sur le CPU et la mauvaise bonne idée qui a été mis en place c'est enfin c'est une très bonne idée c'est de mettre quelque part on a besoin de calmer le jeu sur la facture donc on met en place du R limiting c'est assez connu sauf que l'effet pervers c'est que les dashboard étaient très très lent voire inexploitable donc on est arrivait un petit peu à ce à ce problème là donc des problèm de scalab de performance sur le système existant avec un grosse consommation sur les ressources plus que nécessaire avec un haut niveau de de maintenance et surtout des dashboards qui étaient très très lents et voir inexploitbles qu'il a fallu reprendre euh et comme je le disais en fait il y avait énormément les les les les agents en fait étaient très peu monitorés sur le pipeline de monitoring le pipeline de transformation pardon et donc bah si à un moment donné on n pas standardisé on va dire les formats des log euh pour pouvoir faire du parcing qui fonctionne bien et avoir des agents qui sont parfaitement monitorés mais on peut se retrouver typiquement avec des données qui sont bah pas consistante par pas du tout relayable sur lequel on peut pas du tout construire de métriqu sur lequel on peut pas avoir confiance quoi donc voilà et surtout comme je le disais l'autre défi qu'on a eu c'est que ça s'inscrivait dans un projet move to cloud et le premier projet qui a été choisi vous savez si on fait un Pareto on fait le 8020 et on a choisi le plus gros projet donc normalement quand on choisit de faire une migration on essaie de choisir le plus petit projet on ouvre petit à petit les vannes nous dans notre cas pour pouvoir supporter le projet move to cloud l'observabilité très importante on ne pouvait pas migrer la solution d'observabilité existante dans le move to cloud parce que les patterns d'architecture étaient complètement différents et pas supportable par l'architecture existante il a fallu donc qu'on s'inscrive en fait dans ce projet move to cloud sur on va dire 80 % d'emblé de charge de de données sur sur ce projet donc comme je le disais dans l'état de l'art il y a les piliers dans l'observability donc j'ai fait exprès de mettre en gros matrix parce que pour nous c'est ce qui était vraiment très important euh dans le projet move to club que j'ai participé j'ai eu l'occasion de travailler notamment aussi sur d'autres projets avant sur du distributive tracing et aussi par exemple sur du profiling donc on on a fait pas mal d'expérience on va dire mais plus avec les des produits AWS style xtraay et quod ou par exemple pour la partie profiling euh sur la partie Xray ça a donné des résultats on était un moins content on va dire sur la partie compléude des traces et intégration par exemple avec Open télémetry ou intégration dans d'autres dashboard en dehors du cloud AWS pour ça soit plus exploitable bénéficier de meilleures optimisations et et on verra à la fin que on a des chantiers pour essayer de tenter de de de migrer vers vers du tempo donc qu face à à ces prérequis là là ces problèmes là qu qu solution et conséquences positives qu'on qu'on a pu obenir euh on a utilisé bien évidemment les backend parce que bah open télémetry il y a pas de backend associé donc il faut un backend donc il faut choisir un backend donc de là en fait on a regardé par rapport à notre usage on a regardé un petit peu les les les solutions techniques qui ont émergé ainsi que les compétiteurs en terme SAS et c'est pour ça qu'on a retenu euh la partie graphana Cloud pour des raisons de coût on va dire c'est quand on prenait le tableau des compétitors et qu'on regardait uniquement les coûts nous dans notre situation graphana cloud c'est ce qui c'est ce qui paraissait et c'est ce qui est le plus avantageux pour nous euh surtout par rapport à l'intégration à l'usage de Loki là où d'autres solutions en fait vont tout indexer là vous avez vraiment le choix de se dire je vais intégrer je vais je ne vais indexer que des labels dont j'ai relativement besoin et ainsi du coup diminuer l'usage desx et réduire le tradeof et l'usage DX à son strict usage correct dans le cadre de l'observability ou d'un monitoring classique ok donc fini les scénarios analytiques on va aller s'amuser à à calculer des des choses qui sont en dehors du pyramè du monitoring ou l'observability euh ce qui étit très important pour nous c'est de se dire on a besoin d'avoir des graphes des Matrix et il y a un élément qui est important c'est que on a pu via ces logs là faire un pipeline de transformation pour les Transformers métrique donc ça ça a été très très bénéfique et ce qui étit aussi important c'est l'intégration et l'usage des SDK open télémétri alors j'ai mis SDK on verra il y a une grosse subtilité tout à l'heure il y a cil qui en parlait il y a vraiment un une différence entre le protocole et les instrumentations je vais vous expliquer un peu nous le tradeof qu'on a qu'on a utilisé parce qu'on n' pas utilisé toute la suite d'outil open télémétrie on s'est inscrit dans un cadre de reprise d'un Legacy donc on a utilisé vraiment les parties qui nous semblaient essentielles euh donc le résultat ça a été on a divisé la facture par 8 par rapport à l'existant et aux prévisions qu'on allit avoir on a pu diviser les coûts par 8 alors les coûts c'est important mais on a augmenté l'expérience utilisateur c'est-à-dire que euh en divisant les coûts par 8 on a aussi intégré des métriques qu' nous n'avions pas avant donc en fait le scope il est beaucoup plus large que l'existant on a été beaucoup plus loin et aussi on avait des logs typiquement qui tournaient sur DVM qui n'étaient pas dans aucun SAS si vous faites un move to cloud les logs ils vont pas se retrouver dans un file système les développeurs vont pas pouvoir aller prendre le fichier comme ça ça va soit tomber sur du cloud watch ou soit tomber dans du S3 donc il faut le quantifier il faut l'estimer et quand on a fait l'étude et qu'on a regardé on a vraiment euh eu fois euh des économies mais sur un scope beaucoup plus large que un move to cloud sur notre paramètre on premise euh sur la partie scalabilité et performance euh on a vraiment comme je le disais vraiment on s'est focusé sur le métier de l'observability du monitoring excite les use case style Analytics ok on a répondu architecturellement à cette approche là de manière différente parce que c'était vraiment un problème d'usage du produit euh on a surtout utilisé les recording rules donc c'est un portage des des prometus recording rules mais sur la partie graphana Cloud et ça a été vraiment un un use case très intéressant pour nous parce que bah du coup ça coûte zéro en terme HS j'ai envie de dire puisque l'un des critères importants dans notre situation c'est qu'on avait un la solution d'observability il faut qu'elle soit relativement standelone autonome pour qu'elle se suffise à elle-même et que ça ça requière le minimum de ressources possibles de maintenance rationnel et donc en fait ça nous a permis d'avoir des dashboard respon très rapides euh et surtout ça nous a permis de construire des SL des so fiables qu'on optimise on va dire le bon scénario et que quand on regarde une métrique qu' qu'on soit sûr que ça ça soit vraiment la bonne euh et surtout par rapport euh aux intégrations complexes là où ça a été important ça a été l'usage d'Open tellémetry parce que c'est du push euh l'interface prometus elle est intéressante si vous faites du pool sur kubernetis par exemple ça va bien marcher si vous utilisez une lambda ou de le CS ça risque d'être un c'est possible he mais c'est beaucoup plus complexe à mettre en place et vousz vous pouvez pas dans un cadre move to cloud bah multiplier ces intégrations là ces effort là sur toutes les équipes il vaut mieux une solution on va dire quelque part unifiée qui permet simplement d'intégrer l'observability à moindre coût euh et donc voilà il y a eu un autre avantage aussi en terme de coût c'est que on voulait corréler les M puisquon faisait un move to cloud B on voulait récupérer la télémétrie des services managés par AWS et de faire des corrélations et et du coup on a utiliser les cloud watch data sources vis-à-vis d'autres compétiteurs là où il fallait passer toute la télémétrie de de Cloud watch de votre cloud provider vers votre solution de d'observability là vous pouvez simplement souscrire une Data Source cloudwatch et vous allez pouvoir en bénéficier donc ça vraiment ça a été euh j'ai envie de dire un élément qui a été très très important pour pouvoir faire de la corrélation de log donc là si je reviens on va dire sur les les composants important qui ont été utilisés donc Open télémetry en fait c'est des services grpc ok donc tout est spécifié avec des des des des schémas et des protos donc c'est une spécification pour pour faire du du RPC d'accord c'est une sémantique convention avec des attributs et des ressources donc par exemple si vous vous avez des logs avec un hostname bah clairement dans la spécification ils vont définir les clés pour pouvoir le faire ça peut être énormément de de clés valeur et souvent sur un système de log c'est énormément rébarbatif de répéter cette opération là de le standardiser chez vous il vaut mieux adopter un standard donc c'est ce que nous ça nous a fait gagner sur cette partie là énormément de temps parce qu'on a pu décommissionner toute la partie transformation et passer sur le le protocole Open télémetry et c'est dernier en fait où c'est sur les instrumentations les exporteurs nous dans notre situation on a utilisé que les exporteurs parce que on sur il y avait un choix on a un use case c'est du Java spring boot sur ce P de système d'information là il y avait déjà des instrumentations microméterur en place ok vous installez le projet spring par défaut vous allez avoir du microméteur il y a 2 ans il y avait pas encore les exporteur open télémétri ils existaient mais ils étaient pas encore mature il on va dire qu' commençait à être mature mais pas pas à ce point-là et il y avait les instrumentations donc il y avait deux choix soit on rajoutait une deuxième instrumentation avec potentiellement bah deux instrumentations qui se bagarrent sur un Runtime avec une incertitude sur l'impact sur les performances sur l'application ou soit on vient simplement ajouter une ce qui s'appelle une registry pour pouvoir prendre la télémétrie existante et l'exporter via des services grpc qui sont assurés par un exporteur Open téléméri et c'est la stratégie qu'on a choisi c'estd on touche pas au code on tout juste on va juste rajouter une dépendance avec un exporteur in memory adjintless qui va exporter la télémétrie au format Open télémétrie au TLP euh donc on a en plus pour la partie AWS on s'est intéressé du coup au dashboard donc on s'est dit bah tiens c'est pas mal on va utiliser les fonctionnalités grafana et les grafana dashboard je vous encourage vraiment à aller voir c'est très très intéressant vous allez pouvoir retrouver tout un tas de dashboard pour votre USC j'en suis quasiment sûr et on a nous-même contribué à des dashboards qui nous paraissaient essentiels donc il y a notamment par exemple le dashboard AWS API Gateway donc avec une Data Source cloud watch on a fait enfin j'ai fait le les les dashboards SQS et pour vraiment la partie applicative là j'ai vraiment utilisé en en backend Loki Mimir et tempo avec une instrumentation comme je le disais open tellemetry Java pour des applications style spring boot dans laquelle il y a on a besoin de zéro customisation on a simplement besoin de rajouter une dépendance qui permet d'aller exporter la télémétrie euh alors pardon j'ai je suis passé un peu rapidement mais vous verrez il y a un QR code qui qui s'affiche en haut à droite euh qui s'affiche en haut à droite ici si vous voulez le scanner vous allez arriver sur sur les dashboards sur lesquels on a contribué euh il y en a un pour lequel j'arrive presque à un million de download donc j'aimerais bien comprendre de savoir d'où ça vient mais ça en tout cas je vois que c'est c'est pas ça a l'air d'être pas mal utilisé il y en a un autre là sur dynamo dB que je viens de faire qui commence aussi à pas mal pas mal intéresser de de personnes donc l'y le point essentiel c'est que ça nous a permis comme je le disais de bah de faire des économies par rapport à son type de confil figuration il y a un tradeof qui assez malin c'est de se dire on va pas tout indexer on va juste prendre les labels dont on a besoin il y a l'interface avec le loql qui ressemble de très qui est très très proche à du promql donc quand quand on connaît déjà prométhus il y a quasiment pas grandchose àapprendre quand on fait du locqel donc ça c'est super important c'est compatible avec plein d'agents c'est-à-dire qu'on avait quand même des use cases sur lequel on avait encore des agents et du coup il y a quand même une API Loki qui est très très bien intégrée dans dans les agents donc ça ça nous a permis d'aller vite par exemple sur l'intégration avec fluent bit au départ on n pas utilisé de l'otlp parce qu'il y a 2 ans c'était pas aussi mature que ça c'était pas aussi bien intégré donc on est passé par des agents qui utilisaient la p Loki et là maintenant on va s'intéresser à vraiment avoir toute la télémétrie dans le même protocole LP sur tous les signaux on va essayer d'aligner ça et bah comme je disais voilà les indexes qu'on peut configurer donc il a Mimir Mimir ça nous a permis de faire des les économies parce que c'est ce qui a permis d'introduire un pipeline de transformation trnolognométrique plus accueillir toute la télémétrie des Java euh ça supporte promethus donc ça c'est très très bien en entrée ça supporte otlp donc ça c'est parfait euh dans Don not case comme je le disais sur les recording rolles euh nous ce qui nous a permis de vraiment de faire des économies c'est bah toutes ces gestion des recording roule qui en plus est maintenue et exécuté dans fana cloud donc on a pas besoin de je sais pas de customisation ou de faire quelque chose de particulier on premise ou ou dans le Cloud pour les pour les exécuter il y a simplement besoin de les configurer et tout est tout se passe dans CRAF cloud sans sans sans surcout donc c'est très très intéressant donc comme je disais c'est équivalent au prometus recording ru donc si vous connaissez déjà les recording r promeus c'est exactement pareil sur sur graphana Cloud et du coup voilà ça nous permet de transformer tous nos logs sur la partie Legacy qui sont quand même très importants en maétrique pour afficher des dashboards ou créer des alertes euh et voilà donc ça je l'ai dit c'est donc les datas sources pareil C savings il revient beaucoup hein mais c'est vraiment ce qui a permis de se dire bon bah est-ce qu'à un moment donné on va avant d'aller sur du grafana cloud on a essayé différentes approches différentes combinaisons s'intégrer avec le le l'outil concurrent existant et on a regardé et malheureusement en fait bah rajouter des composants ou même standardiser des composants pour synchroniser de la data de de la télémétrie de de notre de notre SAS Cloud sur notre système d'observability c'est coûteux et en plus faut le faire c'est une grosse maintenance donc c'est des coûts opérationnels mais aussi c'est des coûts sur la facture donc ça c'est important donc comme je le disait ça nécessite pas d'avoir une full data synchronisation en place euh il y a juste besoin de de brancher de faire des API call sur sur cloudwatch et vous allez payer que cette partie d'API call euh donc voilà c'est ce que je disais vous payez que les API col en plus et c'est out of the Box vous le prenez vous configurez la data source euh vous vous configurez les credentials qui vont bien et vous récupérez la la télémétrie euh il y a l'Open télémettri Gateway alors ça c'est assez intéressant c'est comme si vous aviez un open tellémetri collector mais embarquer dans grafana cloud en mode Gateway ça c'est vraiment très très bien comme ça vous avez pas à vous occuper donc ça supporte nativement au TLP si vous avez des US cas un peu particulier vous devez faire de la conversion ben il le fait il peut ça peut permettre de mettre tout en otlp et derrière si vous avez un mimiroki il va quand même faire la conversion donc ça c'est intéressant moi je vous recommande d'utiliser l'Open tellémetry Gateway euh alors j'ai plus que 3 minutes on il y a il y a un un élément qui a été important c'est que on pouvait pas arriver et dire bon bah on va tout faire sur kubernti parce que bah ça nous arrange et puis comme ça on met un appérateur on met un agent et puis terminé nous comme ça en terme d'architecture c'est super simple à faire parce qu'on met une config d'agent et ça va être réglé du coup come on pouvait pas faire ça on a adopté une approche qui a été le Cher de responsability model où il a fallu qu'on supporte on va dire les architectures à observer et donc pourquoi c'est une ch responsability pourquoi on l'a pas délégué sur les équipes bah parce que il y a il faut se conformer à la partie sécurité euh si on a une plateforme grafana bah faut mettre en place le SSO faut quand même [Musique] euh si on fait de l'infrastructure à ce Scotte si on a un système de de multitenany il faut la mettre en place il y a une certaine gouvernance à gérer et on peut pas se permettre de dans dans quand on commence à être structurer à à à la rendre éclaté parce qu'au niveau organisation organisationnellement parlant ça ne fait pas sens ça va être compliqué à à maintenir quoi et surtout on voulait supporter un scénario dans lequel on nallait pas euh faire une stack une instance on voulait plutôt avoir une observability 360°g sur l'ensemble du système d'information euh donc là maintenant ça doit être opéré par une seule équipe et comme je disais on doit supporter beaucoup de styles d'architecture qui vont des lambda de le CS du un peu de kubernetis euh de le C2 et cetera donc on peut pas se permettre de se dire on va d'abord standardiser le SII et ensuite on fera l'observability après donc on a eu tous ces scénarios là à supporter c'est pour ça qu'on a choisi otlp parce que le protocole permet de gérer en fait la majorité de des use cas des architectures à observer et surtout ce qu'on voulait mettre en place c'est you builditit mais aussi avec un système de cashback c'estd you pay for it quand une équipe l'un des Prin principau points de faiblesse de l'ancienne architecture stack d'observabilité ça a été que les cous ont pas bien été ventilés et donc en fait les équipes se sont quelque part pas rendu responsable ou pas rendu compte de leur usage et donc en fait c'est important que à la fin du mois il puissent voir leur facture cloud consolider quoi sur AWS sur grafana du coup on a fait des dashboards aussi qui permettent d'exploiter les Data Source Cloud watchat pour avoir le billing du cloud plus le billing grafana pour leur St euh en B en parlant des stacks je vais vous expliquer un peu très rapidement ce qu'il me reste plus qu'une mite mais toutes les styles de configuration qu'on a eu donc pour des projets super simples style lambda nous on a un trigger chez nous c'est le coûp c'està-dire que si les coups ils vont être trop élevés sur la partie télémétrice ou le use case va pas bien marcher sur AWS c'est Graf an cloud ok donc sur des projets relativement simples où les coûps sont pas importants parce qu'on peut avoir plein de tout petits projets qui popent comme ça ça on va avoir on va quand même avoir du graphana cloud parce que sur la partie front end ça devient intéressant puisqueen fait on peut s'apercevoir que sur AWS on a le graphanronte mais vous aurez pas moi je l'ai essayé vous avez pas la dernière version vous avz un faible niveau de support et en terme d'intégration pour tout ce qui est SSO cas un peu particulier c'est vraiment très enfin moi je trouve ça très complexe si vous avez un graphana Cloud au moins vous avez la partie front de visualisation tout ça ça reste un changer donc ça au moins la standardisé il y a pas une un seul une seule architecture qui n'a pas un un graphana fond sur sur graphana cloud ça n'existe pas chez nous et par contre pour en terme de backend on va utiliser le cloudw et en terme d'organisation ils ont utilisé le W architecture framework donc il y a une stack par environnement par équipe et donc du coup on va avoir minimum trois dat sources par équipe à agréger et on le fait avec de l'infrastructure à SCOD via du terraform pour aller souscrire les les dat sources ensuite après on a plusieurs déclinaisons donc comme je le disais on a introduit Mimir pour la partie métrique donc on a bien évidemment du loky du Mimir avec euh un loky Mimir une stack par équipe on a le Cloud watch comme je le disais on a le AWS cloud watchat par équipe aussi et on a toujours notre graphana front donc en fait dans grafana cloud vous pouvez faire des stacks par équipe et vous pourrez visualiser les coûts par stack et donc quand vous voulez aller faire un dashboard pour aller mesurer euh les coûts d'une une équipe ça devient très facile si vous avez une convention vous adoptez la même convention de nommage entre vos stacks AWS et la stack grafana Cloud et vous pouvez vous êtes en mesure d'avoir le le billing sur sur toute une équipe donc ça c'est pour les étapes futures euh où en fait vous avez du du grafana loky memir et tempo et donc là pour tempo c'est un peu particulier c'est du distribued tracing donc il faut que vous listiez toutes les applications qui participent on va dire au distribued tracing pour qu'elles aillent sur la même data source donc c'est pour ça que nous on a choisi une stack global euh pour avoir le la visualisation et tempo et par contre ça reste un changer l' kimimir reste par équipe et toute la partie AWS identique donc ce qu'on a prévu après je sais pas si on j'ai le temps je vais passer rapidement donc c'est enance root cause analysis bon si jamais j'ai pas le temps j'avais fait la participeré à la graphe anacon je peux scanner vous pouz scanner le le QR code pour aller voir la vidéo mais grosso modo je vais un petit peu répéter ce qui s'est présenté précédemment mais vous pouvez tomber sur un log avec internal serve error dans votre chaîne de dépendance et il faut remonter comme ça la chaîne de dépendance pour vous apercevoir du vrai problème c'est une vraie perte de temps c'est vraiment très pénible et du coup nous on est parti de Laar méthode on mesure rate error and durations et quand vous allez sur le error pass en un clic vous êtes capable avec du distribu tracing d'aller sur la rout de cause là dans ce cas particulier là vous cliquez vous allez sur Server traces vous allez avoir toute la liste des traces et du coup en fait ici vous pourrez vous apercevoir que la table bad table existe pas du coup vous avez vot diagnostic en un clic euh et alors du coup pour résumer euh les donc en terme de backend observability alors là je l'ai mis au TLP c'est récent mais ça devient de moins en moins vrai c'est moi je pense que maintenant c'est car c'est assez mature euh l'oki ça permet de supporter des des stys avec des agents adjun mode où vous pouvez pouer directement la télé métrie depuis vos applications ça reste tout à fait euh envisageable et surtout bah otlp c'est compatible avec quasiment n'importe quel style d'architecture à observer et ça c'est vraiment un gain ça permet vraiment de simplifier l'observabilité mimiè pour moi c'est vraiment la pièce maîtresse vous pouvez pas commencer une stack d'observabilité sans métrique sans mimiè tous les composants intègre au moins un prométeus et mimè est compati prométeus et Open téléméri donc ça c'est vraiment impeccable l'AR méthode je l'ai je l'ai je l'ai dit aussi mais c'est vraiment le le pour moi le premier point d'entrée c'est les métriques et tempo en fait bah faut commencer simple faut commencer par du sampling mais sur un petit périmètre essayer de voir les bénéfices que vous pouvez en tirer et après il y a plein de produits j'ai vu contrôle tail ou t contrôle désolé mais je trouve ça très intéressant comme produit pour simplifier un peu la partie distributive tracing donc voilà si vous voulez me contacter bah comme je disais je vais je je je suis là toute la journée et si vous voulez me contacter après c'est possible euh hésitez pas euh je suis prêt à faire des bronb lunch des Meetup ce genre de chosesl en tout cas je vous remercie je vous remercie beaucoup pour pour m'avoir écouté pour cette session si vous avez des questions nhésitez pas mais à mon avis j'ai dû dépasser [Applaudissements]

