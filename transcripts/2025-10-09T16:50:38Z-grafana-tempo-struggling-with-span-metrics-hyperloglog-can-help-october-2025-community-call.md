# Grafana Tempo: Struggling with span metrics? HyperLogLog can help (October 2025 Community Call)

Published on 2025-10-09T16:50:38Z

## Description

Let's talk about metrics generation and HyperLogLog. Have questions? Please bring them! Can't comment in the chat? You may ...

URL: https://www.youtube.com/watch?v=e248vEvAhfQ

## Summary

In the October Tempo Community Call, hosted by Matt, Tiffany, and Carlos, the team discussed several updates and developments related to Tempo, including the upcoming release of Tempo 2.9. Tiffany announced her upcoming talk at the All Things Open conference, while Carlos presented his research on the Metric Generator component in Tempo, which generates metrics from spans. He explained the implementation of a new metric to estimate active series demand using the HyperLogLog data structure, which allows for efficient memory usage and accurate cardinality estimation. The discussion highlighted the benefits of this new metric for users, especially in understanding and managing cardinality limits in Grafana Cloud. The call concluded with an invitation for community questions and feedback.

## Chapters

Sure! Here are the key moments from the livestream with their respective timestamps:

00:00:00 Introductions and welcome to the October community call  
00:01:30 Announcement of the upcoming release of Tempo 2.9  
00:02:00 Discussion about the release candidate available for review  
00:02:45 Tiffany's upcoming talk at All Things Open  
00:03:30 Instructions for commenting in the live chat  
00:04:15 Introduction of Carlos's presentation on the metric generator  
00:05:00 Overview of the metric generator component in Tempo  
00:08:00 Explanation of how the metric generator creates metrics from spans  
00:10:15 Discussion on using HyperLogLog for cardinality estimation  
00:15:00 Q&A session about the accuracy and scalability of the solution  
00:20:00 Closing remarks and reminders for the next community call  

Feel free to ask if you need more details!

# October Tempo Community Call Transcript

**Host:** Hello everybody and welcome to our October Tempo Community Call! We appreciate everyone joining us, whether you're here live or watching later. This is our third month of live streaming, and we welcome any feedback you have—good or bad. Please feel free to leave questions or comments about the topics we discuss or anything related to Tempo.

**Housekeeping:** 
- We are looking to release Tempo 2.9 next week, assuming everything goes smoothly. Expect blog posts and documentation to come out shortly.
- A release candidate has just been released, so check that out as well.

**Tiffany:** 
Next week, I’ll be speaking at the All Things Open event on Monday. My talk will cover Java, Spring, and Tempo. Additionally, if anyone is having trouble commenting in the chat, you can click on your image in the top right corner and create a channel to enable commenting. This is a YouTube rule that has been in effect for the last few years.

**Host:** Great! Now, Carlos is going to present some interesting research and work he has been doing. If you have any comments or questions as he presents, feel free to post them. We’ll find a good stopping point to address them, and we’ll also have time for questions at the end.

**Community Engagement:**
- If you aren't already part of our community on Slack, please join us at [slack.grafana.com](https://slack.grafana.com). There's a Tempo channel and many others where you can ask questions afterwards.
- You can also visit [community.grafana.com](https://community.grafana.com) for our forums.

---

**Carlos:** 
Hi everyone! I’d like to give a bit of background on the metric generator. For those who may not know, we have excellent documentation on Grafana.com about Tempo, how it works, and the various configuration fields.

In Tempo, the metric generator component generates a number of metric series from spans. The distributor component in Tempo receives spans from the collector, such as Grafana Alloy, and forwards them to the Tempo metric generator. The metric generator creates metric series and pushes them to meters.

For instance, we have three main metrics generated:
- A histogram showing the duration of the spans.
- A counter of the total spans.
- A counter of the total size ingested by the spans.

With this, you can derive the red metrics: request rate, error rate, and duration metrics, which are very useful for all kinds of services. Tempo is fantastic if you've instrumented software with traces but lack metrics. Thanks to Tempo, you can generate metrics from spans, and by default, a number of labels are included, such as the status code. This allows you to quickly get the error rate from the spans.

In the configuration file, under the override section for the metric generator, we have a field called `max active series`. By default, it is set to zero, meaning there is no limit. However, if you set a limit, a tenant will not generate more than the specified maximum in the metric generator. This is to avoid cardinality explosions. 

For example, if you set a `max active series` of 500,000, once you reach this limit, you won’t know how many series are being dropped and the total demand for active series without the limit. To address this, I've opened a pull request that adds a new Tempo metric to show an estimate of the active series demand. This is particularly useful for a tenant that reaches the limit and wants to know the actual demand and how many series are being dropped.

My first approach was to calculate the actual cardinality. Cardinality refers to the unique elements in a set, or in this case, the unique number of metric series. To calculate the actual cardinality, we would need 16 bytes for each series: 8 bytes for a 64-bit hash to identify each series and 8 bytes for a timestamp showing the last time each series was updated. 

For a tenant generating half a million series, this would cost about 8 megabytes in the metric generator. This is where HyperLogLog comes in. HyperLogLog is a probabilistic data structure that allows you to calculate cardinality using very little memory. We found that we only need 5 kilobytes, which is 99.9% less per tenant without storing rejected series in memory. 

The estimate has only a 3% error compared to the actual number. If we look at the code in the pull request, I’ve added a new `cardinality.go` file detailing the implementation and comments on how this works. We are using a sliding window of HyperLogLog instances, known as sketches. This is because HyperLogLog is great at counting, but you cannot subtract elements once they’ve been counted, which is a limitation we need to address since we don’t want to count stale series. 

A series becomes stale by default after 15 minutes of no new data points. Therefore, every five minutes, we drop the oldest count and add a new sketch to count the current five-minute interval. A nice feature of HyperLogLog is that while you cannot subtract, you can merge sketches. By counting the number of metric series in each five-minute interval, you can merge them to get the total cardinality in 15 minutes. 

When creating an instance of HyperLogLog, you must specify the precision. The higher the precision, the more memory it uses, but the smaller the standard error. We have decided to use a precision of 10, which uses only 1 kilobyte per sketch with only a 3% error. 

This data structure has been deployed in our development cluster. I want to show you a couple of graphs illustrating the deviation between the estimate using HyperLogLog and the actual cardinality. The deviation is typically 3% or smaller, with some spikes that converge quickly. In a specific tenant in our development cluster, the actual cardinality closely tracks the estimate.

In conclusion, we expect to have this pull request merged this week, allowing you to access this metric to estimate active series demand, regardless of the limit.

---

**Host:** Awesome! Thank you, Carlos. Let's open the floor for questions. 

I had a question as I listened: Does it work at different scales of metrics and accuracy? Is it pretty accurate at scales like 1,000, 10,000, 100,000, or 1 million? Or do you need a certain threshold for it to make sense?

**Carlos:** It works at any scale, which is the amazing thing about HyperLogLog. It uses constant memory, regardless of cardinality. You can configure the precision based on the memory you can afford to use and the standard error you can tolerate.

**Host:** How much exposure do we have to tuning knobs? Is it pretty much good to go, no matter the scale?

**Carlos:** Currently, we’ve decided to standardize the precision for everyone. It isn’t configurable by the users because we believe a 3% error for the estimate is acceptable. This will help indicate how much to raise the limit for tenants exceeding the maximum active series.

**Host:** What does this mean for our Grafana Cloud customers? Sometimes we get requests to adjust metric limits. 

**Carlos:** This will greatly benefit Grafana Cloud customers. Once this is in production, they will be able to know in advance how many metric series their spans will generate before enabling collection and the metric generator. This will help them decide if they need to reduce cardinality to avoid issues.

**Host:** Did you already know about HyperLogLog, or did you discover it while trying to solve this problem?

**Carlos:** My initial approach was to calculate the actual cardinality. A colleague, Joe, suggested using HyperLogLog. We were skeptical at first, but after working on it, we found it works really well. The main challenge was implementing the sliding window since we cannot remove stale series, but we came up with a solution that tracks well.

**Host:** Great stuff! Thank you so much for sharing today, Carlos. And thank you to everyone who joined! If you have questions after the stream, please visit the Grafana community Slack. Different team members check that regularly, and we’ll do our best to answer any questions.

If you have questions you’d like to see addressed in future community calls, please share those as well. 

**Reminder:** Tempo 2.9 is targeted for release next week, and I’m working on the blog for that, which will include a lot of new information. Check out the release candidate if you haven't already.

See you folks next month!

**Everyone:** Sounds good! See you all next time!

## Raw YouTube Transcript

live. >> Yeah. Hello everybody and welcome to our October already tempo community call. Um, welcome everybody here, everybody watching live. Again, this is something we've tried. I think this is our third month where we're doing the uh live streaming. So, we appreciate any feedback we get from it. Um, good or bad, feel free to add in the comments. We also implore anybody to add uh any questions they may have about any topics we're talking about or anything tempo in general in the comments. A little housekeeping before we get to our kind of main topic here. Um we are looking to release tempo 2.9 next week if everything goes smoothly. So you should be seeing some blog posts and documentation about that and some notes coming out here shortly. >> Also, there's a release candidate that just came out if you want to check that out as well. And yeah, Tiffany, I think you're doing some speaking. >> Yeah. Uh, so next week, if anyone's going to be at all things open, um, on Monday, um, I'm going to be giving a talk that uses Java, Spring, and Tempo. So if you are interested in that, and then also for folks, if you are trying to make a comment in the chat and you are unable to make a comment, um, if you click on your image in the top right corner and then click create a channel, it should let you do that afterwards. It's the Google uh you well YouTube rule at least in the last few years. >> Great. All right. So, uh we're going to have a little topic today. Carlos is going to present some research and some work that he's been doing. Uh that's pretty interesting. I think pretty cool. So, I think it's a great topic here. If you have any comments or questions about it as we go along, feel free to post it and we'll take we'll find a nice little stopping point, answer it, and move on. And uh or if you have any at the end, again, we'll have some wrap up there to answer any questions you have. >> So, without further ado, >> also, if you aren't already in the community in Slack, uh please go to slack.garfano.com. There's a tempo channel in there and a bunch of other ones as well. Um, you can ask questions there afterwards if you didn't think of something during this time or if you're watching this after the stream is live or just want to ask anything else that you may have there. So, you can go there. There is also community.garfono.com which is the forum which you can use as well. All right. Now, back to >> now without further ado, uh, Carag. >> Sure. Hi there. So first of all I would like to give a bit of background on the metric generator. In case you don't know we have in graphana.com some really good docs on tempo how it works and all the different configuration fields. But in tempo we have the metric generator component which generates a number of metric metric series from spans. The way the way it works is we have the distributor component in tempo which receives bands from the collector such as graphana alloy and then forwards them to the tempometric generator and then the metric generator generates a metric series and pushes them to meter as an example. If we go to the span matrix docs here we can see the three main metrics which are generated. For example, we have a histogram showing the duration of the spans a count a counter of the total spans and a counter of the total size ingested by the spans. So with this you can already have the red metrics the request rate error rate and duration metrics which are very useful for all kinds of services. Uh so tempo is great if you have instrumented a software with traces but you don't have metrics. Thanks to tempo you can generate matrix from the spans and by default a number of labels are included in this matrix for example the status code. So this is how you can get the error rate from the spans very quickly. If we go to a configuration file as an example in the override section for the metric generator we have a field called max active series by default it is set to zero. So there's no limit but if you set this limit a tenant will not generate more than the given uh maximum in the in the metric generator. And the reason we have this is because we want to avoid uh cardinality explosions. If you have high cardinality in the span names, the generated matrix will also have higher cardinality. Now the issue we have with with this is that if you have uh let's say max active series of 500,000 half a million once you reach this limit you don't know how many series are being dropped and what is the total demand of active series if you didn't have this limit. So I've uh opened a pull request which adds a new tempo metric which shows you the estimate of the active series demand. So this is particularly useful if you have a tenant that reaches the limit and you want to know what is the actual demand and how many they are dropping. Now my first approach was to calculate try to calculate the actual cardinality and by the way by cardality I mean the unique elements in a set or in this case the unique number of metric series. So to calculate the actual cardality we will need 16 bytes for each series. That would be eight bytes to identify each series with a 64-bit hash and eight bytes for a time stamp showing the last time each series was updated. So for example given a tenant generating half a million series that would cost us 8 megabytes in a metric generators. This is where hyperlo lock comes in. Hyperlo lock lock is a probabilistic data structure that allows you to calculate a cardality using very little memory. In this case, we found that we only need 5 kilobytes. So that's 99 99.9% less uh per tenant without having to store the rejected series in memory. This is actually an estimate and the estimate only has a 3% error versus the actual number. If we look at the code in the pull request, I've added this new cardality.go file and here you can see the implementation and some comments on how this works. But basically we are not we are not using a single hyperlo lock lock instance. We are using a signing window of hyperlo lock lock instances which are called sketches. The reason is that hyperlo lock is great at counting things but you cannot substract elements once they they have been counted. This is a limitation of this data structure. The reason we need to substract metric series from the count is that we don't we don't want to count stale series. A series becomes tails by default after 15 minutes if it hasn't received any new data points. So given that we cannot substract to the count the solution is to have a sliding window of hyperlock log sketches and every 5 minutes we drop the oldest count and we add a new sketch to count the current 5 minute interval. And by the way that's another nice feature of hyperlo lock. You cannot substract but you can merge sketches. So if you count the number of metric series each five in each five minute interval you can merge them to get the total cardality in the 15 minutes in hyperlo lock when you create an instance you have to specify the precision. the precision uh the higher it is the more memory it uses but the smaller the standard error. For this use case we have decided to use a precision of 10 which uses only 1 kilobyte per sketch and the error is only 3%. So that's a great thing about this data structure/ algorithm that it uses constant memory. It doesn't matter if the cardality is 1,000 or 1 trillion. The memory usage stays the same and it's tiny. We have deployed this in our development cluster. And I want to show you a couple of graphs here in our development cluster. I wanted to see the deviation between the estimate using hyper lo and the actual cardinality and we've seen that the the deviation is 3% or smaller most of the time. There are some spikes but this converge very quickly. So the theoretical standard error of 3% matches empirically. If we zoom into a specific tenant in our development cluster here, we can see the actual cardality versus the estimate and we can see that the numbers track really uh closely. So this is a new data structure we've been applying to tempo. If you ever have a problem where you need to estimate a cardality without using much memory, hyperlo is the state-of-the-art in this problem space. If you're interested in knowing more about how it works internally, there are a few papers online. For example, this one from Google which explains the algorithm and basically the state-of-the-art. So that's pretty much it. In conclusion, we expect to have this pull request merged this week and then you'll have access to this metric that will allow you to estimate active series demand regardless of the limit. So that's all over to you Matt. Awesome. Thank you so much. So, we'll open up if anybody has any questions. Um, I did have a question as I was listening that kind of came to my head. Is does it work at like different scales of of of the metrics and accuracy? Like is it pretty accurate like 1,000, 10,000, 100,000, 1 million or do you need like a certain threshold to kind of for it to make sense? Uh no actually it works at any scale and that's the amazing thing about hyperlock lock that it uses constant memory no matter your cardality. What you can configure is the precision. This is something you configure once you create the instance of a paral lock depending on the memory you can afford to use and the standard error or deviation that you can afford. M >> um so yeah it's really amazing. It's quite counterintuitive because you wouldn't expect to be able to count uh one trillion or one quadrillion cardality using only one kilobyte or less but it works. It's uh it's basically proistic methods. Um in the paper it's explained quite well. Mhm. Do we how much um exposure to like knobs to tune it do we have or is it pretty much just good to go um no matter the scale >> we have decided to stay with the precision for everyone. It is not configurable by the users right now because we believe that for the estimate a 3% error is uh acceptable. Uh basically this will tell you how much you need to raise the limit >> for tenants which exceed the the maximum active series. >> Yeah. And what does it mean for our like graphana cloud customers? Um you know sometimes we get requests to adjust the the metric limits. So what what what happens there when this rolls out? >> Right. Since this will be really nice for graphana card customers because once this is in production uh they will able to know they will be able to know uh in advance also. So before enabling collection and enabling the metric generator how much uh metric series their spans would generate. Um this will also be useful to understand if they need to reduce cardality because sometimes there's a cardinality explosion and there's there are some obvious ways to reduce it. So if you see in advance that the number the estimate will be 1 million this will tell you that okay maybe you should reduce it reduce it a bit if you cannot afford it. Uh in addition in graphana we have the application observability app which uses the metrics generator internally. So that's another uh usage. Uh so yeah hopefully we can export this uh soon in the future and customers in graph cloud will be able to see it as well. So did you already know about a hyper log or was it like hey I have this problem I need to solve and then therefore you started looking into it and then found hyperlo log >> actually my first approach approach was to calculate the the actual cardality and our colleague Joe proposed to use hyperlock we were a bit skeptical skeptical in the beginning but I've been uh working on this recently and it works really well the main challenge was using the sliding window given that in hyperlo you cannot remove elements uh and we have to to remove a stale series. So we came up with this solution and it's working great. It tracks really well >> and Z says great stuff. >> Thanks Raj. >> So yeah thanks so much for doing this today. And thanks everyone who joined. Um, if you have questions after the stream again, just go to the Graphana community slack. You can ask us questions there. And if if will be different people on the team go and look at that every so often and then we'll see like if someone can answer whatever it is that you have. And if you have questions that you want specifically asked in a community call in the future, please share those with us as well. Anything else you all have? I guess another reminder that 29 is targeted for next week. Um I am working on the blog for that. So all a bunch of new stuff and check out the release candidate if you haven't already done that. >> So yeah, I guess see you folks next month. >> Sounds good. See you all. >> See you next time.

