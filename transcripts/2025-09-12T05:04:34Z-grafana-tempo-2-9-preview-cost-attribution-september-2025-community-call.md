# Grafana Tempo: 2.9 Preview + Cost Attribution (September 2025 Community Call)

Published on 2025-09-12T05:04:34Z

## Description

We'll be giving a sneak preview of the upcoming Grafana Tempo 2.9. We'll also be talking about cost attribution. Please bring your ...

URL: https://www.youtube.com/watch?v=fdmLmJMlUjI

## Summary

In the September Tempo community call, hosts Tiffany and Matt, along with Tempo members Joe, Siraj, and others, discussed upcoming release 2.9, focusing on changes and enhancements. Joe highlighted key features like TraceQL improvements, a shift in how metrics are calculated, and the transition to the new OpenTelemetry SDK, which aims to enhance trace quality. Siraj introduced the cost attribution feature that allows users to track data ingestion by various labels in their traces, detailing its easy configuration and benefits for monitoring usage. The team also addressed questions regarding Tempo's capabilities for client-side tracing and future plans for trace-based alerts. Overall, the session provided insights into improvements in Tempo's functionality and encouraged community engagement.

# September Tempo Community Call Transcript

Hello everybody and welcome to the September Tempo community call. We're doing this a little differently than we have previously, using a live streaming format. Feel free to watch, add any comments, or questions you have while we're going through it, and we will be more than happy to answer them.

Today, Tiffany and I are joined by Siraj and Joe, who are Tempo members. Joe is our esteemed Tempo mastermind, and today we're going to cover cost attribution and discuss some details about version 2.9. To kick us off, I believe we're starting with Joe, who will cover the upcoming changes in 2.9 and provide some details about that release.

---

### Joe's Overview of Version 2.9

So Joe, take us away!

**Note:** If you don’t see a chat box to participate, you may need to click on your picture in the top right and create a channel due to YouTube rules to enable chatting. Thanks!

*Cool. And Joe, you need to share your screen again.* 

Oh, shoot. Thank you. I took it off because I did. All right. Hey everybody. I'm alone now. We're going to talk about 2.9. Let's get to the right stuff here.

Version 2.9 is kind of on the horizon. Version 2.8 was released a little bit ago, and it’s about time we cut a new one. Toward the end of this month, I think my guess is late September, maybe the first week of October, we should see version 2.9. Today, we want to go over some of the major features of 2.9.

I’m going to look at the change log. Lots of stuff is coming in 2.9. I won’t go over all of it—there will be release notes and a blog post that highlight some of the major things. The release notes will have a lot of technical details, and then the final raw change log will show everything.

We won't go over everything since there's so much, but I’ll cover five or six features that I thought were fun to discuss. 

1. **TraceQL Improvements**: Marty has spent time this quarter putting together some performance improvements for TraceQL, so get excited about that!

2. **Breaking Change - TraceQL Metrics**: In version 2.8, metrics counted forward from the step. This meant if your step is aligned at a minute, it counted future data and rolled it up into that minute. We realized this was different from Prometheus and Loki, so we decided to flip it. The bucket will now count the last 15 seconds, 30 seconds, or one minute, rolling it up forward.

   Rouslon, a newer Tempo developer, has been doing great work on this. If you want to check out his PR, it has great details.

3. **Traces for Tempo**: Tempo has struggled with generating solid traces for about a year. Zach put in time recently to completely get rid of the old Jaeger client and focus entirely on the new OpenTelemetry (OTEL) SDK. The tracing looks much better now for debugging, which is a great change.

4. **Search Tag Values v2 Endpoint Fix**: We had a bug where querying the search tag values v2 endpoint with an invalid TraceQL query would break a tenant, causing a deadlock. Carlos fixed this so that it now returns errors correctly instead of causing issues.

5. **Trace Quality Metrics**: To help ensure high-quality trace data, we’ve added metrics that indicate if data is being sent outside of acceptable time ranges. This will help operators know if bad data is coming in.

6. **Native MCP Support**: This is a major feature in 2.9 that allows you to spin up Tempo directly with no shims. You can point your cloud code directly at the Tempo MCP server for improved interactions with your LLMs.

7. **Performance Improvement**: Marty has also added an explicit sampling hint called `with sample=true`, which will dynamically choose how to sample your tracing data for the best quality signal.

That was the overview of version 2.9! Expect to see it at the end of this month. 

---

### Questions and Answers

If anyone has questions, we’ll give just a minute here for someone to post them up. If not, we will move to Siraj shortly.

*Great, thank you so much for the 2.9 overview, Joe. Now let’s turn our gaze to Siraj.*

---

### Siraj on Cost Tracking

Hello everyone, I’m Siraj, one of the developers on the Tempo team. I’ll tell you how to track the usage of your trace data. Tempo has a feature called the **Usage Tracker**, which allows you to break down your injection volume by any label in your trace.

**Configuration Steps**:
1. **Enable Cost Attribution**: Go to your distributor configuration and set `users.cost attribution enabled = true`. This includes controls for cardinality to protect your downstream systems.
2. **Configure Dimensions**: Set up the dimensions on which you want your users to be broken down, like `service.name`. This label will show up in the Prometheus metrics.
3. **Expose Metrics Endpoint**: Once configured, Tempo's distributors will expose an endpoint called `users_metrics`, which provides Prometheus metrics about your injection broken down by the specified dimensions.

You can write queries against these metrics to see who is driving your injection data. If you are a Grafana Cloud customer, you don’t need to do anything extra; it will just work for you.

---

### Open Forum

We'll take a minute for any questions. If anyone has questions about what Joe or Siraj covered, feel free to ask!

*I see a question coming up* 

1. **Can Tempo be used to trace and debug user actions on the client side?**  
   Tempo is designed for backend systems, focusing on how requests are serviced across multiple services. For frontend user interactions, Grafana has a product called **Pharo** that’s more suited for real user monitoring.

2. **Is Grafana planning to have trace-based alerts?**  
   Currently, the answer is no, but we do intend to implement this. There have been discussions and some accidental implementations in Grafana Cloud, but it's not a stable feature yet.

3. **Usage Metrics Feedback**:  
   One user commented they are using the usage metrics in production, and it’s great. Thank you for that feedback!

---

### Final Notes

If you have more questions later, please visit the Grafana Slack in the Tempo channel. 

We have a presence at KubeCon coming up in November, and we will also be at OBS-CON in London from November 7th to 9th. 

Thank you everyone for joining, and we’ll see you next month!

## Raw YouTube Transcript

Hello everybody and welcome to the September Tempo community call. Again, we're doing this a little bit differently than we have previously. So, we're using uh kind of this live streaming format. So, feel free to watch, add any comments, any questions you have while we're going through it, and we will be more than happy to answer them. Today, uh, Tiffany and myself are joined by Siraj and Joe, who are Tempo members. Joe being our esteemed Tempo mastermind. That's what we're going to go with. And um, today we're going to cover um, some cost attribution and talk a little bit about 2.9. So to kick us off, I believe we're going to start with Joe. He's gonna cover some of the changes that are coming up in 2.9 and maybe some details about that release. >> So Joe, >> yeah, >> take us away. >> And also, if you're if you don't have the ability to chat, it just shows you the chat, but there's not a little box to be able to chat in there. Um, you may need to click on your picture in the top right and create a channel due to just YouTube rules. And then you can um chat in there. So, thanks. >> Cool. >> And Joe, you need to share your screen again. >> Oh, shoot. Thank you. I took it off because well because I did. All right. Hey everybody. I'm alone now. Um Ter community call. Uh we're going to talk about 2.9. Let's give it get to the right stuff here. So 2.9 is kind of on the horizon. Um 2.8 was I don't know a little bit ago. Um and 2.9 it's about time we kind of cut a new one. So towards the end of this month I think my guess is late September maybe like the first week of October. uh somewhere in that universe. Um we should see a 2.9 and today we just want to go over uh the some of the major things of 2.9 basically. So we're gonna give you a little preview talk about I'm going to look at the change log. I got some tabs up here ready to show. Um let's see 2.8. I was double checking June 10th. Yeah. So middle of summerish. Uh and so like late September I'm hoping to get or we we are hoping to get the next one out. So to the change log. Um where's that? I should know. Doink. There it is. Uh lots of stuff coming in 29. So this is kind of I'm not going to go over all this, right? There'll be release notes. Um there'll be a blog post. Blog post will kind of highlight some of the major things. Release notes is one step below. It'll have a lot of technical details. And then the final like raw change log is the you know the the lowest level you can go to. So as always for all tempo releases you'll see all these kind of you know all these things produced. Um and the change log here. We're not going to go over all this because there's so much. Um, but I'm going to co cover I think I got like five or six of the ones that I thought were kind of fun to talk about. Um, the one one one of one of the ones I'm not going to talk about is some TraceQL improvements. So, Marty in particular spent some time this quarter or last couple months uh putting together some performance improvements for TraceQL. So, get excited about that. Um, and there's lots of other great things here, but that's one I kind of didn't want to cover, but uh I thought was worth at least pointing out since they're all kind of very nuanced and there's quite a few of them. But moving on, um this is a breaking change that's worth discussing. So TraceQL metrics um currently in 2.8 uh count forward from the step basically. So if your step is aligned at a minute like he's showing here, um it's actually going to be counting the data in the future and rolling it up into the data point aligned with the minute. And that's kind of a little I don't I'm not going to say it's unintuitive or not, but it's just different than Prometheus and Loki. So, we realized this at some point and decided we're going to flip. And so, this breaking change might not even be noticed. And in my opinion, it makes things more intuitive. Like, I realized this was actually looking at a graph of Loki metrics with tempo metrics and they look different in a way that I didn't expect. I thought there was a bug. And the reason was simply because of this difference in how we roll up into the bucket. So, 2.9 will follow suit with Loki and Prometheus. Uh the bucket will basically be counting the last 15 seconds or 30 seconds or one minute or whatever. It'll be rolling it up forward into the bucket instead of kind of like backwards into the bucket. Uh Rouslon is a newer tempo developer. He joined us I think this summer. Um and he writes these awesome PR. So if you want to check out this PR um great details here and he he's a wizard with whatever this tool is he's using. Somebody knows. We all use it except me. >> It's Excal. >> Thank you. Thank you. And he joined in February. >> That was kind It's like the summer, right? It's >> February is basically the summer. >> It feels like it for me, too, because that's when I started basically. >> Okay. I mean, like in the southern hemisphere is the summer. I was thinking the southern hemisphere when I said that. Anyways, um so whatever. This is a neat change. Uh you may have noticed some alignment issues in the past and you may have thought there was some weird behavior and it may have been this. So, we're hoping this change will make Temple more consistent with other um databases and make uh some like kind of intu intuitions or some kind of like instincts you draw from the metrics to be more consistent across uh your data. Um this one's cool. We have been struggling with this is I love this story because Tempo has struggled with making traces for Tempo for uh I swear it's been like a year since we had really solid strong traces and Zach put some time in uh in the last few weeks and we completely got rid of the old Jagger um client. We completely swapped only to the new hotel SDK. We made sure all our contacts were connected correctly and currently our traces are were looking really good. They're great for debugging and it's been a while since that's been true, which is a little embarrassing for a tracing database, but here we are. Um, and Zach's put put some time in. He cleaned all this up nicely. So, he got rid of all the old SDKs, all the old clients, focused entirely on the new hotel client. It's the only client you can use now with Tempo, and he cleaned up all the context connections, and our tracing looks far better than it has in a while. So, this is a great change. Um, we are tracing Tempo. Obviously, you probably aren't, but you might be. And if you are, you'll be excited to see much improved traces in the next uh release in 2.9. >> For someone who might be unfamiliar with tracing bridges, how would you what exactly is that? >> Okay, so um previously uh we were using the open tracing bridge I believe to open telemetry. So open tracing was an API right like a in like a code API that um different uh clients implemented. So Jerger implemented open tracing and Zipkin implemented open tracing and we were using hotel through this bridge I believe. Um so Zach got rid of all that got rid of the the like open tracing bridge the shim in between hotel code and tempo code. um and focused like um made it so we used the OTEL SDK directly which has really removed a lot of bugs and really kind of just cleaned up our setup generally. So it's basically like threw out all the complexity. I'm sure we could have gotten it to work with all that but it just felt unnecessary. We're moving on from the Jerger client into the hotel client. Um and so we got rid of all that old complexity, all that old code and now everything is way way cleaner. Cool. And this one's fun. You may or may not have run into this. Uh we ran into this in production which was fun. The the title doesn't do justice to this bug. I'm mentioning it because um I'm mentioning it because it something you may have seen as an operator of tempo. But uh if you query the search tag values v2 endpoint within which takes a Q parameter. So you can filter down the tag values returned like the spa uh yeah the tag values returned by a trace query. But Graphfana never ran this and so we didn't really have um good experience with it. It just was there. And if you passed an invalid TraceQL query, it would actually break a tenant basically. So we had a tenant in production curl search tag values v2 manually not use it through graphana passed a query that wasn't valid trace which broke the parser and basically um started like basically creates a deadlock in their tenant. Nothing else is impacted. all of their tenants work, but this one tenant can't do queries and can't write traces anymore because there's like a deadlock in the tenant. So, we never realized this because Graphfana just doesn't support this and you know, we have some very basic basic smoke testing to make sure it works, but we didn't like push it very hard obviously and we didn't write like invalid queries into it. So, uh, Carlos fixed this for us and, um, now if you do search tags, um, it will return errors correctly instead of incorrectly, you know, deadlocking a tenant. So, if you have seen this, I'm wondering now if some issues filed at Tempo were the result of this. Um, but if you have seen this, we fixed it in 29. Um, I like this one too. Uh we one of the challenges with tracing data generally is uh getting good trace quality understanding like if you don't send good traces to a tracing database you're not using it for the right reasons and you're not going to get a lot of value you're going to be frustrated if you send good quality data which can be challenging we know this right if you're if you're using your clients well in fact we were just talking about how tempo sends bad traces to tracing databases and we fix that and our traces are way better we're getting a lot more value from them um so one of The challenges with setting up your tenants or like maybe you're a single tenant, you're just like a single or with one tenant um is m making sure this trace data is is high value um so that you can get good good data or it's a high quality so you can get good value out of it is a good thing to say. So this counter is neat because it's we've added a lot of these and if you're unsure of these we should maybe make a read me somewhere about them but there's a handful of metrics in tempo designed to give feedback about trace quality and one of the things is sometimes people just create bizarre spans with time stamps in the future or time stamps two years in the past. So this is a set of counters. It says future only, but it's a set of counters that um that uh tell you clearly uh if data is 30 seconds, 1 minute or 5 minutes outside of the sorry my bad. I think it's like 5 minutes, 30 minutes, 1 hour, whatever. It's a set of um time ranges outside of now so that you get a really good sense of if somebody's sending you bad quality data, it's not being stored well, it's not coming in searches, they're frustrated with you, the operator, um this particular metric is going to be good. it's going to give you one more signal for the quality of the data you're putting into tempo um so that you can um you know know that you're going to get good data or value out of it. So there's a handful of these. Like I said, I do wonder if we want to make a dashboard out of this or maybe make a doc or a readme um that wraps up all of our different kind of trace value things. We expose all these in cloud to our customers. I'm not sure if we've done a good job of um uh documenting them for open source users yet. Uh this is one we've talked about extensively. This is one of the major features in 29. Uh native MCP support. Uh we've talked about the last few community calls. Please go back and watch them if you're interested in like live demos uh and some other like detailed discussions. But the native MCP server means you can spin up Tempo um directly with no shims and no special stuff and point cloud code, point cursor, point whatever your favorite tool is over here at MC at the Tempo MCP server. Um and uh And you can get uh and you can use your LLM, whatever your favorite LLM agent, you can use it with tempo data. You can ask questions. It's really neat. Um there's a blog post I think somewhere. There's a lot of documentation. There's a nice um what do I want to say? There's a nice uh there's a nice docker compose in the tempo repo. You could pull the tempo repo, spin up a docker compose, fool with an MC with cloud code with some fake synthetic data to get a feel for it. lot of uh great ways to experiment with this and we definitely will highlight it in the blog post as well as ways to play with it. Um so have have some fun with this and give some feedback. We think there's a lot of work to do here. We've done the basic thing. We really want to improve this over the next couple months and releases. Uh finally final thing that I think is super cool in 29 is uh Marty did a lot of those performance improvements we talked about, but there's one that you can call explicitly and that's to add this uh with sample equals true hint. So um sample equals true will dynamically and automatically choose how to sample your tracing data to give you the highest quality signal with examining as little data as possible basically. Um real super quick demo here um I have I have a just a basic rate of everything. So this you know open close parenthesis is going to pipe to rate. So I want the rate of all spans. This is some random dev cell that we have um excuse me in internally in graphana. And I've added this with sample equals true. So this is the the hint that Marty was pointing out in the PR that's now part of tempo and will be in 29 sample equals true. And I've cheated a little bit. I've added this hint which just bus cache. So sorry. Uh so this cache equals 6 is just a cash flushing thing. This prevents it from you know the hash of the query from matching. It's going to prevent cache from playing a part in here. So rating with sample equals true um should give us a decent time. This is 24 hours and it's about 80,000 spans per second. Um a little over three seconds to do that. That's pretty strong in 2.8. If you were to run that same query right now, so same deal. I got my little cache buster here. I'm going to increment it to cache equals 6 and I'm going to run this one. So it's going to take significantly longer. And with a little bit of luck, it'll be really, really similar, right? We should see much better performance and only a very small difference in the data returned. And we can see these lines are really close to lining up, right? Like ah, you know, if you look real close, maybe a couple very small changes here and there. Um, if I look at the query inspector, it took 11 seconds, so over three times as long. Um, and this uh improvement, the sample equals true on our metrics endpoint, allows us to get very high quality data um with uh by dynamically choosing the sampling thing. You don't have to put any parameters in here. You're not going to tell it 10% or 100% or 50%. It's going to make all the choices for you. Um, and it gives a really high quality signal for, you know, a fraction of the cost. Um, I think we've also baked this into drill down already. So um if you use the drill down app, the open source drill down app in graphana or in cloud, it should automatically start doing this and speed up a lot of drill down to make that experience um snappier and better. Cool. Uh so that was 29. Um I don't know if we're doing questions now. I think Matt or Tiffany probably are way more on top of this, but we'll either do questions now or kind of at the end we'll wrap it up. But that was 29. We're really excited for it. Expect to see it uh at the end of this month. and I will hand it off to Matt Durham, I think. >> Yeah. So, uh, if anyone has any questions, we'll give just a m minute minute here for someone to post them up. And if not, uh, we will move to Siraj here in just a minute. I'm going to go ahead and remove your screen here, Joe. All right. Uh, maybe there'll be questions by the end. You're welcome to post them at any time. I'll I'm going to be here the whole time. It'll look like I disappear, but I don't. Um, and I'll be glad to answer anything and we'll pass it off to Matt, Tiffany, and Siraj or however we're gonna do. >> Awesome. Well, thank you so much for the 2.9 view. Now, let's turn our gaze to Siraj. Oh, uh, we do have a question. >> Okay, >> let's go ahead and address that. Uh, >> sure. Um, any chance MCP will make it into Docker Hub? So this is one of the neat things I think about our MCP server is that it's not a second process outside of Tempo. So you're not going to run, you know, a different thing that acts as a proxy in between your agent and Tempo. You just run Tempo. Tempo natively supports the MCP server. So when you start up 2.9, there will be an MCP endpoint. Uh there is a configuration option to turn it on. It's off by default. Um, but if you turn that on, it tempo itself directly supports the newer HTTP um, uh, MCP protocol and you can point Claude at it directly. The blog post did we Oh, we did have the MCP blog post. Thank you, Tiffany. Um, that blog post walks you through step by step of how to set up Cloud Code with what? Yeah, that's right. That's true. Um, like you're excited about. Um, yeah. So, uh, the blog post will take you step by step through how to connect to, I think, cloud, but it'll be basically the exact same steps for open source except way simpler because you have no O, you'll point directly at the port. So, yeah, no special anything. Start Tempo 2.9 up. Um, and you should be able to connect Claude or whatever your favorite agent is directly to it. >> Cool. Thanks for the question, Matt. >> Great question. >> All right. And if you think of any questions uh throughout the um Siraj presentation, feel free to post them up in the chat and we will get to them at the end. So now I'm going to turn it over to Siraj to tell us a little bit about cost or usage tracking. >> Yeah. Uh hello everyone. Uh I'm Sur. Uh I'm one of the developer on tempo team and uh I'll tell you about how to track the uses of uh your traces data. So uh tempo has a feature uh it's it's called uses tracker uh and it allows you to break down your injection volume by any label that you have in your trace. uh you know as you can see in this screen uh and uh allows you to see you know who's ingesting more data, who's ingesting less data, who's causing the spike in your injection data uh and stuff like that. Uh it's pretty easy to configure. Uh if you already have tempo running uh which I hope you are uh you you just need to follow these three simple steps. Step one, uh go to your distributor configuration and then set users.cost attribution enabled true. Uh we also have controls to control the max cardality and this is the cardinality of the the metrics that are produced by the the users trackers uh cost attribution feature uh because let's say you do cost attribution span name or or something high cardality label like uh you know tenant ID or something you you can have lot of cardality. So this this allows you to protect your downstream systems uh who are consuming uh these metrics and then the stale duration is like the Prometheus stale duration uh that you can control. The defaults are are good enough uh for most people but you can change them if you want. Uh the step two is you need to configure the dimensions on which uh you want your uh users to be broken down. Uh in this example I'm saying break down uh my injection by service name. Uh and it says service.name. This is the label that is on my traces. This is empty but I can put a label and this is the label that will show up in the Prometheus matrix. Uh but if I don't put anything it will just replace dots with underscore and it will show up as service name. And uh when you do th these things uh tempos distributors will uh expose an endpoint called users_metrics. And this will be an endpoint that exposes Prometheus metrics about uh your injection broken down by the dimensions that you've configured here. And then you can just point your Prometheus at that endpoint. Uh that would look something like this. Uh obviously these values would be different in your setup. Uh but yeah uh we have docs uh you can scan this QR code to go to docs uh and then the docs uh have details on you know how to actually configure it which is and this is what the metrics will look like and yeah uh so then once you have these metrics you can write a query like this that says hey you know uh show me the rate of the the bytes that I received and then break them down by service uh name tenant and the tracker and here I'm seeing uh the different ser uh metrics uh and the volume ingested by by services basically and then I have a little demo and the demo code is nothing but uh the docker compose uh example that we have in our tempo repository and it is configured to you know expose uh the cost attribution by default you just need to scrape it and uh this is my demo de demo environment I can show you what this metric looks like by default. I have it running for a couple of hours. So you can see the counter keeps growing. It's like a regular counter and then I can just say show me the rate and then you'll see the the rate and then you see all the labels. So there is a job label, instance label, service name uh you know this these are like stand from this label and uh this is the label that is coming from that dimensions config and uh it's looking at service.name name in my traces getting the value for that uh you know span or uh resource attribute and then populating it here. So then I can see you know who's driving my injection. You can see like most of my injection is driven by shop back end. uh this is just dummy data uh that I have in here and then I can you know like only have the labels that I care about because this is demo I have a single tenant but if you are running a multi-tenant setup you can see you know which tenant and then which labels that you've configured are driving your injection uh you know so yeah uh this this goes uh really well uh and you can see like this sums up to to match the the data that you see in the metric that shows the overall bytes ingested in the tempo. So you can look at the the rate of bytes ingested and this is around 2 2.5k uh you know if if we submit uh and then if we do the same for the users tracker bytes and then we stack them. Yeah, it's it's 2.5k uhish. So yeah it's it's accurate uh you know and uh that's that's the demo. Uh one thing that I want to point out that if you are using graphana cloud uh then you don't need to do anything. Uh we have a feature called cost attribution. Uh looks something like this. Uh it's uh not GA yet. It's in private preview. Uh so you can file a support ticket to get uh to get it enabled. But it'll be G soon. Uh so if you are a graphan cloud customer you can just uh configure your uh cost attribution labels and then it would just work magically. Uh there are talks and you can scan this QR code and I can show you the talks. Uh yeah so there are bunch of labels that are uh prohibited. Uh so yeah you you need to be aware of these labels but other than that if you are in cloud it would just magically work for you. And uh that's that's all uh over to you Matt. >> Thank you so much S. So we'll give just a minute here uh if anybody want to ask any questions or if anyone wants to ask any questions about what Joe covered or anything in general. Kind of open forum here for anybody who wants to just uh ask something to the Tempo team. >> Yeah. And of course, if you think of questions afterwards, um you can go to the uh Graphana Slack and go to the Tempo channel and ask anything there as well. And I see a question coming up. So, kind of new to Tempo, but I'm going to ask anyway. Can Tempo be used to trace and debug user actions on the client side? Example, I want to know what has happened on users mobile after failing to create a assuming that says resource. >> I can take that question mark. Maybe I know part of it at least somebody else here might have a be able to help as well. So tempo can kind of trace anything, right? It's designed for like backend systems and the way um you know like HP request might be serviced by you know a database and a cache and a couple microservices talking to each other from that level all the way to something significantly more complex. So that's the traditional use case for distributed tracing. I think you could do what you're saying but there's also kind of more focused products and I think they call that rum. Is that correct? real user monitoring. So um at Graphana we have something called Pharaoh which I think would be more designed for what you're saying like on a mobile or on a web page right so like on a user uh facing client a tool to get an understanding of how they're interacting and where the latencies are and what their experience is so I think Pharaoh from graphano would be like something that would fit your niche what you're asking about more directly and then if you just search for rum generally you'll find all kinds of projects and ideas in this area. >> And then another question is Graphana plan to have trace based alerts. >> Uh do we have Okay, I don't even know like some Okay, it got turned on accidentally. Somebody figured out how to do it in cloud, but you weren't supposed to. So I think somebody is currently running. Don't tell anybody that I shouldn't say that. Um I think there is a way technically to trick Graphana to do alerts right now on TraceQL in Tempo in cloud. somebody figured it out. But for the most part, the I think the answer is currently no, you can't. But yes, we intend this, right? Um I don't know the timeline. Like I've heard so many stories about it like getting enabled and disabled and turned on and somebody's using it not that I've completely lost track of when it's supposed to go and what the real stuff is. Um and then um I was going to say, yeah, like we have a PM somewhere that's supposed to know this. Uh, does anybody know does anybody know when the alerts go or real in Graphana? No. Okay. There was a PR a bit ago that they merged and then reverted or something. It's a very bizarre story. Maybe we should get you a real answer for the next community call. Um, because I like I said, I've lost track of the true state because it's changed so much in the past three months and a lot of people want it too. This is one last one last final thing to say about alerting with tracing. got to be careful with trace alerting. Um when you alert on something like metrics or logs, right, you generally have like a consistent signal. Uh if you were to build some kind of alert like more than one error per second or something, you know, alert me please. Uh things like sampling rates impact the data in tempo. So tracing is a differentish signal and you got to be a little bit more careful about the alerts you write to avoid a lot of noisiness. Right? If you were to a lot of alerts can be impacted by your sample rates, head sampling, sampling in pipeline, right? Um or the different places you would put this. So you gota be thoughtful about the kind of alerts you build on tracing. Um so do what you will, but I would say think for a bit about what your tailbase sampling policy might do or what would happen if one of the one of your clients went from 1% to 2% to 10%. Would that impact your alerts or not? um and try to write alerts that are like uh not unimpacted by changes in scale like that. >> And then Mal made a comment of we use the uses metrics in production. It's great. Thanks for that. So good job. And I wanted to jump quickly back to the question before so that Siraj can show Pharaoh. >> Yeah. Uh so this is the the page. It shows uh stuff like uh total uh what you say time to first bite I think first content full pen paint. I'm not a front-end developer so I don't know what this means. Uh but these are the the metrics I think the the front end folks really care about and uh it's it's pretty easy. I think there is a JavaScript uh SDK that you include and then you add uh events and then you ingest and then it generates like logs and uh traces both. So yeah, that's that's all. And then like for folks if you have like questions about things maybe like you're looking through the docs and you're wanting to ask more if you go to um graphana.comgrat uh there's gratbot and you can ask the aihelper questions and it'll pull things from the docs and such. >> All right, we'll give just a minute or two to see if any more questions come in. Um, but in general, uh, I'll take an opportunity to, uh, rep some of our conferences coming up. We obviously have a presence at CubeCon coming up in November. So, if you're going to be attending that, uh, I will be working the booth there. So, feel free to stop by, say hello. And of course, we have OBSCON in London at a date I don't remember off the top of my head. It is the 7th through 9th. I'm posting the link in the chat. >> Awesome. Um, plus any number of smaller uh other events we attend. So, if you attend one of those, find us. More than happy to talk to you. All right. I don't see any more questions coming through. So, anything else? Anything anybody'd like to add? See you next month. >> All right. See you next month and thank you for showing up. >> Thanks everyone.

