# Grafana Tempo Community call 2024-10-10

Join our next Tempo community call: ...

Published on 2024-10-10T19:14:07Z

URL: https://www.youtube.com/watch?v=jDi_Wy9mPWs

Transcript: cool all right hey everybody Welcome to Tempo Community call October 2024 Edition uh we have a pretty solid agenda today we're going to go through some of the uh obcon observability con happened a couple weeks back we're g to talk about that I was there as a lot of fun um we have a list of performance improvements we've been pretty heavily focused on performance and I think the next release will be very performance focused couple features but mostly performance um and I think Marty perhaps is going to review that SJ as well I'm not sure we'll kind of hand it over to that part of the group see what they say and then we have an api2 that jav worked on jav's gonna talk us about to us assuming he wants to if he doesn't I'll say some words if not jav's got it he he wrote the code um and we can also demo the full explore traces app it's live in graphon Cloud it is open source or repos been open sourced it's available on the grafana plug-in catalog it is 100% ready to go uh and as usual um please feel free to type questions in chat uh feel free to save them for the end this is always kind of an AMA always uh you're always welcome to chat and discuss um uh you don't need explicit permission to do that cool so we'll start with observability con um it was it's like a conference we put on every fall uh I think there's a handful of smaller ones too called observability con but this is the big one it was in New York was a lot of fun and we had two big tracing announcements uh one was the explore traces app was open sourced and you can find that now um let me share that link uh I think you can also um here's the repo I'll put that in the I'll put that in the chat here oh Kim typ in a bunch of cool stuff thanks uh so there's the repo I I linked it on the Explorer traces there uh it's open source you can also get it from the app catalog I'm pretty sure there's like a plug-in catalog for grafana super easy to go find it there and download it and give it a shop um really cool use of tempo shows off a lot of the trace ql metrics features that 99.9% of the population don't know but those of us who do really enjoy and so that was kind of the major point of the app app I think want to do a quick demo of it at the end we've shown it before in this call but I will I really want to kind of show the full thing like what we eventually release because you're only seeing partial uh results in this call the other big tracing announcement was the acquisition of pil control um which is to uh kind of get our adaptive tracing story going um admittedly that's a cloud that's going to be a mix of closed and open source proprietary so less of an interest to this call which is tempo but just a heads up that's there and we are excited about building out some of the the features there um for adaptive tracing and I think that's underway now if I recall correctly uh generally there was tons of Interest it was really cool lots and lots of people talking about the explor traces app um I had a lot of fun showing people how to use it on their data found a lot of issues which is the whole point very quickly identifying issues what's going on we'll look at some of that later and then generally there's a it was really an engineering heavy conference which I didn't expect I was expecting more markety but there was tons of Engineers there's tons of Open Source Tempo questions people were one group is running it at a million plus spans per second um I was helping them tobg issues they were having with their clusters it was a really cool uh conference uh if you had an opportunity to go I would encourage you to try it um cool I think that's enough on obson 2024 and I will pass this off to anyone who would like to talk about performance improvements oh yeah real quick I wanted to mentioned that I think we've demoed the explore tracing app in like earlier forms on these Community calls too so in past recordings like there would be probably some good Demos in there too yeah you could probably see the evolution of it through the through the uh past couple months we've been giving you sneak peeks for a while now uh what we have released I think is very strong I'm very happy what we released we'll we'll do a full review of that at the end cool all right uh Marty do you want to do some performance stuff I think uh also if you're interested yeah s since you're here do you want to cover the top half which is kind of like your PRS uh yeah sure uh so if you are doing uh tag lookups and typing in queries where you want to you know filter down on values like I don't know uh whatever values you have in your traces uh they will be faster now so hopefully you'll have to wait less for the auto complete to return uh another major Improvement is that is more noticeable when you acquiring High cardinality tags so let's say you're typing in something like I don't know block ID and it has like a bazillion values uh now it will be a lot more faster uh that's most of it and we did it with uh I guess fixing a bug that we had uh and then paralyzing the work that we were doing of you know collecting the results and also adding a cache so if you do the same query again and again so let's say you're asking for service name again and again uh now we have like a disk based cache in ingester where the files are on the disk we give you the results no need to you know look it up again and then the cache files fall off once the block is flushed so it's pretty neat yeah over to you Martin oh cool um yeah so I was going to add kind of like uh the auto complete lick UPS or we we are kind of pushing Tempo to its limit um in some of our installs with lookups of like 100,000 300,000 values for autocomplete so that's really what drove a lot of this um so yeah I think if you're in that situation you'll really appreciate some of these changes um yeah Joe since you're sharing yeah that's kind of like this section here so yeah I wanted to talk about um some Trace CU performance improvements that we had if maybe you want to click like the first PR or uh yeah so if you have particularly complex queries um I think these things will help um we've spent a lot of time like over the past couple of years really working on the storage layer right so um now we're kind of circling back and getting back into a little bit things that weren't at the top but now the next bottleneck is in the engine so if you've already doing things like using the latest parket you have dedicated columns um you know your repath is tuned this will be the next bottleneck and so that's kind of where we are so uh there's not an example here but like um yeah just you know showing some of the promising numbers for that one and you could expand that Benchmark little expanded B tab it's too tiny uh can you make it bigger oh sure yeah it's okay let me stop presenting oh it's okay Mar's got okay sure I was just wanted to show the dock I didn't realize there was a whole thing go go sir no it's all good I don't know okay oh sure yeah no it's okay uh I was just going to say yeah so we did you know kind of like the way to evaluate some logic and this is kind of like our suite of benchmarks and so like pretty happy about that I did put an example of what I mean like with a complex query so you have a lot of or statements um different rexes different conditions things like that so that's really where this is going to help a lot um this other PR is linked in here um take a look at that one but just together uh I think like there should be really helpful so if you already have a query that's like really slimmed down scoped variables it's pretty optimal already I don't think this will really help but if you have anything else like I think this would be good y excuse me I think uh today something like this right tempo's been really good at a bunch of anded conditions and we're starting to attack some of the right other queries that we've seen a lot of people executing in cloud and it's not fast as fast as we want and so right Marty is kind of uh making some good progress on that the second one of those is really cool the second one it's uh for each B binary operation it's prioritizing the one that is allowing us to skip the most comparisons and it's kind of doing that dynamically by tracking how much they're true or false as well as um how quick they are to run so reg X's will be deprioritized over like integer Compares automatically things like that yeah neat stuff good work everybody Javi the newest member of our team excited to present uh the V2 traces API which I think a lot of people have been asking for I think it was one of our more upvoted issues so uh jav if you want to talk about that a second yeah Okay so until now when you tried to query for a trise that is uh very big and this TR exceeded the limit that you uh config in in Temple you will get a 500 error saying that the TR is too large so we decided to uh create a new endpoint this traces version two and then we allow to return the partial TR so we sto uh adding more data to the TR but we just return the TR as a partial uh as a partial one um also this version now is fully oel compatible previously we had like an we structure when we are like a returning the trace as a Json um and I think that grafana is almost ready to use it Joe I think there is a PR uh yep almost ready yeah so Andre has been working on it and I think there's a PR up to support right the new endpoint so instead of just getting that era you're talking about you at least see some part of your Trace um which can be very helpful in determining why it's so large for instance what caused this what created all the spans there's some neat Trace Kil metrics queries you can do too but I think it's just overall a much better experience than you know a big red box which is what you're getting previously in grafana yeah awesome uh yeah good work hoby we appreciate that he's uh got a couple other great PRS in and he is also working on some good performance improvements too so it's kind of like I said the focus of the team at the moment uh to meet a lot of the growth we've seen in Tempo um and feature-wise we really feel like we've pushed it for the past couple years and uh we're taking a breather this release I think couple features in fact hob is also working on average overtime so there will be some features in 27 um but very but a whole lot of performance Focus as well all right uh oh is traces is the V2 API and 26 did you say that jav I think it is I I think it's not yet oh I think I told somebody it was I think that the version two is there but the the partial one is not there oh that's right it was two PRS I got to go find who I told that and tell them that's not the truth okay cool um not in two six sadly um all right next uh let's get into uh the explore traces app like I said we demo this a few times uh hopefully I will be able to share and not lock up like I did lock up hello everybody can you hear me uh oh no I can I can hear oh you can hear me okay so I can my whole computer now this is so sad I apologize took about I'll just I'll just talk about the next item on the list while uh my computer decides to start working again um Pavo and a red hat uh and actually not even sure who PR this may have been from Red Hat a fix for IBM Z series s390x which everyone was clamoring for in Tempo um so our Paro uh dependency assumed one of either little Indian or big Indian architecture whichever one's more popular uh and the Z series is the opposite and so it was crashing essentially on the Z series so uh I am kind of shocked and impressed that somebody went and figured figured out all the little assembly and corrected it all um and now we have support for uh some more weird architectures in Tempo and the question is can we update this in 26 and I think yes is going to be the answer I'll put it on my list we there was one small issue we had with a header that jav fixed that we also wanted in 26 so we can we'll do a 26 out one um it'll have this very tiny bug fix that most people haven't noticed but somebody did where we dropped a header uh on of our end points search or Trace I'm not sure and then we'll get this Z series um s390x fix in it's easy very easy to release a patch release um and we can get both of those out no problem cool uh thanks to Red Hat and whoever the who was this who was the original I don't know if this is a red hat engineer or not uh this was the original PR I'm not sure who V one of is but thanks to this person for putting so much time into this I think they worked on it for months but super cool they get this [Music] fixing all right um yeah grafana or graphon explore traces here is the final product not the final product I'm sorry here's what we released as the initial product we have a lot of ideas still to come um but this was uh a lot of work from a group Kim's on the call she put some work into it uh as a doc person we had some product people we had some ux people a lot of people poured time into this very proud of the team um and the idea was to encapsulate the things that those of us who work on Tempo every day know how to use Trace ql metrics to find problems and Trace ql select queries to find problems take those ideas and build them into an app so um anyone can use it and you don't have to go learn some new special cool language you can just uh you can just use Tempo immediately and get value out of it uh without you know learning a language first um so this is is the basic uh structure here uh we have different methods to like explore your tracing data so you just kind of look at the rate this is some fake data in SC demo is what we call it that's a demo environment error rates you can look durations um we have tools where you can see down here like if I'm looking at errors it's broken down by all these different attributes so I have two Services here one's erroring a bit more than the other we could kind of filter down and Home in on the one if we were interested and similar for uh duration so for our duration broken down by all these attributes we can see this particular service uh seeing spikes some of these other services not so much we can make decisions here about what we want to investigate what we want to dig on um we really built this uh around I think uh two major kind of like exploration activities uh the ability to like dig deep down through a trace so traces of course propagate context they have all this structure so that's what this tab does it lets us look down the trace to figure out why errors occurring or latency and the ability to look horizontally across attributes so the ability to correlate our errors against any attributes in our spans basically so we have these two kind of like investigation modes and then a third option here is I don't care about any of that stuff I just want to find some erroring traces and so I can very quickly and easily just look at this Trace list right here um to look at the we talk about this the errors this idea of like looking down through your uh down through your traces so this is an aggregate view of all erroring traces and it's showing us the path of Errors basically so I can see across I have some number of errors here it looks about one half an error second to an error second something in that range I can see in aggregate all of these are the same path it's always these two services and it's always terminating in this postgress um query right here so clearly I have some kind of issue with my postgress database right uh I can also excuse me run this comparison and this is going to compare um or it's going to correlate my attributes against errors so shocker uh the status code 500 is perfectly correlated uh to errors whereas some of these other status codes not so much 200 um response size that's interesting errors always have response size of 21 uh let's find something more interesting like span name okay span name might give us a nice hint like this particular span is airing a whole bunch uh and succeeding some and look whatever this span name is ingress uh has a few errors but mostly successes and these other ones are all fine they're all successes we look at service names and this is automatically comparing to all attributes both resource and span level um in our data so we can kind of quickly say okay uh let's see this span is a problem I'm GNA add this to filters real fast so now my data is refiltered around that uh span name uh glancing through here I can see I kind of have no more options right everything's pretty equal meaning there's no strong correlations between my attributes and errors and I think that's where this kind of plays a role more like I can correlate uh across horizontally what other attributes correlate filter down and now I'm going to use that like filtered uh that filtered view to dig deeper into my Trace cool and a very similar experience for duration so for duration um I can do this root cause latency which is going to show me like what the longest spans are it's going to throw away all the tiny spans and show me what the longest spans are in my data that are pushing my root span uh and comparison is going to do something similar it's going to look for those attributes that correlate to my um slowest traces essentially so very similar functionality here between these two one is more focused if you if I'm looking at duration they're focused on duration latency if I'm looking at errors then they become focused on errors basically and then finally the I don't want your fancy features I just want to look at slow traces that exists over here cool um all right uh one last thing I want to show uh those of us on this call know tracing data quite well and we know it's highly um structured data especially if you're using standard tagging like open Telemetry semantic conventions then all your database calls are decorated the same way and all your server like your cu calls are decorated the same way and we can use that to not just look at traces so at that what everything we look at so far was traces we needed to look at specific signals that occur in our infrastructure so I'm going to look at database calls and now I'm only looking at spans all of this now only represents spam that um uh were calls to a database I can see errors per second on databases across my org I can see the rate of database calls uh here's the all this this duration nowor reflex database calls I can perform some similar operations here that I did before to figure out well what's going wrong so now I'm doing my comparison tab on database calls I can see what attributes are correlating I can see this postgress query insert which we saw before is having issues I can see these other names like select uh delete some of these other names are fine so I'm using this information to uh hopefully very quickly home in on um which data datas are having issues I can see database name I can see uh this ffs is fine whatever this is fine uh but this postgress database is having issues so this view is going to very quickly home me in on the databases that are having trouble uh as well as maybe the operations uh so we have full traces view we have databases view uh we have all these other ones in here and I think we've built a very very cool and Powerful application for um uh for using Tempo showing off the the powerful features of tempo basically um oh uh one last thing is we are kind of specking out thinking about the next set of features here and uh no promises but I think the team is leaning towards a um kinder landing page right now you start here and it's a lot of graphs and a lot of detailed information and it might not be quite apparent where to go from here to learn something so I think the next step for this is a landing page that might just show top 10 slowest services or top 10 slowest endpoints or something like that you click on one of those and then you enter this second page with um with the filters already in place basically if you picked a service and a namespace and a cluster or something those are automatically going to be added up here and now I have a much more like um focus view on what I may be interested in finding cool so this is available in Cloud it's available open source I think it's a one-click install from the app catalog or the plugin catalog in grafana um please please get involved in this play with this if you're a Tempo user install it for your or give us feedback uh I would love to hear that people are using this and are successful and enjoying this for sure uh we are quite quite proud of this we've spent a lot of time building this I think it's awesome cool all right um I think that's it for the agenda I see I do have one question uh please if anybody else has questions throw them in chat unmute and ask whatever you're interested in Kim is asking for a time frame for 261 probably because she has to write the release notes for 261 if we were to release it um I don't know maybe we'll try to get it out um I don't want to say this week right because it's Wednesday or no God it's Thursday what year is it um next week I think is fine because it's not that big of an operation it's backboarding a couple people um and then the release notes should hopefully be pretty small as well because it's only going to be two PRS and they're they're pretty simple changes basically uh so hopefully next week we'll have a 261 um out there and I imagine it's not really going to impact anyone primarily doing this out of respect for our red hat friends who have put a lot of time into Tempo have invested a fair amount of time and we appreciate these fix and some of the features they have added cool that was a lot of talking any other questions thoughts concerns what's up Kim what was the response to the app tracing app at obson uh there's a lot of excitement um I think a lot of people are struggling with learning a new query language um when you work on a query language every day it doesn't occur to you that it's hard to use it because you know it so well uh but uh when you don't and you find Tempo and you think oh this is a cool tool and you want your or to start using it um it can be very difficult right a lot of in fact I bet 90s something percent of tempo users go to the basic search page and they select some attributes they select a span name or status error and that's it right so there's so much depth in what tempo can do and it was very difficult to show this off and so I think that's what people are excited about suddenly the things they knew about Tempo or the things they suspected you could with Tempo are so much easier um I really enjoyed in particular people would bring their laptops to me out there they have this like ask the experts desk thing so I'm behind there because apparently I'm an expert um and they'd bring me my their laptops and they would show me their data and explore traces and it was so much fun looking at everybody's data I could very uh I could very quickly often find errors or other issues help them discover why there was latency or um other problems in their infrastructure it was so cool uh to see for them to see how quickly this application immediately surfaced these problems that were difficult to find otherwise so I encourage everyone to give it a shot please give us feedback feel free to file issues that's an open source repo um it has you know I keep an eye on it kind of it's mainly owned by a fellow named Andre and Joey uh so uh but yeah file some issues get some uh get some excitement there ask for features let let me let us hear what your what you think about it for sure hey can hey you uh cool I think you're just in time for the end of this meeting you is there anything you want to say you want to add anything you want to wrap it up for us just give us a good summary of what happened in the past half hour awesome stuff just it was awesome all right if there's no other questions or uh comments we can wrap it up I really appreciate everyone's time it was a lot of fun like I said uh keep on using Tempo let us know what you think about explore traces uh yeah see you in the repo take care byebye

