# All about Explore Logs for Grafana Loki (Loki Community Call October 2024)

In this Community Call, Senior Software Engineer Trevor Whitney talks to us all about Explore Logs for Grafana Loki, ...

Published on 2024-10-04T04:12:48Z

URL: https://www.youtube.com/watch?v=XJMQbEuBeMc

Transcript: record no hello everyone and welcome to a Loki Community call for October we're trying something new so we'll see how it works let us know in the comments how you like the new format we're going to do introductions first hi I'm Nicole fer Huen I'm a senior developer Advocate and so is my colleague over there my name is Jay Clifford I am one of the other developer Advocates working as part of the Loki engineering team so really happy to be here and today we have a few people on the call actually um I'm going to introduce this person first Trevor Whitney is going to be the one in the hot seat today because we are going to be talking about something that he's worked on quite a bit and this is going to be one Community call All About explore logs um Paul over there if you like to introduce yourself as well I'm a team member on the the Loki team just uh here to support the community awesome awesome and looks like we've also got Julie hi Julie I'm the technical writer that that works on the Loki docs great all right so we we are going to be talking about Explorer logs please ask any questions that you might have in the comments below but first Jay you had a quick announcement yes just want to start off with so we know a lot of you have reached out about a prom tail bug that's been ongoing for a little while we absolutely heard you and we're really sorry we didn't get back to you sooner on this we dug up the issue we found it um we've done some testing over the last evening and day and we believe we now have a fix so the kind of the issue that we had with promil is we actually bumped up to a new build version um which invalidated basically a bunch of libraries within older OSS builds so unfortunately that mean meant de Debian 11 auntu 2004 had a few issues and incompatibilities what we've done is reverted the build image um and we've tested that on newer OSS um newer OS builds as well as the older ones and we look to be good so we have a PR up and waiting I think we just need to get that merged and cut a new re release for promail that being said we would love to get you on alloy I know it's a big lift and shift but we're here to help you through it there's been a lot more work that's gone into all of the features that were impr promail going into alloy we do see alloy is the future since it's going to be the full general purpose agent for grafana in general for all types of telemetry points um and there is a great tool if you don't know about it a migration tool directly in alloy that allows you to migrate your prom tail configs directly to alloy configs so hit hit us up in the community let us know how you feel um and yes we would definitely love love to hear your opinions about the Futures uh future of alloy and promail awesome and I will leave some links in the description below to what we're talking about so you know how to make that switch if you do need to um Trevor why don't you talk to us about what explore logs even is this was actually something that was announced fairly recently well at least the ga it went GA recently yeah like a few days ago in observability con so this is like fresh off the presses we did announce that we that it was in research last year I think um but now it's actually available for pretty much anyone to test and it's not just on Graff Cloud it's also on on OSS grafana so it along with explore logs there also explore metrics and then in research now we also announced explore es explor profiles what's what's with all of these explore apps can you can you tell us from the logs perspective what the problem was that we were trying to solve with this yeah so I think explore logs kind of rounds out our um vision of making um the logs and Loki available to um people with all different levels of experience um so you know when you s the the first sort of entry point into Loki is lql uh um and that is great if you are sort of a you know observability power user and um you know maybe you're an earlier adopter who was you know comfortable with promql and able to make that transition uh really easily um and then the next um sort of development we saw was in um the traditional Explorer view um we offered the sort of um low code mode if you will with the sort of query Builder so this is okay you know um maybe you're comfortable sort of um making customizations where you need to but uh you know maybe you don't always remember all the you know syntactic bits of log ql and so you kind of want to use building blocks to put your query together um and now explore logs is um no code right so query list like you don't need to write anything the idea is you should be able to point and click your way through the logs um and the way that we do this is um by um leaning really heavily into Loki's ability to produce metrics from logs um by looking at sort of like um counts over time um of various different aspects of your logs um and so the idea there being that we are going to draw you pictures that are going to help you um narrow down to the sort of small segment of time that you're um event that you're um investigating happened in and then um you know um that that allows the logs from that you know small time period to actually be useful right as opposed to looking at a wall of logs from you know a bigger time range and a bunch and across all your services yeah I think so I one of the issues with that I had with it initially was when you go to grafana there's like an Explorer section that is different from the Explorer apps can you can you tell us what the difference is and and where we should go for what yeah um so um the explore section right so we have the traditional explore mode which is the explore that everyone's sort of been used to in grafana that's been there forever right um and that's where you're gonna get this Code and low code mode um and then each um database has its own explore app under that section which is going to represent the no code pointand click mode um and so I think the the idea there be that um when you're dealing in like a code or low code mode low code mode the um way the access pattern for the database is similar enough that we can have one UI to sort of cover them all but once we um get to no code um the sort of different dimensions of the the data in our different databases become uh an important enough characteristic that we sort of need to tailor the whole experience and so that's um really what the the difference is so yeah you know explore is everything and then under that you'll see metrics you'll see logs you'll see traces um I think tracers is in resarch right I don't know I'm a logs guy it is so jumping on what Nicole saying there we we sort of refer to them as the explore apps um and we have metrics kind of pre-integrated into grafana do we have to do anything special with the the Explorer explore logs app do we have to install it do we plan to integrate that into grafana into the future um how how do we sort of how do people get use out of this app to yeah yes and yes um so it is um it's built as a plugin to grafana um and so I as of today um you do need to install it um but there are conversations to um incorporated into cor graa and I believe that we are I don't know if we have officially stamped that as go but um last I uh checked we are leaning very heavily towards um including incal so what that means is you know every single grafana installation um it's gonna be in the sort of group of plugins that the first time grafana spins up it's going to go out to the web and it's gonna tatch the plugin and it's gonna install it automatically um so as a end user uh what that means is um you shouldn't really have to do anything um and it should um other than conf configuring a logs data source obviously it needs one of those um and then that should be all you need to to get up and running so talking about log data sources then is there anything that they need to do to their Loki instances as well to make sure that all of this is up and running like any Loki config changes yes um you need to uh have um so the volume endpoint needs to be available and the pattern ingestor needs to be well um for the best experience the pattern adjuster needs to be running um so in the Explorer logs you'll have that um patterns tab um and so that is populated um by observing logs at ingest time and and generating patterns for them and that happens in the pattern ingester so um those two components um they are behind feature Flags I believe as of the latest release that the defaults for both of those are off um and so that's probably something that we will eventually turn on by default as you know we you know build more confidence with these things and more people are running them um but uh yeah for the best experience you'll need to enable those two features okay so you said that um you needed to for the best experience you have to enable the pattern ingestor can you tell us first what that does and what happens if they people don't enable that but still want to use explore logs yeah so the pattern ingestor um it sits in the ingest path and um as logs come in to the distributor we um set up what's called a t um and so basically yeah we just um T logs off into the sort of you can think of it as like a secondary ingestion um path and so the cost of turning it on is you're going to need to run another component if you're running in microservices um or if you're running in SSD or binary you're going to need to allocate more resources to the pods you already have um because we are sort of going to do this like secondary ingestion um and the pattern ingestor um is capable of um doing aggregated metrics um which is a very experimental um feature um that we have right now in Loki um that you know for high volume installations can um improve some of the features of explore logs uh but we expect a very small percentage of installations to need that and then it can also do pattern detection uh which is um it sort of reads these log lines as they come in it runs it through um an algorithm called the drain algorithm to basically sort of tokenize the log line and find similarities between log lines and um develop all the patterns that you see on that pattern um tab in explore logs so if you don't have the pattern ingestor running you're not going to see any patterns um on that patterns tab um and you're also not going to um have aggregated metrics available but again that is uh very experimental and you know um a lot of um open source installations probably aren't like needing that functionality could you also can we take a step back and then also maybe demo what this looks like what explorer logs looks like Julie did you have a question yeah no just wanted to to mention because we haven't said it explicitly you have to be on Loki 3.2 you have to be on uh grafana 11.2 to get um especially the Loki 3.2 the detected Fields API at 3.2 so if you don't have that you're not going to have the best experience but anyway just want to make sure we said that out loud that's perfect good win yeah 3.2 and 11.2 and potentially going forward we may have already just have it baked into grafana that's the plan not the latest release um but I heard a promise there is it just me I hear to Hot Seat yeah I did say hot seat yes H we have a comment from zven who says they're enabling explore logs by default for all on Prem grafana correct yes so all Cloud users um will have have this on by default so that we begin that um enablement um with the ga announce last week um so if you are a cloud user you have it um and then sort of the sort of open um question there is for um on Prem like open source installations uh it is not currently bundled but there is uh I think strong desire to to do that but you know I I'm a lowy guy you gotta go talk to the find think we can add a I add the link in for the docs into the channel so everyone's got the installation guide ready to go yeah that's awesome um Trevor can we would you be able to show us like what explorer logs looks like I can sure try great this is where we put in grafana play literally just like there it is like Julie I have a question for you do we do we have the Dos on this by the way is it is it sharing yeah hang on a second I just wanted to ask Julie if we already have the doc for Explorer logs yes we do okay in the meantime here is play what are we looking at can you make your screen a little bit bigger sure can yeah streamyard told me I should do two monitors so I guess I will how's that too big or perfect good but can you zoom in as well like I can command plus on it yeah how's that awesome thank you all right thus begins my career as a streamer look at this oh um yeah okay so this is playr.org so this is a um sort of demo like sample um grafana instance that anyone can access so you can go here um and you can play around um with graph features and so here is the explore section that we were talking about so the top level explore this is going to be your traditional Code and low code um experience and then each of these apps is going to be the uh no code experience and so we'll go to logs and so the first thing that you see is a um list of um services so one of the new features that um we rolled out in our latest um Loki I think it landed in 32 it might be in 31 as well but I know for sure it's in 32 is this service detection um where we are going to try and Define a service uh for each log stream that comes in and so you can either specify that yourself by setting the service name UMB or we're going to use a list of labels to pick from and I don't remember what that is off the top of my head um it maybe in docs um I can find that um asynchronously and post that later um but uh here are some example of services that it found and so it's going to show you the um a histogram of the log volume um by uh well volume in terms of count so uh count log lines um by level as well as a sample of the log lines um next to there so when I find a service that I care about so let's say I'm going to go in here into this ingester um so then I get a bigger version of that same histogram and I can select a smaller time range um and then these are the log lines for that time range um if I want to search Within These log lines I can do that here um and so that's going to find maybe just the log lines that are match my string that I'm looking for uh and like here we notice the histogram change so none of these log lines for example are erroring um I can get rid of that go back to this full view I can also drill into specific labels uh so for example maybe I know that um there was a problem in prod so I'm going to go here and I can hit include Pro um and so that's G to um now if I go back to my logs view this is only logs coming from the prod environment um another thing that we can then do is Fields so Fields represent um the um sort of field uh the label value pairs that are present in structured log lines uh so this may be a log formatted log line or a Json formatted log line uh we're going to automatically detect the type um if any of your log lines we're going to parse out the fields and then we're going to parse out the values so for example maybe I was um I got a page because of um some sort of like uh Max size of uh of a trace or something like that for a certain tenants so um of course I picked Tempo the service I don't work on uh you know it's good it shows that anyone right can naate any logs uh but let but now that I'm seeing that there's a bunch too large I'm realizing that's a bad example so let's say I was got a a page about something related to getting items from the cach and so I can go ahead and include that message um and now you know I can see like oh maybe like I'm seeing a lot more ER yeah look so this is all errors now um so I've narrowed it down using the detected field of this message and go back to these logs and now I'm sort of like able to to drill into this and maybe the thing I care about is like fetching these IP addresses or something like that um and then the last tab that we have here is the one that I was uh commenting is enabled by that pattern investor before you get to the pattern inest Trevor hate to delay so the with the F with the fields I actually had a question myself so with the um is it the fields detected is it unstructured metadata so do they have to put all of this data in unstructured metadata for it to come up in fields or do we actually just in infer anything from the log body that's considered like a key value pair structure um that's where I was like oh maybe this is where unstructured metadata is really important yeah so let's um also sort of differentiate the so structured medad is a specific thing in Loki so let's uh let's put that to the side for a second um so this is um so look based on the log line content um we are going to run the log line content through our Json parser and our log format parser um now our log format parser so log format is sort of like key value pairs like key equals value in space right that can pick up some Fields um in sort of like if you have a line that's sort of like mixed with some key value pairs in there and then also some just like text at the beginning or something like that um we we are running this format in our nonstrict mode so it will do it's best to pick up some of those values uh but I would say best practice uh to guarantee that your values are detectable in your log line is try to put in as much structure as you can um and then with Json the whole thing does need to be a valid Json object otherwise that will not work um but bringing back structured metadata as a as a um distinct Concept in Loki um so in addition to the content inog you can also um and you have to you have to do this via your push client whatever you're using to push logs into Loki you can attach structured metadata to um the log lines as well and this Fields tab does get both structured metadata and um parsing of the actual log line content so both of those um types of data are available here in the fields tab oh that is really cool so I I actually that's because I always wondered oh maybe you have to have structured metadata on by default to be able to use this service but if you're just creating structured um body Json data as you say with Jason anyway you can still get the use out of this Fields tab no matter what that's wicked okay and on that not question on that I was just gonna say you do want to have structured metadata enabled yeah yeah that's I was gonna ask the best experience um you don't best experience yeah so you don't need to necessarily be sending structured metadata with your log lines but we leverage structured metadata for things like so this is going off the detected level um and that is a field that we put into structured metadata which don't see it in oh here right here so like detected level um this is something that we are um so detected level is something that comes if you have a level lab we'll use that otherwise we do actually parse the log line content to try and get that value out like we look for the the string error or something like that and we do put that in structured metadata so for the best experience um even if you're not sending structured metadata in your pushes you still want to have it on okay I think that I think that actually raises a good question Nicole and that someone put in from the community um they've uh nice there we go so another crappy pianist ask or says eager to have this for our grafana Enterprise deployment but we first need grafana to support detected level with a log volume histogram lest our users complain about the sudden loss of color I mean color is important I I I understand it's humorous but but like you know we're a visualization tool too color matters totally no and I mean it's is very a very colorcentric experience right to like drill into what you want I I'm gonna plead the am Aoki guy card again um so take this with a grain of salt but I do think the detected level support has been merged into grafana um I know that they were working on that so um maybe it's just pending the next release um I don't know Sven is still watching and wants to comment he might know better than I am I do but uh um yeah I do I know that it's I know that it's planned I think that it's merged okay and on the structured metadata thing so it's I thought that it had to be enabled but it doesn't actually it's just a better thing if it's just a better experience because it allows you to do more things but I suppose if if people are using otel then we should recommend that they we would recommend that they enable that absolutely um o the Hotel this structured metadata was introduced into Lo Loki largely um hand in hand with our hotel support I would say you can't really have one without the other yeah okay so another crappy pianist also says oh yes detect level support has merged it's coming in 11.3 they're patiently waiting the code freeze on that is tomorrow so you know we will talk more about 11.3 shortly um so there's one more uh tab I was wanted to show here which is the patterns tab um and so this is where patterns sorry this is also our opportunity to ask you the stuff yeah yeah um the patterns because you said you that you would recommend that patterns the pattern in gestor be enabled so is is there something extra that it that some extra patterns that are ingested specifically for Explorer logs that aren't normally ingested if people aren't that aren't processed I guess if if people aren't using explore logs yeah so all of these patterns are specific to explore logs and um are are produced by the pattern ingestor so um one thing that is um um yeah yeah so yeah basically um all of the patterns so you won't um you won't see any of this on on this page um if you don't have the um pattern ingester enabled is pattern ingestor only for explore logs um it is yes um and we also have added pattern support to the query language so if you come in here you can add one of these patterns um to the um to the actual query and filter log lines based down on that pattern only oh wow did not know that yeah all right let's let's back up here okay so can you tell us what a pattern is and is this in any way related to bloom filters um no um let me you're gonna stretch my knowledge on Bloom filters Bloom go there's a whole topic on log patterns in the help just yeah so Bloom filters are doing a form of tokenization um but and then this is also doing a form of tokenization but they are separate the pattern the the tokenization done to support Bloom filters does not use the pattern ingestor the pattern ingestor logic is solely for the purpose of um producing this page um and the tokenization as you can see here is we look for in we we we pass all the log lines through a drain algorithm um for um the pattern ingestor here and we try to pull out High cardinality values so things like you know the Tim stamp and then these sort of like version old vert like these things change a lot per log line but then these level debug like so so basically what we're finding here is there's a broadcast. go whenever it uh emits this me message um it's sort of always at a level debug right it's always from this file and then there's these pieces that change per uh log line but if you were looking for something like um I mean maybe this one's a better example where it's like I'm again I was looking for those mcache errors uh maybe I don't care the specific text of the error message or I don't care about the specific timestamp but I want to find all of my failed to get keys from mcash uh and so I can include that um there we go so now we're only getting these um failed to get keys from mcash errors so this pattern has now been add added to the actual query um and filtered down and now this is maybe a bad case right because I also have this message here um and so I mean patterns might have been a quicker way to get to sort of vog lines with that message um because we have this nice sort of view of like um how what percent of my logs are are this pattern right um and so you know in this time range 14% of my logs were this error message um so that's sort of the usefulness of patterns um but yeah patterns are being a lot used a lot in Loki in a lot of different ways right now so I can understand that sort of um confusion um and and uh thanks Julia it sounds like we have a whole um whole docs page helping to uh sort of alleviate some of the confusion there so so there was um as a we had a few questions in the community before we get back to the whole install thing again but we what we could do is like jump into the actual docs of where that is and and show that bit the guide and we talked about it a little bit at the beginning um but the first B I wanted to mention which was interesting was someone saying can we we we talked about a little bit of changing say can I have something else inferred as service name and I believe there's about we talked about this earlier about sort of 10 different labels that could be inferred as a service name but we're also working Trevor to actually allow you to use other labels in that section is that correct that's something in the pipeline um because we know not everyone refers uses Service as a as a to like as a naming convention within their company that is correct um so um first let me get those um names um I'm just going to Ping that one up on screen where I got that question from that was from another crappy I love the name it's so good um so yeah if you so the the field that we use is servicecore name um if you send that field we'll use the value you sent if you don't send that field we will look for um in this order Service app application name appor kubernetes iore name um a container container undor name katore container uncore name component workload job katore job uncore name and a lot of those like Kates uncore ones is because um like that's a lot of like otel stuff where you they have the dots and we convert the dots to underscores um because we have that um restriction in Loki that they have to be underscores so those are the list we'll use if you don't provide service name um or or you can s provide your own um but the second question there is um yes we are actively um so designing um sort of uh incremental improvements to explore logs um where you can pick a different starting point so currently you you have to start with service name right um but we are working on sort of a custom um selection of labels that you can start with um my understanding is um where things are at right now is that you're sort of always going to need some sort of like index label for us to like show you the the those top level histograms for uh but that we're going to allow you to um Supply additional labels to sort of narrow down that initial um search um criteria um and that is all under active development right now very okay well in in their case they mentioned that they're using service equals Loki for example and you said that's one of the the labels that we would pick up already so they should be good to go on that front right correct yeah they're setting a service label then yes that will um that will be picked up awesome and a followup from them thanks I definitely need the set of labels to be customizable as there's no way you'll support the label will use oh it has a company prefix because why not God oh um so that list is also customizable um so you can set that in your uh yeah it's it's a it's a per tenant um so runtime override um so it's a per tenant config um so just like we we have um the per tenant limits that you can set you can set the per tenant config for the list that you want to pick from so um so for Environ variable that it's an environment variable uh it's in the um the runtime config file oh okayi yeah the the Loki config yam file let me right okay um all right let we we have a question on pattern as well let's let's do that before we go back and talk about enabling Explorer logs um Andre zani asks does pattern work with unstructured logs or only with Json or fmt um I believe pattern works with unstructured logs yes um like the rules apply is like it needs to have like a key value pair for it to be picked up as like a or yeah I mean I I don't know what the patterns would look like right so if you have unstructured logs that are just free formed text I would imagine you're not going to have have a lot of recurring patterns in them right a a pattern usually requires structure um to pick up but like if you have something like a mix where you have maybe like a time stamp in Brackets and like a level and then you have like key value pairs or something like that like the pattern in just it would pick that up right where um it would recognize sort of like the time stamp is high cardinality and so we're going to we're going to token that just sort of template that out and then um it'll like recognize okay all of these debug lines have this you know key value pair in them um yeah yeah access log so in that case would the patterns would there be patterns shown for an access log there should be yeah um I don't know if we have any data streams to set that as an example um we should do that next tutorial let's get it on all the different stream tpes yeah don't don't promise any tutorial live J come on we says engine X logs are a good example pattern examples are included in the documentation yeah you might have to and this is great CU we can use that we could just directly monitor the patterns from like like low Keys engine X on the lowkey Gateway there we goit of inception yeah L Inception I'm GNA share my screen again real quick Nicole if you could awesome there we go um and here we have uh so here's like engine X Json mixed right um so this is like a mix of we have Json lines we have sort of these lines which are you know I'm going to wrap the lines so they're easier to see there but like uh but now they take way more room so like this line which is just a stack Trace right where this is like a mix right of like a string and then there's some structure with like you know key value pairs then we have more Json so yeah let's see what this does for pattern so yeah we can see here right it picked up this pattern it also picked up this Json pattern um exactly so I guess you know back to Andre's original question I think the confusion is that yes so the answer is yes it does work with unstructured logs and but that's different from like structured metadata is different from structured log so it works without structured metadata um correct it doesn't have you don't have to be using otel basically uh and and you can but you will need some structure sure in the sense that for a pattern to be picked up there have to be things that are structured the same way yep I think that's a good a good synopsis yeah okay can we talk about how to use this thing so I think we could like maybe just I think what we could do is like jump into the docks maybe um and then like Point them at those bits that we discussed at the yeah and and like because we sort of mentioned we sort of mentioned the bits you had to enable in low key and we mentioned sort of like how to install but maybe we can just quickly scan through the document and and sort of paint a picture of those bits yeah okay so first uh and and by the way Trevor let us know if if any of this is wrong but um we do need to have you're going to need Loki and grafana in a specific version so as Julie mentioned earlier it's going to be 11.2 for grafana and then 3.2 for Loki and then the only real um so here it says that pattern ingestion needs to be enabled and you're saying Trevor that that's not actually entirely I could it it's it's better if we do but it's not technically required is that right if you don't enable it you won't have the patterns tab yeah so um for the best experience you I would recommend enabling it and then it looks like we did not document allowing structured metadata so I probably need to update for that um that wasn't mandatory either I we still want to like as a best practice set people up for Success yeah yeah agreed that is what we did for like for the tutorial that Jay and I did by the way I will add a link in the description to the what is Loki video where we go through like I I I showed how to do how to use Explorer logs in oss the only difference there is I used a an earlier version I used the grafana 11 preview version which you don't need to do anymore it's now just going to be from 11.2 um but in there I do allow uh I do allow structured metadata and also the pattern inest Nicole I actually have your tutorial here the Loki fundamentals one if you just want to CH me access and we can see all this in a config we're all sharing yeah so I just I feel like just to show because there was a few bits there that's a little bit confusing around pattern ingestion and how you enable that but you can actually so to pain in context here we have Loki um like basically single of course oh dear command come on come on it's not giving me anything there we go there we go just bringing them all so the the ones that we have so we have Loki just running as a Docker container to keep it easy um and we actually have this as our Loki config so we actually have everything in the limits config at the moment but as Trevor said you can do this on a per tenant basis not just in like the limits config so the two ones that you'll see that are necessary or well one that's the least necessary and a best practice here is we have the limits config and we have our volume enabled true and we also have our allow structured metadata as true as well um the one that I thought was a little bit confusing in the docs maybe we can have an edit there is you can actually enable the patent ingestor inside the Loki config as well rather than just setting it as a flag when you run Loki so you can just add a pattern uncore ingester and enable true as well so this is kind of like a bundled up version of your loky config to make sure that you are ready to use um our log Explorer app um and then I guess we could roll over onto the grafana as well um so in this example here we've got grafana set up as Docker container again um is this I'm hoping this is still this might not be as valid anymore if we should probably go check the the docs again um but essentially what you need to do is install the um basic because we've installed it as an artifact Nicole and I'm not sure if this is still the way that happens in the docs yeah it still is so I'm switching here to the docs we still have the integration artifact it's it's still going to be the [Music] same brilliant so here I'm here it's this is the the as an environment variable uh whereas the one that you are sharing is through Docker compose but yeah nice and I think that is everything so once you have those installed those parts of the Loy config enabled um you are good to go um the the only bit that I'm not so sure about is if you need to restart your grafana instance if you're not using um a grafana container or if you just use say the grafana CLI I believe once you install the plugin it should just load automatically I don't know though that should be tested and J if I could uh if you could bring up your config one more time yeah go for it um if you go back to your Loki config absolutely one thing to call out please do it's nothing bad it's right uh in your schema config you're using schema version 13 and 13 or higher is required for um structured metadata oh that's a really good point oh I didn't know that I think you Haven migrated oh as well ASV is also required okay but this that's for structured metadata not for explore logs we want and this is like getting everyone off like bolt storage and everything like that we kind of want everyone to move across to tsdb anyway and make sure that you're on the latest sort of indexer yes please yeah yes please please I can delete all of that old the storage type Co code is going to be a great thing amazing yeah okay all right so so back to like what exactly we need um so it's the just flipping back so it is the volume enabled true and we would recommend also pattern ingestor should be enabled and the structured metadata and then on on top of that um we need the the two versions of of grafana and Loki that we talked about plus you need to enable the grafana plugin the explore app but hopefully we do we just build this in by default in the future potentially I think that would be better yeah okay what else is coming for explore logs other than baking it into graphine oh and by the way I had a question about uh how this is available in grafana cloud and in the OSS version are there actually differences between the two are the things that you can do with explore logs on grafana cloud that you can't do with OSS grafana nope everything um that we're shipping to the cloud we're also putting in OSS um we think that this is you know we we want this way of interacting with Loki to be available to everyone that's using Loki um and you know because you know we think that um you know you you may um your observability team may be super comfortable with like a a code approach to accessing logs but you're putting this in for your whole team to use right for the whole company to use um and so um you know we believe that you know everyone needs to have ways that they feel comfortable accessing this data and so yeah absolutely um part of our sort of Open Source Mission so they're all they're both the same really love that that's awesome yeah and so it seems like explor logs in general is like um a more opinionated version of like here are the logs that we think you should look at first or here's where you should start at least when you're troubleshooting and we may not get everything that people we probably won't get what people are looking for but it's a good starting point it it's like like um a pack prepackaged dashboards basically but specific to your logs right but is there a way to change which dashboards those are like can you can you say every time anyone in the company or anyone um who uses this instance of grafana uh goes to explore logs I want I want this to be displayed in a slightly different way or something like that or would you recommend that that's when you start to actually build your own dashboards rather than using explore logs yeah so not currently um so you asked like sort of what's in store for the future um so the the big thing that we're actively developing right now is picking another starting point right so if you don't want service you can pick another starting point um I I do know that already um when you pick a service when you come back Explorer logs remembers the services you've picked in the past um so as a user it starts to learn the things you care about and Bubbles those up to the top of the list um I don't know um if that functionality is planed to be extended to like picking other starting points but I could imagine that being pretty useful um like if there are a few different sort of yeah starting points that you have you want to sort of persist those tabs to to be available whenever you sort of come in um I'm not really sure what that experience is going to look like exactly um but yeah I mean the other big thing is um we want people to use it and give us feedback and um let us know I mean you know what one thing if you are a power user who is comfortable with log ql um we would love for you to use explor logs and and sort of tell us which situations it's working for and which ones you still need to sort of bail out um into like a a full code mode um so that we can learn um there is a a button in explore logs um that says open and explore uh and so what I've actually found myself doing throughout this development process is I'll start and explore logs I'll narrow down a label set and maybe I'll get some filters um and then maybe I want to do some complicated metric query you know craziness or something like that and so then I'll bail out to explore logs but I'll already have 70% of my query written for me and then I just need to tweak a few things um and so yeah we I would encourage people to who who are comfortable with the code mode to use it in that way and then um send us feedback on on um things we could add to explore logs that maybe would get them further down that path before they had to bail out and then I would also encourage people who who aren't comfortable with the full code mode to to use it and um sort of let us know the problems that it is solving and the ones that it's not for them yeah and I know internally it's gone through some iterations right like Matt ryer I know is working on like what exactly should should be in the explor logs like which dashboards exactly is it too much is it too little information so that stuff is still like that could still change if we get feedback about what's actually useful and what's not absolutely and and yeah you mentioned Matt he's sort of um working across all the Explorer um apps and stuff and so we're sort of trying to find um similarities where things work things places where things need to be different uh and um yeah that whole Space is just under very um sort of heavy development right now so it's a great time to um chime in with the things you want to see or or the things that you find aren't useful so I have a fun question about the development to sort of like help round this off um so you've been on this since the beginning Trevor at least you know the development process of it um do you have one moment which was a keyboard forehead slap moment where was like this is difficult this is this is never going to work and do you have a moment where you were like this is really cool what we've built you know chair flip moment this is great absolutely so um yeah I mean we went through a few iterations of this app that never saw the light of day um tell us and one of the things that was really hard was uh um you know figuring out how so you know explore logs it it queries a lot out of Loki and so if you're looking at a big log stream like our own uh our own usage of Loki we're we're one of our our own biggest customers and how much we at graan Labs log um and so we have some really big data streams and then you know sort of getting Loki to like you know give us these um query results over especially over like longer time time periods we were just sort of like pushing it to its its Brink um and so we um you know I think there were moments where we were being hesitant about the experience because we were trying to design within the confines of like what was possible uh but then um we had a a hackathon project so um for those who don't know we do I think what we're doing what about three a year hackathons two or three a year and um I this is just a week where graphon Labs employees get to work on whatever they want um and a large percentage of those projects have actually turned into products and the current the iteration of explor logs that we shipped that people are seeing today was exactly that it sort of said okay let's just like forget everything we've done trying to solve this problem and let's just start from the user experience like assuming anything was possible what do we want users to see how do we want users to interact with their logs data um and that's where explore logs came from and then of course we shipped it and then it was like okay now make Loi handle this right which was the fun part um and the challenging part but it uh I think that was really the sort of aha moment um and a great sort of lesson in in product design you know where it's just sort of like okay stop stop limiting yourself to what's possible put out what you want and then make it possible um and so that's what we did and I'm really excited with what we shipped yeah I did some digging recently actually about what we've publicly shared that we handle the amount of logs that we handle with Loki and it's 324 terabytes of Vlogs a day it's a lot yeah so we definitely you know we have to put our own stuff through that so yeah we dog food everything we we're we're right there with you yeah and that's the sort of thing where you know I mentioned uh aggregated metrics earlier in the call like that we have you know in building Loki we are thinking about a lot of different use cases right you have your open source installations which often tend to be small um right and they don't need the same features that 32 terabytes a day needs right um and so you know we're definitely trying to make this sort of like um loky work for all of these these use cases and so it it definitely introduces some complex problems where like like the the version of explore logs that we shipped that so that came out of the hackathon that worked great on small installations and nothing needed to change uh right but it's sort of these bigger um installations and and like a lot of our our um Cloud users or our on Prem Enterprise users that are really sort of stretching um what Loki can do um sort of we needed to to put in considerations for them too so that it works for everyone yeah and another problem that I see for for logs at that scale is that the you know the query times become even more important and I can imagine that that's also something that you had to think about with Explorer logs in the observability con key note gen via actually threw in a little nugget I was like wait wait I want to double click on that because she said what next for explore logs is streaming support for graphs and log lines shown for faster query times can you talk a little bit about that before we end things yeah so uh we are currently running this in our own Ops cluster um so um we of course use all the graffo Su of products to monitor our SAS offerings um and um yeah this is a really neat feature uh that my my colleague Matias worked on um where he sort of um in the front end slice and dices up the query into these a bunch of small chunks so that you can get S of the streaming um result and so um it is it's behind a a feature flag um in the latest release um so you can turn it on and try it but highly experimental um so warning caveat uh and again it's only something that asterisk probably only something that big installations are gonna um really benefit from um but yeah um basically what happens is so um historically what we would do especially for these the metric query so um one distinction that's important here is a log query in Loki is gonna have a line limit uh and the default is a thousand log lines so you're as soon as we get a thousand log lines we're GNA stop querying um because combing through more than a thousand log lines like as a human that's not useful right um but metrics we paint a picture across the whole time range so for metric queries we are G to actually touch you know every log line um and um these are the queries that can sort of as a result be really slow and so these histograms which are these metric queries U what we would traditionally do is we would slice it up into time chunks and then and you might people may have seen this in Explorer where we sort of um will'll fill in the graph from uh right to left right and we'll do sort of the the most recent hour and then afterwards or I think maybe it's a 24-h hour split I forget exactly what the splits are but we sort of do it in these chunks what we have um what we're planning for the future um and currently testing is instead of splitting it vertically in these time slices we actually split it horiz ially by using the this concept of sharding that we have so um as Loki ingests data um it wants to create streams of equal size and in order to do that it um adds the this this sharding aspect to the streams um so big streams get sh sharded into many smaller streams and by leveraging that same sharding in the front end we can sort of put produce these graphs um that like fill in so when you first go to the page you might see like a small Spike and then as the rest of the shards come back you'll see that Spike maybe gets a lot bigger or maybe that Spike goes away and like sort of levels out uh and the whole goal there is to get you to the time range that you care about quicker um because you might only need three shards um to find out that there's a huge Spike of Errors because assuming your your system is spewing out errors there's going to be a large Spike of errors in Every Single Shard um so maybe we only you only need three to be able to zoom in and go deeper and at that point we can cancel all these other queries um and then you know issue new queries in this smaller time range and and get you to your logs that you care about faster um so that's sort of some of the sort of like the the streaming features that that we're excited for um we're also going to um working on on that for um log results as well so we've we're currently testing this for metrics um and in active development is doing this for logs um so this is really exciting if you are sort of looking for you know what we call like a needle in a Hy stack um you're looking over this time this large time range you're looking for a single string um we have a bunch of features coming out to help with that blooms being one of them but then another one being sort of this same sharting approach so that we're going to increase our chances of finding this needle quicker and then showing it to you as soon as we found it as opposed to you having to wait um for all the all the shards to have been processed before we show you anything so very excited about all of that awesome thanks for the look into the future um thank you also for just coming on and answering everybody's questions including our own the hot seator logs yeah thanks for agreeing to sit in the hot seat yeah I'll be careful before I do that again geez we're gonna have to we're going to have to give him I don't know something that he likes so he'll keep coming back Trevis says he wants his sponsorship deals so next time he comes on he'll be repping something and it like he'll have a little advertisement time so Trevor will be good yeah click on the link over there for Trevor's patreon I mean ski season is almost here so you know I would love you know to like you know okay that's what Trevor wants if you would like to to learn more about explore logs everything that we've mentioned the documentations page the link to the community forum for Loki uh to our slack as well you can reach all of us there all of that will be in the description below and also if you didn't notice the this was a new format of community call we're trying a few things out did you like joining in this way leave us a comment to say what you liked and what you didn't like and we will respond and try to adjust accordingly anybody else have any party messages yeah just really quickly I know that there are probably some um comments that we didn't get to um we do have an explore logs channel in the um public Community slack um so I I try to keep an eye on that um so if you had a question that didn't get answered please just send it over there like this one do you have a quick answer this yeah the short answer stream rate limiting will what you describe become problematic you're so good at just like reading the question and suppos they just jumping in answer um yeah so I my my hunch is that if you don't need to charge your streams that you probably don't need it at query time like you you my guess is your volume is not problematic enough that you're going to need the feature on either end um and so um I think you know um I no I don't think it will be problematic um because the the the logic works such that um it also will it looks to see if your streams are shed before trying to do the started queries so if your aren't shed it's just going to do the regular queries and your volume probably fine um all right now we're really saying goodbye thank you everybody go bug Trevor on community slack and give him some love so he'll come back on the hot seat thanks everybody we'll see you next one for the next by bye

