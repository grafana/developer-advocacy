# Grafana Loki: Best Practices for Recording and Alerting Rules (Loki Community Call February 2025)

In this Loki Community Call, we talk about the best practices for recording and alerting rules with Loki Staff Engineer George ...

Published on 2025-02-27T05:27:09Z

URL: https://www.youtube.com/watch?v=3_DtwTOppiI

Transcript: hello hello everyone it's my second time on the hot seat and I am going to get it right this time my name is Jay Clifford I'm a developer advocate here at grafana labs this session is all about alerting today but first I'd like to introduce you to my co-host once again oh that way yeah hi everyone I'm Nicole ven uh good job Jay remembering your name this time today we have over here George here to talk to us all about alerting this is actually Jay and me asking all the lowkey questions to to George um alerting is something that's super confusing and George is going to be um he's actually in a in a pretty good position because he used to be on the alerting team so we're going to be asking him some basic questions about how to do a alerting on logs and all the different methods for doing alerting but first we have some announcements um first is that Jay and I are both speaking at cubec Con in London in April so that's really if you are going then why don't you check us out do you want to talk about your thing j yeah I mean um so this is with my colleague Tom we had uh a lot of fun with this so essentially what we wanted to do was teach people about the different Telemetry signals in observability and specifically about open Telemetry so we made a text-based adventure game to teach people how to use each of the Telemetry signals logs metrics traces and eventually profiles so we'll be playing a live game at cubec Con on stage so quite excited for that and I'm going to be talking about Asar zeroth law of Robotics of observability for AI I am a huge Isaac azimov fan he had the three of Robotics and I'm going to propose law zero which is that a robot must be observable before we know whether or not it can rise up and kill humans but that's good that should be fine I'm going to be tying in Ai and Ai observability and testing so if you're going to cubec con come see both of those talks as well as six other Talks by graphin estas so pretty well attended from our side this time we've also got Loki 3.4 which just released that's kind of relevant to this um Jay did you want to say anything more about that yeah Loki 3.4 was a a solid release it actually included um basically the new Thanos API um we're basically trying to unify the object storage API across the board so it now works alongside mamir um pyroscope and Loki all now share the same object storage API um there's no need to move to it just yet um it's pretty easy to configure in your Helm values files and into your Loki config it's just something to consider in the future that makes things a lot more easier for us to maintain going forward having sort of a unified API to work on so thank you to the fosf maintainers for doing all that work on the API it's made um Loki mimir and um pyroscope um better DBS because of it that was really good and we' o got graphicon in May this is the last one uh if you'd like to go to graphicon then then go to this link it's gr. fancon and you can you can still sign up to go to it the cfps have closed um I I'm not sure exactly who's going to be there yet because they haven't released who's who has who is going to be speaking but it is one of the most awesome conferences because it's all Community Based and everyone there is like a super user of some graph or multiple grafana projects and it's quite unique in that normally when I'm at the booth I'm the like grafana person that people are asking questions to and in this conference everyone knows a lot more about gra well not everyone but like these are people that that really use grafana stuff heavily so I learned so much just by going um but why don't we jump into the our topic for today let's talk about George George what are you doing here how long have you been working at gravana the hot seat George the hot seat tell us about yourself yeah so uh my name is George um I'm a software engineer on the Loki team but I've been at graan Labs now for I think this will be my fourth year or the end of my fourth year um and I spent the first three years of that actually on the alerting team before I joined the Loki team so I was hired in on the Loki team um but I've been using grafana now I think because I was thinking about this the other day I think it was 2016 serious I think 2016 and at the time I was working as a Sr and we were using the tick stack which for those people who don't know that's Telegraph influx DB um capacitor and chronograph um but we use grafana as our visualization tool so we were using influx DB and Telegraph Telegraph is like the agent influx DB is like Prometheus but we using grafana to you do our dashboards and stuff like this and I've been using grafana ever since that ever since 2016 every job I've worked at and now I work at grafana so and you've also got some fans in the the chat George as well um so L was like Gana is lucky to have you George so the appreciation and love is already in the chat oh that's very kind so today alerting um I must admit I'm gonna I'm going to break the cardinal rule here and say I am terrible at alerting I have I feel like with Loki I do a lot of querying I do a lot of ingesting but when it comes to alerting I have no idea so I think like the cool thing would be to break it down to its Basics and then we're going to build it up talk about the different components litter in all of these we've actually I must admit thank you to everyone who put in questions we've had like the most amount of questions from the community on alerting so clearly it's a big topic for you all so we're trying and get through as many as we can but let's talk a little bit about the basics first um can you tell us exactly what an alert is in Loki sure so an alert well even just beyond Loki an alert is essentially a rule that is continuously evaluated at a frequency on interval that you choose and it's looking for a condition um and when the condition is met the alert fires and the idea is that you can have a sort of continuously running query where you get to set the parameters of when does this fire and then when it fires the idea is that the alert lets you know that it's fired so it's a complete hands-off way to know about problems without having to be continuously monitoring for them I.E you don't have to have your screen on 24 hours a day watching a dashboard just to see if a graph spikes up the alert takes care of that for you runs autonomously in the background and then it lets you know there's a problem rather than you having to go yourself and check if there's a problem that's that's like a very basic high level so we're like being proactive rather than reactive it's like we're getting our notification first yeah but the computer is doing the proactivity you're not yeah you're reactive in that sense yeah yeah I really this is why I sometimes hate the term observability because it makes you think that the goal is for someone to be constantly watching I don't want to be watching I don't actually I think I think that's pretty crappy if if there's a system that you have to be observing all the time alerting is the thing that really frees you because you don't want to be you don't want to have to babysit it you want to just be told when things go wrong and that's that you really can't can't do any good observability without the alerting part of it exactly yeah exactly and so okay so alerting is one side of it with Loki there's also another notion of recording rules could you tell us what we' use recording rules for what are they so recording rules have well I think they have a couple of use cases the most common use case people think about is essentially saving the result of competen intensive Loki queries so for example you're running aggregations over our windows or lots of data and this can be quite slow you obviously have to read a lot of data and if you're running in gra Cloud your build if you read too much compared to what you write for example so recording rules are a way for you to take frequently use queries so you might run these queries every day or they might be feeding dashboards that you look at all the time you can take those queries and you can essentially save the results as a prometheous metric so when you then come back to load that dashboard it doesn't have to run the really expensive query again on all that log data it just loads the intermediate result from Prometheus it can be much faster can save you a lot of money um so you you really just benefit but you can also also beneficial for alerts as well sometimes if your alerts are running very intensive l q queries over large time ranges well they can also be slow you want to make sure that your alert query is relatively fast because if the alert query takes longer than the alert evaluation interval so your alert's meant to run every minute but the cury takes one minute and 30 seconds well then the rule is going to fall behind because it takes longer to run it than the frequency it's meant to run it and then you might miss things um Can something be a recording Rule and an alert so in general the way it works is you use the recording rule to calculate the metric that you want to alert on and then you have an alert rule that uses the result of your recording rule so if you like the recording rule feeds into the alert so you've got your lowkey logs you calculate some metrics with your recording Rule and then you alert on those metrics that your recording rule has just produced so it's like a chain it's pretty Co and then okay and then so so that's basically you would obviously send you would do we're really getting into the weed of it hit so going to extract us back but we can definitely get back to this with the recording rule part um so I guess sort of bringing us back back what actually handles all of this in Loki what component of um Loki handles both recording and alerting rules yeah so it's the ruler component um the ruler is an optional component in Loki but you don't have to download another piece of software it's included in the lowkey installation whether you run a single binary or whether you run a Docker image whether a Docker compos you run it from a Helm chart it is included in the lowy software tweet if you like um and the Loki ruler runs both your alerting rules and your recording rules um and the actual way you define alerting and recording rules is almost identical um in the fact that your yaml the actual yaml that you write it's almost identical um and you specify a group and a group contains a series of rules for each group you say how frequently I want the rules in this group to be evaluated and then you specify a list of rules that are evaluated in that group um something that people Miss I think time couple times is the rules in a group are all Val evaluated in series sequentially one after the other so it's like a list it goes rule one rule two rule three rule four so rules in a group are not evaluated um in parallel but groups are so you got to be careful if you've got an alert rule that depends on the recording Rule and they're in different groups you can have situations where the alert rule is evaluated first before the recording rule so its result is not ready so often it's important to put the recording rule in the alerting on the same group so they happen seti sequentially yeah and then the output can be ready when the next rule runs dang so and that's sort of like the dependency chain going down I they didn't realize that that's actually really good to know um so evaluating group keeping it so groups are triggered parallel depending upon timings but anything inside a group sequentially run in that group so there is a real quick question I just want to answer someone asked can you use the result of One recording rule yeah well let's um let's read that out loud uh y r check says can you use the result of One recording Rule and use it as a part of different recording rule so the the answer is yes but in low key ruler no and the reason for that is is that the Loki ruler queries logs so when you have a recording rule in Loki your input is locks and your output is metrics so there's no way to do like a recursive rule where you because it lowkey recording rules don't output logs you can't then use the output of a recording rule in Loki as the input to another recording rule but in M or Prometheus you can so once your recording rule has been exported to Prometheus or exported to mimir you can then use the output of your Loi recording rule as the input to a mimir recording rule if you really want it to in mimir you can do this in Prometheus you can do this because everything is metrics but a loky recording rule actually translates logs into Matrix which isn't the case in Prometheus orir where you have the same input and the same output it's both metrics it's not translating it between logs and metrics and that's what is different about Loki recording rules is they translate logs to metrics so I feel what would be good um and especially for me as well is do you think we could kind of like because the the next sort of question we have for you is sort of like how does an alert manager tie into this and we've talked a little bit about you know moving using recording rules and putting those into promethus you could sketch out a bit of a diagram of these components and we can just sort of see sort of how these all a bit of lay of the land yeah sure so I have a um I have a tab open we can switch that okay let's go draw so you would like to see how the ruler fits in with the alert manager and like the data sources should I also include grafo at this stage or just keep it with Loki ruler yeah go for it I think and then like especially we talked bit about recording rules there as well so if you want to add say like a Prometheus um component in there that would be great yeah alert manager because it's really confusing it's like there's so many it feels like there's so many different ways to do alerts like what what should we be using I don't know sure so let's let's get drawing so we've got I'm going to draw this big box as Loki um and obviously inside Loki you've got your little components like your aquera can you use a a bigger font size I think I can I can also oh it's quite small isn't it let me zoom in I have to zoom in and out every I think a little bit but you got you know you've got your quera you've got your ingestor gester and you've got I haven't dra everything because I'm going to around space but you got your ruler yeah um and here you have like your store and this is where your logs are kept this could be loky could be a local disc it could be cloud storage bucket like any a ws3 bucket or a gcp bucket or an AO bucket um and your Loki ruler actually queries the store and indexes directly it can use the querier but by default it doesn't by default it actually has its own querier um and the ruler goes and it gets all the rules you have and it evaluates them one after the other um and it will get logs from the store and read the indexes and compute if it's a recording rule it will compute a result and if it's an alert rule it might fire and alert so with recording rules once you have the output you can then send them somewhere you might want to send them to Prometheus Prometheus is a Time series database you can send it to mimir and I think a couple of other backends doesn't have to be Prometheus and just m um and then what you get out of this is metrics and in your recording rule you you specify what the metric should be called you can then well if just looking at recording rules you can then use the metrics you've got from the recording Rule and you can combine it with other metrics you've got from somewhere else say I don't know your your agent um and you can write alerts also in Prometheus or also in m so you could because Prometheus has a ruler as well you can then do alerts like this so you can use the output of your recording rule in a Prometheus ruling but you can't do it in a Loki ruler because the Loki ruler only queries logs but you can do it in a Prometheus ruler so you get the output of your recording rules in a Prometheus ruler and then you can combine it with other metrics to alert on those if you go back here and we look at the Loki ruler or it can produce alerts and then this together combined goes to an alert manager so you can send all your alerts whether it's from Loki whether it's from Prometheus whether it's from rir to a single alert manager so the job of the ruler is to evaluate rules and either produce metrics if it's a recording rule or to produce alerts if it's an alerting rule the job of the alert manager is not to evaluate rules but it's to receive alerts and then group them together um based on your configuration file in your alert manager and the idea of grouping is you can take hundreds or thousands of alerts and reduce them to a much smaller number of notifications because if you've got a thousand alerts firing you don't want a thousand slack messages in your channel you don't want a thousand phone calls from P DT you want to reduce them into something meaningful that says hey these systems are down take a look and that's what alert manager does it will take all these alerts it will then pass them through all your roots which you define yourself it will separate them into little groups the font's getting very small now but you get the idea imagine these are your little groups color those and then from there each group will send its own notification and that can be to slack it can be to email um any of the supported receivers basically um and in addition you have other features like silencing inhibition rules active time intervals mute time intervals um we can talk about those in a little bit as well if you like so I mean this is I think this actually sort of relates really well to a question we' just had in um just because you've got the architecture and you mentioned about how the ruler um has its own querier or you can root the ruler through the queria was that is that sort of the two two modes um so someone was were so Herve nickol was asking what's the pros and cons of each way of doing this um and it would be great to sort of get your there's there's been some answers in the chat about it it'd be great to see um what you think about this as well George yeah so Andre is pretty much totally correct so when you have rulers with their own quera you you can sh you can um horizontally scale the rulers and they will Shard the role evaluations by by group not by rule but by group so one ruler takes these groups another ruler takes other groups so you can sort of Shard the query load that way but like Andre says you won't be take advantage of caching um you won't be take advantage of things like query splitting and query sharding where the query and the query front end can take your complex queries and then split them even more among amongst other queries so because you only have a single ruler valuation a rule without the query and the query front end only that ruler can evaluate that query it can't split them any further than that but by getting a ruler to use the query front end it can then subdivide the work even further and parallelize it so if you have sorry go ah oh if you have a very expensive query a slow query you can parallelize it and speed it up on the other hand that's also a great use case for a recording rule because if you have a slow query you might be able to cach its results intermediately using a recording rule so you have a bunch of different dials you can turn when you're writing your Lo your recording rules as a way to essentially avoid having to run slow queries in the first place or run them less frequently um and then you also have the scaling of the query or the query front end um so that's what you would do and in general when we when when we recommend people to use an a query with their rulers we actually recommend that they set up a separate set of queries independent of their dashboards the reason we suggest this is because alerts and recording rules are time sensitive your dashboard isn't as time sensitive if the query takes a little bit longer to show on the panel doesn't really matter but you want your alert rule crws to be running with the highest priority and essentially the easiest way to do that is to given dedicated queriers and that's what we do that's how we that's how we run it oh interesting just say that Andre is one of who answered the question is one of our grafana Champions yeah so totally awesome that that our Champions are in the chat as well um Jay do you want to say something I just thought that was super interesting that's what we did within graffan and Cloud when we host Loki is we actually have dedicated queriers for our rulers that we are basically we root all of our traffic through I didn't realize I guess for the sheer scale of our lowkey deployment that that makes sense so we're always making sure that we hit um people's alerts on time um but yeah dang that's that's really good to know um we have another question from well another answer from or comment from sroy who I think your name is Carla I if I'm not mistaken um this is an answer or comment to A's question uh for me the pro is that my query load depends on my users so having the ruler querier avoids a Noisy Neighbor problem in my experience yeah so I think if I understand correctly um this person's just having the ruler do their querying so they can avoid noisy neighbors on their queriers from affecting their alert R evaluation so that's why I recommended potentially having a separate set of queriers it's another way around that Noisy Neighbor problem but it also gives you the scaling advantages of splitting and shouting and N adds and recording rules can generate a lot of S3 traffic and end up being expensive hence the usefulness of the cash z c also asks so is it running a separate set of querier pools so the ruler forwards its rules I I think I think I just answered that because they've also said yeah yes like you're running a separate set of query a separate pool just for your rule evaluation and your recording rules yeah okay um we also had a few questions earlier that I wanted to get to Suman also a graina champion yay asks how ruler how does the ruler work with multi-tenant lowkey SSD or distributed deployment modes I could not find the proper configuration to Define two tenants in the Loki ruler config uh how does ruler work with multi-tenant Loki SSD I'm not necessarily sure I actually understand that question I'm not super familiar with lowy stuff um in distribute deployment can find configuration to find two tenants in the Loki ruler config well the way the way we do it is the tenants interact with a Loki ruler via their own basically per tenant API in terms of config fars on disk I don't know off the top of my head if that's the way you're doing it um with com files on dis I'm not sure off the top of my head I thought that it works if you're doing the the SSD or the file system based rules you have a folder per tenant and then you have the files in each folder so you would have like rules slash tenant one and then in each folder You' have all their rules um and then You' have rules tenant two and tenant two would have all their rules in there and so on um I don't know if I've missed understood your question I I think uh George so SSD I know unfortunately is like Le scalable deployment um and they've just sort of like condensed the name so I think it was just the idea of so that if we've got like say I know tenant production tenant um Dev like is is the they basically want to understand is their way of like configuring rulers to to work with specific tenants um so if we have like how how do we do it in our own cloud setup when we have multiple tenants um interacting with to configure rules with specific tenants I again I don't off the top of my head um what I do know is we basically for us we don't have dedicated rulers to specific tenants like that um we use the tradition we just use the standard sharding mechanism where rule groups are split um across um different ruler replicas and if we do have noisy neighbors we just tend to scale up the rulers and sometimes aquarious yeah nice hopefully that answers your question and we can definitely follow up afterwards to look at um a ruler config we can uh I'll add that to my backlog um the the cool thing about this call with George is what we're doing here is everything that we're aggregating here is our plan is to rewrite the alerting part of the Loki docs so this is like our initial call to get all of this information out there so thank you guys for asking the tough questions to George because that helps us out in the process so um so I guess sort of taking it back L we've talked a lot about the ruler and its configuration um and setup um if we sort of jump back um up to the sort of bubble back to alerting and recording rules once again um let's sort of talk about alerting first um what are some of the issues that you see people doing in their alerting what's it you know if a bit of a war in the trenches when you've worked with customers or seen community members what what can go wrong in alerting and what should they be doing to change it um okay so I think one very interesting problem we see and it's not something people think about to start with is what exactly are you alerting on what what do you want to know from this alert it's very easy just to set up loads of alerts without really thinking about it but remember usually an alert is there to PID you it's usually there to tell you something and something urgently so my my advice always is when you're setting up alerts sort of take a step back like you want your alerts to be relevant you want them to be in generally General specific and actionable so by relevant I mean like they should be you should be alerting on something that is important to you or to your service specific I mean like say you have an alert that just says HTTP error rates are ele ated well where are they elevated is it in an API somewhere which API which service which endpoint how long has it been elevated so you get a lot of information early on before you start investigating and then the other one is actionable you need to have an alert where when it fires and you get paged or you get pinged you need to be able to take an action based on that alert if you get woken up at 3:00 a.m. with an alert and there's nothing you can do about it then there's it's meaningless like what how is that valuable to you or to solving the problem so when you're setting up an alert think like what what is this for what does it mean when this fires and what action am I going to take when this fires to solve the problem and if you can't really answer those three often times it shouldn't be an alert it should be in a dashboard somewhere and you can go check it the next day um once you've decided that you this needs to be an alert um and you got your you know you know what you're going to feed into it there's a couple of things that I think people I don't say mess up but um sometimes misunderstand or it's it's a bit vague so one of them obviously is the evaluation interval we did see a lot of cases where people want to have the lowest possible evaluation interval on that rule like they want to evaluate the alert every 10 seconds every five seconds um but they weren't sending us data every 10 seconds or every five seconds so your evaluation interval is only as useful as your scrape interval or your collection rate and this applies a little bit more to metrics than it does to logs but if you've got recording rules it's still relevant because your recording rule might run only once a minute so it's only giving you one metric per rule depending on the cardinality per minute so evaluating the alert every 10 seconds gives you no additional value because you're only getting new data points once a minute but then you're cering everything every 10 seconds and you're paying that cost of crewing every 10 seconds for nothing so evaluation interval is important a good saying default is usually one is is once per minute and that usually matches people's scrape intervals of 15 seconds um where you send metrics to us every 15 seconds or you know your agent sends logs to us every X seconds you usually want to be a multiple of your dat to ingestion interval your scrape interval and four times is generally considered a same default um the other one is the for duration and the for duration says how long after the condition has been met for myal or rather how long should the condition be met for the alert to fire I.E the condition shouldn't be met just once but it should be continuously met for a certain number of minutes say five minutes and the reason for that is you don't necessarily want to alert on a small blip something went wrong for so you you had elevated error rates for 10 seconds and then everything returned to normal that's not really a good alert because you don't want to be paged about that you there's again that's an example of an unactionable alert because there's nothing wrong you might have just had a small blip maybe there's lot contention in your database or something like that that caused a very small blip in latency or errors but right now there's nothing for you really to do and it's not worth being woken up about so so this is like alerting fatigue is kind of like you you just receive too many alerts for the sake of exactly and it will fatigue you and then you will start to discredit them discount them and be like oh it's just that again but you're like wol when it actually counts you're going to look at it and go ah it's just that look again but this time it really matters and something's actually broken and you need to look at it so the four interval is a great way to avoid um or prevent sort of overly eager alerts overly sensitive alerts so don't be afraid to set that to something like five minutes you know like if if this has been broken for at least five minutes something's wrong and don't forget you might be evaluating your rule just once a minute so that's five evaluations continuously where the condition has been met and that's usually quite a same default 5 minutes 10 minutes 15 minutes depends on what you're monitoring but those are usually pretty samee defaults um another very common one is labels versus annotations so in the whole Prometheus ecosystem and Loki's based on Prometheus at the end of the day alerts have labels and they have annotations um the difference between the two is labels are sort of like a primary key if you're familiar with databases and they are key value pairs that essentially become the the alerts ID it's like unique key it's Unique name um so you don't want to put highly Dynamic data into a label for example the result from a query because every time the rule runs you'll make a new alert but it'll be for the same problem so say you've got crashes and you're you're alerting on crashes but you include the whole log line in as a label and that log line will have a time stamp well every time the rule runs you're going to make a new alert so you're going to have lots of duplicate alerts for the same problem um so that's an example where you don't want highly Dynamic data in your labels and that happens a lot where people try and put the value in their label because they want to get the value out of the query into their notification um a great place for that is annotations anotations are structurally identical to labels but they don't form the ID they don't form the part of that primary key they they're descriptive so for example you would put your summary in there and the summary would be a piece of text you write when you define your rule that says this is what the alert means this is what it means for this to Fire and usually it's about a sentence long and you'd say something like HTTP error rates on this API have been elevated for more than 5 minutes and then you have the description which is sort of a longer piece of text and in that you sort of write what what this alert means and then what you can start to think about think to do about it right so so labels are sort of the unique key and annotations sort of the descriptive part of the alert um and annotation is a great place for summaries descriptions like links to run books links to finer explore with the query so you can have an alert that fires and then you link it to a graph finer explore page and then you can click it and you can see all the log lines from that alert rather than having to try and force them into the alert so I think that kind of answered a question that we had from one of our community members pre-loaded um which was how to include log lines um I don't know if we got it here um Nicole Andreas um Jager how to include log lines into alert notification um maybe just a few samples about modifying or complicating alert conditions but I'm assuming that's the best practice here in itself is you shouldn't you should just be creating a like an annotation that's a good description and then linking out to somewhere that contains that information rather than trying to push that all inside an alert message yeah that's what I recommend today the reason for this is is because what you have to remember in Loki is that Loki alerting is actually on time series data it's not on logs so it's like it is Prometheus in that sense it's Prometheus for logs and Loki was always you know the slogan was always Prometheus for logs um so your alert rule is not actually on logs it's on the time series data produced by the query produced by the logs um and that's where it gets really complicated to put things like log lines in there high cardinality data what I recommend is sort of what I said is create your alert write good summaries write good descriptions and then link to a log ql query in gra explore where you can then go and see the logs if you really really have to you can try and extract parts of the log line as a label but you have to be very careful about having highly Dynamic data in that because you will end up with hundreds or thousands of alerts like every time a log line gets produced that Max us like query you'll get a new alert and you'll end up with thousand are hundreds of alerts coming in and you don't want that um but we also understand that this is a popular feature request and we are thinking about ways for you to do this in future without running into the problem of putting it in labels because it's clearly beneficial to users be able to get a notification and just see a sample of the log lines rather than all of them because you could have thousands right and that's not going to be useful to you but at least a sample where you can see okay here's here's the line in the file where the crash happened I saw that last week you know and you can immediately become familiar with the context rather than having to go into grafana explore so we are thinking about it but today I would say try to avoid putting log lines in your alert for that reason we also had a question from Andre that I want to make sure we get to um Andre said he would really like to use the recording rule but he has a delay receiving logs uses a WS ALB application load balancer so they get pushed every five minutes so that means that um he has to use an offset and then the metric ends up with a wrong Tim stamp so that's great because um I actually thought about this earlier before before I um we joined so for you I think it sounds like the offset modifier would be great and the offset modifier says each time the rule runs you can actually get the rule to look further back normally it looks at the current time and it says what's happened in the last say you got something for the last five minutes it starts at the current time and then Works back but if you've got a five minute delay on your logs you you want to shift the current time back you want to shift the window back because you've got a 5 minute DeLay So when every time the rule runs you don't want to look at the current time but you might want look at five minutes ago because you've got a 5 minute delay um and the offset modifier is great for that the offset modifier lets you change that window that the query is looking at from now to sometime in the past and by doing that what happens is your record is sort of lagging lagging behind real time but it's lagging behind real time just enough for those delayed logs to come in and to be ingested so when the rule runs the logs you need for that time window are there now so it's constantly lagging behind real world time a little bit but not too much and then you actually get to run your career against the logs that you wanted to um and if you look at the documentation for log ql you should find it under under offset modifier okay and there was some discussion in the chat as well about using grafana managed alert manager versus mimir's alert manager um and a Nicole was suggesting to use M's alert manager just for the multi-tenancy purposes so that they you only need one alert manager I mean can you can you shed some light on this sure um would it be good just to also give some high level context about the differences first and then we can go into that question yeah that one of the topics I think we were going to talk about so in the grafana ecosystem you have graina managed alerts um and we abbreviate these a lot as GMA gravana managed alerts and then you have data source managed alerts um we call dma um grafana managed alerts are alerts that are managed stored and evaluated within grafana so your grafana installation has the alert it evaluates the alerts and then it sends them to an alert manager in data source managed alerts the alerts are stored and evaluated in the data source and that might be mimir it might be Loki um data source manage alerts kind of predate grafana manag alerts Graf's always had an alerting Suite but graan manag alerts are quite new um it was the product I was working on when I was in the team um but there are a couple things to consider when choosing between them and the answer kind of depends a little bit depending on whether you're using grafana cloud or whether you're using open source um so for example grafana managed alerts has its own alert manager built in so if you just want to deploy one application for all your alerting you don't to have to deploy a separate alert manager you don't have to deploy any of that you graph manag alerts has it all built in so you just run install grafana and it's up and running that's obviously not the case with lowkey because you need the lowkey um query you need the ruler and then you need an alert manager gravana manag alerts also has a number of features that are not available in things like mimir lowkey so for example you can have role based access control um the alerting state history feature you've got um screenshots of dashboards and panels when an alert fires um you can obviously join data across multiple data sources so your alert query can actually query two different data sources at the same time um you got some other features like pausing and resuming of alert rules and then OB you've got the really tight integration with like grafana incident and gra on call um until recently grafana M alerts didn't support recording rules um but this is coming I spoke to the team this is coming and they're expecting it to be available in grafana 12 which is this year right um so that will make gra managed alerts almost feature complete with data source managed alerts now the only exception is if you're using grafana open source an important factor here is that the open source G managed alerts doesn't have things like horizontal scaling and sharding of your rules where the mimir and loky rul is do so if you've got thousands thousands of thousands of alerts in your OSS or Enterprise deployment then running graphon marage alerts can be a challenge and what you would probably have to do is run separate installations of GRA managed alerts and then split your rules manually yourself across the installations um so with data source man alerts the sharding is taken care for you um but that's not true in gon Cloud so in gra Cloud the horizontal scaling of GRA manag alerts is taken care for you it's just the overset product where where that's not possible um and obviously when you use data source managed alerts you can still use grafana as your UI there's nothing to stop you using the grafana UI to manage those alerts in Loki in M all you have to do is just add it as a connection in the connections tab um so those are sort the differences should we go back to the question just and then we can follow up on that question which one was it uh yes so the question was whether or not um they should use mimir alert manager for multi-tenancy or the grafana managed alerts because Andre was saying uh that uses grafana managed alerts so grafana managed alerts doesn't have the concept of tenants as such but it has a concept of organizations um so you can have separate orgs but also within grai M alerts you have role based access control and I'm not completely familiar with the ins and outs of role based access control but I would imagine that would mean that even though you don't have tenants you can restrict which users can see which rules so even though it's technically a single tenant the the roles in that access control system let you um separate who can see what and who can do what um but like if if the tenant system inir works for you then great like don't change it like there's no need to change it unless you really want those features from Graff managed alerts and if it works for you today stick with what works with you today you know Yi also says um grafana managed alert manager does not support inhibition rules amongst other things which makes it the biggest argument for us to use the Prometheus alert manager so I know the alerting team um is aware that inhibition rules are not supported right now um because it's something I we talked about when I was on the team I do think they are planning to add it because it's it's on their it's on their issue board but I don't know when um in Gana Cloud I believe it's coming very very soon in grafana OSS again it'll take a little bit longer because the way they're going to implement it is slightly different so they'll have to do something for cloud and something else for OSS but I I do believe it's coming if you really want to use graph mans today with inhibtion rules you can point it to an external alert manager so you would still use graph managed alerts but rather than using the built-in alert manager with grafana you would run your own Prometheus alert manager and then grafana would send its alerts to the Prometheus alert manager instead of using the internal one but then you could use your inhibition rules if you wanted to so it's a little bit complicated but is possible is possible okay uh we have another question from um Community member Matt Brown is anyone aware of what might cause loky recording rules that you define in grafana to disappear is there a way to recover recording rules so I I I saw your question earlier uh Matt um the thing is if they if they disappear it would suggest to me they're not loaded in the ruler um because that would suggest that they're not being returned by the rules API the rules API in the ruler basically says give me all the rules you know about that you've loaded if it's not in that API response like you can't see it in the graani UI or you can't see it in um the Loki rules API or you can't even see it in Loki tool it suggests to me that they're not registered um the question is then why um if you're using like the local SSD rule um back end they should still be in the file system system somewhere somewhere on your desk if you're using one of the like um object storage back ends it should be somewhere somewhere there um if they've been accidentally deleted I don't think there's a way to recover them but if that hasn't happened they they should be there somewhere but for some reason they're not being loaded into the ruler so so George could there be like a weird limbo like I'm thinking about like this from like say you've got like your ruler TI object storage and like an s and that's where you're saving your rules and you're using the ruler API in order to like push new rules into the ruler could there be a period where the ruler acknowledges the fact that it has a new rule and it's within say the the the permissive memory or the W of like it's its function but because they've got a problem with their object storage that rule is no that rule is not being propagated into S3 for whatever reason and if the ruler restarts or kills it like it could potentially the rules or is that does does the ruler need to have them hit object storage before it starts yeah I think I think if my memory is correct I think the ruler essentially commits it to object storage as part of the upload stage so if it cannot put it in object storage I think it should reject it and return an error I think okay so you shouldn't be able to they shouldn't be to go missing in that respect yeah we can check that we can check that but I'm pretty sure it's been built that way so that was my only thought it's like how do they how do they disappear if not because yeah I couldn't I couldn't think of another reason except for someone's gone in one of his colleagues has gone in there and deleted them all here's another question it's a troubleshooting type question uh okay well it's going to be difficult to show in one thing but I will read it out from ctil I'm trying to figure out why my Loki setup isn't running recording rules and not sending the resulting metrics to the Prometheus remote right endpoint so the rules don't get added to the slash rules directory by the sidecar container but I don't see anything related in the logs of the Loki container in Lo the Loki backend pod or the Loki SE rules container even after enabling debug logging for both any ideas on how to troubleshoot this yes so I was actually looking at your specific problem this morning I had some free time this morning and I was looking at it specifically um so if rules are being evaluated you should see the bug log lines for evaluating rule they L you just say evaluating rule if you see this but remote remote right isn't working then I would start to look for problems with remote right if you don't see the evaluating rule log line I would start to look for problems with rules being loaded in Loki so I did find a bug report about this specifically related to the Loki sidecar rules container what I had time to test this morning um someone mentioned in that issue is that what they thought is the rule is being loaded and then the side car container is running and downloading the rules so what I checked this morning was is the Loki ruler discovering rules that are written to the file system after the ruler has started running and that I could confirm is working so even if you the FES are created after the ruler has started it does pick them up which is what I expect because that's how it works with object storage right you know you're going through the API you're putting stuff in object storage and then it's syncing them from object storage and then loading those rules into memory so that bit worked um what I haven't had time to look into then is if that bit Works what is the interaction with the side car why is it not working with the side car so I think it seems to me that you're not the only problem not the only person who has this problem because um the issue that's being linked um 11508 so I think we need to look into that to make sure there's not a problem there because it doesn't necessarily seem like you've done anything wrong it could be a bug and we will look into that I think and get back to that issue so follow that issue for updates um I should have some time tomorrow maybe to have a look at it in in more detail and we can look into oh wow committing to things I've never used the side car before so it's good it's good to um get some experience okay we have another question from Peter corus shouldn't there be a revision control system that would allow you to look at the rules yikes and essentially you're looking for S verion control aren't you yes I think that's what he means inside Loki there's no I don't think there's any revision control um what you can do today is If you provision your rules as code using git and then some kind of cicd system is an excellent way to do that that's what we do for a lot of our infrastructure alerts they're all managed in GitHub and then they get published via C cicd Pipeline and it works great and then we can see the history of the rules we see you changed them obviously but then we can also lock lock out people from manually making undocumented or unauthorized changes to rules they have to go through the pipeline their changes have to be approved by a colleague and then everything after that is locked down um so I think it's something we can definitely consider but for now if you really need this now that's what I would recommend and it works great for us yeah I I think that probably isn't as much a Loki question as much as it is just like a you know a process or yeah Best Practices get Ops is is your friend on that one uh we have another another question from pili sairan hi team I've been using ruler valuation mode as local and also tested remote using the query frontend URL having pros and cons with both what would you recommend for us to use so I think we kind of talked about this a little bit earlier we we talked about when you might want to use a um the query front end with creu splitting and sharding versus just using the ruler I mean a really good way to um understand if your ruler is not being able to keep up with it workload and you've say you've already scaled it to some degree is to look at metrics for like rule group evaluations missed and it will tell you um whether rules are being essentially um they're falling behind they're getting getting delayed because the previous valuation took too long um and you can go and look at the query and be like oh actually I can optimize this query and then you can sort of push defer the need to use external queriers but sometimes if your query needs to look at large time ranges then that's when something like the query with the query front end is really great because you can split the query over that time range and get different queriers to process little chunks at a time feed their results back and then you can compute it all together so you're basically breaking up the work um so that's what i' be looking at things like M evaluations is your ruler running out of memory because it's trying to do read too much um read too much log data into memory do aggregations on them is it getting OED is it getting killed by um your process manager right so things like that are any of those included in the Loki mixings or is that something they should go pull for to sort of monitor um those specific components of the Loki metric uh top of my head I'm not sure it sounds like a great idea if we haven't already done it um it's in our documentation I know that because I checked it um whether it's in the mixing I'm not sure I'll have a look okay that's cool that's a to-do list for Jay I can find that doc as well on scating the rout real quick we can put it in the in the chat sweet another question from a Nicole any advice on tools to check Loki rules within a cicd pipeline okay can you be can you like expand on what you mean by check um I I think you mean something specific but I'm not understanding I think this was the um this was basically what we was talking about earlier about how how you guys in the team basically to approve new rules going in you use get to basically sync rules up um into approve new rules in and someone has to check them how are you guys currently going about that method the way that it actually gets provisioned I'm not 100% sure because it's mostly managed by a platform team but what what I do know is obviously it's it's in a file essentially it's in a yaml file it's reviewed by colleagues and then we use um tanker which is like our configuration language and sort of um management system and from there it goes I think goes through um kab actions and it pushed essentially to our infrastructure but it's it's mostly taken care of by a platform team so I don't know the ins and outs of exactly exactly how we do it but that's sort of the big picture um and obviously you don't have to use GitHub actions you don't have to use tanker um you can use loads of other tools but the big picture is sort of the same um and even if I wasn't know Eon Labs that probably how I would do it if I was working somewhere else and then setting that up because it's just a great way to do it love it so I guess Nicole do you want to do your favorite segment where we quickfire the last of the questions with George oh um can I quickly um jump in because um ni mentioned so if you're using um to check the syntax is valid if you're using the Loki rules API that should be validated for you I'm pretty sure when you push the rules it shouldn't let you say save an invalid logl um invalid logl alert um or rule um if you do it file based it will fail to load it should fail to load and the feedback mechanism is a bit more asynchronous in that respect so you might have to look at metrics and logs so the the rules API is a great way to do that um for perthus rules to use pint I'm familiar with pint um I wonder if pint even works because it is the the quy language is very similar between Loki and Prometheus um and if it doesn't work directly you could probably Fork pint because there's not a huge amount of difference between log ql and promql um Okay cool so mimir tool and cortex tool handles that I would expect loky tool to handle it too um yeah okay so just a few quick ones I've been I've been I've been going at it and hacking away at them already um I think we already sort of talked about this one but let's let's mention it again s Croy who had to go but you know he might watch the recording um said had a question about the alerting in multi-tenant environments that he mentioned earlier he said I run loky in justing logs from multiple clusters each cluster is identical but hosted in different zones or regions and each one has a tenant ID and at the moment I need to create one alert for each cluster so 20 clusters because the rule rer component doesn't support multi- tendency are there any PL to support multi-tenant queries in the ruler component so I I saw your question earlier before we got started I it sounds like you've got different tenants for each cluster um but it sounds like it's all it's all your like these not separate customers of yours or separate teams it sounds like um this is all or your stuff so in general tenants are really meant be isolated from one another like think of them like customers think of them like teams they should never be able to see each other's stuff so there's quite a strict level of isolation um and if you don't need that strict level of isolation I don't think you actually need different tenants for each cluster um we don't do that for example so then you can have a single alert rule that just the Lo cre um Aggregates by cluster so you have a single rule for all your clusters and you just aggregate it by the cluster which is exactly what we do for our internal monitoring um so yeah I would consider actually having one tenant for all clusters unless there's a really strong need for the isolation between them and if you do need that strong isolation between clusters um you could then look at provisioning the rules using something like loky tool or again a cicd pipeline where you manage it in git or GitHub or wherever git hosting you're using and then you have a list of your tenants and you publish that rule every time you make a change for all tenants automatically so even though you have the rule replicated per tenant you only have to change it in one place once and then your automate automation takes care of the rest so that that's probably what I would advise for that okay couple more quick ones might not be able to get super into it because we're already a little bit over but I still wanted to address it a little bit Yi roich says is it possible to perform aggregation queries like some count overtime on labels extracted from metadata I was unsuccessful when attempting it this week is this this is structured metad dator I assume yeah I believe so yeah I must be very honest I am not super familiar with quering structured metadata um so my hon answer is actually I don't know um I would suggest answer asking that potentially on the community Forum or we can find out and get back to you um we can ask someone in the team who would know and get back to yeah you you should be you should be able to just useing metric queries but um you know maybe it would be useful to to show the actual query that you're using and and what do you mean unsuccessful if you're querying I might have misunderstood the question as well because you're not trying to just do a sum or count over time of the metadata but of the logs identified by the metadata right and in that case you should definitely be able to um yes if you're yeah because just aggregating the method of itself doesn't necessarily make sense to me but yeah yeah I think that's what he means but yes more information maybe in a community post uh last one Tomic DKI are all alerting rule settings available through the grafana UI also available as configuration as code and terraform for example setting an SNS contact point okay so SNS I think is only in grafana managed alerts I I think and terraform um only works with grafana managed alerts there are some Community providers by the way for data source managed alerts but they're not maintained by grafana Labs um if the terraform provider is not up to dat then SNS may be missing because I remember it was added when I was still in the team but it was about a year ago so if the terraform provid is not up to date it will be missing um usually when features get added in in alerting then the terraform provider gets updated afterwards sometimes things can be missed if if it's not there definitely raise an issue on our terraform provider repo and the alerting team can work on getting that getting that um filled in because it could it could it could be missing yeah this that actually makes that actually makes a really good plug for tomorrow uh funny enough we have grafana campfire which is is also doing an alerting segment for an hour so if you're along for that tomorrow um there hopefully should be one of the maintainers there that can definitely you can fire that question at again and hopefully get that sorted um so yeah definitely speak to them as well um but thank you for answering that one George I I wasn't sure if we we do or not yeah so um it's actually going to be one of our colleagues Usman uh who will be the developer Advocate on that with a bunch of other cool fol from grafana and they're going to be focusing on grafana alerting whereas this one was really more focused on on Loki and logs but there's going to be some overlap so there are some of these questions that might might be better suited to to that one so catch that as well um I put it in the chat there so you can you can do that um and it's going to be on on Friday at uh well just so that don't have to do different time zone just go to that that video and you'll see it in your time zone um we are a little bit over already thank you George for staying a little bit longer with us George thank you yeah and if there are any other questions leave them in the comments and we'll we'll try to answer them if there's something that requires like you know more information more steps to reproduce or something like that the community forum is better for that because then we can really get into what exactly you're doing and what exactly you're getting back um but more conceptual type stuff could also be be answered here I'll make sure we watch this um thank you everybody for for being here lot of questions before before we go before we go um I have one more thing because we now actually have a schedule woohoo um yes so next month we have poison coming back for mcash so if you have any questions about scaling mem cash with your loky deployment or cashing in general come join us for that one she's really excited to come back and join us and then after that it's going to be cubicon and we plan to do a special episode in person so I finally get to see Nicole again there um with sirel and we have a very exciting that's gonna be really cool very exciting feature coming to open source so very look well it is actually already available in the main branch uh just FY if you can go digging for it it's all in the open source of course but yeah very special announcement in cubec con so looking forward to that all right thank you everyone for for coming along and we'll see you at the next one thanks Squad see you later

