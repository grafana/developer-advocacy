# Grafana â¤ï¸â€ðŸ”¥ Tempo â¤ï¸â€ðŸ”¥ OTel: Practical distributed tracing from the field (Feb 2026 Community Call)

Published on 2026-02-13T06:57:58Z

## Description

Ever wondered what actually happens when a request travels through a chain of services, and how to debug it when something breaks?

In this episode, we sit down with field engineers who onboard customers and solve real tracing problems every day. Itâ€™s practical, opinionated, and grounded in real-world experience.

What we cover:
- Distributed tracing basics, with a live demo of services talking to each other using Tempo
- How to enable tracing with OpenTelemetry and what you get out of the box
- How to use traces in practice: debugging flows, service graphs, SLIs, and alerting
- Cost control strategies that scale: head and tail-based sampling
- Using tracing together with metrics, logs, and profiles

ðŸ’¬ A great opportunity to ask questions and learn from the field.


-----
â˜ï¸ Grafana Cloud is the easiest way to get started with Grafana dashboards, metrics, logs, traces, and profiles. Our forever-free tier includes access to 10k metrics, 50GB logs, 50GB traces and more. Sign up: https://grafana.com/get/?src=yt&mdm=social&cnt=description

â“ Have a question that isn't related to this video? Check out the Official Grafana Community Forums: https://community.grafana.com/?src=yt&mdm=social&cnt=description

-----
ðŸ‘ If you found this video useful, be sure to give it a thumbs up and subscribe to our channel for more helpful Grafana videos. 

ðŸ“± Follow us for the latest and greatest on all things Grafana and our other OSS projects. 

X: (https://www.twitter.com/grafana)   
LinkedIn: (https://www.linkedin.com/company/grafana-labs/)
Facebook: (https://www.facebook.com/grafana)

#Grafana #Observability

URL: https://www.youtube.com/watch?v=1lMHX8CRHsc

## Summary

In the February community call for Tempo, hosted by Matt, Tiffany, and field engineers LM Miller, Ya, and Ed, the discussion centered around distributed tracing and its integration with OpenTelemetry and Grafana's Tempo. This call marked the first collaboration between the Tempo and OpenTelemetry communities, emphasizing the importance of tracing in understanding and visualizing requests within distributed systems. Key topics included an overview of distributed tracing, the differences between metrics and tracing, the significance of instrumentation methods (including zero code, auto, manual, and hybrid), and the benefits of sampling for managing trace volume. The participants encouraged audience interaction and highlighted the value of community feedback, while also promoting upcoming events and future calls on specific topics like sampling. Overall, the session aimed to foster a conversational atmosphere while educating attendees about the intricacies of tracing and its practical applications.

## Chapters

Sure! Here are 10 key moments from the livestream along with their timestamps:

00:00:00 Introductions to the February community call  
00:02:30 Overview of the joint call between the hotel and tempo communities  
00:05:15 Explanation of distributed tracing  
00:10:00 Visual representation of how a trace works  
00:20:30 Discussion on metrics and logs vs. traces  
00:25:45 Introduction to OpenTelemetry and its significance  
00:32:15 Overview of trace collection backends and Tempo  
00:40:00 Explanation of sampling (head sampling and tail sampling)  
00:50:30 Discussion on instrumentation types: zero code, auto, manual, and hybrid  
01:00:00 Wrap-up and invitation for future community calls  

Feel free to ask for any specific details or further breakdowns!

# February Community Call Transcript

---

**Matt:** All right. Hello everyone and welcome to a very special community call. We have the normal Tempo folks you see, but we also have a few new faces. We'll do some introductions here in a second, but welcome to the February community call for Tempo. We're joined by some hotel folks and some of the wonderful field engineers we have to talk about tracing from a variety of viewpoints and angles. 

**Tiffany:** Yeah, itâ€™s our first joint call between the hotel community call and the Tempo community calls, so itâ€™s really cool having this first joint call. If you like this format, let us know, and we can do more of them.

**Matt:** Absolutely. As always, we invite conversation. Weâ€™re going to present some content and talk about some things. If you have questions, feel free to post them in the comments, and we will address them. I believe in having conversations rather than saving QA for the end. 

Since there are a lot of new faces here, most of the content we cover will be relatively high level, but if you want to dig deeper or have more technical questions, please feel free to post them too. We love to have these discussions. There may be some parts where we have conversations among ourselves, as we all have different viewpoints.

Also, if youâ€™re trying to comment and it doesnâ€™t show any way to do that, you need to create a channel. Click on the top right corner where your profile image is, and then go create a channel.

---

**Matt:** Letâ€™s do some introductions to the new faces. Iâ€™m Matt, kind of the MC and a developer here at the Tempo team, formerly a Valerian agent. Youâ€™ve probably seen my face if youâ€™ve been on these calls a few times. Tiffany, why donâ€™t you go next?

**Tiffany:** Sure! Iâ€™m Tiffany, also a host of the Tempo community call, and a developer advocate for multiple things, including Pyroscope, where we had a call this morning.

**LM Miller:** Hi everyone! Iâ€™m LM Miller, and Iâ€™m excited to join this amazing Tempo call. You might see me on the OpenTelemetry community call we will host in a few weeks. Iâ€™m looking forward to hearing real-life stories about OpenTelemetry and Tempo adoption from our field engineers.

**Ya:** Hi everyone, Iâ€™m Ya, a senior field engineer here at Grafana. I mainly work with tracing and Iâ€™m excited to share insights from our day-to-day interactions with customers and how cool OpenTelemetry and Tempo are.

**Head:** Hi, my nameâ€™s Head Simons, and Iâ€™m also a field engineer here at Grafana. Longtime listener, first-time caller, so Iâ€™m quite excited to be here as well.

---

**Matt:** With that, I think we can kick it off. Today, weâ€™ll cover what tracing is, why itâ€™s useful, how we can use it to observe our systems, instrumentation to get traces out of services and applications, and a bit about sampling as well. 

This is also the first time weâ€™re using Streamyard, so please bear with us if we move past slides too quickly. 

---

**Matt:** The first thing we want to cover is what exactly distributed tracing is. Distributed tracing is a strategy that tracks the journey of a request as it makes its way through a distributed system. It provides a visual representation of that request as it flows through different parts of our system.

To illustrate how a trace works, think about how a distributed system in your application functions. You have various services, especially in an internet application. A request comes into your application, passes through a load balancer, hits an API service, and then calls downstream services to get data and send a response back to the caller.

What is a trace in this context? We can visualize a trace as a flowchart where we see how a request comes in from the internet, goes through the load balancer, and then to one of our API service replicas. 

We can start a trace in the load balancer or the API service depending on your setup. Traces are based around the time spans that occur in each of these services, allowing us to identify how long each service takes to handle a request.

---

**Matt:** If I may pause here, most people are likely familiar with metrics and logs. Ed, can you explain how tracing differs from these?

**Ed:** Sure! With metrics, youâ€™d have to compile multiple metrics together to get a full view. The concept of cardinality comes into play here, which refers to how many variations of data you have. Tracing, on the other hand, gives you a self-contained environment to see all the pieces of work and enables you to deep dive into each of those spans.

**Ya:** Distributed tracing also provides correlation, allowing you to jump from metrics to traces and see logs correlated through the tracing context.

**Matt:** Exactly! Observability works best when you can correlate all data types rather than looking at one signal in isolation. 

---

**Matt:** Now, letâ€™s explore what a trace looks like. This diagram shows how a trace starts at the front-end client, calling the cart service, with spans indicating where work is done. Each span provides valuable information, including attributes like the gRPC method being called and its status code.

---

**Tiffany:** Just to clarify, span attributes are inherent to the span itself, while resource attributes provide context about the infrastructure where the span is running.

---

**Matt:** Absolutely. The separation between span and resource attributes helps us gain insights into performance issues and facilitates correlation across different telemetry signals.

---

**Matt:** Letâ€™s move on to the next slide. The reason for this joint call today is to discuss the relationship between Tempo and OpenTelemetry. OpenTelemetry sets the standards for metrics, logs, and traces, utilizing the OpenTelemetry Protocol (OTLP) for emitting and receiving telemetry data.

---

**LM Miller:** If you're interested in OpenTelemetry, we invite you to join the community calls and share your feedback. Weâ€™d love to hear your thoughts!

---

**Matt:** Now letâ€™s delve into instrumentation. We have various types: zero code instrumentation, auto instrumentation, eBPF, manual, and hybrid. Zero code and eBPF do not require code changes, while auto instrumentation requires an SDK. Manual instrumentation involves changes made by the developer.

---

**Head:** It's essential to understand that auto instrumentation uses libraries or frameworks that already have instrumentation built-in. This differs from zero code, which does not necessitate any code changes.

---

**Tiffany:** We also see that many people start with auto instrumentation to gain visibility into their applications before moving to manual instrumentation for more specific insights.

---

**Ed:** There are questions regarding syncing span and trace IDs with logs. In Grafana, different telemetry signals are treated as different data sources. You can configure them to correlate trace IDs with log lines to ensure traceability.

---

**Matt:** Regarding sampling, it allows us to reduce the number of traces collected, which is important for managing costs. We typically see two sampling types: head sampling and tail sampling. Head sampling makes decisions early in the request lifecycle, while tail sampling examines all spans before making sampling decisions.

---

**Tiffany:** Tail sampling offers more flexibility, allowing us to focus on specific attributes such as error rates and latencies.

---

**Matt:** Letâ€™s wrap up here since weâ€™re close to time. Sampling is a significant topic we should explore further in another session. 

We appreciate everyone for joining, and feel free to connect with us in the Grafana community. If you have questions or topics for future calls, let us know.

---

**Tiffany:** Just a reminder, if youâ€™re interested in Grafana, the registration for the event in Barcelona is open. It will be a great opportunity to connect with others in the community.

---

**Matt:** Thank you all for participating! Weâ€™ll see everyone next month!

---

## Raw YouTube Transcript

All right. Hello everyone and welcome to a very special uh community call. Uh we're the normal tempo folks you see, but you'll see we have a few new faces. Um we'll do some introductions here in a second, but uh welcome to the what is this the February community call tempo and we're joined by some hotel folks and some of the uh wonderful field engineers we have uh to talk about tracing from a variety of viewpoints and angles. So >> yeah, it's our first joint call >> between the hotel community call and the tempo community calls. So it's not just our tempo call too. So it's really cool having this first joint call. And if you like this joint call, we should do we should do more of them. Let us know. >> Absolutely. Um as always uh or well to newcomers uh we invite conversation. So we're going to present some content. Uh we're going to talk about some things. If you have questions, feel free to post them in the comments and we will take a look at them and address them there. Uh I'm a big firm believer in that we should have conversation, not stay QA till the end for these type of things. So I would love to uh see what y'all have to say. Um, and again, since there's going to be a lot of new faces here, uh, most of the content we're going to cover is relatively high level, but if there's something you want to dig in deeper to or you want questions that are more technical or maybe more, uh, in-depth, feel free to post them up too. Again, we want conversation if at all possible, and we love to talk about it. Uh, there'll be some statements, some parts here. We'll have conversation among ourselves because, you know, we all have kind of different viewpoints on this. And I think it's good to kind of have that conversation. >> So, >> and also >> if you're trying to comment and it doesn't show any way to do that, um it means you need to have a channel. So, if you click on your top right corner where your uh profile image is and then go and create a channel, then you should be able to comment in the chat. >> All right, let's do some introductions to new faces. So, I'm Matt. I'm kind of the MC. I'm a developer here at the Tempo team, formerly a Valerian agent. And yeah, you've probably seen my face if you've been on these calls a few times. So, Tiffany, why don't you go next? >> Yeah. So, um I'm Tiffany. I also am a host of the uh Tempo community call and I'm a dev advocate for multiple things such as Pyroscope as well where we had a call this morning. So, their stream is there. I'm going to kick it off to LM Miller. >> Yeah. Hi everyone. Um I'm here joining this amazing Tampa call. Uh you could see me on the open telemetry uh community call we will host in a few weeks. Um, I'm super excited to be part of this stream today and um, I'm also super excited that we have some field engineers that could tell us like the real life stories about uh, open telemetry and tempo adoption. So with this, I'll hand it over to Ya. >> Hi everyone, I'm excited to be here. I'm Ya. I'm a senior field engineer here at Graphana. uh mainly work with tracing and I'm really excited to be here and share some of the things that we typically see on a day-to-day uh when we work with our customers uh but also tell you about how cool open telemetry and tool are and uh heads thanks hello my name's head Simons I'm also a field engineer here at Grafana uh longtime listener first- time caller so uh I'm quite excited to be here as well um and yeah as a says I'm you know we work with our customers to ensure that we can get them the best tracing uh uh solutions with hotel and tempo that we have. So with that I think we can we can kick it off. So I think today we're going to kind of go through as Matt said at a kind of high level um just a bit about what tracing is, why it's useful, um how we can actually use that to to observe our systems and then a little bit about instrumentation to actually get some traces out of your services and applications uh and into whichever observability vendor you're using. Uh and then a little bit about sampling as well. So let's >> move this forward. This is sorry this is also the first time that we've used Streamyard. So we're kind of learning as we go here. >> Yeah. So please bear with us if we move past slides too quickly. Uh so yeah the first thing that we want to cover is what exactly is distributed tracing. Uh so this is kind of a quick summary of what it is and what we're going to talk about here. Uh but distributed tracing is a strategy that tracks the journey of a request as it makes its way through a distributed system. Um so really it gives us the ability to kind of get a visual representation of that request um as it flows through different parts of our system. >> So I need to unfortunately change the slide very quickly here because uh we've got an animation and uh we can't get that working stream. So to get an idea of how a trace actually works, think about how a distributed system in your application works. you have many different services especially if it's an internet application you probably have a request comes into uh your application it goes through a load balancer it hits a service which is probably your API service and then from there depending on what the request actually is there's probably a number of different services that you're actually going to go and request data from calling downstream to get some data and then send a response back to whoever the caller is. So what is a trace in respect to that? So we can see here at the top there's a diagram of an application which has several different services and we can see at the bottom there's actually a trace representation. This is u a graphana representation of a trace in tempo. So we can see that there's several different services. The names are slightly different. Don't worry too much about that. But we can kind of visualize a trace by saying okay something comes in from the internet. Uh and let me just start making that work. There we go. We got an arrow. We got a request coming in. Goes into a load balancer. Um, and then from the load balancer, it will go out to one of our API service replicas. So, at this point, we want to actually start a trace. We might start it in the load balancer. Some do, some traces do, some traces don't. Depends on your load balancer. Engine X, for example, uh, is a good example of a balancer that actually will support traces as well. But say in this particular example, we only want to trace from an API through to a database and then back again. We're going to start a timer because what traces are based around are the time lengths uh the time spans that actually take uh place in each of these different services to get an idea of exactly how long each of these services are taking to handle that request. So we're going to start a timer. We can kind of look at the the top level mythical request there at the very top the green bar to say okay we're starting in here. Um and we can kind of get an idea of well how long does it take to actually decode the request for the data that's that's coming in. And in this particular case, you can see that there's a length of that first green bar. That's the total length of the trace. So sometimes traces have that first root span. We call it a span, that very first uh green line at the top. Sometimes they don't. But in this particular case, it is the total 10 the the span. And for the first 4.24 milliseconds, it's just a case of that first service decoding the request. It will then based on the request that we've got send a downstream call to another service. So we can kind of look at mythical uh creatures there at the bottom. That's in yellow. So you'll notice also that the color has changed. That's because um the way that we're denoting each different service here in a trace is based around color. So we can very quickly see that okay, we have one service that called another service. This is the the green service calling the yellow service. And now we can see that we're doing work in another service, mythical creatures, because it's in a different color. And we can see that the first thing that it did is handle the request coming from the upstream service. That takes us 59 microsconds. Uh then we sorry um yeah microsconds. Then we kind of go into processing the header. That takes us 64 millconds. And you can kind of see now on the bottom diagram that these different pieces of work are being represented by these different um bars, these different spans. So as we go through we can see that all of these things take a different amount of time. We eventually go and call a database that takes us about 1.68 milliseconds. In this particular case we're not showing the work done in the database. That's because we haven't instrumented and we'll come back to instrumentation a little bit later. But that takes 1.68 milliseconds and that's why the final bar here takes 1.68 milliseconds because it's making the call to the database. We then send that data back. We process the response we've got from the database. We send it back to the API service and then we package it all back up. That takes us 0.4 milliseconds and then we send it back to via the load balancer the person that requested it. And then we stop the timer because that's basically the total length of our trace. We've seen exactly how long it's taken to get from a request coming in to response getting back. And as you can see, that's basically how we can see on the bottom diagram exactly how long each of those pieces of work took individually to get that response back. Hey Ed, while we got a little pause here, what what is like let's maybe take a step back and like most people are probably familiar with like metrics and and logs and it feels like with like metrics maybe I could get this timing, you know, from one service to another kind of like have some kind of duration or some metrics like that. Um kind of what what does this bias like from from this kind of high level? Well, you'd have to probably put a lot of different metrics together to actually get the full view of what we're seeing. And the other thing is cardality. So cardality is uh how many different um variations of data you actually have. So if you look at your average metric, um you could have maybe different labels on it. So let's say that we wanted a total request metric. Um if you wanted this for each of the service that it's going through, you would need labels that have something like a service name. You would possibly need the type of operation that's actually occurring. you would possibly need a status code saying whether something was okay, whether it was not. And then, you know, you would kind of get into things like this was an HTTP operation, so I have an HTTP status code or I have a database status code or I have all of these different values and these different keys that would actually be useful to me in a metric. Now, you can do things with log lines, right? So logs actually before traces really became very popular. You could print a log line for every single operation that you were doing as well, metrics and logs, right? The problem with that though is it becomes very expensive very quickly. Whereas with a trace, you can actually see in the form of a, as we can kind of see a Gant chart, all of this different data. Um, I think a is going to go in a little bit in a minute into actually visualizing a trace in Graphana via Tempo. And you'll be able to see that there's lots of different data values associated with each of those little pieces of work, each of those spans. To do this with metrics would be incredibly costly. There would be a lot of metrics, a lot of different cardality, a lot of active series in kind of Prometheus Parliament. So it would very very quickly spin out of control. you'd end up with with millions and millions of active series which is not very viable. So the whole point of a trace is to ensure that you can see in a self enclosed environment in a self-encclosed trace itself all the different pieces of work and then deep dive into each of those different pieces of work to see exactly what was happening in the context of each of those in each span. >> Awesome that go ahead. >> Oh sorry. Yeah to add to this I think distributed tracing provides something very unique that nothing no other signal provides. It provides correlation right. So you can jump from metrics to traces because you can see an exampler on the metric. You can also see all the logs correlated uh using the the context tracing uh propagates. And even if you record 0% of spans we will talk about something later. It's still quite valuable because it can show you all the logs associated with some operation and you can make your conclusions from just the log lines. >> Absolutely. I think that's a really important point because um being able to correlate between all of the different data that you have and all the different types of telemetry is what makes observability work. You know, having one signal in isolation usually isn't particularly useful. Although there are things that you can do. So for example, you can actually turn um different traces and different spans that come in for a trace via the hotel collector for example or graphana alloy into metrics if you wanted to. We do have span metrics but you lose some of the granularity um that comes along with the trace spans. So it gives you a high level overview of what might be happening with your system as a whole which is very useful but then being able to actually dig into what errors what latencies are occurring that's where the trace becomes very useful. So that's a really great point to raise. Thank you. Yeah. And then I was just going to mention, yeah, because like you can also go from like traces over to profiles. Like in tempo you can have the which we'll explain more in later, but you can have settings to be able to go from traces to metrics, traces to profile, traces to logs to be able to correlate those things together. >> Yeah, absolutely. And something else that we can do with uh tracing specifically is later being able to sample for specific uh data that we want to see. So if we're looking at specifically erroring spans, which we'll get into later on, um or requests that are taking longer than what we would like them to, uh with tracing, we can kind of control the data that we get, uh so we can get see the most value out of it. Yeah. And I think I it might be good to show a good example of a trace at this point, don't you think? Uh let's see here. Again, please bear with us. We're learning how to use this. So, um is this coming through? Okay, awesome. Um so, this here is the um diagram of how a trace would look. Um this is one that starts um at the front end client. We can see that we have the front end proxy here which then calls the front end calling the cart service. Um and these are all the different spans. We can see here there's something um that we would like to or that we call the critical path which is how uh the duration or the time that the uh or it's when the span is actually doing the work. Um sorry not the span the trace is actually [laughter] doing the work. So all of these little black lines here is where uh work is actually being done. Um and from here you can see uh the distributed tracing as we go from one service to another we're being called. Now we can go inside of it. We can look at the different attributes that we have. We have span attributes and we have resource attributes. Now the span attributes are those that are inherent to span itself. Um so for example here we have the gRPC method that is calling. We can see the status code here being a 200. Um but we can see what cluster it's also running on. So this is a lot of valuable information um that we get to see and also we can see the correlation with logs uh like Tiffany mentioned earlier um that is awesome to have um and then we get the name we get the what kind it is we get the status so these are all like really important um things um that we can see in a span um and this just kind of >> shows yes >> um like so if you go back to where I had the span uh the like attribute like the different attributes that you just had for your resource and span attributes. >> Yeah, sure. >> So like things like this are some of them So some of them are basically um added by default and some are ones that you have to create or the one like for the ones here like I guess is it kind of like a mixture of both? Uh so these are auto sura this is in player.com. So we use the open telemetry uh uh what is it called? Heads um the one with the cart service the telescope application is what >> it's the it's the open telemetry telescope application. >> Yeah. Uh so this is auto instrumented. Uh we didn't manually add any um of these attributes oursel but that is something that we can do if we wanted to. we can add um some things that are useful to us um either as a span attribute or a resource attribute that is not um given to us automatically and we can dig deeper into instrumentation later on as well >> and talk about the different instrumentation methods. I think a really good way to to kind of think about the difference between a span attribute and a resource attribute is a span attribute is literally the context around the span itself in the piece of work that it was doing. Whereas the resource attributes are essentially what was the infrastructure it was running in? What are the uh what how how is it being instrumented? What which open telemetry version is being used to actually instrument this? What's the language? Which the which is the operating system type? What cloud service is it running on? Which region is it running on? So it's the separation between the specific work that's going on in the service for the application that you're running and the actual uh the the overlying metadata around the infrastructure that was it was running on and the environment that it was running within. >> Yeah. And the cool part about resources that they are common across all signals. So they they provide yet another way to correlate things. And I think we have a product that leveragees something similar where we show okay the are your nodes the are the entities that produce telemetry like this is your service A calling to service B we can see A and B because of the resource attributes and we can see that A calls B because of spans between them. Yeah. All right. Um, I guess we'll go on to the next slide unless you guys have any other questions here or any any part of the span that we would like to look into or any part of this trace that anybody wants to see. >> I'm curious how would I use a critical path because it's not like a typical the most common thing people have on the the Gant chart for traces. >> That's a good question. So there could be a lot of different uh pieces of work that are going on in a particular span. And I think it's a really good example of this actually is the routter front end here. You can see the uh the second blue span, the third one down is that's that's a particularly long span. And without the critical path, you might think that there's quite a lot of work going on in there. Whereas in actual fact, being able to see that in fact all of these other ch child spans here are taking some of the work as well is quite important because you can get an idea of from that particular span um and the length of it. what are the proportions of all of the other child spans that we have here through the different services actually creating that work. Um, this is particularly important again if you're doing correlation work between different signals. Maybe you've actually got a dashboard that um is looking at the the the front-end proxy and the spans inside it and you can think okay well that's actually taking an awful long time. Maybe there's something wrong with that. But actually then as you said earlier Lid Miller putting an examplier and pulling that up and then moving to the trace the critical path can very quickly show you actually no that's not the case. it's not actually that service that's taking a long time. There's actually services underneath that are maybe causing the problem. So, it's a really good way of ensuring that you know different software engineering teams that are um responsible for different services can very quickly say okay is this something we need to look at or maybe we contact another team and say actually it looks like your service might need some optimization. So having the critical path there just makes it a lot easier to figure out what's going on. It's not so much obvious in this particular um trace, but where you have traces where you have lots of different sibling spans, where you have many different child fanouts occurring as well, it gives you a very good idea of what actually was happening when um instead of kind of having a guess of of looking at all of those different spans and trying to figure out exactly what was happening for each of those individually. So is the critical path the one thing that we should be relying on to kind of debug u those high latency operations or is there something else that we can also do? >> I mean this is again this is a case of which signals do we have and what can we use as Tiffany pointed out earlier know we can embed profiles as well into traces. So it's again the story is having all of those different observability signals to ensure that you get a full picture of everything rather than just kind of relying on one. >> Good. All right. Um we'll go on to the next slide then. All right. Um so the reason that we're doing this call today is because it it's a joint call with the open telemetry community call and tempo as well. Um and tracing and tempo kind of go hand in hand with open telemetry. Uh so it is a CNCF project um that's developed and overseen by a large group of observability players. So Graphana being one of them. Uh but there are many others. Um and open telemetry itself um sets the standards, the schema, the semantic conventions that we use for metrics and logs and in this case traces specifically. Um we use the open telemetry protocol or the OTLP. um for you know emitting and receiving that telemetry data. Um and when we talk about instrumentation we use a lot of the um open telemetry SDKs for some of the popular languages that we have um listed here like Java, Rust and Python. Um and it also has the open telemetry collector um that we use to collect um the telemetry data. uh and if we wanted to do any additional processing to it, if we wanted to do uh spanmetrics generation like has mentioned earlier or if we wanted to do any sort of sampling, we can do that at the collector uh before forwarding those traces over um to our obserability platform. In this case, you know, we would use graphana. And uh LMill, I know this is one of your uh specialties. Anything any shout outs you want to give to the open telemetry community? Uh yeah, we would love to see you also on the open telemetry community call. But I I'm I'm here uh very interested in the feedback you folks want to provide or the thoughts you have about open telemetry. U I'm a technical committee member. I work on semantic conventions and I I take this as an opportunity to uh listen and learn from you. Yeah. Thank you. Um and yeah, we have a couple different um trace collection backends. Um when we talk about instrumentation um and hex, would you actually mind covering this topic? >> Yeah, so I mean >> we've kind of talked a little bit about what is a span, what is a trace, so and so forth, and why is that useful in the context of different applications. Um and obviously we're going to we're going to do a little bit on instrumentation um in a second or so, but the trace collection back end itself is where all of these traces are stored. So obviously different vendors have different backends. Graphanas is tempo. Uh obviously this is the tempo call which we're we're kind of co-hosting with open telemetry. But so the common uh assets that basically trace collection backends have is the ability to actually receive any trace span data from the clients that are actually collecting all of the the instrumented data and the traces uh mostly span data and then passing it on. So these generally group uh all of the different spans that are occurring for different pieces of work together via a trace. A trace can really be thought of as a as a piece of metadata. It's an ID that wraps a lot of different spans together. It doesn't intrinsically um have data of its own as far as a service is concerned, but there's lots of different data that you can kind of attach to that. The trace ID, how long the trace length is, uh so on and so forth, the total collection or the span lengths. Um but it also allows you to um determine exactly how long a trace is. So traces don't necessarily always end. Uh a trace actually can be something that is added to hours later, days later potentially. Not always a good idea to do that, but essentially the idea of a trace is that you can always add a span to it if you need to if it's later on. So the other thing that uh trace collection backends generally tend to do is ensure that all of that data is collectible so that you can then store it sensibly uh and then actually retrieve it again. It's very important to be able to go away and you know for a particular trace ids if you know what the trace ID is or characteristics of a trace to be able to return all of the traces that match those characteristics. So for example you might say um I want to see all of the traces in the last 30 minutes that have an error on them and are over 10 seconds long because that's not right. We know we're having a problem. We know that customers or that uh users are having an issue with the time that it's actually taking to make requests into our system. So being able to give uh characteristics to a trace backend and say show me all the anomaly, show me all the things that are actually going wrong. Um is able is basically what the trace backend does. So it's it's receiving data, it's storing data, it's indexing that data in such a way that you can then retrieve that data very rapidly as well. And this is kind of where Tempo comes into the picture because obviously Tempo is a trace storage backend. So there's a few things that obviously change with each of those trace storage backends. Lots of different uh trace storage systems out there. Um some things that makes Tempo a little bit unique is that we use object storage. So uh obviously object storage is very ubiquitous within um the cloud industry. Um there's a variety of different ways that you can use it. Amazon has S3. Uh Google have GCP. Azure have their Azure uh blob storage. Um, and object storage actually makes it quite cost- effective because it tends to be a fairly cheap storage medium as opposed to using traditional block media. Um, obviously Tempo's built for Grafana. It's one of our OBS products. It seamlessly integrates with it as well. So, we've got a lot of support inside Grafana for that. Um, interestingly, the way that it actually stores data is based upon Apache Parket, which is a Colmer um, database format. Um, and there's some really good reasons for that because it actually allows us to break up spans and the attributes within spans and the resource attributes into such a way that makes it pretty easy to index and then also pretty easy to query. It also means that we can kind of look at specific characteristics of a trace via their spans and pull back particular span attributes very quickly to get all of the spans that constitute a particular trace. Since you were mentioning uh just attributes and such, we have a question in the chat that is when I have a lot of traces with the same resource attributes or I have a lot of resource attributes in general, will it cost me a lot of storage and tempo performance? Will it do dduplication? >> Really good questions. So, um generally it kind of depends on the size of your spans. So if you do have very many spans with the same attributes and you know they start to build up yeah it will it will start to cost a little bit more um in terms of storage because obviously you've got all those together. But the great thing about having those attributes actually on both you know for the span context and the resource context itself is it allows you if you're running many different systems to get an idea of exactly where things may be going wrong with those particular resources. So maybe you're running uh a set of Kubernetes clusters, but maybe you're also running a set of serverless lambdas, maybe you're also running a set of VMs. It's very easy then to kind of use those resource attributes to get an idea of where things may be going wrong. It might be the VMs, it might be that you don't have enough um Lambda functions replicated. There is though if you're kind of running um in one particular infrastructure system and you know that those resources are um always going to be very consistent, you could do some things in your collector to kind of filter those down to compact the number of attributes uh the resource attributes that you have as well so that only maybe one or two of those spans actually have those resource attributes if you know you're going to be looking at a direct trace. But generally with spans, you know, more attributes tend to be better. Here's a question because you just opened up opened it up. What's a collector? >> I think there's uh several answers here from you know a few different viewpoints. >> Uh well to be a collector is something that will receive data and then allow you to do some pre-processing on it before you send it to a final trace storage backend. So the open telemetry collector is the obvious one. You can receive OTLP data. You can see many different data actually. There's lots of different receivers. But if we're talking about open telemetry, you'll receive OTLP data. You can batch it up. That's a really good reason for having a collector in local infrastructure because instead of just sending in drips and drabs over the internet, which could cost you extra in egress, you can batch all of these spans together into a set of a set number of spans, then send it. It's going to be a low amount of traffic going over the internet, especially if you're zipping this stuff up. But also, you can do pre-processing. So, it means that you can do things like filtering. You could change the names of a spam, for example. You could change the attributes. So you could delete attributes, you can mutate them, you can rename them, you can add extra ones if you know that there are labels, for example, that you want to put on resources, etc. It allows you to do pre-processing of that data before you actually go and store it. >> And speaking of batching, uh when when we send batches of spans to graphfan or any other back end, the resources are appear once per batch, right? And I think getting back to the question of uh Robin, I think do we store them as part of the span or do we store them independently? >> We tend to store them as part of a span, but Matt knows way more about this than I do. >> Yeah, they they'll be stored in there, but we there should there is some sort of forgive me. Uh there is some logic there that will help you duplicate that. That's part of the reason like you can you can query by uh kind of unanchored whether it's uh names where I have to check both the span and like the resource attributes or values and that can be quite a bit more expensive because it kind of has to check in multiple places. So um that's more of a tempo optimization of TSQL is to try to be as specific as possible when querying data um just to have better performance. Um, and I'm speaking of someone who used to work on the little squiggly icon at the top of this chart, which is alloy, um, which is a, um, you know, kind of our collector that we use. Um, but I want to be clear, you do not have to use alloy or the hotel collector. Anything that kind of can handle that performs a collector uh, kind of functionality can be used to send us. It's we're all about this. The spec, the specification is mainly what we consume. But I would very highly recommend anyone have a collector. Um, I think headed briefly touched on this, but I will reinforce that even if you're not doing any of these drops or any of this kind of attribute setting or cross control from a retry standpoint, from an efficiency standpoint, having a collector that you can write like local data within the same region to a collector that lives within your same region and then you batch that, you're going to see much better network cost and in general have a lot more control over that data if you do that from the beginning instead of trying to add it later. >> Okay, we have >> question in the chat. Yep. Um, hotel and tempo is really cool, but what considerations including resilience should we keep in mind under high load? Maybe use the hotel load balancer, ascending queue between layers, and so on. >> Yeah, I can >> No, Matt, you go for it. [laughter] >> I'll go for it. Uh and somehow my name tags disappeared. Anyway, um the um so I would say always try to have a queue. Uh hotel collector itself has a support for a storage queue. Uh it also has support for an in-memory queue. Um use both. Um you know your memory Q is eventually going to overfill and then it will start dropping the oldest entries to make room for the new ones. Pretty sure that's behavior. It might be configurable. Um but in general have both um you know having kind of that long-term heavy load um will help you that if for some reason Grafana cloud has an outage and you're not getting data or it's a hiccup or something like that that protects your side and it also protects you gives you a buffer that if you suddenly increase a lot of load um if you write to disk it's you know going to be stored there for be able to be replayed um in a more timely manner. So yeah, and there's a whole lot of of knobs in that queue like the number of consumers, the number of riders. Um, so that could be maybe a conversation for a future talk on how to optimize the hotel collector. Also kind of intersects with the two teensy groups we're talking about here. >> Great question. The load balancing especially is oh sorry is incredibly useful when you get into tail sampling because you want to ensure that you've got enough um instances of your collector or alloy or whatever you're using as your OTLP processor to ensure that you can actually um store spans for long enough to be able to make sampling decisions. We're going to get on to sampling a little bit later. But the load balancer becomes incredibly useful then because it ensures that you can fan out um the space the spans that are coming in for traces out to different instances of collectors so that you don't end up with memory resource issues. Okay, so kind of just ending off on um graphana uh tempo and cloud traces. Um as we kind of said earlier, you can actually generate metrics from the spans that are coming as well. So this allows you to generate things like service graphs. So you can very quickly get an idea of exactly how all of your application is um being tied together, which services are calling, which are the dependent services, so on and so forth. It also generates exemplars for you, which LEA mentioned to start with earlier, which is uh sets of data points that basically show you latencies, for example, of exactly how long particular spans are taking to process the work that's actually going on. This is really useful because if you come in and you start looking at a graph and you can see that your latencies are going up, you can use one of these examplers to select an exact trace that is showing you these latency problems so that you can very quickly get an idea of which service in your application is actually starting to have problems with um servicing the load that's going on. Um, you can also add custom metric labels and spans, that kind of thing as well to, you know, if you wanted to see what the HTTP status codes were from a particular span on your metrics, you can also do that to allow you to kind of chop and change those metrics as you go. And then there's TraceQL, obviously, which is in Tempo, which is the way that we query the the span and the trace data from uh, Tempo's backend storage to allow you to actually get to those traces of interest. It was the world's first dedicated tracing language. I think that's uh has been for like 3 years now uh TraceQL but we've added an awful lot of of uh different functionality to TraceQL as it's gone along and actually TraceQL also allows you now to generate metrics so instead of just uh using the the metrics generator in tempo itself you can at runtime also generate um real-time metrics [clears throat] which allow you to dig into specific traces that you've stored to get an idea of what's going on without having to generate metrics that go to a Prometheus back end. A quick question. It comes up every once in a while. Can we do alerts based on traces alone? >> That's a very good question and it's it's coming up a lot more at the moment. So uh traditionally we haven't been able to. Uh you've used the uh metrics generation and then store that data in Prometheus or Mamir which is our uh our also our um Prometheus large storage and and scalable solution. But we are actually getting to a point now where we are in private preview for doing trace metric alerting as well. So you can hang um alerts off traces if you so desire. The one thing that I would point out with that is if you are doing any sampling, you have to ensure that you know that the traces that you're going to be alerting from the span characteristics that you're going to be alerting from are going to be present because that's one thing that maybe there's a bit of a decoupling there. If you're going to do some sampling and then you're going to set alerts on traces, you have to know that those traces will actually be around to exist so that you can alert off them. And generally I think we're and Matt might correct me if I'm wrong. We're kind of saying that really if you want to do TraceQL alerting. Um the best way to do this is for you know very um very short short past. Uh make sure that you're doing this from things that have happened you know the last 30 minutes or so. Don't try and do this on things that may have occurred a couple of hours ago. really if you want to do that kind of thing and do a long running alert you need to really do that from metrics generation >> and to add some additional context um it is in public preview prop preview whichever it's still kind of uh getting worked on and and the kind of the wrinkles being ironed out um if you are running an OSS and kind of want to to handle that um we definitely recommend as we've talked about on several of these tempo calls our new rhythm architecture which will become kind of the default in 3.0 because it solidifies um kind of that injection pipeline a little bit better. And we're also adding some additional functionality um to make queries more consistent um because anytime you're dealing with a cube which is what project rhythm essentially is you can have lag which can make your alerts maybe not be as consistent as you want. So we're adding some additional um logic and functionality to uh basically evaluate the lag characteristics and then only return when we think the lag is reasonable or it's you're essentially what you're um looking for is path in the ingestion queue. So that's a little aside. It is ongoing. Uh feel free to play with it. We'd love to have feedback. Um but it is something that I wouldn't know if it is it is uh in progress. So you know don't make it the most critical part of your infrastructure that if it doesn't fire for some reason or doesn't work um you know things will break. Okay cool. So we've chatted a lot about instrumentation. So let's get into what it actually is and what you guys can uh get out of it. Um so there are five different types of instrumentation um that we can talk about. Uh zero code instrumentation, auto instrumentation, EVPF, manual and hybrid. Um so zero code and ePF um are ones that don't require any code changes. Automatic instrumentation technically also doesn't require you to do um any code changes on your own. Um it just requires um an SDK um and you have a list of languages uh that you can automatically instrument. Uh manual instrumentation is when you actually make changes to your code yourself. Um and that can become valuable um when there's no auto or zero code instrumentation available for you. um or if you want to add any additional attributes or um instrument your services in a way that's not really um available for you um in like an out of the box way. Um so manual instrumentation comes in really handy for that. Um and then hybrid instrumentation um is when you do a little bit of both. Um or a little bit of zero or auto instrumentation and some manual instrumentation. Um and yeah, so uh we see a lot of this typically people start off with some form of auto instrumentation uh just to get that visibility into your applications. Um, and it will show all of your services if you have um a language that is that has an available SDK um you you know if you're trying to instrument a Python application or a Java application um you can automatically get that. Now if you wanted to add a little bit more um then you can you know take the next step into manual instrumentation and add additional information that you would like to see. Um EDPF instrumentation is usually at the kernel level. Um so zero code and eBPF are stuff that you can start off in the beginning uh but they won't give you as much visibility um into your systems as auto instrumentation or manual instrumentation uh typically would you agree with that heads and lid miller from what you guys have seen? Yeah, I I think that that that's kind of true. I kind of I want to I want to poke on the difference between zero code and auto automatic instrumentation though because this comes up an awful lot [laughter] where because people say well surely autom automatic instrumentation is the same as zero code auto uh zero code instrumentation and there's kind of a subtle difference and I think the way that I think about this is auto instrumentation automatic instrumentation is where you are using uh frameworks or or libraries third party vendor frameworks or libraries but you don't have to go away and instrument it. So these are instrumentations where usually the maintainers of the the particular framework or library and obviously these differ depending on which languages you're using, they've gone away and they've actually added all of the code to instrument the specific work that's going on in those libraries. So they will add new span context, they will add span attributes um to that context to ensure that you can see all of the things that are coming out of that. For example, uh the express library in no.js JS um that's all been instrumented already by by folks that maintain that to ensure that you can see when a request is coming in what the h >> oh last year >> you got muted a little bit >> well sorry I don't I don't know how that happened >> unmuted you [laughter] >> thank you I wasn't I wasn't okay um so I don't know where I'd got to but yeah so for example Example, the Express Node.js library. The maintainers will have added span context to ensure that you can see the different span attributes that will be interesting to you. What the HTTP route is, what the path was, what the status code returned was, what the method was, so on and so forth. It means you don't have to do it. You can just pull in one of these libraries in your code. And as long as you're doing um one of the other the formats of instrumentation, be it zero code, manual, or hybrid, that all will be instrumented for you as you make calls to those libraries. Whereas zero code is more kind of like not changing any of the code itself um as opposed to you know pulling in manually open telemetry library so on and so forth but maybe changing the way that the execution parameters for the underlying service work. So again taking NodeJS or Java um there is basically an execution path change that you would make in the case of Node.js it's requiring- require for the top level auto instrumentation API for uh the likes of Java. as a open telemetry Java SDK which you can just add as a d-jar and that means that you haven't actually had to change any code but you'll still get all the benefits of tracing your application along with those automatic instrumentations. The only difference with that is that obviously um if you have uh code of your own which is long running and you really need to kind of get an idea of how long bits of work are taking in your code, you still need to instrument that yourself. So you'll need to do kind of like a little bit of manual instrumentation or hybrid instrumentation if you're using auto instrumentation. The final thing about zero code is if you're in Kubernetes um there is actually an own telemetry operator that works for five different languages that allows you to go and say okay as long as I'm running and as long as I've annotated my podspec uh I will automatically get all of this zero code instrumentation as well. So it's very easy to add especially in in frameworks such as Kubernetes. Awesome. Yeah, >> I [snorts] figured I would pop in because there's a few questions that are in the chat that aren't necessarily just specific to what we were just talking about, but I figured I would try to ask them sooner than later. Um, so there was one that's asking if there anyone's had issues with the relation between span and trace ID and getting it synced with logs. I don't know if that's something specific to an issue that they're having, if maybe there can be more detail on it, but um, >> yeah, we can we could talk about that. So in Grafana there's there's different ways that this occurs kind of depending on the vendor and and the back end. Um in Graphfana u obviously if you've got familiarity with it uh every different type of telemetry signal is a different data source and so what you do is you configure the data sources to ensure that um you understand what the contextual information is between a trace and log for example. So you might have log lines that have trace ids in them. Uh and they might be trace underscore id equals it could be trace capital id colon whatever the trace ID is. But the data sources allow you to configure uh via different patterns via different reax is exactly how that you correlate between um the trace ids and the span ids in a trace and those in a log line. And that's what allows you to go between trace to a log line and a log line to a trace. It kind of differs in every diff uh backend platform. But generally um the the uh the convention is to ensure that if you want to correlate between one signal and another, you have to have that contextual information to be able to switch between those different signals. >> I'm going to jump in really quick. I think usually it happens when people get logs from pods or from a CD out and at this point you you've lost all the structure. You've lost correlation. there is nothing uh usually in the um log output that uh preserves the context locked happened in. So you could switch to recording logs from your login library to open telemetry inside your process or you would need to stamp the trace context and then parse it back. >> So I can speak a little bit of what we've seen issues with this from a from a cloud perspective. We occasionally see issues with this with padding zeros on trace ids where essentially the um whether you let's you know if the first four or five characters or whatever it is is a zero then if the log line essentially writes the zeros and the trace ID doesn't or vice versa that can cause problems or if you've enabled adaptive telemetry you can be dropping one side of it essentially um and so you you naturally [clears throat] lose that association. So that's probably the two biggest things I have seen from like a graphana cloud perspective. >> Okay, I am going to switch it over to the next question. Um so is it more stable and or mature? Any pros/cons to use temposmetrics generator or to generate metrics in hotel/alloy collector spanmetrics connector. >> This is a very good question because it kind of also depends on whether you're doing something like sampling. So um yeah you can use absolutely tempo or graphana cloud traces metrics generator to generate all of the metrics that are coming from your trace bands. There's a caveat to that though, which is that if you want a full view of the observability over your entire system, the number of requests coming in, the number of spans that are being used to process that data and how that's actually all working and get your red metrics from that. Um, you really need the full view of all of the traces that are being output because without those, you're only going to be getting a subset of actually what's going on in your application rather than the full view. So generally what we recommend at the moment is that if you are going to be doing some tail sampling or some head sampling that you actually ensure that the metrics generation are carried out in your local infrastructure using the spammetrics generator or the service graph generator rather than sending it to graphfana cloud for example to do that yourself because you won't get a full view and you'll kind of have to mentally uh scale up depending on how you know you're actually doing your sampling on how many requests you're actually getting. So kind of like the short view is if you're not doing any sort of sampling whatsoever and you're sending us 100% of all of your data or you're sending tempo 100% of all of your data absolutely use the metrics generator within tempo or graphfana cloud to do that if you are doing any sort of sampling it's best to do it um to carry it out in your local collector or graphana alloy instance because before you do any tail sampling before because then you'll get the full metrics output of all of the requests that are coming in so that you can see that data in you know something like graphana >> to make this a a little more complicated. Uh the formats, the naming is very slightly different between the two which is slightly problematic in trying to switch between them. I would say if you're a graphana cloud customer then using the um kind of what we provide um which is necessary to use the app observability um product. So that kind of makes it a slightly more complicated But um yeah, if you're if you're running it at home essentially, then if you can put it in the collector, it's easier to scale collectors than it is to scale tempo. So this is a it depends at least from kind of my perspective. Um but if you're I think it's an easier story if you're graphana cloud customer is maybe what I would say. Yeah, there's a >> sorry that I was just going to say based on the application um observability for example if I obviously don't want to get into products too detailed but if you're using kind of like some of the products that we have application observability or knowledge graph there's actually configuration options in there that say I'm actually either generating it from graphana cloud traces or I'm generating it from a local collector or graphana alloy where the prefixes are slightly different where the names of the metrics are slightly different so it will automatically um ensure that it does the translation for you so that you don't have to reconfigure all of that yourself. But yeah, if you're running it kind of like locally, as Matt says, probably collectors are the way to go. >> Yeah. One thing I will add though, if uh sorry, [laughter] >> if you're planning on doing metrics generation on in the collector and planning on doing sampling as well, make sure that the metrics generation happens way before sampling. If you sample and then have the metrics generator, then you are generating metrics on sample data. So, you're going to miss a lot of uh the information. it's not going to be um as uh I guess what's the word that I want to use but it's not um very a very good insight into everything that is happening into your application. So it's very important to generate the metrics first and then sample um because that is uh what we recommend and that is the best way to kind of get u the best information out of that. [snorts] Okay. And then there's another question. There's two more questions. So um one is because the name is different does the trace dashboard need to be updated to support the different names depending on what is used. >> Yeah. Uh you can override the behavior of the naming I believe in alloy. Um I forget what the configuration is called. I believe it's namespace. Um I think that can override the behavior. Um, and I can try to look at the documentation here real quick to see if that's uh if it will actually do what I think it does. You Oh, and you can always re uh you know, if you feed the uh remote write to uh a Prometheus endpoint that you have a you know metrics relable rule on, you can rewrite the metric there fundamentally. Um so yes, there there are ways to get around it. uh but they're less than ideal. >> Okay. And then another question is are there any lightweight open telemetry SDK patterns for edge devices or low power nodes or is custom instrumentation usually required? >> Yeah, I can take this one. This is a great question. I think if you're doing up telemetry on edge devices or anything you need some it's very to be very efficient. It's probably the best choice to just use SDK and do manual instrumentations. You also wouldn't gain much from auto instrumentation. So you wouldn't be able to run it. There is C++ SDK, there is rust SDK. Uh they should be able to provide very efficient um telemetry. U there are also always a questions of what kind of telemetry do you emit from edge devices? Like it's not matrix probably, it's probably some sort of events. It's also not spans usually. Um, so this is all pretty uh custom and manual but I have heard that some uh open telemetry SDK uh version has been sent to the space and people are getting actual telemetry from from the space using it. Awesome. All right. Um, I know we're running short on time. Tiffany, are there any more questions that we should probably think about asking for moving on? >> Uh, there's none in the chat at the moment. So, feel free to carry on. >> Okay. Um I think I'll skip a few things because in the interest of time um so we can jump right into sampling maybe because that is a topic that we've covered or that we've spoke on a little bit um but we haven't really touched on [laughter] here to kind of explain what it is. Um so sampling um you know uh put simply is just a way to reduce the number of traces that you have. Um this is really important um if you are conscious of how much you want to spend on your observability budget. Uh reducing the number of traces will reduce um uh infrastruct infrastructure costs and um how much you will be spending on maintaining that level of data. Um so there are two different kinds of sampling that you'll see often. Uh one is head sampling and the other being tail sampling. Um and you can use both uh if you wanted to. Um so HUD sampling is where decisions are made as early as possible. Um and spans are sampled at a consistent rate. So this is more uh probabilistic. Uh so you will sample a percentage of your traces. So if you wanted to, you can sample only 10% of your traces, 5% of your traces. Um and it would just sample at a consistent rate. Uh tail sampling on the other hand uh the decisions are made after all the spans within the trace are examined. Um and there are a lot more policies. It's not only probabilistic. Uh so there are things like uh sampling how many uh or sampling erring traces uh latency. Um you can get into vertex expressions um and look at specific string attributes, numeric attributes. Uh there are a lot of different policies um that you can use uh to kind of control what kind of uh trace data that you are getting. Um and this will heavily improve uh your trace volume and what kind of um traces you're actually looking at. If you only care about your traces that are erroring, that is a great thing to do. You can put tail sampling um on the collector and uh you can sample just whatever it is that you're looking for. All right. And there are Oh, sorry, Matt. I didn't know if you wanted to say something. >> Yeah, I think we'll um probably wrap up here uh since we're getting really close to time and just be cognizant of everybody. [clears throat] >> Absolutely. >> If everybody wants to join in. Uh we can have a I think Head's having a little computer trouble, but Lumia, if you want to join us back. Yeah, >> I didn't give her a choice. [laughter] >> Bye. >> Oh, she can remove herself. That's it. [laughter] >> Uh >> yeah, we Yeah, we should definitely continue this on another stream, but yeah, I figure Yeah, it's been in our um people might have other meetings, other things, but it's definitely stuff that we should dive into more. I think if you're up for another joint call, >> yeah, sampling deserves I don't know how many calls. I think this is super important and like I mean you cannot record all the spans so there should be some sampling but pe people who watch uh who listen come comment if you are interested in another stream on sampling let us know >> yeah [clears throat] or if someone wants to get involved in open telemetry um how can they do that >> uh it depends on what you want to work on right so like if you are interested in instrumentations there is an endless amount of work streams related to instrumentation. We are stabilizing things around let's say RPC gRPC calls. Um if you want to work on let's say collector there is also a lot of efforts around collector you can come join open telemetry slack uh I don't have a link right now I'm sorry if anybody can post a link that would be awesome um come join slack come join any community calls we have a community repo where you can find active projects uh you could also just write and talk about open up telemetry or share what you have um in the social channels. We would be very interested to learn what you're working on. >> Awesome. Well, uh >> yeah. Um so let's see. Yes, there's a comment of sampling is kind of important topic. Be great to have another call. Okay. >> Absolutely. >> And then >> could have a whole session on that probably. >> Yeah. And then again for time. [laughter] >> Yeah. first slackdographo.com. Um, if you have any questions for us after the fact, um, please go in there and like there's the tempo channel, there's open telemetry channel. We also have, uh, community.carfana.com. Um, LM Miller posted on there for this specific call. So, you can um, make if you wanted to chat about this specifically, you can make comments on there for instance. So, there's a bunch of things we have there. And then Luna, did you want to talk about Graphana? >> Uh yes, we uh the registration for Graphana just opened up. Uh you can get uh I think 50% uh for the early bird registration. If you are interested uh to come and talk to us there there will be a lot of hands-on lab socializing and um other things about graphana and open telemetry as well. >> Yeah, the focus of a graphana though it says graphana um is obviously on things in the graphana ecosystem but the focus is on open source. This is also a great place to talk and listen about Tampa uh the OSS version. >> And do we have dates and where it's at? >> Yes, it is April 20th through 22nd in Barcelona. Sorry, Venilla. [laughter] >> Thank you. >> Which is one of my favorite cities in the world. >> And they're also working like obviously not everyone can go manage to go there. Um, there's also going to be some local Graphana related events um as well, so you don't have to completely miss out. [snorts] >> All right. Well, I appreciate everybody, especially our guests. Unfortunately, Head's having some computer issues, so he couldn't be here for the uh kind of the wrap up. I'm sure we'll see him again soon. But again, appreciate everybody showing up uh and and presenting some great topics. Thank you everybody for having kind of the conversations. Uh we love to see any comments and if you're watching this in the future, add more comments. We'll take a look at them and try to answer them either there on future calls. Um yeah, Tiffany, anything you want to add before we wrap up? >> Even if it's not specific to this call, if you have general questions for Tempo, you can ask them. Yeah. again in this slack for instance if you want topics if you have topics that you think would be interesting to talk about or want to discuss for a future community call please mention there if you have worked on something super cool and really want to show it we can also try discussing that as a possibility as well so basically come talk to us >> all right we'll see everybody next March then >> this March March [laughter] next March sound. We'll see everybody in a month. How about that? >> Yeah. >> Thanks a lot. >> Not everybody.

