# With OpenTelemetry, ComplyAdvantage overhauled its observability (twice)

ComplyAdvantage, which provides compliance and risk management tools, has overhauled its observability platform twice in two ...

Published on 2024-01-03T00:43:05Z

URL: https://www.youtube.com/watch?v=Wyvq7Mka3vI

Transcript: hello uh yep as Matt just said my name's Adam Wilson I'm the principal Sal at comply Advantage um I'm not here announcing any cool products of any kind um but I'm just telling you how we've ended up using um quite a cool product and how we went from on Prem grafana to Cloud Pana uh cloud grafana with a little bump in the middle um just for a bit of background on myself uh I'm actually from Medway in Kent it's a lovely picture of someone from Medway in uh lovely picture of Rochester which is the only place in the UK I think that has actually managed to lose its City status um there's quite an interesting Tom Scott video on that if you if you want to know the details behind that um and purely based on that um I ended up moving to Brighton and that's a lovely picture of Concord 2 in Brighton which is where I spend um as much of my free time as I possibly can uh I play drums and go to as many gigs as I can but anyway when I'm not at gigs and playing drums um I work for a company called comply Advantage um we're all about trying to find bad guys in the financial system uh so we kind of work with a lot of um different fexs um across the globe really um to try and find anti-money laundering and or help provide anti anti-money laundering um and yeah like I said try and root out anybody in the financial system that we'd rather wasn't there um so after all of that Spiel I'm going to talk about uh what we've ended up doing uh with observability um and yeah I gave it away in the title of the talk really which is that I did a gigantic observability migration with the entire company got everybody onto um one vendor which I'm not going to name um but there's some Clues hidden throughout the slideshow and uh 10 points to anyone that can work out which observability vendor we were using uh in the middle of this process uh and just as we'd finished that project um I got the enviable task of turning around to the entire company and saying well guess what we're about to do it again um and we've ended up now migrating on the grafana cloud uh which is why I'm here um so in order to talk about all of that I'm going to break down there a million different ways of breaking down uh the parts of an observability system but the three things that I'm interested in today are how we export data from our systems our applications and uh you know the various different things that we want to uh collect information about how we collect that information um and then how we ship that into our storage uh which has a big impact on how we end up um querying the data um so why did we end up migrating twice and and you know why migrate or think about changing your observability stack um at all um this is one of my favorite diagrams um ever I bring it up very very regularly um simply because so if anybody doesn't recognize this sorry this comes from the Google SRE book and this is kind of like their summary of all of the different like the maso's hierarchy of needs this is like the maso's hierarchy of Sr um and the bottom three parts of this uh pyramid are all um directly related to or kind of directly tied into um your observability stack so for me it's all about making sure that you absolutely nail the bottom parts of this stack before you start trying to do too much at the top and start trying to be too clever um you I'm a really big believer in absolutely nailing those Basics um before you go too crazy um so to give you an idea of like our infrastructure um and and what we're dealing with uh at the moment uh we deal with about that many spans per day uh that doesn't include the spans that are flowing through ISO because we've had to sample them all away because there's just so many of them and they weren't given us the information that we actually cared about at the time so that's taken directly from our span metrics and I'll talk a little bit more about our span metrics later uh we've got about 41 kubernetes clusters that many nodes um about 20% we reckon of our metric series are in open Telemetry the rest of them are in Prometheus uh and to give you an idea of the size of some of our on Prem Promethea servers that are still hanging around uh this is a screenshot from from our Argo CD uh so you can see there's two replicas in the stateful set 110 gigs of uh memory and we're requesting for CPUs so that if in terms of like the number of um Prometheus servers I'm not sure if they're still in every single cluster but yeah we're talking about 82 that'll become relevant later so where like our starting point where do we start off we started with an on-prem grafana and this is a basic overview very high level um diagram of what our observability stack looked like when I first joined the company uh it was set up like this for a reason um and it's got a little 41x over one over there on the right hand side because basically um there was no Federation everything here was deployed into every single cluster so if you wanted to know about something that was going on across two different clusters you were logging into uh let's have a look there so you're logging into gabana grafana you're probably running some Pro um some Cube C uh commands uh directly against the kubernetes API um as well to understand what was going on in bits that hadn't made it through because there was an error in the fluent stack somewhere um so yeah in in terms of like how everything worked we had what I felt was like all of the constituent parts of a great open- source observability stack um but just not really serving us uh during an incident um yeah you had to hold all of that information in your head head go into one tab then the next tab then the next tab um and you know that was all going on um with people trying to uh put screenshots from k9's which is something people use for cc commands on the command line so yeah K9 screenshots were flying across slack all over the place which is uh not great because you can't click on them and get anything out of them really you just have to take their word for it that they haven't um engineered that or they've run the right k9's commands um and yeah basically at the end of the day we had too many people with too much access to ra production clusters um and in a fintech that's focused on compliance and security and um data security uh that wasn't going to hold up and also just basically because of the culture uh around observability at the time everybody wanted their logs forever they wanted more and more and more metrics so the Prometheus servers the reason why there's two replicas is because they fall over constantly so when I first joined I haven't got the exact data because we were very bad at collecting data back then um but basically every single day I had an Sr who was off because they'd been up all night trying to keep the Prometheus servers up because they just constantly fall over anyway so that's where we were and this is what it looked like during an incident there you go so you've got tabs opening and you had to try and collect this context and go from one tab to the next and look at all the different things and try and work out what was going on right there we go so um started looking into this thing called open Telemetry uh I'm going to very briefly I'm sure most of you here know what open Telemetry is but for anybody who doesn't um it covers now it covers traces metrics and logs it didn't when we first um started using it uh but logs has come in uh more recently uh it really is geared everything around traces as like a first class citizen within observability um it allows you to do drop in instrumentation so you can start off with just a few lines of code um get maximum benefit from it um and then when you've kind of sold the idea of using open Telemetry to your to your organization people then and I can tell you this from experience people start to use it more and more and more and they want more out of it because they start to realize the power of it but you know often trying to get over that first hurdle uh the drop in instrumentation is absolutely key um and another thing that was really important to us is that it was open source and vendor neutral and we'll be talking a bit more about the vendor neutrality uh as this talk goes on um but that fact that it was open source was really important to us we're a company with a um a big history of using as much open source technology as we possibly can uh we contribute regularly back to open source our Engineers like to jump into the GitHub and find out if something isn't working and we're sure it's a bug in you know the systems that we're using we want to go and find the bug raise an issue raise a PR see what we can do to help um so yeah this was really key for us in terms oh sorry yeah open dmetry came about as a fusion of open tracing and uh open sensus um I'm doing a massive Whistle Stop tour through this by the way if you want to know any more of the details I can bang on about this for ever and ever and ever um anyway so yeah this is what's ended up running for us so a big a big piece of the open Telemetry um ecosystem again for anybody who's not already using it is the open Telemetry collector and this is a visualization that I've pulled from um a tool called otel bin which if you're using the collector is a really cool Tool uh interesting one so you can so the collector is a go binary that basically absorbs a hell of a different hell of a lot of different observability uh data types um so it can absorb for us for our case we've got open sensors and pure open Telemetry uh coming in you can then run a lot of um different processes on it uh within your within the collector itself and then you can export to all sorts of different places for us I think we got what have I got one there doing to the load balancing exporter which again talk a bit more about later and one going out to otel http so the open cat protocol works over at HTTP or grpc this one is exporting V HTTP um and at the top there is a pipeline that's running traces and I think the second one is doing metrics uh from I can't quite read the slide but it's probably metrics um so yeah that is all I want to say on that so in terms of like what this looks like in the back end this is a quick screenshot from grafana cloud just in case anybody isn't already using distributed tracing uh in their graan Cloud instances this is um actually just touching one application but applications don't run on their own uh in our kubernetes clusters they've got sto uh gateways that the requests touch before they come into the application pod then when it hits the so this top one uh right at the top here is the iso gateway then underneath that in a different color we've got the esto proxy which is a side car that runs inside the same pod as our application and then all of these other little bits and Bobs uh underneath are um the application operations that happening within the app so the whole thing um to keep us all on the same page that whole thing is called a trace uh and each of the different bits underneath it are called spans um so they're all tied together with IDs and all of this is just driven through uh request headers set out by the open Telemetry um schema so um yeah I mentioned there was going to be a few Clues and if anyone can guess who we ended up migrating to but I don't want to get sued so I haven't got anyone's actual logo on there so anyway so in terms of how this works uh for our purposes internally um we have our app pods so in this case we've got a cotlin app and it's got the open cry SD K running inside the appp pod that's over there on the left hand side and this developer has actually read my documentation and added the annotations that I asked them to to their application pods so well done them 10 out of 10 um and what that happens over here we've got the open Telemetry operator which has a number of um crds so in comply Advantage we've got a dedicated observability team and they are able to control the deployment of the operator and they deploy the configuration that's in these side cars and what the side car uh what the operator will do is actually inject the uh open Telemetry collector binary into the application pod sitting alongside the application itself so there's it's the exact same binary every time you see the op Telemetry logo on this slide it's the same go binary with slightly different configuration so that's what's really cool about them you can SN you can sort of point one to the next to the next and you can build pipelines that feed into pipelines that feed into pipelines anyway what happened for our purposes was we tried to keep it as simp simp as possible app exports to Local Host hopefully 4317 on the grpc open Telemetry protocol endpoint there's a sidec car sitting listening there it's going to chat to uh the open slet Tre what we call the hotel Gateway uh via stto that's what the little Sailing Boat is if anyone's not familiar with that logo so anyway everything goes out so what this means is that the observability team are able to control the side car config to make sure that no one does anything silly there um but you know if we trust an application developer they might be able to add that config themselves to their Helm chart the thing that we really care about though is the open Telemetry Gateway and that's where we have our sort of final controls in place we make sure that we're tagging it with the right environment um when I first started migrating we ended up with like a drop down list um in our UI of like 10,000 different environments because everybody across the company was deciding no hang on that's not a name so that's QA 01 and that's not playground that's you know Q a whatever I've decided it is today so we tell them we don't care about that anymore we're going to tell you what the environment's called um so we have three environments in our drop downs which makes things 10 times easier when we're communicating across the company makes things easier for our support teams and more importantly for um my boss and his boss to uh you know the people that pay our money anyway so then how do we handle everything else I mentioned at the time early on we didn't trust open Telemetry with logs and I don't think open telemetry trusted itself with logs um so we did logs via the um unknown vendors uh proprietary agents so we did we tried to keep things as open source as we possibly could but we did have you know some necessary evils and we used that to ship logs um what else did I want to say there yeah I think that's it for that so um very soon after uh we started implementing all of that I started having having lots and lots of conversations with people about sampling um and this is one of the the more interesting things that has changed most recently has been that people have started to get on board with the fact that sampling is going to happen whether you like it or not um no application developer seems to want any of their application uh operations to be sampled um but if anybody here is running you know a non-rival observability infrastructure um you're probably going to be sampling or um you've got way more money than I have to play with um so yeah but anyway in in most cases you do not need every single one of your traces you need an example of what works and then during an incident you need an example of something that went wrong um and you can compare the two and go Ah that's probably the issue so yeah very quick on sampling I don't have the time to go into detail but I can talk about this all day um early on that first um diagram that you were just looking at we were using what's called probabilistic sampling so effectively every time a trace hits our Gateway we roll a dice and if the numbers come up the way we want uh we sample it away and if they don't um we send it onto our back end um we've actually um moved to uh Al so probabilistic is really good because it's very easy to do so we had so many different things going on we were doing the first migration um so the observability team we didn't have time to get really complicated and we were actually able to use some of the tools that were in the V that we were using at the time to um get away with just using as much probabilistic sampling as possible because the vendor that we were using would sample our data for us which meant that we could keep our infrastructure internally quite uh simple um but yeah as time went on we ended up moving um to grafana Cloud anyway timeline migration on the first the first migration took us ages I don't want to go into the details of it it doesn't really matter to any of you I don't think you're going to get much out of how long it took us to migrate to a vendor that we're not here to talk about today um but just to talk really really quickly some of the things that took the most time um I always say you know I can reprogram computers tomorrow they do what I tell them to it takes you about 18 months to two years to reprogram the people and in any proper observability um infrastructure the people are you know a massive key aspect of getting this stuff to work properly um so yeah it's it was a long what I'm trying to say with this slide is it took us many many months to migrate there you go that's all we need to know there so anyway we're on grafana cloud now so yeah after that first migration um I give so some of the reasons that we ended up moving was were more around um like our feeling around the culture of this uh other vendor and the culture that we had internally and it was very much like we were we were talking different languages and um as a result the relationship with this vendor didn't go very well um and started kind of looking around at different people that we could use for our observability back in now that we were in open Telemetry and we're vendor neutral you know we could we could migrate on a whim like for you know zero cost um yeah uh yeah um so yeah we started looking around at different different companies and it as as soon as I started talking to graer again it made absolute sense to migrate to to um work with those guys they came from an open source background um a similar size in terms in ter or more similar than with our previous vendor uh to us and um as soon as I kind of got a lot of our technical teams talking with some of their technical teams spoke to Tom um as part of the onboarding process um and it just everything gelled um much much much better so anyway moving swiftly on in terms of what we needed to do though to migrate to grafana with our vendor neutral open Telemetry that wasn't uh that vendor neutral at all um we had to solve for our sampling issues that we would that we didn't need to solve to begin with so we actually had to implement a much more complex setup of our open cemetry gateways uh so we this is where the load balancing exporter came in and um yeah so what we do now is we similar kind of thing we get the the stuff out of the apps they dump all sorts of rubbish it goes into our gateways and the gateways now use something called spam metrics so every single trace that flows through that first layer of open Telemetry gateways we extract all of the metadata and the numbers and the metrics about it we ship them straight to grafana Cloud then um the uh first layer of open gateways then uses the low balancing exporter to send all of the spans from the same Trace into the second layer of open Elementary gateways uh where we make our sampling decisions uh and this is important because both of these are run as kubernetes deployments and uh we need some kind of way of basically we're hacking State into our observability infrastructure at the moment using request request headers um we're constantly debating this though so if anybody wants to come up and talk um with me I've got a couple of um guys from my team here as well Edwin's in the audience who put a lot of this together so we're always happy to have a conversation about anybody else's experience uh with running this kind of stuff um and it'd be cool to to bounce some ideas uh back and forth but we do we we and we we use all of this um and we've swapped in the graffer agent where we were using a proprietary agent uh previously uh and it dropped in quite neatly it does the same thing that we did with the proprietary agent stuff it handles the logs for us uh and actually what we've done is we've added Prometheus running in agent mode back into our clusters uh to handle our metrics uh specifically for the Prometheus endpoints uh so I've got a little picture of rabbit mq there that runs in some of our runs on some of our nodes and we can scrape the um the rabbit mq metrics and ship them up to grafana Cloud along with everything else um so that's what our infrastructure looks like uh at the moment um so all of those changes uh that I was talking about there like why what changed across the organization like as we did these migrations um and I think the big thing for us was the change in the way that people started to tell stories with data so these screenshots are from an earlier uh talk that I used to give um and it's basically so we got the trace on the left hand side there and we extract the metrics from the trace and we're able to build pie charts what I found was that these pie charts actually opened up a conversation across the organization that had never happened before because previously when people used to talk about like for example when our CEO wanted to talk to us and say um you know how many searches would run across the systems um they'd get some kind of complex answer well we've got you know 300 HTTP responses from this system and actually know only 30% of our requests go via that service so we have to add that to that and extract with this and I don't know really start talking about traces because the traces don't matter about which kind of Route the operation took through your infrastructure so you're able to answer very simple questions how many searches would run this many and this is the breakdown um by customer that's uh actually fake customer IDs up there so um hopefully I've not revealed anything to proprietary but anyway the cool thing about this was the CEO could go from our engineering infrastructure data and go and have conversations with our customers and our partners and going hang on a minute you know that discount that I gave you last year it's quite an interesting pie chart considering how many searches you're running on my infrastructure um you know is this is this really um the best relationship for us to take into the next year year um and just being able to have that impact on product on you know sales on you know all of that kind of side of the business just from getting some request headers into our applications was was revolutionary really um this is just going to be a quick slide it's basically in terms of like pulling off the migrations and then migrating again um it's a standard it change management project um yeah so identify your allies every time people come up with problems because they will you need a team that they know they can go to solve the problems quickly we actually ended up building an observability team as I mentioned earlier purely to do I mean they're not just there to do migrations because we're not doing anymore like I'm done now um I think we've all aged about 10 years um but yeah so we we need you just need the team there so that you can solve the problems quickly help people with any of the um questions that they have we actually got two slack channels one of them for like Tech supporty type questions and one for just general observability discussions so that people can go has anybody solved this problem before or across the org like how do people think we should get this into the python data science teams you know how do we do tracing there how does it work and you know people that aren't anything to do with the observability team can just chat about observability and I can keep an eye and go oh that's an interesting conversation um and yeah we try to dog food as much of our stuff as we possibly can like we should be the experts at using whatever we're trying to get everybody else to use uh what else have I got coming up uh yes so yeah so apparently I've completely got the slides in the wrong order um so this is a bit Meandering and disorganized much like the migrations were but you know we pulled it off so just work with me here so these were these were the concerns I was talking about before we started use more and more of the previous vendors proprietary technology and this is the thing that you do need to bear in mind when you're starting to use things like open Telemetry and it starts talking about vendor neutrality is it's like the technology might work with multiple vendors but can you and do you end up in a place where you're like actually I quite like this particular vendor magic like some of the the um features that graan were announcing earlier you know as you start to bake more and more and you start to rely more or more on those features you just you don't need to be like oh my God like we're not vendor neutral at all like you can migrate um you just have to bear that in mind um yeah and we didn't we we actually got quite a lot out of implementing spam metrics we didn't have to do spam metrics before um but yeah it's it's just it's one of those things to bear in mind I'm just providing food for thought here um yeah that's an AI uh diagram I'm not quite as good at generating AI slides as uh the grafana team are so this is as good as I could get I think most of the words are actually actually real words in that as well um so yeah that was the timeline of our migration onto grafana Cloud um it was much shorter because we were using over Elemetary so we didn't need to reimplement the sdks again um so yeah we've reached the end uh I'm going to reflect a little bit on some of the things that happened uh throughout the migration so yeah we've ended up needing to build more and more tooling uh as we went from the previous vendor into Cloud um so you know bear that in mind we originally did a lot of metric under budgeting um as we sort of started to move into um Graal we ended up with way too many infrastructure metrics this is one of the reasons why we started turning off we've turned off the Prometheus metrics that come out of our open Elementary side cars we've turned off the spam metrics that are generated from our sjo proxies because that's not they weren't giving us the story that we wanted to generate with our data that that was just not interesting enough for us at the time or currently it's not but we're going to continuously review uh this side of things um the other problem with like unmoving from on Prem to Cloud especially in the the setup that we had was we had uh 41 different clusters and yeah within each cluster we had the replicas but almost every single one of those Prometheus deployments was a little bit snowflakey this is something that can happen over time and so trying to really understand how many how or you know our total metrics volume it wasn't going to be a 1 to one we never we didn't need to migrate everything from on Prem Prometheus because it was a total Dumping Ground which is why it fell over all the time so going across the graph Cloud I basically had to just generate a spreadsheet of all of the metrics that we were using across the company and I went through it by hand and just went no no yes okay maybe no no no no no um which isn't super scientific um but it got us through uh for a little while um yeah in terms of like what we want to do um in the future as well the thing that we're kind of looking at the moment we have we are using the grafo SLO uh product and it's been quite um cool as part of our relationship with gra is that we're actually able to that they're willing to work with us and um we're able to provide feedback on new produ uh projects and products uh that they're developing um so yeah we were sort of heavily into SLO before it was released um and we're now looking at Can We sync between the slos so if a team sets up an SLO that says actually 400 milliseconds is a slow uh operation for us uh can we sync that into tail sampling so that we make sure that anything that's over 400 milliseconds is sent to grafana Cloud for review um because the concept of a bad Trace that needs a review is different and can be completely uh service specific uh I'm rushing through the the last couple of slides here uh and yeah I think in terms of like where We've Ended up now we've ended up with an observability vendor um and it that has a really great culture match with us internally very important because it's a it's our second biggest uh contract after our Cloud contracts um to give you an idea of the importance of this relationship and this was why it was so disappointing to see uh it sour before when we were with the previous uh vendor um but also really nice to see that it's that we're now with somewhere that we're really happy and we're able to kind of work together um into much more of a partnership um and I think there you go that's my final slide that was a fairly okay one uh from the AI tools I think but like I said so this has been a really really Swift Whistle Stop tour slightly Meandering a bit disorganized um but yeah um I'm really happy to talk about any aspect of this much more in depth if you want to grab me I'll be floating around for the next couple of days um but apart from that um I think I'll end it there and um yeah thank you

