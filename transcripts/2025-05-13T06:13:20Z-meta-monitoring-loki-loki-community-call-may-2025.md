# Meta-monitoring Loki (Loki Community Call May 2025)

In this Loki Community Call, we talk about the need for meta-monitoring Loki: why Loki needs to be monitored, what to watch out ...

Published on 2025-05-13T06:13:20Z

URL: https://www.youtube.com/watch?v=OzYHV9xHIms

Transcript: We are live. Hello everyone. Welcome to this month's Loki community call. In case you hadn't figured it out, we're actually in the same space right now. It's quite special, right? Live. Yeah, it's super special and also super nerve-wracking because we're on hotel internet. So, we just arrived like Jay and I just just landed. We're worried about the Wi-Fi because you could bet anything with Google Drive. Well, we'll see how this goes. So, tell us if like any of the audio is messed up or anything like that because we're kind of just like figuring things out. Um, but yeah. Hi, I'm Nicole Vanderhovven. I'm Jaylen. And I'm Jay Clifford. Partner in crime as always. Yeah. And just because we already asked him for his last name then did not pronounce it at all. This is Dylan. Gades. Gades. Gadis. Gades. Oh my god. G. I already pronounced it wrong. Dylan, why don't you Well, actually, before we do that, let's talk about some some announcements. We actually, maybe Jay, you should talk about Grafana. Yeah. I mean, we just celebrated Grafana in Seattle. Um, so we was there all of last week. Uh, I think we had 531 people that actually came in attendance, which was wild. And then um over a thousand people on the actual live stream attended which was awesome. Um so yeah we had some crazy announcements. We sort of announced for Graphfana 12. Um we had like git sync so you can now like programmatically sort of deploy your dashboards. We announced Premiere 3.0. Um loads of crazy announcements. We're not going to cover them here because this is a Loki community call but there's loads coming. All of the videos are going to come out in about three weeks. I think the legend sorry the content team said so um if you're if you're feeling the FOMO of Graphana as well totally did um you can definitely see them there but there was so many Loki um community members there and so many Loki sort of graphana aces um sugar graphana champions graphana champions for Loki so it was super cool seeing them there so it was brilliant actually the the keynote is already up because it was livereamed so you can already watch that on channel and then I did like a one minute YouTube short to just be like here's all the things you need to know that that was announced in the keynote. Um what else we for on the Loki front we also got a new release 3.5 is out and there are some there's a lot of the main things that I think we want to talk about is some improvements to the helm charts in particular um and then maybe talk a little bit about the storage. Yeah, I mean uh we had a lot of improvements in the meta monitoring site too, but uh I'm very excited for the improvements that we made to the delete uh to the delete part of Loki. We are now being able to like uh to recover things for SQLI database and that is speeding up the delete parties by a lot. So uh users will probably notice a lot of improvements on that side. Oh, amazing. So, so Dylan, sorry, we've got you to speak about features without actually introducing yourself. So, tell us a little bit more about yourself. When did you join Graphana and what do you do as part of the Loki team? Yeah, so I joined almost three years and a half ago. Congrats. Yeah, the beginning I was already a contributed to the local log side of Graphana. So, low key enterprise logs um and graphana call logs, all of that I I try to help with. Yeah. Oh, fantastic. more on the ingest or query side or a bit of everything. In the in the beginning it was uh it was on both your ingest and on the query side but at these days it is mostly focused on the inest part of it. Oh fantastic. Brilliant. So you're you're the one to blame for the the 254 kilobyte limit on low key log inestion. Right with you. We've had some interesting places in the community at the moment where people are trying to ingest logs of three megabytes in size as body and I'm like oh wow guys. So that was actually singlehandedly so today we're going to talk about just what it takes to to build a to maintain a reliable Loki. We use Loki internally too for our own stuff for for our customers but also for us like we just use our all of our own stuff all of our stack to monitor production our production environments too and we're I think we're not actually the largest runner of Loki I think we we'll probably the second Yeah. Yeah. Um but that's why meta monitoring is so important right but can you talk a little bit more about what that is? So meta monitoring is uh basically you having a dedicated observability stack just to observe your own low key uh instance. Um and yeah it is pretty much required once you reach some um some level of seriousness on your low key instance. uh otherwise like you won't be able to tell what is going on if your instance gets unhealthy. So and and in terms of meta monitoring, you sort of alluded to it there. There's the ability to we we use another stack to monitor Loki itself and is that like metrics and logs? Do we look at those and um and what so what do we actually use our own stack to monitor um Loki itself? I mean so uh it is important to have an isolated environment just to monitor your own low key environments because um if something goes wrong you can still like rely on your separated instance to like find out what's going on. Uh let's say like you deploy something wrong to another location. If your meta monitor is not separated from that you won't get like pages for it. So that's that's why it is important to have it isolated. Ah interesting. Um and and I'm assuming this like this is a practice that we deploy in our own cloud environments for ourselves. We do the same thing on our side. Yeah. Uh so the names are not the same but yeah we try to so for all the production environment we have like a thing that you can consider meta monitoring just to monitor those things. And oh, we we didn't mention traces, but we also like to look at that too. Oh, wow. So like I guess we got free telemetry signals there then. Metrics logs and traces from Lucky. Fantastic. So why why monitor meta monitor Loki specifically? Like if we think it's so reli I mean sometimes I get this question like why do I have to meta monitor it if it's supposed to be reliable? That's what it's built for. Yeah. Uh that's a good question honestly. But uh I think it is because of uh the complexity nature of like the the system. Uh depending on how you are using it, you can just make it to consume too many resources or like let's say you want to update something. Maybe your update has a regression on it. Yeah. And and you you you have to have ways of like telling what is going on. So that's why it is so important. I I also imagine that it would also have compliance things like some people might need to be able to show some logs like if it's security access logs or something not being able to access those logs or losing those logs that's that's really critical like that could affect compliance. It has other implications not just for for being reliable, right? So we definitely want to make sure that we are we still have those logs. Absolutely. Yeah. So, there's actually quite an interesting question in the community there and it's kind of um Dylan, you can allude to how much we're allowed to talk about our own infrastructure and not um but um Jiffy Ji sorry J says, "I have a question. Do you actually run multiple Loki instances per service team or environment or is it rather one large?" Uh we have a single big meta monitoring for all the the different environments. And then so do we keep separate clusters of Loki and then we have everyone as a tenant on those those different clusters depending upon the region exactly that but but like uh we have multiple of those but the meta monitoring itself we only need one like we could have a single meta monitoring for all of those but so far we we haven't like find the necessity of it. Fantastic. So yes, so one central LGTM stack where we basically aggregate all the metrics from all of our Loki deployments and make sure we have uptime for each of those. Exactly. Brilliant. We also have a question from Harold or a comment that says that they've just been discussing that Zabex monitoring LGTM infra which in turns turn monitors LGTM prod in case creative application devs log very large stack traces and such. Yeah, because like sometime I mean if you if you're logging absolutely everything that's a great way to to cause problems or just also kind of cloud issues if you're logging too much even queries right if if someone is is has written and and executes a very poor performing query you don't want that to stop everything stop everybody else from being able to run those queries that's those are good reasons to monitor Loki. Exactly. Um, okay. So, what's it So, you're on call. Yeah. What's that like for our own stuff? Like what's what's the setup there for me monitoring Loki? Yeah. So, in general like being on call so we have been doing a lot of improvements. So, it is not super stressful these days. Uh, it is quite often very calm. Uh but uh it is so normally what happens is if something goes bad which doesn't happen often but when it happens uh we rely on our tools and all the data that we have like to tell what is going on. Yeah. And then we try to like stuff and yeah act based on what we found. Yeah. Well, I really like one cool thing about Graphana is that we um we have the Loki team actually monitor Loki in production. So, it's we don't have like a we do have an overarching like platform engineering team, but they're not responsible for the individual components. The people who are responsible for making sure it's up and running are the people that are building it. So, you don't get the silo of like, well, we just do the code, you deploy it. like, no, we're making the code and and we're also making sure it's it's being maintained. So, that's really good. So, Dylan, like um when you're when you're on call, what are some common issues that you see? Is it is it noisy neighbors? Is it like what what are some of the common issues you've had to solve on call? I mean, fortunately, as I said, like these days, it is a lot of days they are super calm. Yeah. But uh yeah when it happens let let me think like um yeah in some cases it is something very user behavior driven like bad queries for like uh something is going really wrong in the user side and they start to emit a lot of logs and like your limit and stuff are not like defined for that. Yeah. So we have to like to act regarding that. Uh another issue that we received not super often but it can happen is like when you emit a delete request and your delete request is like to uh it is too too much data uh we might get paid for that and in that case the solution is like to tune the delete request a little bit. Yeah. Okay. We have a a question here. Um, Louise Luis Schwiggard says, "Our Loki read performance tends to spike by a factor of greater than 10 after the read pods in SSD mode have been running a long time and restarting them solves the issue. Any ideas what this might be about? First of all, just a note there, the SSD mode, which Louise mentioned, is technically not supported anymore. That single scalable deployment we would recommend um well, I I don't know if I'd say it's not supported. it's just not going to be supported further. Uh we would recommend switching to the microser the distributed mode just because that doesn't um just separate it into the read and write paths. You can then tune every component separately. So that's one thing. Sometimes if you're using SSD it might like cloud a whole bunch of things. So Louise said it's the read path but like what part of the read path that kind of gets lost sometimes. Yeah. Um I mean we'll have to look at your metrics to really tell what is going on but uh just to double down on what Nicole just said like uh we are slowly uh reducing the contributions to SSD because uh it is not the the deployment mode that we use on our infrastructure and we have been finding hard to like to uh to duck food it. So it is always like the community has to find the issue for us to work on it. it is not super practical. So we are uh like encouraging people to move to distributed or even like you can have single binary but run multiple instances of it. Uh but regarding your issue itself uh if it is because you've been using low key and now u something is going on on your intra and killing the pots fixes it. uh it might be that you are uh retaining too much data somewhere like on your index uh for some reason. Uh so I look into that like uh what your uh what the the cardality of your logs are maybe and what your quires are looking like because maybe you are requesting data from too many different indexes. I think this is like a cool plug for the meta monitoring that we'll we'll talk about because I think as I think Louis is probably already meta monitoring but you know if he can start taking these snapshots especially of the logs and like where the metrics are at and if we can we can put them in an issue we can get have a look at them after the fact have a bit more specific issue. Yeah. I was going to say I I feel like this is a very good scenario to justify like having meta monitoring honestly. Yeah. Well okay so let's talk about that. Um, what exactly should we be monitoring? We have a question from Matt on our Graphana community Slack. I believe he's also in in chat. Hey, hi Matt. Don't don't worry, we didn't forget. We've got your question cued. We got you covered. What log levels are worth capturing and looking at within Loki? Like the follow-up question is, should I only care about errors and above or like also warn or debug? Uh I mean it is worth have all the different levels but uh but at like the more details you need uh the lower level you have to look for to to know what is going on. So for instance if you just if you just notice it that something is not working you you can start by just looking at the errors. But let's say you already have an hypothesis of what is going on. Uh that's where like you probably want to look at the debug logs too. So in in that sense then so I I guess the the debug logs are just going to emanate just way like the volume of it is just not necessary. So is there an easy way of triggering like turning debug logs on? Is it purely just like a config change in low key? It is. Yeah it is super easy. Uh it is a a dash flag that you can use. It is log level and so it is pretty easy but maybe you have to restart your your pod. That's the only problem with it. And then you can um and then basically you probably just take for a snapshot try and reproduce the issue in that debug time and then disable go back to info or something like that. Exactly. Another approach like uh you can do if you are using graph alloy is uh you can tell low key to generate all the different log levels but you can say alloy to like to discard all the debug logs. Yeah, that that's so cool because one of alloy strengths is that it is also it also has a data pipeline. So it's not just a forwarder of of data. You can also say like okay from this subset of data now only take only send through this smaller one and you can also change that with configuration. Um, I think in general maybe capture as as few logs as you can get away with, but then if you see that the same components are are raising alerts because they're they're getting degraded all the time, then maybe you should think about increasing that um so that you just have more more context in the next case. Um, so talk we've sort of talked a little bit about logs as a signal. Um, and there's also metrics that come from Loki, right? So it's like Prometheus style metrics. Um I think we can like sort of add this into um Matt's other question which is uh what are the top indicators that Loki is healthy versus degraded and I wondered if you had sort of um an overview Dylan of sort of what's the type of metrics that we log in Loki/metrics and which are the ones you think are you use quite a lot which ones are important in the health of Loki. I I feel like the best answer for that is uh so we have the the Loki mixing which is a collection of dashboards that are very similar to the ones that you use internally and uh I I feel like this is the best starting point for someone like uh trying to understand what is going on with your lost instance. Um so on the dashboard you'll find like the most useful metrics uh which are the best indicators of how your lost instance is going. I guess what we could do is we could flip it. We could talk about Loki mixing this now first if you wanted and then we could then go how did we get all these metrics in the first place with the drill. Sweet. And also I I have a question that um about the the mixin because like we also now have the drill the logs drill down which is part of the drill down apps which are on graphonicad but they're also oss um because that's also kind of the premise of logs drill down right that it automatically tries to determine what the services are and then provides you with like dashboards and stuff. Um, and so like what what's the difference between the mix in for Loki and the logs drill down? Uh, because the mixing it is um it is a more there's a more direct uh definition of things. uh for you to know them, you would have to use drill down metrics and you you look like at a bunch of metrics, but you still need to know which are the ones that you should be attention to. Uh but if you are using the mix instead, you can like pretty much just look at the dashboards and all of that will be useful for you to know. So I feel like that's the biggest difference. You don't have to filter data. Okay, what are we looking at here? So, uh, this is the So, we have the compile. Yeah. Sorry, my bad. No worries. There we go. How's that? That's good. Yeah. So, uh, on a rile on Loki repo, you'll find the a folder like with the mixings are compiled. You can just import them on your graphana by going to new and then import and you can import the JSON files and you have those things. But uh but yeah, those are like the main dashboard. This is very similar to what we have internally and the two ones I think most users will benefit from uh are the reads and rights one. uh they are very focused on the rhythm on the right path and they will give you a very good indicator of what is going on with your instance. So I guess if we sort of drill maybe let's start with the the right path um and uh I think this sort of plays into the question well um about what Loki metrics looking at. So could you sort of take us through a bit of a a tour of the the right dashboards and tell us what we're looking at and potential issues that you might see and you know how people can read them and understand what's going on. Sure. So so in this case it is a distributed uh mode. So this is not super applicable to if you are using a different mode but imagine you are using the microservices thing. Um what you find is it is separated by components as you know LUI has a distributor an injuster and many other components. So but for the right path the two ones that you should be aware of for the distributors and injusters. By the way plug Jay and I did an architecture video for Loki. So we're going to be talking about different components but we go through all of them in the video. So check that out. I will add it below after the streaming. go for it. Um, so for all the components you'll find like the QPS which is how many queries are occurring for that component. This is really use for you to tell like if something is going bad because someone is doing a lot of work like you see the QPS is spiking. Uh we have the latency uh which is separated by every pod or in general like everything aggregated. This is super useful for you because sometimes you're not seeing errors but your queries are taking super long or your rights are like timing out and this is how you can say if that's the issue or not. Uh you we are not seeing data here for the ingesters because on this deployment uh we are not using zone awareness but uh we would see like the data here if we were. So which of these have alerts on them? Because I I Matt had the question, what are the top key indicators that Loki is healthy versus degraded? I imagine like you're not just watching this the whole time when you're on call, right? You get alerts. So what are the alerts on? Uh I'm not sure if in the mixing it is the same alerts that we have internally, but uh internally we try to so we have SLOs's for all the different services and stuff. uh we try to page like if we are getting the SLOs's super degradated so we can act upon that uh we also have alerts for like uh when hardware issues going on like let's say we are opening too many contract connections or or pods are getting killed or panicking we have also alerts for that and I I recommend like having alerts for those things interesting that there's an alert for having too many compactors because we only expect one ever. Yeah. And I think that's an interesting one people have to realize is like they have more than one cause lots of internal issues with Loki that so yeah if you have more than one compactor running check that you probably have issues running with Loki. Um but there's there's a few that we sort of g out of the box and I think this is something that we sort of hope to expand in the future. I think like these cover a broad set of like when Loki is fully collapsing, right? Um, we have like Loki request errors which kind of takes a look at like the different paths whether it be query and write um and checks if the percentage if there's a number of like failed requests over a certain amount of time um or there's like the Loki panics where there's literally like one of the components is just falling over. Um so I think we covered general uh catastrophic scenarios. Um but is it is is the goal here you kind of if you get an alert that fires from IRM um would would the option be to then go to that dashboard based upon the specific um path that's been errors exactly like by the by the alert name you can tell if it is more related to the ingest or the query path and you should like start from the from the two dashboards I mentioned like the right and the read dashboard nice I guess So, say they was having an issue with the ingest path. Um, and if we jumped back into our dashboards, looked back at Loki mixins. Um, so we talked a little bit about the distributors. We I don't we don't have Sonyware in Jesters, but I'm assuming it's kind of like the the same sort of setup for for both distributors, and um is it I mean would you would you check would you start with distributors first and then move to injustice and work in sort of like a sequential order to see where things go wrong? Exactly. like uh in some situations you will see that distributors there are showing perfectly but the investors are the ones struggling and and you can easily tell this by this dashboard so yeah that's pretty much what I would do so when you say that the that the distributors are showing up perfectly healthy like what is it are you looking at CPU utilization memory utilization errors latency uh mostly errors and the latency like for instance we can say like 5 milliseconds. That's really good. And we can see here we only see two XX and no four or five because we haven't seen any errors on this instance. But if we had we would see that in the dashboard. So that's how you can judge like that distributors are healthy. Uh they have no errors and the latency is good. We have errors now. So I use K6 plug for this. And I think what I did here at this specific time was I said to K6 I was like generate logs with a trace ID label and I generated up to 100,000 trace IDs and put those in the index. Um Impressively hard to like upset Loki which struggle with um I think inest is super good right it's like it's the query side it's like fails more than the the ingest side. Yeah, it is very hard to do the right wrong for the queries. Yeah, the pain like you won't need a lot of work to like to make a very bad query and and then like so the so for so when we sort of split out the errors here any 2xx errors is okay I'm assuming is any four xx errors usually like limiting issues like if you're you're um if you're being blocked by one of limits exactly in most cases it will be for xx ah interesting so that's like if you're trying to address a log line that's too big based upon the limit or like the your your throughput is too much for what your current limit. Exactly. And and this distinction like it is quite important because uh if we really want to retry errors that shouldn't be retryable or let's say we are like rejecting your log line because it is too long. Uh you're retrying it won't make it shorter like it will be the same log line. So we don't want to retry that. Gotcha. You just yeah keep reiterating. And then any 5xx errors is just we're we're action stations. This is returning 5xx gateway. Nice. We got any questions in the Nola as we go through? Well, there's one from Roman Roman Mikolovich. How do how do you prevent injustice pod from running out of disk space? So, I did also want to I wanted to say this one because I also wanted to ask like do we use Pyroscope? We do. Okay. So we also use profiling. So it's actually the for telemetry signals use metrics traces and profiles for just for lowkey stuff. Okay. Right. Uh it is not on the mixing but u but like if you manage to integrate it it is not a lot of work but if you manage to integrate it it will be really useful for you to tell with more details what is going on with your instance. But to your question uh how do you prevent the injuster from running out of this space? Uh so the inester will use the space probably because of the wall to write a head log. So I would say if your disk space is not too low is probably that you have to tune the the wall usage but but yeah you can the other situation is you can't just run too little on this space because uh it will rely on that. So yeah interesting there it's like your your right head should keep increasing like your right head should flush right and then like you should then that reset. So this is might be a potential issue to look at why their right head log is constantly increasing. Yeah. But but like you you have to have at least a minimum amount of disk space for that case and but then yeah it's supposed to like to never run in other again and I'm assum assuming that well is dependent upon like the size of Loki that you're running. So like is it if you depending on the amount of the log volume would dictate your wow size just based upon say if you have a problem and you can't flush you want a decent enough wow size to be able to I mean after a certain volume uh you don't have to to care about like increasing your wall because it won't ever like be the cause of like an issue you're seeing but yeah if your cell is like your third is too small maybe you have to tune that. Okay. Yeah. Interesting. So yeah, I think the question that's a really interesting point. So yeah, it'd be interesting to check if they did meta monitoring to check whether their well has actually reduced at all or if it's like are they actually seeing flushes to object storage or there might be a bigger problem there that they need to check out. Um so this would be quite cool for them to see what comes back with meta monitoring. Okay. So shall we move on to how to get this data? Yeah. Um, was there any Oh, what did you want to also go through the reads dashboard? I think that'd be cool. I think just to just to finish up on the um rights there so we we complete the the process through um so I think we had the ingest which is very similar the index oh the index yeah index looks interest what is the index I'm assuming based on upon the index component index API index gateway component it is yeah uh basically like while we are in jazz and stuff we are keeping some data in memory so that's why you are seeing some data there uh and it's still the same again sort of queries um and so with the index gateways the queries per second is purely either a query um looking at the index gateway to see what like retrieve the current index for those queries or exactly but I mean from my experience like uh the right path has very little to do with the index gateway so I I recommend users to not uh bother with that if you are if you know that your issue you are experiencing is right related gotcha Okay. And that's purely like so the right will will basically the right path will forward the new label um series sorry the new label flows to the index well forgot the words not label um label streams log streams um yeah um and so and then bolt index was deprecated so we're just ignoring that odd stuff. Sweet. Um and I guess we could talk a little bit about like how data comes in and we can always revisit the other dashboard components. Yeah. Yeah. Okay. Yeah. So, it seems like uh like the question of where to get meta monitoring for met meta monitoring metrics and and logs for Loki is is something to to think about. Um like it seemed like and and I'm asking both of you here. It seemed like there were two main places. You already mentioned slashmetrics. So your deployment of of Loki SLmetrics um that's like a handy API endpoint for Prometheus style metrics and then when we had Ed Walsh on he also mentioned metrics.go for for logs. Um are those the two main main places that you would get information about Loki? Absolutely. Like especially the metrics.go go log line. It is extremely useful and this is a good example like in the beginning of the meeting we were talking about like the debug logs. Yeah, the mat is a is a one which is which has the bug level and it's still like it is really useful for for stuff. And yeah, I I think like for me as well is like the the one thing that kind of like doesn't get covered in the Loki metric side is if you've deployed each component then it's good to collect the pod logs as well so you get most of the information about like general state of those Loki components running. So we kind of uh enable by default in the Kubernetes monitoring helm pod logs. So you can at least see those in the and we can always revisit the dashboard later. We have like a logs dashboard panel to see it. I guess my own question from the metrics go is there anything that you need to do in the low key Helm config to enable the the ability to query from metrics.go or is that always enabled and you can just pull? Um, no, you should like be have the the metrics go log line like by default. By default. Yeah. Sweet. Because that was like that was a question from one of our solution architects was like um they had specific um Helm parts enabled and that gave them specific stats over query performance and like when a specific query runs they could see like the like the latency like how many um what how much sharding is broken into in different parts and and I was just wondering if that's something people need to enable or No, I mean fortunately it is enabled by default. Yeah. Fantastic. Oh, that's pretty cool. Okay. So, um what other what things can would you get from metrics go that you think are particularly important? I I think the mo most important things from metrics.go is like um it will tell you how big the query is like how much data it is. Uh the query itself so you can see if it is a bad query or not. So it's mostly for the reading if if you know that it's read errors. Exactly. Yeah. Okay. Yeah. Yeah. So for instance uh if you are painted for reads degradation or something you would go to the loads dashboard and if you if you have the hypothesis that it might be a one specific query. The next thing to do would be to go to the metric.go log line and search for like what is the bad query that my system is looking for. Do you ever use logs delay? I I do but like since we have a graphana set up don't uh we don't we don't need to use the CLI terminal but but yeah for for some cases we use it like what would you use it for? Uh for instance, we have a product called uh the cloud logs export uh which allows you to export data from your graphana cloud to your own bucket and to retrieve that data. I find it more more like easier to just use CLI to for your data remotely than just spinning up and your refus just for that. Okay. Um let's talk about this question from Jerry. Do you decide it's time to add labels only based on the speed of filling the chunks or do you look at other metrics as well? Um it it depends. Uh so I'm assuming like the trying to understand the question a little bit better. Is it based upon like is it like a labels best practice question of like are you only adding labels to making sure that your log streams fill chunks um to an efficient level. You're not you're not creating too many log streams for the sake of um I guess like I guess I would try to go back to like what is the problem that you're trying to solve? Like are you trying to do this because you are seeing degradation like what what degradation are you seeing? Are you having performance issues or um I I wouldn't jump to adding labels? Yeah, I I I personally would want to know exactly what's what's going on here that makes you think you want to add labels. Yeah, I was going to say like filling the chunks too fast is not a problem per se. So, you don't have to add new labels just because of that. Maybe your set of labels is perfect for your use case. It is just that your volume is getting higher. So you will eventually feel chance faster. Oh, some extra context. Speed of queries, but we're running monolith deployment still. Uh we did uh we did a Loki community call on sort of best practices for querying since that seems to be specifically what you're asking for. We did an hour on that and a lot of it was actually not architectural. It was more like here's how to formulate a query because it might just be an educational thing. Um there's a lot of things that that were not super intrusive to me that I'm like, "Oh, I didn't know that." That's how you make Yeah. like start with reject. Of course. Please don't do that. No, don't do that. I'm joking. I'm joking. Um but yeah, that might that might be helpful. Um Harold has a question. Do you recommend to look for queries looking at a large amount of log data similar to temp space or unindexed query monitoring in a database? So in same same answer look at that that um Loki community caller we talk about queries. This is one of the unintuitive things because I thought it had to be like um start with a very I think we did a a postcode lookup example. So I thought like maybe I should query first for the country and then the region and then the post code and something and then we were told no if you know the post code just go for the post code like don't worry about about doing a large like go as specific as you can already so you're looking through less data to begin with which that's not initially how I started writing queries. Um maybe we could talk about how to deploy metam monitoring like what what does that look like and we can talk about helm stuff. So I think we can talk a little bit about the history of where we was. Um so when I joined which was only a year ago which is wild um we had the Loki meta monitoring um helm and this kind of sat as an independent um repo that contained like a very custom helm for Loki um that had the ability to like scrape those metrics like the Loki endpoints for each component. Um it had the ability to do like the logs and and so forth and it it worked for a while. The problem was is that the the re the actual project itself wasn't kept up to date. Um and so we had lots of people that were putting issues in there and we weren't getting around to fixing them quick enough. Um so what we wanted to do was find a better way of doing a more unified meta monitoring um solution for not just Loki but for Tempo and Mamir. Um and so what we realized was is that you know we're advising people a lot of the time that you know you should be running in distributed mode. You should be running on Kubernetes to have a production ready deployment. There are plenty of other ways you can run Loki but this is the way we personally would like you to run Loki in order to be able to support you in the best way in the community. Um and so what we realized the best route forward was the K8's monitoring helm. Um and so the the K8 monitoring helm has been around for a while. Um we originally created it as part of our cl our K8 monitoring stack for Grafana Cloud. Um it's like so when you use the K8 monitoring wizard um I can even don't if I share the screen. Yeah, there we go. Um oh inception inception. Um, if we look at this uh Kubernetes app here, I haven't uh you might see some of the metrics because I've got I've got it deployed, but this app runs based upon the Kubernetes monitoring helm. Now, the cool thing about the Kubernetes monitoring helm is although the Kubernetes app is a paid for item on Grafana Cloud, the Kubernetes monitoring helm isn't. It's completely open source. Um, and you can go to the repo here. So, we go, hey, K8 monitoring helm chart. Um, you can find it here. And what's interesting about it is there is a ton of features that's included in the KX monitoring helm. Um, especially shout out to Pete here. He's awesome. He actually does his own office hours which you should check out. Um, there's a ton of features that are in the KX monitoring helm that's actually not accessible via the UI currently and that's what we tap into. So um there's a ton of uh inte integrations in the KH mon that was contributed by a few of our solution architects as well as people um which are specific to Loki Mamir Tempo and Pyroscope um and they basically allow you to tap into each of those databases like independent metrics and logs. I also put a link in the description already because I knew this is coming up. He does the Jay just mentioned that Pete does the Kubernetes monitoring helm office hours and so there's already a playlist in on our YouTube channel for that. Yes. Um and I and like it's it's one of those things Pete is super integrained into the community. So please go along and if you have anything to add to it, he is all about it. Um I think the without getting too deep into I feel we're stealing his fun a little bit but to give a bit of like an overview is we decided to take this and said look this would be a very nice way a generic way of monitoring Loki in production. Um and so what we decided to do was curate a the K8 monitoring helm values file specifically for meta monitoring Loki. Um, and so if I jump over on screen, um, here, uh, if you jump into our Loki repo and go to Loki production helm meta monitoring values.yaml file, um, you can see the values file here. Um, and we have now just released documentation as well, which we can talk about in a bit on how to deploy this KX monitoring helm. Awesome. Run us through it. Oh, we've got we've got Prometheus instance. Yeah, absolutely. So the um so essentially what we have is you kind of break it out into a couple of areas. Um global parameters that you see at the top is basically enabling how um each component that you have like each source will operate. So at the moment we're basically telling everything to have a 15-second scrape interval. Um and that's based upon currently how the Loki meta monitoring recording rules are set up. they the queries expect you to have points like at least three to four points in a minute. Um which is not always great for graphana cloud and we'll get to that at the end in the gotchas for the the values file. Um but just just for the the sake of showing here um cluster is sort of like um is basically when you're doing meta monitoring. So say we take our example of Grafana cloud. Um our cluster name would be based upon I'm assuming which region um our Loki is deployed in and it's just an arbitrary name of showing like where your um your Loki is located or which one you're specifically monitoring. It's just like an arbitrary label for identifying that's assigned to all metrics and logs. Um destinations. This is where you're writing your telemetry signals to. Um, and so you can write these temporary signals to your own meta LGTM stack that could be like an open source version. So you could keep another K8 cluster somewhere that has say a monolithic key um mamir graphana and tempo and you could point all of these um production metrics to that meta monitoring or the advice we give is to use graphana crowd um because then you're not under any worry about meta monitoring your meta monitoring stack you did a talk on the meta monitor meta monitor and where do you stop you need to make sure everyone has up time. It's like different. It's paranoid observability. Yeah, the talk is called watching the watchers. You can look it up on YouTube. It was at CubeCon. Um and then like in the destinations here, we you basically the way we've done it is we've done it in its most basic form. So if you're using Graphana Cloud, um you have the ability to tap in pre-created Kubernetes secrets. Um, and so in this case, we're saving like our tokens and our, you know, username for Grafana Cloud in those, but you can just remove those and you can just use basic rights, um, if you need to. Um, and then there is over authentication methods. There's a OOTH 2. There's actually loads. I'm trying to see where I've left these. Yeah, there's like a comment in the docs where you can actually go see the different methods. Don't know if we can actually just search that now. Uh where is the let me go to yeah so bearer tokens or two version 4 um external secrets is what I'm using and embedded secrets as well. Oh look it's Pete Pete again. Um so scrolling down from that we sort of like get into like the juice of the the config. So integrations is basically where we're scraping those curated metrics. And I think the one thing that we don't talk about is we're actually using the alloy helm underneath it all. Um, and basically what these integrations are doing is they're abstracting you away from writing the alloy config. So these when you basically create an integration, we put it down. We have a pre-anned alloy configuration behind the scenes. And so when you like specify your parameters, we'll put that alloy config in. and then we allow you to tweak some of those parameters through the abstraction of the the values.yamel file. Um so in this case we actually have two integrations running. Um you don't need the alloy one. I like the alloy one personally because that's what's doing the leg work in collecting your meta metrics. So it's good to know that you're actually scraping your Loki metrics and you're actually performing well and writing. Um so we have the alloy one here. Um it's only writing from the namespace meta because that's where we're deploying the meta monitoring. Um we have the Loki integration here. Um which is basically saying scrape my production Loki from the the namespace Loki. Um and select the the labels from Loki particularly there. Um in terms of tuning um the interesting things to keep an eye on here is structured metadata. Um, and that's basically saying when you're ingesting logs with the Loki integration is what goes into Loki labels for your I find this really hard to say like we have I'm going to say production Loki and metalloki. So these the ones we dictate what goes into metalokes labels or structured metadata to stop your cardality ballooning out of control because I don't know how many tenants do we have now Dylan like I don't I don't know the exact number but 100 for sure and it's like one of those things that would be like runaway cardality for us is to have like every tenant and and like or ID like Yeah, if it was a label. No, it wouldn't work like not even a single day. Yeah. Um and then so so actually for with that question I that raises a question for me like yeah because do we not have like doubles of everything in that case for the cardality point of view? Um you're kind of making it so that those become so those those fields just put them in the structured metadata. But like are those blogs actually getting duplicated still or? I don't think they are. Okay. No, it is just that uh you have a a faster way of calling for for those when you are searching for those things like let's say you want to filter for a specific tenant you'll be able to speed up your query without having to index the whole thing but that doesn't shouldn't like cause duplicates. Okay. Okay. Because you'll still be like emanating for that specific user but like that tenant will just be saved in the structure metadata rather than you creating a new log stream for it. Um and so that's kind of like the specifics and then the last bit because we are using the K8's monitoring helm is we actually have like cluster events cluster metrics um where you can basically bring in Kubernetes resources and monitor those directly. Um so the ones we advise we actually kind of tell you which ones are required such as CAD visor which actually also deploys CAD visor to your Kubernetes cluster. Um, you can turn that on and off depending upon if you already have CAD visor installed in your K8 cluster and that creates the resource metrics for um, your Loki deployment so you can see how well they're performing. Um, and then there are a ton of other features you can enable and disable here. Um, my gotchas here is what we've tried to do is enable the ones that you need. You can have more data, but you have to be careful of it's it's like a it's a um, like a catch 22. Yeah. It's like you increase your cardality and you potentially increase your cost by collecting all the rest of these metrics. And it's like, do you actually really need them? Probably not. Um, but they're they're optional metrics if you do need them for your use case. Um, and then finally, I'm just checking on time where we're at. Um, the last bit is to check out pod logs. That's how we scrape our pod logs, labels to keep versus and where to scrape them from. And then the last bit to advise on here is like alloy singleton versus the other alloy deployment methods. Um we recommend using alloy singleton for most community members setups just because it just deploys a single alloy down. It's like low resources um and will do most of the heavy lifting that you need it to do. If you have a crazy large key deployment and you have say you know a couple um you know couple 30 replicas of ingesters and queryers and god knows how many then we advise you to look at alloy metrics and alloys which deploy them as a demon set um and have a like basically deploys them as like a alloy cluster and takes away some of like the heavy lifting from that single alloy singleton which might fall over from the pressure of too much logs and metrics. Um, so yeah, that's kind of like a an overview of like the config. Um, we actually have docs here which take you through the deployment method. Really, all you need to do is make sure you match up the name spaces to your name spaces that where you've deployed Loki. Um, make sure you have our repo um, and where you want to install the meta monitoring. We recommend you install it in meta. Um, and then depending upon where you're writing your data to, you might have to set up some authentication. So, we'll take you through that. So, in this case, we're setting up generic secrets for Grafana Cloud. Um, and then all it simply is is a deployment of the Helm, which is I can find it in the docs. Helm install monitoring Loki and away you go. And you should start seeing data in the Loki mixins. Um, that it's designed to populate the Loki mixins. So if you're not seeing data there, then we need to there's either something that we need to look at that's not collecting in the helm or you might have a weird label that we're not accounting for. Okay. Well, what about the gotchas you mentioned earlier about the um the scrape interval for graphana cloud? Yeah. So there's there's one thing you have to be careful with graphfana cloud and we like to keep it like keep it transparent here is we expecting if you're using graphana cloud for meta monitoring it's like we expect most people to be having a metric a minute um and we have this thing this notion in graphana cloud it's like if you're ingesting metrics at a higher resolution than a minute um that actually comes as an extra cost in graphana cloud and so you have to be careful of that cost so at the moment we're doing a 15sec second interval. So, we'll have four points a minute because my math's right. You you have to be careful of that that um extra interval or those extra points because that goes into your total cost. The cost is marginal, but if as you you know bring in more and more metrics, that's something like I don't want the community to feel like they're caught out on. Um so, that's something to keep in mind. What you can do as a workaround is you can set it to the default one minute um interval, but you'll have to change the the way the recording rules function. So the recording rules um currently expect you to have like four points in a minute. You can change the recording rules. So you do like I don't know four points every five minutes or something like that. Um but it's just because we keep recording rules equivalent to what we have internally. Um, if you're using your own OSS deployments, this is not something you have to worry about, but it's just something you have to worry about if you're using Graphana Cloud. Okay, we have some some questions here. We're we're short on time, but maybe we could go through these quickly. Um, Andre Zeani, one of our Grafana champions, one of our favorites, has a question. Any tips on how to find what query is generating load on the cluster, ideally from which dashboard or alert rule? Yeah. So once you know there's a query generating uh enough load uh the right thing to do would be to look at the matrix go log line that we mentioned earlier and you can look at the uh which queries are like loing uh the most amount of data. Okay. And then also there was a question from Kelvin earlier. Kelvin Fedaz says, "Do you have some time to compare Graphana Loki versus the EL services?" I think um there's a there's an extra question from adding to that one business. This is a migration question like is there any best practices for logging for applications in the context of migrating from L to Loki? I think so. I think one thing you should be careful with is that like if I'm not mistaken on ELK like uh everything is indexed. Um so indexing more stuff is probably better in your case in Loki like you have to be very careful with that. Uh you should look at uh the the labels and the indexes on Loki to help you filtering out data that you don't need. Uh but you at the same time you have to be careful to not like exploding cardality. Uh so yeah the the recording with that that you mentioned will probably be really helpful for you. Okay. Uh a question from Oh man, my Hebrew is not up to date. Sorry. Since query front end such as quer results, does it hold the entire query memory? Possibly quer memory. Can we thro large queries without hard limits like max lines or bytes? Let's parsing this. So the query front end. Okay. The query front end to query your results. Does it hold the entire query in memory? Um you you can say that but uh the way it will store the data in memory is very like uh optimal. So so I don't think you have to run like double the query in memory. I don't Yeah, that's you don't you don't need that I would say. Uh and then you asked throttle large queries without hard limits. Um I it is hard because uh this is the the goal of the limits to helping you like throttling bad usage. Uh maybe the question is why don't you want to use the limits? Yeah. Yeah. There's an extra context because I think that they also asked this in the community um not just on YouTube. They said I know that's a bit naive and I guess that there's some type of lazy iteration but I still want to understand this. Yeah, I sorry I don't remember from memory if it holds like most of it. What I remember is that like it is in a more optimal uh way of storing it. I think the question is uh does the does the query front end need to have double memory double the memory of querers? No, not currently. Okay. Yeah, I think we talked about this in um our Kubernetes deployment with poison. We did a local looking community call about some of the sizing. Yeah, this sounds like a sizing question. Yeah. Um and she does talk about the query front end there. I think what would be cool is if they could give sort of some examples of some heavy load queries that end up getting run. Um, and I kind of point it's like it's like they can't prevent people from running crazy queries and that's like you know you know I agree it's like why don't we just set hard limits but perhaps there's like you know maybe like once a month they need to run a big report or something and pull a ton of locks from low key. Um so yeah be cool to see what queries they want to run and maybe we could look at um another community call at some point about you know how we how you can sort of balance heavier queries with with standard usage of of Loki. There was an interesting um use case. So I talked to someone who presented at Graphana but I wasn't there so I don't know if they actually talked about it so let's not mention the customer. Um but he said that with Loki what they do is they actually have a proxy. They wrote a program they have a proxy and instead of the queries going directly to Loki it goes to the proxy and the proxy says this is a really bad query rejected. So then so then they really um it's like they they have a they're gating a bit they're they have a bit more of a safeguard so that people can't just bring down the whole installation just from a poorly performing query and I just thought like we should have we should not need to have people do this convincing to open source it wasn't you they said they had some work to do but they were hoping to contribute it back. Yeah that's a cool one. Yeah, that would be really cool. That would be a cool thing to open source. Um, Andre Zuani says, "From my experience, front end does not use much memory." Exactly. Yeah, I agree with you. So, maybe that's it when we need to look at there might be another underlining reason. And it'd be interesting to see once they bring back their meta monitoring dashboards whether it is truly the front end that's the the issue here. If there's other causations in the background that are boiling up to make it feel like it is the the query front end that's causing problems. Agreed. And uh on the mix that we showed like for the loy you can pretty much tell if the front end is the one struggling and we also have the reads resources which will tell you how much like resource every one of those components are using. Uh see that see perfect. I think that's what if you can get back to us for those dashboards set up test the best for us and then we can help you out further. I think that'd be cool. Yeah, because if you you need to have this stuff deployed to get the information otherwise it's just too difficult to like it's just conjecture basically. Yeah, exactly. Um all right, I think that's that's us. I'm happy that you were able to hear and see us and that we're like in there's no like we're not in separate screens. There's no latency. Are you sure? Maybe this is all just like Gen AI and we're all just like also I have a personal reason for for logging off because I'm hungry. Yeah, we literally just flew in like a few hours ago. Okay. Thank you everybody for for joining us. Thank you Dylan for joining us too. Um thank you for letting us rope you in. We're just like ah Dylan. Yeah. Um if anyone has any questions on this topic uh where should they go? Uh community we have like if you can share the link for them. Yeah the pre it was really a rhetorical question. Okay. Yeah. Community.grafano.com you can find the links to our forum and to our Slack and we're we're there too. So, if you have specific questions, you can you can ask us about that or YouTube comments. We we do try to monitor those as well. Um, thank you everyone for watching and we'll see you in the next one next month. Bye everyone.

