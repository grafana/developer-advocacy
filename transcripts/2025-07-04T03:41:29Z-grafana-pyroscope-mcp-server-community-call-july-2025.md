# Grafana Pyroscope: MCP Server (Community Call July 2025)

Published on 2025-07-04T03:41:29Z

## Description

You may have been hearing folks talking lately about MCP servers. Let's look into what that is and what Pyroscope tools now exist ...

URL: https://www.youtube.com/watch?v=XNj298hyIaQ

## Summary

In the July Pyroscope community call, hosted by Tiffany and featuring Brian from the engineering team, the discussion focused on updates and advancements in the Pyroscope project, particularly relating to AI integration and the upcoming Pyroscope V2. They announced that community calls will now occur on the first Thursday of every month. Brian provided insights into the ongoing work regarding profiling as a signal in OpenTelemetry, server-side symbolization, and the new Pyroscope V2, which aims to enhance efficiency in handling profiling data. The call also included a demonstration of Grafana's Model Context Protocol (MCP) server, explaining how it allows large language models (LLMs) to interact with external systems. The session concluded with a Q&A, where attendees asked about the integration of Pyroscope data with Grafana dashboards and the feasibility of generating flame graphs directly from the MCP server. The next community call is scheduled for August 7th.

# July Pyroscope Community Call Transcript

**Welcome to the July Pyroscope community call!**  
Sorry we missed you for the last one. We were changing things around a bit with the scheduling since we had three different alternating times. We had one for North America, one for APAC, and other variations. 

To make things more consistent, we plan to run the community call on the **first Thursday of every month** at **8:00 AM Pacific Time**, which is **5:00 PM Central European Time**. Please join us every month as we aim to continue having these calls regularly.

Today, we have **Brian** from the engineering team, who will be talking about some exciting developments. Brian, would you like to introduce yourself?

**Brian:**  
Sure! As Tiffany mentioned, I'm Brian, a backend engineer for Pyroscope. I've been focusing a lot on integrating AI into Pyroscope recently, and I'm excited to demo some of that work today. If anyone has questions, feel free to ask in the chat. I'll do my best to address them as we go along. If you have questions afterwards, please visit our website at **selectdark.com** and ask there as well.

### Agenda Overview
This is the July community call, and our agenda today includes:

- Quick roadmap updates
- A demo on Grafana's MCP server and how to use it in conjunction with Pyroscope to analyze profiling data
- Concluding with a Q&A session

### Roadmap Updates
We currently have three major open-source workstreams ongoing:

1. **Profiling as a Signal in OpenTelemetry**:  
   This work is still ongoing. The SIG group in OpenTelemetry, as mentioned in a previous community call, is aiming to commit the V1 spec by **August 2025**. The spec is quite unstable at this point, and we hope it stabilizes soon so we can start advertising it as a viable option for collecting profiling data.

2. **Server-side Symbolization**:  
   This is still in the experimental phase and is quite dependent on Pyroscope V2, which I will touch on shortly. We hope to get this into a production V2 deployment and test it on top of that.

3. **Pyroscope V2**:  
   This is our upgraded storage engine. We've been conducting a lot of internal testing and migration on production clusters. If you follow our PRs, you'll see we've been focused on performance tuning and bug fixing. We're excited to release this soon, as it is a significant step forward in making continuous profiling more efficient with constrained resources.

**Brian:**  
To answer a question about what initiated the idea of creating V2: We encountered numerous scale issues with the original Pyroscope. When Pyroscope joined Grafana, we adjusted our storage engine's architecture to align more closely with other proven architectures like Loki and Mimir. However, we found that those architectures didn't lend themselves well to profiling data. 

We faced challenges with queries that spanned long time ranges, making it difficult for the backend to scale properly. As a result, we had to pre-scale our servers for what we anticipated as the largest queries, leading to over-provisioning. Pyroscope V2 aims to improve the efficiency of long queries and overall scalability.

**Tiffany:**  
For users of V2, will they notice many changes, or is it mostly backend improvements?

**Brian:**  
For users deploying V2 themselves, there will be some new elements to learn, but it won't be overly complicated. For those just using Pyroscope without worrying about deployments, the changes should be mostly transparent, with noticeable improvements in query performance.

**Tiffany:**  
Is there an estimated time frame for when this will be available for open source?

**Brian:**  
We don't have a concrete deadline for the open-source release yet. We're aiming for sometime this year, but it will depend on our internal testing and confidence in the stability of the architecture.

### Moving on to Grafana's MCP Server
Before diving deeper, what is an MCP server? 

The **Model Context Protocol (MCP)** server is a way for LLMs (Large Language Models) to interact with other systems. It allows for structuring resources and data in a way that LLMs can reliably use. An MCP server operates similarly to an API server, like REST or GraphQL, but instead of you calling the server, the LLM calls the server.

We refer to these as "tools." The LLM can enumerate these tools and understand how to use them, providing vendor-agnostic ways for LLMs to interact with external systems.

For those interested in learning more about MCP servers, I've added some links that can serve as good entry points to understand how they work.

### Demo of the MCP Server
Now, let's move on to the demo. The first step is to download the MCP server. One of the easiest ways to do this is by using `go install`. In the demo, I'll show you how to connect Cursor to Grafana's MCP server.

1. **Creating the MCP Configuration**:  
   Inside the Cursor directory, create a new file named `mcp.json`. I’ll show you how to structure this file. 

2. **Configuration Setup**:  
   In the configuration, you’ll specify the type of server (standard IO is the most common), provide the path to the MCP server binary, and any additional arguments if necessary. You will also need to set the environment, including the Grafana instance you want to connect to and generate a Grafana key.

3. **Creating a Service Account in Grafana**:  
   In Grafana, go to **Users and Access > Service Accounts** to create one and generate a token. Paste this token into your configuration.

4. **Validating the Configuration**:  
   Once everything is set up, you can check the tools and integrations in Cursor to confirm that the MCP server is properly connected.

### Using the MCP Server
With the MCP server connected, you can start interacting with Grafana's data sources. For example, you can request a list of all data sources available in Grafana and retrieve specific metrics or service names.

The demo will illustrate how to query for specific service names and analyze performance hotspots using the profiling data.

### Future Improvements
Currently, there are limitations regarding the integration between Cursor and Grafana, especially concerning visual data like flame graphs. In the future, we could enhance this by allowing cursor to render a flame graph directly or provide parameters for different formats.

### Conclusion
Thank you for joining today’s community call! If you have any questions afterwards, please join us on Slack in the Pyroscope channel. Our next call is scheduled for **August 7th** at the same time, and we hope to see you all there! 

Thanks, everyone!

## Raw YouTube Transcript

and we're live. Uh, welcome to the July Pyroscope community call. Sorry we missed you. Um, for the last one, we were changing things around a little bit as to when they were since we had three different uh times that were alternating. So, there was one for like uh like within North America and APAC, there's in North America. There's just a bunch of that changing. So, um, to try to have things be more consistent, we are planning on running the community call on the first Thursday of every month. Um, and that is at 8:00 a.m. Pacific time. So, 5:00 pm, uh, Europe, Central European. So, uh, please join us every month if we manage to continue to have it every single month. Um, so yeah, today we have Brian from the engineering team who is gonna be talking about a bunch of cool stuff. want to tell folks a little bit about yourself? Yeah. So, like Tiffany said, I'm Brian. Uh, I work on Pyroscope and a backend engineer for Pyroscope. Um, and I've been doing a lot of work recently with with Pyroscope uh with AI to continue the AI hype train. And I'm excited to demo some of that stuff today. Yeah. So, if anyone has questions, please ask in the chat. Um, we'll I'll try asking things as we go along. Um, if you have questions afterwards, um, please go on to select darker.com and join us over there and ask whatever questions you may have as well. Cool. So, this is the July community call. Our agenda today, I want to just quick touch on a few roadmap updates. Um, and then I want to dive into Grafana's MCP server and do a little demo on how to use that in conjunction with Pyroscope to help analyze um, profiling data and we'll conclude with a little Q&A. So, roadmap updates. Um we have three major open source work streams going at this right now. Uh we we're trying to get profiling as a signal in open telemetry and so this work here is still uh ongoing and um the the SIG group uh in open telemetry as alluded to in a previous community call is still working to commit the V1 spec by August 2025. Um the spec at this point is quite unstable. So we're hoping this will stabilize a little bit more and we can start uh you know advertising this as a viable option for collecting profiling profiling data. Uh in other news, serverside symbolization which is semi-related um is still in experimental phase. Uh, and this is actually dependent quite a bit on on Pyroscope V2, which I'll touch in a second. Um, but work is still ongoing here. Um, we are hoping to get this in a production v2. We're hoping to get a a production v2 uh deployment and then test this on top of it. So, we're still under construction here. And uh I guess one other point I should quick touch on with server side symbolization. Um for those who don't know uh this is this will uh symbolize uh profiling data on the server side. So releasing some resources from the client. So the client doesn't have to try uh find symbols for functions um and module names uh while also trying to serve whatever application it's running. And then lastly, oh sorry, no you go and then I'll ask my question. Okay. And then lastly, a big one, Pyroscope V2, which is our upgra our um upgraded storage engine. And so this has been we've been doing a lot of internal testing and migration on production clusters running this internally. Um, I'm sure you'll have seen if you follow our PRs on the Pyroscope project, there's there's been a plethora of of various performance tuning and bug squashing uh going on there. And so we're really excited to uh release this soon. Um, this this is a big step forwards in making Pyroscope making continuous profiling more efficient uh and more viable with constrained resources. And then since some of the previous community calls at least for this year uh did not make their way onto the internet um I guess I'll just check on something. So like what started the idea of like creating a V2 in the first place? Yeah. So we had we had a lot of problems with scale um with the original Pyroscope. So when let me let me back up a little bit. When Pyroscope joined Graphana, there was a slight architecture change to adopt um how Pyroscope's storage engine worked to mirror a lot of other databases like Loki and Mimir and Grafana. Um and this because those were proven architectures at that point in time. Uh as time went on, we found out that those architectures didn't lend well for profiling data in general. We had a lot of problems uh with uh queries that expand long time ranges. Um we we found it hard just for the the backend to scale up properly. Um so you'd end up scaling you'd have to scale pre-scale your your servers um to accommodate you know what you considered the largest data so or largest queries. So then you'd end up having these servers that were overprovisioned. Pyroscope V2 helps um make those long queries a little bit more efficient. And there's some other efficiencies too, but yeah, V1 had a really hard time handling scale properly and we anticipate that V2 can do this much more efficiently. for the V2 for the users will they notice is there a lot of stuff that's changed or just is it more things that is happening in the back end that makes things better overall but they don't have to really learn new stuff too much or how like I guess how much change is there for people who are using it so the deployment if for folks who are deploying V2 themselves or who are deploying Pyroscope themselves there's going to have there's going to be some stuff to learn um it will be a little bit different uh not necessarily really complicated or or difficult but definitely different. Um for those who are just using Pyroscope and aren't worrying about deployments of Pyroscope, uh this this should be relatively transparent for them. They should hopefully all they see is uh an improvement in query performance and then is there like a estimated time frame when this will be available for folks in open source? Uh I'm not we don't have a concrete deadline for open source. We're hoping um sometime this year. Uh so you know I guess to scope it somewhat but we c at this point we don't have a concrete time a deadline for when when we'll release v2 and open source. Really it's going to depend on how our testing internally goes and how confident we are that the architecture is at a pretty stable point. Sounds good. Thanks. Cool. And so that's just a quick touch on the road map and ongoing work we have as relates to open source. Um, and so let's move on to Graphana's MCP server. So before we get too far into the weeds, what is an MCP server? Um the the TLDDR is that an MTC MPC MCP server is a way um for LLMs to interact with other systems. Uh and so you can expose resources and and data from and behaviors from other systems in uh structured ways that the LLM's can reliably use. So think of you know an M MCP server being like your API server with REST or GraphQL where um those servers provide structured ways for you to collect the data or perform actions on the data that the server has and uh you there's there's well- definfined formats for how you how you do that and so an MCP server does the same thing but instead of uh you calling the server. The LLM calls the server. And so we c we call these things tools. Um and so the LLM will the the server will define tools that can be used. The LLM can enumerate those tools and understand how to use them properly. And this is a good way to provide uh vendor agnostic ways um for LLMs to interact with with external systems that um otherwise might be difficult uh for the LM to get access to. So um I was just trying to do some googling. Okay. Yeah, exactly what you were going to put there. I was going to check, hey, is that the place for people to go? And I will post that into the chat for folks. Yeah. So, model context protocol idle. Um, it gets pretty dense at times, but I added two links that are good entry points to trying to understand how MCP servers work. Um, I don't want to derail the community call too much and and and dive into uh all the nitty-g gritties of MCP server. Um, but here's some here's some jumping off points that can help. And so this what does architecture look like for you visual learners? Um you can imagine we have uh your LLM of choice um operating kind of as a client. Um there's actually a tiny MCP client that each of these uh applications has. Oh, and uh these can go and interact with MCP servers um that sit in front in front of external resources. So, for example, if you're, you know, in cloud and you want to go look up a file uh on GitHub, uh if you have the GitHub MCP server installed on Cloud, then uh Claude can use that server to make calls out to GitHub using GitHub's API and collect data. So, this server will more or less um you know, think think of it as being many prompts for different API calls out to GitHub. Um likewise, you know, maybe we're sitting inside cursor like we'll see in a second and we want to go talk with go collect resources from graphana. Um cursor can go talk to the graphana mcp server and that server can then proxy requests to graphana to collect data. So, Graphfana's MCP server, uh, Graphana has an OSS MCP server for interacting with Grafana resources. Um, I've linked it there. Uh, at the at the time of writing, we have 39 tools um to or things like dashboards and and metrics data sources and metric data and loc and logging data sources and and logging data. Um, we can list data sources themselves. We can list users and teams and we can also uh interact with Pyroscope data. So if you have a Pyroscope data source on your graphana then you can uh use the MCP server to interact with that data. Okay, cool. So a demo of how this works. Uh so the first thing we want to do um is you can download uh the MCP server by in a few different ways. Um but one one of the easiest ways is just use go install. So we can install the MCP server and then uh I'll I guess in this demo I'll show you how how we can connect cursor to to gra uh to graphfana's mcp server or enable graphana's mcp server in cursor. Um so for cursor what we can do is we can create a new file and we can call it mcp.json inside the cursor directory. And so we've installed we've installed the server locally and now we have this file and uh what we can do is um so we can we so we create this structure here um there this configuration here uh and here under here we can name we can list all of our the servers we we want to create. So here um we can create the type of server is standard IO. Um I'll autofill some stuff. I don't think this will be correct and we'll get to the we'll get to the rest in a second. Uh so the type of the server is standard IO. Um if you if you go to those resources I linked earlier, you'll you'll you can figure out the different types that are available, but a standard IO is is the most common one. Um, and then the command is this is the link to the the MCP server binary. Uh, and so I just happen to have mine installed uh at this path, but um, wherever yours happens to be installed, this is where you this is just a path to that binary. Args, if your server create requires additional arguments or has the option to pass additional arguments, um, you would to to the binary itself, you would put them here. And then the environment uh is here. So I actually have a graphana running at local host. And so um I want to this MCP server to use that that particular graphana. And so lastly we need a graphana key. And so we need to generate uh this key. And so when we do this in graphana I can go to localhost. Let me make this a little bit bigger so it's easier to see. Um, and so on the side we can come down to uh users and access under administration users and access service accounts. And so then I can create a service account. I can call this demo 2. And I'll just make an admin for now. And so now we have a service account. Um, but this account has no tokens yet. So we can just add a token. and we'll create no expiration. And we can copy this token. And then we can go back and paste that token here. And so now um we we're telling the MCP server knows where the graphana is located at and it knows how to authorize with that graphana instance. So we can validate if this is going to work by coming up to settings, cursor settings, and we can go to tools and integrations. And so now here we can see this is our MCP server named Grafana just like it is here. And we can see that there's 39 tools enabled. So I can ask uh ask cursor can I I can say stuff something like can you list all the data sources available in group and it'll think for a second here and so you can see that uh it made this request to the list the list data sources tool and you you can open this up and see uh the parameters it might it used and the result that was returned. Um and here it listed out all the data sources that are available in this graphana instance. We can double check this. Uh so you can see we have one for logs, one for metrics, another for profiles and development and another prof profiles and staging. So we can double check this if we come back to um our graphana instance uh and look at our data sources. So then we can see here we have one for uh logs metrics um some graphana development ones uh or pyroscope development pyroscope ops and so is able to the MCP server allowed cursor to collect this information and so now I can I can ask it more questions. I can say something like um what are the service names? What are uh maybe 10 service names in the Pyroscope dev data source and it'll think for a second And so now you can see that it's uh calling tools related to the Pyroscope data source. So you can see it actually found it's using the correct data source. Um so it's it's asking the tool to use this particular data source. And here's the result that came back. Um then it further then it used that result to pipe into a new tool and it's it's calling uh so it's it's using um same data source but now it's providing different parameters for this new tool and here's the results of that new tool and now it can it provides the total results right so 10 services in the Pyroscope development data source um I can I can keep keep drilling down here and say I I know that uh Fire Firedev O1 is the namespace for um the Pyroscope instance deployed in this data source or that this data source has. So I can ask it um what are the ser service names for the fire dev01 uh namespace and it'll think some more and it should give me back all the service names for uh the fire dev data source or the fire dev name space I should And so you can see uh here's here's all the names for this this um namespace and I might want to ask about uh a a particular name so or a particular service name. Um so in this case I know that maybe compaction worker. So this is the thing compaction worker is the application or the service that um when blocks are saved to uh bucket storage um or object storage uh compaction worker will go through these blocks and compact them um down and dduplicate those blocks. So I might want to know you know is there any performance improvements we can make to this service? So I can ask um can you give me uh CPU hot uh functions that that have CPU hotspots or comp compaction order And there are a couple of questions. I I'm not sure how much you may know since it's not specifically for uh Pyroscope, but um for instance um someone's asking if you can if I can ask about metric data from a dashboard inside of graphana from curser since like I guess it's graphana mcp which is not specific to just periscope but you're adding things from periscope on there. But do you happen to know for that inside a dashboard? Um, I think you can. I I don't know all the tools for interacting with dashboards. There's a lot. Um, so if you go to the repository that I linked a couple slides ago, and maybe Tiffany, you can post that link. Yeah, I'm going to grab that. Um, it lists out all the tools that are possible. And so I would browse that tool list and see if you can if you can ask it about metrics or panels inside a dash inside a dashboard. Now I know if you just know about metrics in general, you can absolutely ask the the MCP server to or use the MCT server to help your L1 understand about metrics in general. And then the followup is whether you can ask questions from a web UI or if you need to use cursor. So you you can use it from any LLM uh that that supports MCP servers. So in this example I'm using cursor uh which I'll get to in a sec why I chose cursor in particular but if you know with cloud desktop for example you can if you have I think a pro account or higher you can you can create MCP servers there or no chatbt I think with if you have a pro account or higher you can create MCP servers and you can if you're using cloud desktop if they support they support MCP servers you can configure them theirs as Um so yeah this is not um unique to cursor at home. Okay cool. So, I just asked it um about some CPU hotspots, and it came back with a list of various functions that it suggests uh might need could or are are basically on the hot path uh in this particular uh service. And so, um I might scan through here and try try find one that seems seems reasonable. So I like this one for example, I could say, you know, hey, this this function, I wonder if we can take a peek at at this function. So I can I can open it up here. Um, and I can see that uh this this particular function is a lot of um uh CPU time spent on this particular line right here uh in 584. And so I I might want to ask uh cursor hey can we can we improve this function in some way. So I can say I would like to try optimize this function and we'll see if this can suggest some some optimizations for that function. And then like so right now everything that you're doing is inside of cursor and it's actually going through and doing things related to like your code. Is there a way to actually be able to see how some of this stuff corresponds with what you would see if you were like in the graphana dashboard looking at Pyroscope and like how you can see like some either the flame graphs or being able to see like how the different like the breakdown for like the different functions of like this is taking too much time like is there a way to easily go between the two or you a little bit stuck? So at the moment um uh so there that's a there's a multi-party answer there. Uh at the moment you you pretty much would have to manually make the link between um the analysis being performed here and the corresponding flame graph in graphana. Uh in the future though we can this tool can be improved further. So uh the flame graph could be presented directly from inside cursor. um that would be obviously a a a better state to be in because then then you can see you know a visual representation of that profile within cursor. Um but at this at this moment we you'd have to uh ma make that link manually and I guess I can quick demo how how we might do that. Um if we come down to drill down and and go to profiles um we can search for uh we were in fire dev 01 compaction worker and so yeah come here ignore all that um and so here's uh maybe I can make this just a tiny bit smaller um so here's uh the corresponding flame graph uh for what cursor is analy cursor is more or less analyzing this specific graph right here. And then with your uh with cursor can you ask it things like hey give me like the flameql that I would want that you could then dump into um to use as a query or like I guess can you have it give you queries? Yeah, your success there is going to be somewhat mixed, but um uh I I could ask it. Can you give me the query for a CPU profile for the uh compact compaction worker profile? Um, sometimes it gets it right and sometimes uh it struggles a little bit. And so you can see here it's um it's uh going off the rails a tiny bit. Um, but yes, so sometimes it'll it'll provide you I've tested before and sometimes it'll provide you something you can kind of just copy and paste. Uh, other times it doesn't quite do that. Um, I think we can improve this a little bit though by uh making the tool the tool prompts a little bit more um refined. Uh it does get tricky though because depending on which model you use, you're can the prompts need to be tuned in slightly different ways. So there's um there's some work to be done to explore that area. But yeah, I think a lot of folks probably aren't unfamiliar with your your model kind of going off the rails and and and starting to give you data or information that like like here it's giving me information that's specific to how you would use the tool to query data, but that's not relevant to a human. A human is not going to want to use the tool directly. Um, yeah. So, uh, slight tangent, but if we jump over to our code, we can see that, um, and maybe I can close, uh, close this out a little bit to give us some more space, some more breathing room. But you can see that um obviously we would need to analyze uh and measure the effectiveness of this perform of this this change that uh cursor has suggested. You can see it it's suggesting um you know it looks like it's pre it's uh pre-sizing or pre-allocating some memory here. So so we we don't do a few initial memory Alexs. Um it looks like uh you know it it created some slightly more optimized possibly more optimized functions for specific cases and um injected them into the original uh rewrite rewrite row function. And so at this point we have a set of possibly optimized code that we can we can review and and test uh and validate that we actually indeed got a performance improvement out of. Um I will say that uh sometimes the LLM are actually quite effective at finding these performance improvements and and we have uh examples of um times when we apply used an LLM against Pyroscope and uh got results that uh optimization results that did help us with CPU and memory. Um but do take these with a grain of salt. because the LLM generated them doesn't mean they're automatically flawless and will work. So you you do need to analyze whether this suggestion is going to work or not. Um but this is an example of how we can use the tool uh and your profiling data that lives inside Pyroscope as a feedback loop into your code itself. Um, and this uh can be quite helpful especially if you uh don't know where you need to start with when when you're trying to optimize a particular application and then I have a question um so uh for the so tempo is working on their own MCP server and what I noticed is that for that one at least it seems to be separate from the FCP graphana but they are pulling in some of their information specifically from the docs. Um does how much does the Pyroscope one integrate with like docs that are there? Yeah, so Tempo let let me just speak quickly to the differences between Pyroscopes's approach here and Tempo's approach. Um Tempo integrated created an MCP server that runs in Tempo. So in the tempo service like when you deploy it you have the option to turn on an MCP server. Uh Pyroscope the Pyroscope's MCP server lives uh in a different repository from Pyroscope which is more or less a collection of tools to interact with graphana in general. So this means you need to go through a graphana data source in order to get access to the data the profiling data. Whereas temp contrast that with tempo as long as you have a running tempo instance you can hit you can make requests directly to that running tempo uh and get data directly from tempo and so you aren't proxying through graphana and a graphana data source. Um there's pros and cons to each. Uh on the one hand the Pyroscope approach is much simpler to um to implement and it also means that you have a variety of if you launch the Graphana MCP server you not only have the Pyroscope tools but you also have all of the other tools that right alongside it that uh your your model can use and infer from. So you see that in in the the demo I showed it didn't just use the Pyroscope tools. It used several different tools to answer questions I had about profiling questions. With Tempo's approach, um you need to have Tempo's MCP server configured and also Graphana's MCP server configured if you want those two to kind of work together. Uh, and so yeah, there's I don't know if I fully answered the question, but um there's pros and cons to each approach. Uh, and they're slightly different between the two. Since you mentioned it, in order for them to work together, you need to have both of them running together. How have you tried to see how well that works? Because a lot of people talk about profiling, a lot of people talk about um tracing, but then there's that whole bridge between tracing and profiling. And do you know if it's complicated with them being separate like that? I have not personally tried to have them both interact, but I have had uh two distinct MCP servers running within cursor before, one for GitHub and one for Grafana. And your results are mixed. Sometimes the model can intuit it how to connect the two um you know discrete tool sets of tools. Other times it's less good uh and needs quite a bit of handholding to to say to use a specific tool to make um a particular action work which uh can be difficult or can be kind of counter counteract the um how tools work in general where you want the L1 to just decide which ones to use uh and just make that a relative relatively transparent process where it just uses the tools it thinks is best instead of the user directing what specific tools to use. Okay. Yeah. Lots of stuff that I need to go play around with since like like how long have you been hearing people talking about MCP servers? Um so I'm I'm relatively late to the game. I would say I it's it's this been this quarter where I've really started to dive into it, but uh if you talk to uh Pyroscope founder Ryan Perry, um he'll have he'll have been on it for quite a while. Uh and he he is fully invested in the AI hype train um and produced some pretty pretty incredible results. I've seen some of the projects he's built with with uh AI and um uh various types of integrations with other systems using MCP servers and it's it's been pretty impressive. And then I I was asking earlier like about whether you had a connect correlation between what's happening in cursor and what you would see like in the dashboards. I don't know if I fully asked though. Can you is there a way to be able to I assume not because it's separate in a way but like to be able to see flame graphs of could you ask her like give me the flame graph is that possible so so right now how the profiling so the tool so short answer no not possible right now longer answer right now what cursor or any model goes and asks for profiling data we actually format it in a very specific way that's not how you would the format you would use to build out a flame graph. Um and so we have this we actually put it in dot format. Um so like think graph viz and visualizing thing things with graph vis. Uh so we take the flame graph and convert it to dot format and then give it to the om and we found that they're much better at analyzing profiles in dot format than they are in what we would call flame barrier format. um which is what we use to render a flame graph. Uh so yeah, at this point in time the the data is just structured too differently for the LM to even recreate a flame graph with how it how we format it now. But in the future we could provide extra additional tools or maybe parameters on the existing tool to request specific formats and maybe some guidance on how it could render out a flame graph. Or we could render a flame graph on the back end like as a PNG or something and send it to send it to the the LM of choice. And so then the the LM could just render, you know, a photo of of the flame graph, which could help too. Okay, cool. Well, thank you so much for sharing this. It's really neat and definitely something that I need to go actually play around with. Um, yeah. So folks, if you have any questions afterwards, please again go over and join us in Slack. Um there is a Piscope channel over in there. Um the next call should be on August 7th at the same time and just continue to be recurring. So hope to see you there and thanks everyone. See?

