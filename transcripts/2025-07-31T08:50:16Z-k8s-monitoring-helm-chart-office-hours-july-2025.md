# k8s-monitoring-helm Chart Office Hours (July 2025)

Published on 2025-07-31T08:50:16Z

## Description

In the July edition of the Kubernetes Monitoring Helm chart office hours, we discuss the version 3.2 release as well as the plan for ...

URL: https://www.youtube.com/watch?v=pEj6pgHev6E

## Summary

In the July edition of the Kubernetes monitoring Helm chart meeting, hosted by primary engineer Pete Wall, the main focus was on the recent 3.2 release and upcoming features. Key highlights included the introduction of a profiles receiver that simplifies profile data delivery to Grafana Alloy, enhanced attribute labeling for better metric sourcing, and improvements in the pod logs feature for better integration with OpenTelemetry. Upcoming features discussed include a log gathering reorganization for more flexible logging options, better integration with Grafana Cloud, and enhancements in user control over application instrumentation. The session also included a Q&A segment addressing community feedback and feature requests, particularly regarding the transition from a list to a map structure for destinations in Helm deployments and the overall impact of the Alloy operator framework. The meeting underscored the community-driven nature of the project, encouraging further engagement through public Slack channels and GitHub contributions.

# Kubernetes Monitoring Helm Chart - July Edition

Hello everybody, and welcome to the July edition of the Kubernetes monitoring Helm chart! I'm Pete Wall, the primary engineer for the Helm chart. We have a short agenda for today. I want to talk about the 3.2 release that we cut a few days ago, highlight some of the upcoming features we have planned, and leave time for questions and answers.

## Kubernetes Monitoring Helm Chart Version 3.2

I promised that when we moved to version 3, we would keep a more strict semantic versioning approach. Now, we are cutting new major features and minor releases to capture some of those features. I'm excited about this release because it includes a highly requested feature: the profiles receiver.

### Profiles Receiver

With the profiles receiver, you can now enable this feature to open up an endpoint where profile data can be delivered to Grafana Alloy. This allows for easy integration with tools like Pyroscope. Previously, this capability required extensive configuration, but now it’s built-in and much simpler to use. If you are using profiles in your cluster, I hope this feature will be helpful.

### Profiling Features

There are currently two profiling-related features within the Helm chart:

1. **Profiles Receiver**: This behaves similarly to the application observability feature. It opens up receivers for profile data to be delivered to Alloy, where we can process it before sending it to the final destination.
   
2. **Profiling Feature**: This enables the Alloy profiles daemon set, which can perform eBPF-based profiling, get profiles if your application supports a profiling endpoint, or perform Java profiling. This feature is more of a push model, allowing profiles to be sent directly to Alloy.

### Notable Changes in 3.2

We made some changes to how attributes and labels are set, which may lead to noticeable differences. 

- **Annotation Autodiscovery Feature**: For those unfamiliar, this feature scrapes Prometheus metrics based on annotations set on pods or services. Previously, we hadn't explicitly set the job label for these, leading to unintuitive values. Now, we are setting the job label with sensible defaults, improving the clarity of where metrics are coming from.

- **Pod Logs Feature**: We are aligning the log data we gather with common OpenTelemetry attributes. Specifically, we are now discovering and setting two attributes: `service.namespace` and `service.instance ID`. This should enhance the usability of logs across different platforms.

## Upcoming Features

We are discussing a log gathering reorganization, which I hope to deliver in version 3.3. Currently, the pod logs feature allows log gathering either via file-based gathering (which requires Alloy Logs as a daemon set) or API-based gathering. However, you have to choose one or the other.

Our goal is to provide more flexibility by allowing you to mix and match these approaches. We want to give you control over how logs are gathered based on your cluster's configuration, whether that includes Linux nodes or EKS Fargate.

Additionally, we are working on improving the OpenTelemetry experience, making features more predictable and less surprising. For instance:

- We're changing the auto-instrumentation feature to allow for more targeted control, enabling you to specify which applications to instrument without affecting the entire cluster.

- We’re also excited about introducing **cloud provider label enrichment**, which will pull tags from cloud resources (like EC2 or GKE) and apply them as labels to telemetry data.

## Q&A Session

Now, let's move into the Q&A session. 

### Feature Requests

Carl introduced me to a colleague who suggested changing destinations from an array to a map. This change would simplify the process of mixing multiple value styles or changing individual settings within the destinations. I see a lot of value in this idea and want to ensure any changes we make are backwards compatible and easy to understand.

### Meta Monitoring Questions

One question that came up was about improving our meta monitoring story. The Kubernetes monitoring Helm chart has a feature called integrations, which is where much of the meta monitoring functionality lives. If there are gaps or outdated documentation, please let me know.

### Deployment Patterns

We discussed whether to maintain a separate instance of the Kubernetes monitoring Helm chart for meta monitoring or deploy everything as a single stack. Both approaches are valid, depending on your needs and preferences. It is essential to weigh the logistics of deploying multiple instances against the benefits of having a consolidated monitoring solution.

### Migration Tools

Regarding the migration tool from version 1 to version 2, yes, it should also apply to version 3. The structure of the values file remains unchanged, so the migration tool can assist you in making that transition.

## Final Thoughts

Thank you all for your fantastic questions today! If you have further inquiries or ideas, please reach out via the Grafana public Slack or contribute to the GitHub repository. This project is community-driven, and your input is invaluable in making it better.

Thank you for joining, and have a great rest of your day!

## Raw YouTube Transcript

All right. Hello everybody. Welcome to the July edition of the Kubernetes monitoring Helm chart. I'm Pete Wall, the primary engineer for the Helmchart. And uh we've got a little bit of a short agenda today. Um I want to talk about the 3.2 release that we cut a couple days back and what was in there. Uh and we'll talk about some of the upcoming features that we've got planned. Uh and then leave time for questions and answers. So Kubernetes monitoring Helmchart version 3.2. I promised when we moved to version three that we would keep to a more strict SER type thing. So now we're cutting new features, big features, and uh and so we're cutting minor releases to capture some of those features. And I'm excited about this one. This is actually one uh this includes one that um I've seen GitHub requests for. I've helped out people uh on the community Slack and I've heard internal people asking for things like this. But the big feature in 3.2 is a profiles receiver. So what you can do with the profiles receiver now it's a feature you can enable this feature and then it opens up a uh an endpoint where profile profile data can be delivered to graphana alloy uh and then that ends up being another step that we can deliver off to you know pyroscope or something like that. Um this is possible before but it required you to put all the stuff into like an extra config and set up the ports and stuff like that. So now this is just a built-in feature. Uh, and so it's a it's a a simple one, but I hope that it uh if you're using profiles, if you're sending profiles around in your cluster, then this one will be um something that will help you out. >> So, this would be more like if somebody were to implement profiling in their application, >> they would send that data here as opposed to what currently happens is the um that eBPF damon set >> gets created. Okay, thanks. >> Yeah. Yeah, there's two profiling related features within the count chart right now. It's this new profiles receiver which behaves similarly to kind of the application observability feature where we open up receivers, profile data is delivered to alloy and then we can do some optional processing to it before sending it off to the profiles type destination. Um there's the profiling feature which then enables that alloy profiles Damon set which does uh it can do ebpfbased profiling. It can get uh pro profiles if uh if your application supports like a pro endpoint. Um or it can do java profiling too. Uh so a couple of different options there. Uh but yeah, this is this where the profiling feature is more of a we're going to go out and scrape profiles. This is more of a uh push model feature where profiles can be pushed to alloy beforeh being delivered up. >> Okay, cool. >> Um other notable changes in 3.2 too. We made some changes to how some of the attributes and labels are being set. So, you may see some differences here. Um, I think both of these things make a lot of sense, but just kind of be aware of some of these changes. So if you're using the annotation autodiscocovery feature uh and again for people who uh don't know these things like the back of my hand like I do uh annotation autodiscocovery is the is the Prometheus metrics scraping feature where we will scrape pods or services on your cluster based on annotations being set on those uh pods or services. Um we hadn't been explicitly setting the job label for those. And so the job label would be set by something kind of uh unintuitive. It would be something like Prometheus.scrape.anotation.services.http or something like that. And uh it always kind of bugged me because the job label is supposed to be a meaningful thing within Prometheus metrics is about like where is this coming from? What's the purpose of the sort of metric? Uh so now we are setting the job label uh with some defaults. What we'll do by first is we'll look at a kit.job annotation that matches some of the other annotations that we have in that feature. Uh so that's the first thing that we'll look for. If that's not set, then we will look at the app.cubernetes.io name label. Uh if that's on your pod or your service, uh that's what we'll use. If that's not there, we'll look at just the simple app label. And then finally, if none of those things are there, we'll fall back on the container name. if you're scraping a pod or on the service name. Uh so hopefully this will just give a little bit more of a sensible source of where those metrics are coming from uh when you uh when you use this feature. Um next for the pod logs feature uh in an effort to try and match the log data that we gather through the podlocks feature with some of the open telemetry uh app metrics that come in. uh we are uh finding discovering and setting two attributes two labels service.namespace and service.instance ID. Uh these ones are commonly set in the open telemetry community. Um but when you gather logs through the pod logs feature using the default methods um not with like the uh the more open telemetry native methods um these ones often don't get set. And so we wanted to make sure that these ones were easy to set and we tried to set a sensible default for these as well. Um and so for both of these, >> right, because that'll because that'll help like one one of the things I noticed like in Graphana Cloud when trying to bounce between the different pillars sometimes that doesn't work >> because of the labeling. >> Yeah. >> And um yeah, this should help that too. Yeah. Nice. Thank you. >> Yeah. Yeah. Yeah. We're actively trying to make sure that there's you know a really cohesive experience between uh all of the different uh you know offerings within graphant cloud and this is a step in that direction too. Um so yeah service.namespace uh if you're delivering as lowkey logs it'll show up as service namespace and same thing for the service instance ID. So uh those are what to look out for in 3.2. Uh all right upcoming features. So, we're still gathering some internal discussion around this log gathering reorganization. You know, I just talked about pod logs, but uh the I'll give you a hint of what I'm thinking is going to be the ultimate destination for this and probably will be the thing that we deliver in 3.3. Uh log gathering reorganization. So, there is basically one pod logs feature called pod logs. Uh and that feature gives you the option of uh do you want to gather your pod logs via finding the files the volume based ga log gathering uh that needs you to deploy alloy logs as a damon set. Uh it needs hostpath volume mounts. Uh it's very reliable because it's just reading files off of a file system and it's very tolerant to restart. You're not going to lose things in between. Um the other alternative right now is you know API g based log gathering. So rather than looking for the log files you stream the logs from the API server and that's really useful in situations where you can't get access to the file system because of permissions or the file system doesn't necessarily exist in the way that you expect it to because you're using uh GKE autopilot EKS Fargate or other systems where the the file system is a little bit more ephemeral. um then the API based log gathering is is useful. But currently you're forced to choose one or the other. And so if you have uh EKS Fargate nodes and you want to get pod logs from all of them, uh you're forced to use the API based log gathering for your entire cluster even though some of them are going to be traditional Linux nodes that you could get access to the file system. Uh and I didn't like that as a trade-off. So the the most likely outcome of this reorganization will be new features. We'll keep the original pod logs feature so that that's still backwards compatible, but it'll be new features that you can enable independently. Uh that would let you enable file log gathering for the pods that are on the Linux nodes that you already use today. Uh but then API based log gathering for you know targeting either at the node level. So you know if your nodes match these selectors then use the API based log gathering for that that's like the EKS fargate model or it could be based on pod selectors or something like that. Maybe there's a pod that uh you know it's only this one that you want to gather through the API. So we want to give a little bit more control over how these logs are gathered as well as the flexibility of mixing and matching both of them. I think that'll be really good. Um, along with that is going to be some changes in how we handle, you know, like I just talked about in the 3.2 feature, how we're trying to improve the open telemetry kind of com combination experience. Uh, we're going to be doing more work to make sure that that's still, you know, we push more and more towards that endpoint. Um, where if you enable the pod logs features, we really do want to make it so that it is properly correlated to data that's coming out of those applications. if you have the application observability feature as well. So um another one that we want to do we're trying to make more and more of the features that you enable predictable and you know low surprises right so in 3.1 time frame we changed the way the profiling feature works so it is it turned into an opt-in model rather than opt out model before when you enable profiling we'd be like sweet I'm going to go profile the entire world and that was a little bit a little bit much um so now that one is controlled by annotations. Uh another upcoming feature is we want to do the same thing for the auto instrumentation the ba feature uh to so that uh you're more in control of yeah I want to start instrumenting my apps. I don't want to I want to do it with the zero code uh method which is what graphfana allows you to do but I want to be more targeted to say I want this one and this app uh but don't go out and try to instrument everything on the cluster. Uh more destination types. I'm still got a couple of those kicking around in my mind that I want to add. Um, cloud provider label enrichment. This one I'm pretty excited about. We did a PC recently uh where we added some of this stuff. The the idea behind this is if you are running your Kubernetes cluster on EC2 on EKS for that matter um or in GKE or something like that and you have tags that are on these clusters or tags that are on these nodes uh we should be able to pull those tags down uh and apply them as labels to the telemetry data that's coming out. Um, we've had requests for people from people who said, you know, I set tags on these nodes. This is my, you know, region whatever node or my, you know, purpose is development or something like that. I just want to pull that from the EC2 data and put that on the telemetry data. Um, we've got a feature in mind that uh should hopefully make that work. So again, all of this stuff subject to change. Some of these things were rolled over from the last uh last month's um office hours. Uh, but I just want to show these are kind of the new features that are top of mind for me. Um, as always, you know, reach out in the uh public Slack for things that are that are important to you. Um, so now into Q&A. And actually speaking about the public Slack and features that are important to you, Carl introduced me to one of uh one of his colleagues. Um, and asked about the possibility of changing the destinations from an array to a map. Um, and I just wanted to to talk about this now because there's there's a lot that I really like about that idea. You know, there's a lot of people who talked about it in the community Slack about how that enables the the mixing of multiple value styles or changing individual settings within the destinations. Uh it makes it simpler. Um and so I've been kicking around a lot about how do I do that in a in a technical way uh as well as a way that doesn't uh you know doesn't make for a lot of surprises. The short answer right now is like I do definitely want to get to that future. I just want to make sure that whatever we do is backwards compatible, but also easily understandable for anyone who's picking up the uh the Helm chart for the for the first time. So, um I'm just saying this is my commitment to like I want to see that endpoint. Uh but I don't have a time frame for when we're going to get that delivered. But, uh but yeah, I appreciate uh Carl for introducing me to your colleague um and for the uh the PR contribution there. Um, I say of everyone who puts a PR into the Helm chart, like if you put in the time and effort to make something like that, I want to honor that and make sure that uh, you know, I respect that and see what I can do to to include those sorts of things into the Helm chart. So, thank you for that. >> Yeah. Yeah. And the, uh, the driver behind that is, um, Teddy's actually on our Argo CD um, implementation. So he's running into that issue where in Argo like previously in Terraform we were just using the Terraform and um just doing it that way. So it wasn't as much of a problem for us but now we're switching over to using Argo to do more of our um deployments and everything get away from Terraform. So he was just running into that sort of issue where we have um we got a feature flag where basically we have to change some of the stuff in our destination if people are using a uh a statsd native histogram stuff. So he's like oh it's kind of a pain because because the destinations are the way they are now. He's like oh it'd be great if they had a map. I'm like well here's a public repo here you go. So he he just went off. So, I think it'll be I think it'll be a good feature to have for people that are using Argo to deploy, >> right? >> Um the Kates Monary home chart. >> Yeah. Right. >> And I saw some other people already too there. There was another thread I saw somebody was talking about. I'm like, "Hey, we got this." I was like, "I know. I've been watching it." >> Yeah. Yeah. You know, and honestly, you know, I've been trying to think back. Gosh, it's well over a year ago now when the first code for 2.0 was written where destinations was changed into an array. And I'm trying to remember if I had good reason for an array over a map. Um, and maybe if I can't think of it now, that's probably the that gives me reason enough. Um, my biggest, like I said, my biggest concern at this point is backwards compatibility as well as just readability, understandability, right? I try to make it so that, you know, this this chart's complicated enough that I try to make it so that uh someone doesn't have to spend a whole lot of time reading the documentation to figure out what's going on. Ideally, it's it's easy to understand just when you look at the names. So, I'm going to definitely look into it. All right. Uh other questions? This is the Q&A open time, so feel free to uh to ask or ask in chat. I see a chat. Someone in the chat thread something mine said uh that had the same thing was gonna ask about that. So yeah um definitely this is something that I've heard you know a lot since we released 2.0. So you know yeah we'll be looking into it. I want to make sure that uh that we can deliver on that. >> Yeah we we discussed this in the Slack and in the community Slack. I believe the other thread that Carl mentioned was mine. So someone someone mentioned about yeah someone mentioned about the uh environment variables >> way to go. So I made some uh Argo CD tricks. We are using Argo CD as well for deployment the upset version. So with some Argo CD tricks now I have only one single set of destinations defined with environment variables and everybody sets their environment variable based on their environment. the URL changes or the >> right >> tenant ID changes etc. Everything is done at the cluster level. So cluster set their environment variables and it is deployed with a single definition of destinations >> as an array. >> Yeah. And that >> yeah if if we have the map version that will be of course better. >> Yeah. Yeah. All right. And that definitely works. Um uh so you know I'm glad that that you I did see that thread and I'm glad that you shared that uh you know finding for people who are using it right now. Um you know I'd love to there not to have to be workarounds. So yeah I think we'll still we'll still pull on the thread for the map version. So great. Yeah, thank you for adding adding to that. Other questions? >> Yeah. Hey, good morning everyone. Um we had a couple questions. We've uh been using case monitoring home chart pretty extensively across our org. Um we we maintain a central LGTM stack uh then have maybe seven or eight application clusters all use case monitoring to push into that. >> Okay. >> Um one of our big questions uh coming up is how to improve our meta monitoring story so that we can keep a better eye on what's getting pushed into that central store. Um there seem to be a lot of different mentions of meta monitoring across the graphana product verse. Um the Kate's monitoring helmchart uh has a document on using it for meta monitoring um but the document there seems to be a little bit out of date um or not getting as as much active development. Just curious like at a high level if there's a good place to go for kind of up-to-date recommendations on monitoring our observability stack. >> Yeah. Okay. Great. No, great question. Um, so the meta monitoring story, the Kubernetes monitoring helmchart um has a feature that it calls service integrations or integrations and that's where a lot of the the the meta monitoring stuff that's built into the kids monitoring help chart lives, right? So, there's a tempo option, there's a Loki option, the mirror, all that sort of stuff. Um, there also is in Graphfana's helm charts, there's a meta monitoring helmchart itself. Um, the the the movement is to moving more and more away from that helmchart into the Kubernetes monitoring home chart with the integrations. Um, if there are gaps, I definitely want to know about it. I definitely want to make sure that that gets fixed. Um, why is the movement for that? So the the meta monitoring helmchart the this standalone Helmchart it has a few deployment options um that uh that it has it has one where you can monitor your your big LGTM stack if it's Graphana Enterprise or if it's you know even oss LGTM but it's you know the main things where all your real data is being sent to um there's an option to deploy a mini meta monitoring LDTM stack so you know if you want to get metrics about your you know LG GTM stack you want to store it somewhere else so that there's no overlap or no dependencies interdependencies. Um so this one the one of the options for the for the meta monitoring chart is to deploy a whole separate stack monitor this stuff over here and then send the data over there. I'm not going to try to do that. That's not at all what this helmchart's meant to do. But the second option which I think in the meta monitoring chart they call like the graphana cloud option but it basically is I want to go and monitor LGTM and I'm going to send the metrics and logs and you know data elsewhere you know and so in their in the meta monitoring charts idea that's like oh we're going to go to graphana cloud in the Kates monitoring helmch chart that's basically what it's built to do it's I'm going to monitor something and I'm going to send that data somewhere else it could be to you know another oss source that that that you have set up somewhere or it could be graphana cloud or it could be you know anything else that's compatible. Um that's a little bit of history. So the the goal is to make the Kubernetes monitoring Helmchart be more and more the way that we want to monitor the LGTM distributions um if you've got them you know deployed on site. Uh and like I said if there are things that uh that feel out of date or if there are uh if there's stuff that's missing from that then that's a bug in the Helmchart. We should get that updated. Okay. No, that's super helpful. Um, each different place where you can kind of read about meta monitoring, there's there's a option value in the mamir distributed homechart too that to just enable meta monitoring. >> Um, but kind of each of those different sources uh presents itself as like hey this is a great way to do meta monitoring. So uh yeah, just hearing your input from um the other side that yeah the cakes monitoring chart is kind of the the direction that things are moving is is perfect and just what we're looking for. Thanks. you know, and and and the main reason for that is not necessarily because any of these other sources aren't doing an appropriate job or things like that, but the goal is, you know, between myself and the other people on the Kubernetes monitoring Helmchart team, um, you know, our goal is to make this Helmchart as as useful as possible, as as you know, good as possible. It's something that meets your needs and and gets your work done. Um, the goal of the Mamir team, of the Tempo team is to make a, you know, a metrics database that's really good, is the traces database. The Loki team is making a logs database that's really good. And, you know, they've built their Helm charts to package and deploy that. Um, but like something like the meta monitoring chart is I don't I won't say I mean like it's it's something that it's not their their primary focus. And so that's one of the reasons why we're trying to move more and more of that effort into the Kubernetes monitoring home chart where it's going to get regular testing. It's going to get regular focus on, you know, squads like myself where we are looking at Helm charts all day long. Um the I' I've been working with the Loki team a lot recently on, you know, how we can share some of the lessons learned on improving their own Helmchart distributions and also just like how do we focus some of this effort. So, um, you know, maybe this is me because it's my Helm chart, but like the Kate's monitoring helm chart is where I'd love to see more of, you know, your meta monitoring being done. Um, and, uh, and like I said a couple times now, if you find that there are things that are missing there, if there's metrics or something missing in the dashboards that you load up from that, yeah, let me know and we'll try to get that fixed. >> Okay, fantastic. Yeah, thanks. Thanks for the input there. Um, just kind of maybe a bit of a follow on. Um we we actually run our LGTM stack out of a Kubernetes cluster with several other kind of IT infrastructure applications. Y >> um so in in cases like that, does it feel like a better fit to have um a separate instantiation of the case monitoring Helmchart to do meta monitoring aside from like the typical monitoring instance that's uh you know picking up logs and metrics from all the other applications that run on that Kubernetes cluster, >> right? Um, it's going to come down to I guess personal preference a little bit. In my opinion, there's there's kind of two ways to look at that, right? You deploy a single instance of the Kates monitoring Helm chart, you've got uh let me ask one question first. So, for all of these other IT stuff that you want to monitor, would that be going to your main LGTM stack >> and then like meta monitoring goes somewhere else or something like that? >> Um, yes. the the current pattern that we're looking at it and this is all pretty new so open to change um is we have a large IT infrastructure Kubernetes cluster that's running several kind of commercial enterprise apps as well as our LGTM stack uh right now we have a single deployment of the case monitoring helmchart going there scraping all those things and uh you know for the IT applications and then forwarding that straight into the LGTM stack that runs in the same cluster now we have a separate instantiation case monitoring helmchart to do meta monitoring and we have that set up with a single destination to a separate IT dev cluster uh that runs stack. >> Okay. Yeah. All right. So there's yeah like I said there's two patterns. The pattern one will go with what you've kind of already got set up. You've got the uh Kate's monitoring Helmchart instance that's doing kind of the primary uh telemetry monitoring of your IT stuff and it's sending to the the backends uh where your you know your actual business dashboards are going to be looking at that sort of stuff and then you've got a separate instance that's doing meta monitoring for monitoring LGTM and then sending that data somewhere else. Um that's totally possible. My only concern there is just kind of the logistics of deploying that. Just making sure that you know things that are clusterwide, cluster roles and things like that don't step on each other. Um that should be just fine as long as you just kind of figure out how to do it. Um the option to deploy everything as a single stack um or a single Kates monitoring Helm chart uh should be definitely doable. What you would end up doing is in the destinations list uh or maybe map in the future uh is uh you would define both the primary uh backends for all of your metrics destinations or you know telemetry data destinations as well as you'd define separate destinations for the men and monitoring uh workloads. Um and then you would enable all of the different features. Each feature uh has a an option to specify the destinations list for that for that feature. And so if you're doing things like you know pod logs u most of the pod logs you want to send to destinations ABC you can specify hardcode that list of destinations to say I want the data from this feature to only go to these destinations. Uh and then for the meta monitoring stuff, I want these these features to only go to these destinations. Um by default, the way that we try to wire data sources or the features to the destinations is based on obviously the the data type. It is it logs it goes to logs destinations and it's metrics to metrics destinations. But we try to also keep it within the ecosystem. So if it's Loki style logs, we'll send it to the Loki style destinations. But if you also have an open telemetry log destination, we won't automatically wire that together unless it's the only option. So there's a little bit of a uh I hope it's the right kind of magic uh in the Helmchart for wiring that together. Um but all that being said, you can specify multiple destinations and then you can define at the feature level I want the output of this feature to go to this destination. Um similarly in the destinations you can add uh rules and filters and things like that. And so you can say for this destination drop anything that has these labels or something like that. And so that's another way that you can kind of enforce the routing of that sort of data. What would be the advantage of doing that? The biggest one that I can see is you're really limiting the topology of what the Kates monitoring helm chart's going to deploy. You have one instance of the alloy operator. You get one instance of the alloy receiver. You get one instance of the alloy, you know, logs, metrics, all these sorts of things. You don't need to duplicate that. that that's you know to some people that's a really great feature to some people like they want the separate topology just so that they can you know match things separately and there's no interdependencies about that um but I'll answer the question is it's definitely possible it's up to you to decide if that's what you want to do or not >> okay yeah that's excellent uh thanks thanks for the insight >> yeah no good questions >> I' I've got one more but I've I've uh kind of peppered you with them so I'll pause and see Anybody else had some thoughts? >> All right, there's a question in the chat here. Matt just posted a question here. >> He said that he wanted to ask if the values.yamel migration tool for the v1 to v2 migrations also applies to v3. Um that's a great question and you know honestly I should just update some of the docs and some of the things. Uh yeah the tool to migrate from v1 to v2. So there's a separate like a web-based tool where you can paste in your values.yamel YAML from V1 and it spits out a valuesy that's compatible with V2 should be compatible with V3 as well. Um there and I want to say like with an asterisk as far as I as far as I know off the top of my head there's no requirements for you to change your values file from V2 to V3. Uh the reason why we made it a V3 is because of the inclusion of the alloy operator. It changes kind of the system which we're using to deploy alloy. We wanted to appropriately highlight that this kind of you know felt like a major change to a lot of people. Uh but there is no changes to the structure of the values.yamel from V2 to V3. So it should all apply going from uh using the migrator tool from V1 to V3. Uh and uh we're up to 3.2. uh we have obviously no uh no dependencies or no backwards uh compatibility problems or values problems from 3.0 to 3.2. Uh but yeah, I need to go in and update the migrator tool in Docs to just highlight that that's compatible with three. All right, Michael, do you want to do you want to ask this one live so that I don't have to read it? Um we we had some back and forth in the Slack channel about this and there's an open issue. Uh we put a bunch of you know a gist and some documentation on how to reproduce. Um but basically the issue is with multiple destinations for Loki logs. Uh if any of any one of those destinations becomes unavailable or starts rejecting logs um all of the destinations will stop receiving logs. Um so this commonly comes up we in in an attempt to in improve uh like reliability or redundancy we have a lot of our critical application clusters deploy a local Loki a small local Loki um so that if shipping logs to our central stack ever you know fails or or network prevents that um they can continue sending logs to that small local uh piece and we've commonly seen that that local piece actually you a volume will fill up or it will otherwise go down because it's not getting as much attention. Um, and when that happens, it actually impacts the delivery of logs to our central stack. So, um, I guess just curious if, uh, that issue has affected others or if, uh, people have like a a good proper workaround other than just, you know, duplicating everything entirely independently. Um, or if that multiple destination log writes, uh, bug is getting looked at or planned for work anytime soon. Um uh honestly I'm not I haven't been aware of that one. Um but uh but I will p pull on that thread. Uh thanks for posting the uh the GitHub issue there. I'll take a look at that. Um can you do you have a a thread in like the community Slack or something like that? >> I do. Yeah. Um I had some back and forth with Sam Dhan there. Um, >> can you just like at pew wall me there and just that keep me uh that'll just give me the the bookmark to look at it? >> Yep. Thanks. >> Awesome. Thank you. Yeah, it in my my gut would say that using multiple destinations makes a ton of sense for having like a local redundancy and things like that. Um, I'll uh I I I'll work with Sam. I work with Sam a lot. Sam's great. So, I'll work with Sam to try and figure out uh what's going on. >> Okay, cool. Thanks. >> Yeah. All right. Fantastic questions today. Thrilled. All right. Anybody else? >> I have one related question about the meta monitoring. Um I'm now I wasn't aware that there are integrations within Kubernetes monitoring hand chart for Mim Loki. So I was using auto discovery with the annotations to scrape everything from mirror and loi. We have a central open source one running but the integrations are available I believe in version 3.2 right not in v2. >> Um they the integrations may be available in later versions of v2. Yeah take a look. Okay, I will check. I'm now with the uh uh early version of V2. We are upgrading from V1. Okay, >> but my question is there is the dashboard mixins in Mim and Loki repositories for the dashboards and alerting rules. >> When I when I have the integration enabled in Kubernetes monitoring helm, >> will it be compatible with those? Because for now what I uh scrape by auto discovery there are some differences between the uh mixing for the dashboards and what I have on >> mir itself for the meta monitoring. So I updated the config file for the mixing quite a lot in order to make it work with my current uh >> sure >> metrics. >> Yeah. Yeah. Yeah, >> what is the uh what is the intention on that? Keeping the mixins in Mimir's own repo for the dashboards and alerting rules but integrating the scraping part with Kubernetes monitoring. >> Yeah. >> How will how will that work? >> Yeah. No, the good question. Um, so I will say that the we don't I mean like you you've all know this, we don't store mixins or or you know reabeling rule or um sorry recording rules uh or alerting rules. We don't store those in the case monitoring helm chart. Um as an aside there's been talk about can we push those sorts of things up to there and I just that hasn't been the focus on what the case monitoring helm chart's supposed to do. Um that's as an aside is so the main question I think there's kind of two questions there. Um is there compatibility between Mamir's mixin and like the mir mirror integration and the same thing applies to the other services too but we'll just focus and the answers will be the same. >> Um I will say that using annotation auto discovery to scrape those services great idea. I mean I'm glad that uh that that's been working for you. um that obviously is it's a much more generic method for scraping those services using the integration features. Uh I will say that th those features were written by someone from our professional services team. Um and so the the goal there really was let's make it so that it's easy to uh get you know meta monitoring for Mamir, for tempo, for Loki, those sorts of things. Um I I wish I could say oh yeah totally it's going to work great. Um I haven't done it enough recently to to give you that but I will say my gut says it's going to be written much more towards the mamir mixin and towards the the upstream mixin deployment right. Um, yeah, because it was written with the intention that you enable this service integration, it's going to work with the one that's going to be, you know, run the the mixin that's going to be built to give you the dashboards for that. >> Um, >> I see. >> So, >> do we >> what I would say is, yeah, check it out in when you do the migration to 2.0, uh check out if uh the uh the service integration take a look at the integration feature uh and the mamir piece within the integration feature. Um I would say enable that uh have it you know there's going to be a a selector rule or something like that or two that you'll need to to specify to you know make sure that you're targeting the specific thing that you want. Um, and you know, maybe try with a clean deployment of the mix and and see how that works. Um, and uh, like I said before, you know, if you encounter things that don't quite match up, you know, let me know. Uh, highlight something in the public Slack or file an issue on the Helmchart repo. Um, and fix because, you know, ideally all of these things work seamlessly together. >> Yeah, I will definitely uh do that. the integration and uh clean install of the mixing in my dev cluster. >> One more thing related does the integration depends on a specific version of mirror or loi or tempo or profiles or works any version >> as far as I know it should work with uh any version. Yeah. >> Okay. Okay. Thank you. >> Mhm. >> Yeah. Another good question. Really good. All right. Anybody else? >> I see Teddy join. >> Nobody else has the paging. Oh, go ahead. >> I was gonna say I see Teddy joined. We were talking Teddy, we were talking about your PR a little earlier. >> Yeah. Uh just for people that uh for some context. So uh we we're moving to Argo CD. Uh we were previously deploying the the Helm chart via Terraform and basically just generating a very large uh values file via Terraform's templating function um which obviously is very different from Argo. With Argo we're using Helm and we have the ability to provide um kind of like a default values.yamel YL that applies to pretty much every cluster in our uh uh sorry any one of our clusters but then we also provide the ability to pass overrides and we found um that for example my my PRs with destinations because destinations is a list it's uh kind somewhat unfriendly to GitOps based workflows because with lists and helm when you want to change one thing you have to overwrite the whole thing um so I basically created a a PR that adds a destinations map um that takes that map that you provide and it uses the same definition in the value schema um and it essentially updates the values.estinations with extra list items. So it doesn't it it changes relatively little of the actual Helm chart and uses a a neat little hack where where um Helm does a reverse parse of the templates. So my uh template is like ZZZ destinations which enables it to be run first. it'll run through the map and then add them to the to the to the destinations list. And for GitOps based workflows, um that that that's very useful. I've tested out locally and it allows you to override, you know, a single field instead of having to provide the entire list. Um yeah. >> No. Yeah. Awesome. Um yeah, we we chatted a little bit about this uh and so catch the recording uh on YouTube in a couple days. U no no, thank you so much for putting that PR into there, Teddy. I'm really thrilled that you uh that you took the time to do that. Um the what we were saying before about this is that uh you know this is not the first time that I've heard about use of a list versus a map for especially for the destinations. Um I you know beyond I'm I'm trying to come up honestly with a reason beyond like well it's you know backwards compatibility and how do we make it an easy to understand addition to the Helmchart. Um, and so I'm I'm going to be looking into this probably next week. I'll spend some time, you know, focusing on this. Um, and uh, you know, maybe I'll find you on the public Slack and we can ping back and forth on some of the some of the the discussion about it. So, um, but, uh, but what I said earlier today uh, on the office hours is that like my commitment is like I think I've heard enough that, you know, people really want this. We're going to have a pathway that that brings us to this point. So um whether it's through directly your PR or some other kind of mix of that and something else um you know we'll get we'll get some sort of map based option. >> Sounds lovely. Thank you. >> No and again thank you so much for taking the time you know building a PR. Um the this is not a straightforward Helm chart. It is a very complex thing. Um, I had somebody I was I was working with somebody and uh they they sat back, they took five seconds and they go, "You did this with a with a templating language?" I mean, Helm is a templating language. So, it's a there's a lot to it. Um, I'm really proud of what we've built. It's a very uh it's a very powerful tool. >> Yeah. Certainly pushing Helm to its it its limits, but in a very smart way. >> Yeah. Yeah. Yeah. No, and I appreciate that. So, uh, I I applaud anyone who's who's got taken the time to put in, um, PR and to build stuff of that. Um, I I really try, especially, you know, this is a very much a communitydriven project. Um, yes, it's working with Graphfana's uh, Graphana Cloud and the things that, you know, we do as a as a as a company, but it's really also meant to to support people who are using it for their open source distributions or for Grafana enterprise distributions or for whatever. And so I really focus the effort on it being a communitydriven project. And so whenever somebody takes the time to put in a PR, um I really take that seriously and I honor that. And you know, even if it's not nec not necessarily the uh the the direction I want to take things, I try to find ways to incorporate what, you know, what was important to that person. Um and so like I said, I'm going to make sure that I can do whatever I can to get a map based option. Um I will find you on the public Slack and we can maybe collaborate and chat on some of that stuff. Wonderful. >> Great. Uh, Michael, did you have one other thing? >> I think we're at time, but I was just going to ask your thoughts on like moving to the operator framework and how how that's changed things or enabled new things, but it's more just kind of a softball interest. So, time we can wrap up. >> Yeah. Yeah. Uh, it's a soft time. I'm not too worried about it. Um, the operator based the the change to use the alloy operator. Um this really came out because um Robbie Langford who was the original writer of the graphana sampling helmch chart. Graphfana sampling uh is a separate helm chart that we have that was built for doing tail sampling and also service graph metrics generation. Um, and for the longest time, the method that if you wanted to do tail sampling on the cluster before you delivered off to tempo or to graphic cloud or for wherever, the method that you had to do is you would deploy the Kubernetes monitoring helmchart and then you would deploy the graphana sampling helmchart in parallel and then on the cage monitoring helm chart you would say I want to send my traces to this alloy. I don't you know no mention of tempo. it it kind of felt weird like all of your traces were going to just another alloy distribution and then the graphana sampling chart would say you know I'm going to send the traces up to tempo. So there was this like coordination that you'd have to do between these two charts to do that. The reason for that to do tail sampling specifically is that all of the traces from a certain source needs to be processed by the same instance of the sampler. Because if you want to do policies that says from this given application I want only 10% of that application you need to make sure that that uh the same alloy that's student and tail sampling is counting from the same application. Um and so before 3.0 uh we had to to have these kind of two charts that were deployed in parallel. Um it was hard to maintain both of them at the same time. Um they often had different versions of alloy and things like that. Um, and so the thing that Robbie and I got really excited about and we were chatting a lot about internally is is there a smart way that we could deploy things within the same Helm chart. Um, we knew that in order to do tail sampling specifically, but also other features like service graph metrics and things like that. In order to do those things intelligently, we would need a way to dynamically deploy alloy instances. We didn't want to keep adding on to the list that was on the the growing values aty file of the case monitoring helmchart in 2.0. when before we had the alloy operator, I want to say it was like 86% of the values.yaml file is just alloy definitions. I mean that's that's the tool that we use. That's not even the real meat of what the the feature is there for the the Helm charts there for. And so the idea of adding, you know, another alloy line for doing tail sampling that was only enabled when you needed that feature, just didn't really love that. Um, so what we came up with is if there's a way to dynamically deploy alloy instances, uh, what could we get with that? And so that's where the idea of the operator SDK, uh, came from and building the alloy operator. So what beyond just tail sampling, it gives us a bunch of really cool stuff, too. So we're able to dynamically deploy alloy instances. You enable tail sampling as a feature of your trace destinations. Um, so now in the case monitoring home chart, you say, I want my traces to go to tempo wherever that lives. Uh, but I also want to do tail sampling. And so then behind the scenes, we can deploy the extra infrastructure to do the load balancing to do the tail sampling. And we rewire all of the the destinations internally. So traces that you receive go to the sampler first before being delivered up to tempo. Like all of that stuff just kind of happens and you don't have to be the one who's orchestrating that. Same thing for servicecraft metrics. uh we can deploy extra alloy instances to handle generating all of those things before sending the metrics up to the destination where you want those metrics to go. Um the other really cool thing that we get with new dynamic alloy instances is that if you enable features that need specific configuration, we can just put that configuration onto the alloy and not have to say like, oh, you enabled uh you know receiving zipin traces over this port. you're gonna have to go down to Alloy and enable that port. Like it felt like a weird twostep that you'd have to do every single time. Enable the feature and then enable Alloy to use that feature. And now since we control how Alloy gets deployed, we can just do all that sort of stuff automatically. We can set the environment variables that are required. We can open ports. We can um you know in the future with the podlog stuff, we can do the right host path volume ups and things like that. Um so there's a lot of cool things that that Alloy and the Alloy operator bring. There are some nitpicks that I have with the operator pattern. Uh, and I want to see better ways to solve this in the future. These are definitely things I think about a lot. Um, the uninstall story is not great because there is the cage monitoring helm chart deploys the operator, but it also deploys the CRDs that that invoke that operator to do stuff. And so there needs to be and I think with Helm hooks there's a way to do this in the right way but when you uninstall the Helm chart we need to first say the operator needs to uninstall the alloy instances and when those are gone and clean then we can finish the the uninstall story. Um so that's one where it's not it's not great yet. Um, also we're using the operator SDK's Helm operator as a pattern, as a template. Uh, which made it extremely easy to get up and started. Uh, I don't love that. Uh, if you install the case monitoring Helm chart and then you do a Helm list, you actually see all the alloy instances too. So, that's a little bit of an annoyance. It's it's benign, but you know, it's you see more Helm deployed instances than you maybe expect to. Um, so those are things that I'd love to address in the future. Uh, I am working with the alloy squad internally in Graphfana. Um, and you know the between my squad and their squad, we're working with the, you know, where we want where do we want to see the alloy operator go in long term in the future. We may end up building our own using more of the Go libraries rather than the the Helm operator shortcuts. Uh, which would give us a little bit more control over some of that stuff. But um you know also I want to get stuff that's up and running and you know freely available and uh easy to deploy at the same time. >> Cool. Yeah. Awesome. Thanks for the thoughts. Um big fan of the operator framework in general even with some of its difficulties with CRD management and whatnot. But um no excited to see it. We're we're still on V2. Um but looking forward to upgrading to V3 and bringing on the operator and everything. So >> yeah. >> Um love to get your feedback on that. Yeah. the joke uh Helm charts and Kubernetes and CRDs and operators and stuff that I'm sure they're the old joke is I'm sure they they'll fix that in 2.0. >> Well, thanks so much for uh all the thoughts and all the great work, Pete. Um it's a awesome Helm chart and has really gotten us off the ground uh really quickly uh in a lot of places. So, appreciate all the work, >> man. What a better wrap-up than that. Thank you so much for that. Uh let me click this button. Awesome. Thank you so much for joining. I love the questions. They're uh really really great questions today. Um if you have questions, if you're watching the recording and you want to engage with us on this, find find us on the Grafana's public Slack, graphfana.slack.com. Um I hang out in the Kubernetes channel. Um if you have issues or if you have poll requests that you want to contribute into the project, uh go to the GitHub at graphana monitoringhelm. Um like I've said a number of times, this is a hugely communitydriven project. Uh I build this to make it easy for you to get your work done, but if there are things that uh you would love to see improvements like please interact with us. That's the fastest way for you to see those changes get happen. Uh but yeah, until next time, thank you so much for joining and uh have a great rest of your day. >> Thank you. Thank you. And we

