# Grafana Tempo Community Call 2024-08-15

Join our next Grafana Tempo community call: ...

Published on 2024-08-15T20:14:35Z

URL: https://www.youtube.com/watch?v=GHZFKV53CUk

Transcript: all right here we go uh August 2024 Tempo Community call uh how many of these have we done it's been a few years 30 40 I don't know I'm gonna look that up after this call uh today we have an action-packed agenda Tempo 2.6 should be coming out I think in the next week or two so we'll look at some of the uh uh improvements there uh we have some trq improvements we have a lot of performance improvements and we have our very own Zack Leslie on call who will talk about okay I did not prep him for this we'll talk about uh we recently made a really nice Improvement to polling uh Baseline polling memory usage uh that he did in the past couple weeks we'll talk about in a second and we'll look at an explore traces app preview this will be officially announced it doesn't exist yet so don't tell anyone but it will be officially announced uh I think the end of September at Observer buildon New York uh so we'll do a big reveal there and announcement but I want to show you all the community kind of where we are I think we did a preview last month you'll see how much progress we've made and talk about what we're looking to get done uh in the next couple weeks before we release for real all right so Tempo 26 um we'll start there has a lot of nice SK features I was really hoping Jenny fam would be on because Jenny fam wrote A lot of these I think she do a good job of demoing them but I will quickly discuss some of them these are some uh longer term requests from the community uh and from ourselves internally as well uh for support for some of the more I would say esoteric otel open Telemetry data types maybe not even esoteric but certainly uh less common than just standard attributes so here's our doc I'll use this as kind of a guide here we're going to talk about links and attributes talk about arrays and scope scope so links are an awesome addition um we can do um sorry links and events I should change that shouldn't I links events so in a given um span you can have a set of links to other spans I'm going to look for them like this real fast oops I didn't mean to do that notice that grafana does not know about this syntax yet we're still working on that being added on the Graal side but we can now query for uh links so any span uh in the old open tracing as well as the modern open Telemetry um object model can reference other spam so we have the ability to uh link or query those directly now you can query them by Trace ID so that's what I did here I just asked for really anything right does not equal blank but if we had a specific Trace ID we could same with span ID if we had a specific span ID I could do a span ID here like this and I could search for links that have a span ID and then all um links can also have attributes so this has for some reason an empty follows from attribute uh but you can attach attributes to your links which maybe you are used to describe them um and you can bre those nicely as well so I'm gonna actually do that real fast uh I don't know why but these links a lot of time have an attribute name follow from that's empty so we're going to look for link the dot means attribute follows from we should be able to find all of these links so if for some reason we had attached um some kind of attributes to our links we could search by those or maybe you just want to find any Trace anywhere that links to a specific Trace maybe have a problematic one and you want to see uh traces that surround that you can use this new syntax um also exciting events so event has a name this is an intrinsic field in the event so I'm going to use the colon event colon name does not equal empty and this will let me find all kinds of different um events in my data so here's a span with two events one named query and one named status uh sorry this is another thing we you need to improve in grafana this message here is the name essentially and the query another attribute so we can query by the intrinsic name we can query by attributes so let's try this query one query by the query event. query let's say does not equal empy that's simple enough and will let us find all kinds of neat things um so we can do these attributes we can do the name and this really clever addition I love we can also say how long has it been since when has the event occurred since the start of the SP since start I believe is what it's called um and this might let us find perhaps a let's say we have an event that marks a mutex uh we could use this to find times where it took like a second or two seconds or five seconds after the beginning of the span to acquire that mutex this could help us find all kinds of great situations where uh an event was very late in the span which might indicate it took a long time to establish TCP connection or collect a mutex or some other internal uh uh event on the span during the processing of the span so uh event has name and time since start intrinsics and all attributes link has Trace ID span ID intrinsics and all attributes um and then another addition with v par A4 the as the default in 2.6 all attributes will be arrays now so span dot I don't even know an array I apologize but if I had an array attribute this will now search the array attribute for a value so let's say I had an array attribute with some values this will now correctly return but this did not work before V park4 so we have this limited array attribute support we're looking to extend that right now it will now search correctly inside of array attributes so we're looking for more precise syntax there um but you can with v Park A4 and 2.6 correctly search arrays which you could not do before so events links arrays are nice new awesome things with Trace ql uh in 2.6 uh this is borderline we have a PR of now I'll just mention it quickly but we're also looking atting the scope scope um the scope scope is the open Telemetry scope that uh Records the um I thought I had some good examples here but I do not seem to but it records the instrumentation so if I had some span this is not merged yet but I think it's quite close in fact I am going to review it later today um but o uh yeah I can't find a good span with it but you can record your instrumentation library and instrumentation name uh and that information is on your span we will make that quable as well hopefully on 26 I think it's going to happen but I'm not sure Teddy fam has been killing it lately uh metrics performance we have some performance improvements in six for metrics uh these performance improvements will increase TCO I think the blog post will have some details uh Kim on this call is now very mad at me because I've not mentioned this before and she needs to write the release notes um and so I think we're gonna get some details about how to improve performance it's something of a TCO increase but we're really looking forward to the future in the next couple quarters of releasing a version of tempo built around rf1 that has top tier metrics and um has top tier metrics and search performance with TCO reduction so there will be some options in 26 to make metrics faster at the cost of more blocks more CPU more memory and then 26 or sorry a future version uh that we are actively working on it's our primary project I would say right now is to reduce total Tempo TCO by building an rf1 architecture uh and building metrics and Trace search around it um and then I will hand off to uh Zack Leslie who recently released a very massive performance Improvement in terms of memory consumption on most Tempo components uh by mucking around at the Block Meadow what do you got Zach yeah I was just trying to pull up the screenshot let me good call good call let me stop presenting here here I'll just get our time window so yeah we took the opportunity to find some improvements in our block list we kind of bumped into into some challenges we weren't expecting a year ago we'll say and um our block list has really been growing internally for various reasons and so we we wanted to reduce the memory of the block list itself and so um we noticed that we had these common commonalities between um you know a lot of the blocks and one of those pieces was the dedicated columns um so we started interning I guess you would call it interning uh dedicated column so that we now only UNM Marshall when we have not seen that that string of Json before um and then also we dropped a couple fields that were uh not used or or very lightly used and um replace their their implementations a little bit uh clean that up shrink the struct size overall and I think we also reduced a um like an in 32 to an int8 if I recall and so that that helped us strength us up so you can see pretty good Improvement here I think if you're running a large cluster this is only showing the queriers um but if you're show if you're running a large cluster you will probably notice a memory improvement um yeah so we're going from two to 1.25 at the floor so that's you know not not terrible uh depending on how many queriers and poers you're running yep I think our largest cluster is like two million or so blocks is that right yeah I think I think so yeah we've got a cou scary number of blocks so we were seeing a lot of overhead on both marshalling unmarshalling uh as well as just holding those in memory and z uh clean that up a lot hopefully maybe even some future stuff maybe a different encoding I think there's some options on the table for even breaking that down a bit yeah a little bit of exploration it'll probably take me a couple weeks to get back to that but um yeah I think there's some there's stuff on the on the horizon anyway very cool thanks thanks s all right team uh lastly and mostly uh demo app this is I think the most exciting work I going to say that's happening in Tempo uh it's kind of not in Tempo it's outside of tempo it's in grafana but it's really the culmination of years of work in Tempo the ability to do metrics queries uh the all that Trace Q language the structural operators all of the features that you have seen roll out and Tempo over the past two years three years now uh have enabled the creation of this application right here so this is pretty busy I think we have a lot of work here to clean this up tighten it up communicate What's Happening Here better but uh I think the power of this is amazing I use it now basically every day I'll sit down in the morning and find some new issue in our applications and our infrastructure and I use this app to try to track those down so I can uh improve the app and improve the experience but let's talk a little bit about what it does now kind of what the goals are in the next month or so so from our uh from our traces we can uh a view all kinds of different details of our applications right like the number of errors that are occurring the duration of our traces and just the total rate let's focus on errors Let's Pretend We're in this situation where we have a spike in errors on some service or maybe we just want to go look at errors generally let's let's do that uh I I just want to find something that's failing in my infrastructure um this is all demo data you can see SE demos the the cluster up here this is all demo data so this is just synthetic data at the moment but I use this in a real cluster like I said kind of daily so this particular service here seems to be failing I have something on frontend proxy but look at mythical requestor having a lot more let me add that and filter down some um I can then do breakdowns I can look at um I can look at uh this error graph broken down by all kinds of attributes I can click any of these things and see how these errors are split or I can just click this investigate button here and it will automatically uh show me the error split across all different kinds of attributes so I can see for instance kind or name span name has both errors and not errors in fact in about the same so we can correlate across uh the attributes on the span and see where the error is occurring and uh like span level okay the error level is correlated with errors the error status code is correlated with errors what this is telling me is there's no strong correlation between an attribute any attribute on the span and the errors which is a good clue it means that what is happening may be happening below uh below this particular span the error isn't occurring on the span it's kind of hard to tell but we're Ed we're using tracing here right tracing is involves context propagation we can see our whole tree or whole request and so the application will allow me to then look below this mythical requester so now I'm looking for failures um in the tree anywhere beneath it this is an aggregate view of all errors in in my infrastructure that is below this span and now I can immediately see that the thing that's failing is this postgress query insert so I can uh jump to a number of different traces that are having this failure um and I can maybe use this to diagnose maybe there's an error here uh maybe I'm going to now go review that database metrics or other signals uh related to this database to see perhaps what's going on um another very cool feature of this application since these are traces and tracing is highly um highly structured metadata now that I know there's a database failure I could also do this um because this is so structured I can go look at all database calls across all my applications uh and see uh failers so now I can see uh the error rate of um the error rate of all database calls this is that postgress database failure let's me let me try my breakdown now so before I wasn't getting anything because the error was actually kind of below my current span but now I'm on the current span now I'm looking at the errors of the database I can see now clearly that postgress query inserts are failing a lot more than other queries so like selects are fine uh these deletes seem to be fine uh whatever this stuff is fine but something about inserts uh on this database is definitely having issues I can see this postgress database itself maybe I have a bunch of databases I could see maybe if one is failing they would all be up here I able to see the Red Bar was taller on the one that's were failing the green bar would be shorter this would let me quickly see is this a query issue is there a specific query that's failing is this a database issue I can immediately understand is this a is it in a specific cluster is it a specific uh namespace is a specific resource uh all of these uh this attribute view is going to immediately visually let me see that and what I've discovered is that these inserts seem to be failing more than succeeding selects are fine other things are fine maybe go dig in some database logs I have a lot of very strong clues that I can now use to further my investigation in moments I was able to identify a failing database the kind of query that was failing I have all these good details I'm going to go figure out now you know why that issue is failing um similar I'll do one quick additional one let's talk about duration real fast I don't want to do I could do duration of database calls I could see uh which database calls were slow but I really really like the uh the I really like the um uh the the structural thing so we're going to go back to the full traces let's find out why I didn't know this thing is ditl demo let's see if we can't figure out why this thing is slow so on the errors when I click structure I was able to see the structure of Errors beneath my root span right I was able to immediately see what was going on there for duration I am able to see the um the longest running spans beneath the thing I care about so I was looking at this bitl demo thing um and this frontend proxy is uh one of my longer spans and this uh document load seems quite long it's identifying for me in aggregate that resource fetch whatever that thing is is a longer running span beneath this maybe I need to go investigate this uh dit this resource fetch thing it's also identifying to me the average length of all these different spans it's getting rid see these small ones it's getting rid of the small ones throwing out anything I don't is less impactful and it's highlighting the longest running spans um so this structure tab lets me see descendant long spans long duration spans descendant errors um and allows me to quickly hopefully quickly uh jump to traces and then maybe determine what's happening I'm G to click investigate slowest for fun real fast um I can see that uh this is showing me posts are significantly slower than other methods and it looks like Gs are quite quick so something about my posts are slow um this particular spam name document load is very slow maybe I want to uh add that let me add document load real fast so now I'm filtered down to document load and I'm able to again do my breakdown again uh so we have different tools here this investigation allows me to uh look for correlations in the span attributes and then the structural allows me to look down the tree for longer running spans as well as errors and other things that might be impacting the top of the trace so I think this is super cool um if you are anywhere in the New York area towards the end of August come to observability con find me let's chat some I'm going to be demoing this there as well it'll look a lot better uh a lot more clean I hope get rid of some of these rougher edges um and I think I think that's basically it uh We've covered most of the stuff Jenny F fam room joined which I appreciate thank you Jenny fam room for joining the call in the last few minutes um if there are any questions which I'm sure there are many questions but if there are any questions feel free to ask um otherwise sure I got a question yeah um so you you were showing off the events and the links yeah and there was this um a syntax I hadn't I don't think I've seen before where it was the colon because it was an intrinsic so can you talk a little bit about the difference between when you're searching for intrinsic event compared to an event name on the on the spans yep good call uh good good eyeballs there uh this is a change we made recently to trace ql by recently I mean three to six months ago or maybe even longer we spe it out and it's slowly been rolling out in the past few Tempo releases even grafana is not quite aware of this but initially we named all of our intrinsics in kind of a global name space right like name is really span name right I can search for span names essentially oh I'm G temp it it it doesn't matter um but uh and duration is span duration and then we also added this thing called Trace duration and it got this kind of unwieldy name Trace duration and then we started adding events and links and we knew those had intrinsic values as well and we knew scope was going to have intrinsic values and we realized this was going to turn into a mess Event eration Event name name what what all these things mean right so we decided to extend language a bit uh and add the ability to scope intrinsics and it's for clarity essentially so TR span name is uh this I don't know's greater than whatever food it's not GNA match anything so span colon name is an intrinsic anytime you see colon it's an intrinsic we can do event name now our names are a bit cleaner they communicate better what is occurring we don't have this like kind of frustrating candle Casas situation where you're wondering a bit uh I I just find that the names themselves to be a little bit more aesthetically pleasing and like I said I think they communicate better what's going on because you see the colon you know immediately intrinsic versus dot would be attribute um so that is a change we made and has been kind of rolling out I don't think we have we have any plans to remove the old ones we'll probably leave them there forever um but we'll probably slowly encourage the new ones through documentations and examples so because I just think this is way clear span col and duration is telling me a lot more than just duration right and then Trace Fen duration a user if they saw this would I think have a much better understanding of what was being quered versus the older names so we kind of realiz I think we I was frustrated a bit with their name Trace duration when I first put it in but that so valuable we put it in uh I think once we were addressing how to do events and links we saw very clearly that it was not a good path forward to just Chuck a giant bag of intrinsics at the user and think the the scoping is really going to help uh people understand better that's happening definitely cool yeah thanks for the explanation yeah good question Cool anything else team I haven't eaten lunch I haven't eaten lunch I'm starving it's one o'clock here go get some food I had a lemon blueberry bread for breakfast go get some yeah you said you've been baking this I this has been in the freezer for a while blueberry season's over but um it was delicious awesome yeah that sounds great all right team take care everybody I will see you next month um we will have the community call on time next month I swear and uh we'll be talking about 26 surely so everybody take care and we'll see you what we see you all right bye everybody

